{"id": "C-1", "keywords": "uddi;dht;web servic;grid comput;md;discoveri", "abstract": "Efficient discovery of grid services is essential for the success of grid computing. The standardization of grids based on web services has resulted in the need for scalable web service discovery mechanisms to be deployed in grids Even though UDDI has been the de facto industry standard for web-services discovery, imposed requirements of tight-replication among registries and lack of autonomous control has severely hindered its widespread deployment and usage. With the advent of grid computing the scalability issue of UDDI will become a roadblock that will prevent its deployment in grids. In this paper we present our distributed web-service discovery architecture, called DUDE (Distributed UDDI Deployment Engine). DUDE leverages DHT (Distributed Hash Tables) as a rendezvous mechanism between multiple UDDI registries. DUDE enables consumers to query multiple registries, still at the same time allowing organizations to have autonomous control over their registries.. Based on preliminary prototype on PlanetLab, we believe that DUDE architecture can support effective distribution of UDDI registries thereby making UDDI more robust and also addressing its scaling issues. Furthermore, The DUDE architecture for scalable distribution can be applied beyond UDDI to any Grid Service Discovery mechanism.", "references": ["P-grid: A self-organizing structured p2p system", "GridVine: Building Internet-Scale Semantic Overlay Networks", "PlanetP: Using Gossiping to Build Content Addressable Peer-to-Peer Information Sharing Communities", "Self-Managing Federated Services", "On Death, Taxes, and the Convergence of P2P and Grid Computing", "The Physiology of the Grid: An Open Grid Services Architecture for Distributed Systems Integration", "Was the Universal Service Registry a Dream?", "Pastry: Scalable, distributed object location and routing for large scale peer-to-peer systems", "Grid information services for distributed resource sharing", "Handling churn in a DHT", "Ontology-based Resource Matching in the Grid - The Grid Meets the Semantic Web", "Web Services Dynamic Discovery (WS-Discovery) Specification", "Information Services (MDS): Key Concepts", "G- QoSM: Grid Service Discovery using QoS Properties", "UDDIe: An Extended Registry for Web Services", "A Market-Oriented Grid Directory Service for Publication and Discovery of Grid Service Providers and their Services", "An Architecture Providing QoS-Aware and Federated Support for UDDI", "Service Discovery Supporting Open Scalability Using FIPA Compliant Agent Platform for Ubiquitous Networks"], "title": "Scalable Grid Service Discovery Based on UDDI"}
{"id": "C-3", "keywords": "self-adapt;grid comput", "abstract": "Grids are inherently heterogeneous and dynamic. One important problem in grid computing is resource selection, that is, finding an appropriate resource set for the application. Another problem is adaptation to the changing characteristics of the grid environ- ment. Existing solutions to these two problems require that a per- formance model for an application is known. However, construct- ing such models is a complex task. In this paper, we investigate an approach that does not require performance models. We start an application on any set of resources. During the application run, we periodically collect the statistics about the application run and de- duce application requirements from these statistics. Then, we adjust the resource set to better fit the application needs. This approach al- lows us to avoid performance bottlenecks, such as overloaded WAN links or very slow processors, and therefore can yield significant performance improvements. We evaluate our approach in a number of scenarios typical for the Grid.", "references": ["Parallel program/componentadaptivity management", "The cactus worm: Experiments with resource discovery and allocation in a grid environment", "Enabling applications on the grid - a gridlab overview", "ATLAS: An Infrastructure for Global Computing", "Adaptive Computing on the Grid Using AppLeS", "Experiences in programming a traffic shaper", "GridSAT: A Chaff-based Distributed SAT Solver for the Grid", "The Distributed ASCI Supercomputer (DAS)", "Simple localityaware co-allocation in peer-to-peer supercomputing", "Speedup versus efficiency in parallel systems", "Speedup versus efficiency in parallel systems", "An Enabling Framework for Master-Worker Applications on the Computational Grid", "Adaptive scheduling for master-worker applications on the computational grid", "A framework for adaptive execution in grids", "Experiences with the KOALA Co-Allocating Scheduler in Multiclusters", "Sensitivity of parallel applications to large differences in bandwidth and latency in two-layer interconnects", "A performance analysis of transposition-table-driven work scheduling in distributed search", "Self adaptivity in Grid computing", "Efficient load balancing for wide-area divide-and-conquer applications", "Satin: Simple and Efficient Java-based Grid Programming", "Ibis: a Flexible and Efficient Java-based Grid Programming Environment", "The network weather service: A distributed resource performance forecasting service for metacomputing", "Fault-tolerance, Malleability and Migration for Divideand-Conquer Applications on the Grid"], "title": "Self-Adaptive Applications on the Grid"}
{"id": "C-4", "keywords": "voic over ip;loss sensit;loss conceal;loss metric;object speech qualiti measur;queue manag;differenti servic", "abstract": "Best effort packet-switched networks, like the Internet, do not offer a reliable transmission of packets to applications with real-time constraints such as voice. Thus, the loss of packets impairs the application-level utility. For voice this utility impairment is twofold: on one hand, even short bursts of lost packets may decrease significantly the ability of the receiver to conceal the packet loss and the speech signal playout is interrupted. On the other hand, some packets may be particular sensitive to loss as they carry more important information in terms of user perception than other packets. We first develop an end-to-end model based on loss runlengths with which we can describe the loss distribution within a flow. These packet-level metrics are then linked to user-level objective speech quality metrics. Using this framework, we find that for low-compressing sample-based codecs (PCM) with loss concealment isolated packet losses can be concealed well, whereas burst losses have a higher perceptual impact. For high-compressing frame-based codecs (G.729) on one hand the impact of loss is amplified through error propagation caused by the decoder filter memories, though on the other hand such coding schemes help to perform loss concealment by extrapolation of decoder state. Contrary to sample-based codecs we show that the concealment performance may "break" at transitions within the speech signal however. We then propose mechanisms which differentiate between packets within a voice data ow to minimize the impact of packet loss. We designate these methods as "intra-flow" loss recovery and control. At the end-to-end level, identification of packets sensitive to loss (sender) as well as loss concealment (receiver) takes place. Hop-by-hop support schemes then allow to (statistically) trade the loss of one packet, which is considered more important, against another one of the same flow which is of lower importance. As both packets require the same cost in terms of network transmission, a gain in user perception is obtainable. We show that significant speech quality improvements can be achieved and additional data and delay overhead can be avoided while still maintaining a network service which is virtually identical to best effort in the long term.", "references": ["Understanding end-to-end internet traffic dynamics", "Adaptive FEC-based error control for interactive audio in the Internet", "Explicit allocation of best effort packet delivery service", "Low bit-rate speech coders for multimedia communication", "Low bit-rate speech coders for multimedia communication", "QoS measurement of Internet real-time multimedia services", "RTP payload for redundant audio data", "Simulation of FEC-based error control for packet audio on the nternet", "729 error recovery for Internet Telephony", "Integrating packet FEC into adaptive voice playout buffer algorithms on the Internet", "Integrating packet FEC into adaptive voice playout buffer algorithms on the Internet", "Packet Loss Recovery and Control for Voice tinsmission over the Internet", "Packet Loss Recovery and Control for Voice tinsmission over the Internet", "Efficient QoS support for Voice-over-IP applications using selective packet marking", "Loss correlation for queues with bursty input streams", "LDAf TCP-friendly adaptation: A measurement and comparison study", "Coding of speech at 8 kbit/s using conjugate-structure algebraic-code-excited linear-prediction (CS-ACELP)", "Objective oualitv measurement of telephone-band (300-3400 Hz) speech codecs", "Towsley. Packet loss correlation in the MBone multicast network: Experimental measurements and markov chain models", "Measurement and modelling of the temporal dependence in packet loss", "Improvement of MBSD scaling noise masking threshold and correlation analysis with MOS difference instead of MOS"], "title": "Intra-flow Loss Recovery and Control for VolP"}
{"id": "C-6", "keywords": "distribut content manag;continu media storag", "abstract": "The convergence of advances in storage, encoding, and networking technologies has brought us to an environment where huge amounts of continuous media content is routinely stored and exchanged between network enabled devices. Keeping track of (or managing) such content remains challenging due to the sheer volume of data. Storing "live" continuous media (such as TV or radio content) adds to the complexity in that this content has no well defined start or end and is therefore cumbersome to deal with. Networked storage allows content that is logically viewed as part of the same collection to in fact be distributed across a network, making the task of content management all but impossible to deal with without a content management system. In this paper we present the design and implementation of the Spectrum content management system, which deals with rich media content effectively in this environment.Spectrum has a modular architecture that allows its application to both stand- alone and various networked scenarios. A unique aspect of Spectrum is that it requires one (or more) retention policies to apply to every piece of content that is stored in the system. This means that there are no eviction policies. Content that no longer has a retention policy applied to it is simply removed from the system. Different retention policies can easily be applied to the same content thus naturally facilitating sharing without duplication. This approach also allows Spectrum to easily apply time based policies which are basic building blocks required to deal with the storage of live continuous media, to content. We not only describe the details of the Spectrum architecture but also give typical use cases.", "references": ["Multicache-based Content Management for Web Caching", "PRISM Architecture: Supporting Enhanced Streaming Services in a Content Distribution Network", "NED: a Network-Enabled Digital Video Recorder", "A Demand Adaptive and Locality Aware (DALA) Streaming Media Server Cluster Architecture", "A multi-agent TV recommender"], "title": "Design and Implementation of a Distributed Content Management System"}
{"id": "C-8", "keywords": "oper context;ot;context-base ot;consist mainten;undo;group editor;distribut applic", "abstract": "Operational Transformation (OT) is a technique for consistency maintenance and group undo, and is being applied to an increasing number of collaborative applications. The theoretical foundation for OT is crucial in determining its capability to solve existing and new problems, as well as the quality of those solutions. The theory of causality has been the foundation of all prior OT systems, but it is inadequate to capture essential correctness requirements. Past research had invented various patches to work around this problem, resulting in increasingly intricate and complicated OT algorithms. After having designed, implemented, and experimented with a series of OT algorithms, we reflected on what had been learned and set out to develop a new theoretical framework for better understanding and resolving OT problems, reducing its complexity, and supporting its continual evolution. In this paper, we report the main results of this effort: the theory of operation context and the COT (Context-based OT) algorithm. The COT algorithm is capable of supporting both do and undo of any operations at anytime, without requiring transformation functions to preserve Reversibility Property, Convergence Property 2, Inverse Properties 2 and 3. The COT algorithm is not only simpler and more efficient than prior OT control algorithms, but also simplifies the design of transformation functions. We have implemented the COT algorithm in a generic collaboration engine and used it for supporting a range of novel collaborative applications.", "references": ["Flexible collaboration transparency: supporting worker independence in replicated application-sharing systems", "A calculus for concurrent update", "Generalizing operational transformation to the standard general markup language", "Concurrency control in groupware systems", "Consistency maintenance based on the mark & retrace technique in groupware systems", "On the consistency problem in mobile distributed computing", "Customizable collaborative editor relying on treeOPT algorithm", "Proving correctness of  transformation functions in real-time groupware", "Time, clocks, and the ordering of events in a distributed system", "Transparent sharing and interoperation of heterogeneous single-user applications", "Preserving operation effects relation in group editors", "A time interval based consistency control algorithm for interactive groupware applications", "Proof of correctness of Ressels adOPTed algorithm", "Operation transforms for a distributed shared spreadsheet", "A framework for undoing actions in collaborative systems", "Automating semantics-based reconciliation for mobile databases", "Logical time: capturing causality in distributed systems", "Reducing the problems of group undo", "An integrating, transformation-oriented approach to concurrency control and undo in group editors", "A flexible notification framework for collaborative systems", "Undo as concurrent inverse in group editors", "Operational transformation in real-time group editors: issues, algorithms, and achievements", "Achieving convergence, causality-preservation, and intention-preservation in real-time cooperative editing systems", "Transparent adaptation of single-user applications for multi-user real-time collaboration", "Operational transformation for collaborative word processing", "Copies convergence in a distributed real-time collaborative environment", "A collaborative table editing technique based on transparent adaptation", "Object-associated telepointer for real-time collaborative document editing systems", "Leveraging single-user applications for multi-user collaboration: the CoWord approach"], "title": "Operation context and context-based operational transformation"}
{"id": "C-9", "keywords": "grid comput;fragment object;resourc manag;adapt", "abstract": "As the idea of virtualisation of compute power, storage and bandwidth becomes more and more important, grid comput-ing evolves and is applied to a rising number of applications. The environment for decentralized adaptive services (EDAS) provides a grid-like infrastructure for user-accessed, long-term services (e.g. webserver, source-code repository etc.). It aims at supporting the autonomous execution and evo-lution of services in terms of scalability and resource-aware distribution. EDAS offers flexible service models based on distributed mobile objects ranging from a traditional client-server scenario to a fully peer-to-peer based approach. Auto-matic, dynamic resource management allows optimized use of available resources while minimizing the administrative complexity.", "references": ["A new major seti project based on project serendip data and 100,000 personal computers", "Decentralized, Adaptive Services: The Aspect IX Approach for a Flexible and Secure Grid Environment", "Integrating fragmented objects into a CORBA environment", "Dynamic load balancing by diffusion in heterogeneous systems", "Fragmented objects for distributed abstractions", "Ten actions when grid scheduling: the user as a grid scheduler", "Optimal online bounded space multidimensional packing", "IDL flex: a flexible and generic compiler for CORBA IDL", "Platform-independent object migration in CORBA", "Stable, time-bound references in context of dynamically changing environments", "The anatomy of the Grid: Enabling scalable virtual organizations", "Javasymphony: new directives to control and synchronize locality, parallelism, and load balancing for cluster and grid-computing", "Ibis: an efficient java-based grid programming environment", "Middleware for dependable network services in partitionable distributed systems", "Arm: Autonomous replication management in jgroup", "Fragmented objects for distributed abstractions", "An object model for flexible distributed systems"], "title": "EDAS: Providing an environment for decentralized adaptive services"}
{"id": "C-14", "keywords": "collabor target detect;deploy;exposur;sensor network;valu fusion", "abstract": "In order to monitor a region for tra#c traversal, sensors can be deployed to perform collaborative target detection. Such a sensor network achieves a certain level of detection performance with an associated cost of deployment. This paper addresses this problem by proposing path exposure as a measure of the goodness of a deployment and presents an approach for sequential deployment in steps. It illustrates that the cost of deployment can be minimized to achieve the desired detection performance by appropriately choosing the number of sensors deployed in each step.", "references": ["Computer algorithms: Introduction to design and analysis", "Multi-Sensor Fusion: Fundamentals and Applications with Software", "Value-fusion versus decision-fusion for fault-tolerance in collaborative target detection in sensor networks", "Empirical formula for propagation loss in land mobile radio services", "Exposure in wireless ad-hoc sensor networks", "Sensor Information Technology Website", "Distributed Detection and Data Fusion"], "title": "Sensor Deployment Strategy for Target Detection"}
{"id": "C-17", "keywords": "vce;voip;real-time audio;simultan speaker;sip;confer server", "abstract": "Real-time services have been supported by and large on circuit-switched networks. Recent trends favour services ported on packet-switched networks. For audio conferencing, we need to consider many issues -- scalability, quality of the conference application, floor control and load on the clients/servers -- to name a few. In this paper, we describe an audio service framework designed to provide a Virtual Conferencing Environment (VCE). The system is designed to accommodate a large number of end users speaking at the same time and spread across the Internet. The framework is based on Conference Servers [14], which facilitate the audio handling, while we exploit the SIP capabilities for signaling purposes. Client selection is based on a recent quantifier called "Loudness Number" that helps mimic a physical face-to-face conference. We deal with deployment issues of the proposed solution both in terms of scalability and interactivity, while explaining the techniques we use to reduce the traffic. We have implemented a Conference Server (CS) application on a campus-wide network at our Institute.", "references": ["Architecture for a Multimedia Teleconferencing System", "Simple Conference Control Protocol", "Voice over Internet Protocol and Human Assisted E-Commerce", "An Empirical Comparison of Copresent and Technologically-mediated Interaction based on Communicative Breakdown", "Floor Control for Multimedia Conferencing and Collaboration", "Virtual Meetings with desktop conferencing", "SDP: Session Description Protocol", "Very large conferences on the Internet: the Internet multimedia conferencing architecture", "Packet based Multimedia Communications Systems", "A SIP-based Conference Control Framework", "Control Protocol for VoIP Audio Conferencing Support", "Automatic Addition and Deletion of Clients in VoIP Conferencing", "Comparison of Voice Activity Detection Algorithms", "A Scalable Distributed VoIP Conferencing using SIP", "On Problem of Specifying Number of Floors in a Voice Only Conference", "A Proposal for Distributed Conferencing on SIP using Conference Servers", "A Distributed VoIP Conferencing Support Using Loudness Number", "Scaleable and Adaptable Audio Service for Supporting Collaborative Work and Entertainment over the Internet", "Deployment Issues for Multi-User Audio Support in CVEs", "Designing Communication Architectures for Interorganizational Multimedia Collaboration", "Designing Communication Architectures for Interorganizational Multimedia Collaboration", "SIP: Session Initiation Protocol", "Models for Multy Party Conferencing in SIP", "RTP: a transport protocol for realtime applications", "Coming of Age: Conferencing Solutions Cut Corporate Costs", "Centralized Conferencing using SIP", "The Internet Multicast Address Allocation Architecture"], "title": "Deployment issues of a VoIP conferencing system in a Virtual Conferencing Environment"}
{"id": "C-18", "keywords": "emerg behavior;internet worm;malwar;swarm intellig;swarm worm", "abstract": "The Slammer, which is currently the fastest computer worm in recorded history, was observed to infect 90 percent of all vulnerable Internets hosts within 10 minutes. Although the main action that the Slammer worm takes is a relatively unsophisticated replication of itself, it still spreads so quickly that human response was ineffective. Most proposed countermeasures strategies are based primarily on rate detection and limiting algorithms. However, such strategies are being designed and developed to effectively contain worms whose behaviors are similar to that of Slammer.In our work, we put forth the hypothesis that next generation worms will be radically different, and potentially such techniques will prove ineffective. Specifically, we propose to study a new generation of worms called "Swarm Worms", whose behavior is predicated on the concept of "emergent intelligence". Emergent Intelligence is the behavior of systems, very much like biological systems such as ants or bees, where simple local interactions of autonomous members, with simple primitive actions, gives rise to complex and intelligent global behavior. In this manuscript we will introduce the basic principles behind the idea of "Swarm Worms", as well as the basic structure required in order to be considered a "swarm worm". In addition, we will present preliminary results on the propagation speeds of one such swarm worm, called the ZachiK worm. We will show that ZachiK is capable of propagating at a rate 2 orders of magnitude faster than similar worms without swarm capabilities.", "references": ["Monitoring and early warning for internet worms", "Swarm intelligence: Literature overview", "The spread of the saphire/slammer worm", "A taxonomy of computer worms"], "title": "An Initial Analysis and Presentation of Malware Exhibiting Swarm-Like Behavior"}
{"id": "C-19", "keywords": "protocol framework;modular;dynam protocol replac", "abstract": "In this paper we compare two approaches to the design of protocol frameworks -- tools for implementing modular network protocols. The most common approach uses events as the main abstraction for a local interaction between protocol modules. We argue that an alternative approach, that is based on service abstraction, is more suitable for expressing modular protocols. It also facilitates advanced features in the design of protocols, such as dynamic update of distributed protocols. We then describe an experimental implementation of a service-based protocol framework in Java.", "references": ["The Appia project", "Coyote: a system for constructing fine-grain configurable communication services", "Eva: An event-based framework for developing specialized communication protocols", "Protocol composition frameworks", "The Cactus project", "The Neko project", "The SAMOA project", " The SDL project", "The weakest failure detector for solving consensus", "Unreliable failure detectors for reliable distributed systems", "Constructing adaptive software in distributed systems", "The Ensemble project", "Towards flexible finite-state-machine-based protocol composition", "Composing adaptive software", "A step towards a new generation of group communication systems", "Appia, a flexible protocol kernel supporting multiple coordinated channels", "The architecture and performance of security protocols in the Ensemble group communication system", "Dynamic update of distributed agreement protocols", "Dynamic Group Communication", "Neko: A single environment to simulate and prototype distributed algorithms", "On correctness of dynamic protocol update"], "title": "Service Interface: A New Abstraction for Implementing and Composing Protocols"}
{"id": "C-20", "keywords": "data center migrat;virtual server;storag", "abstract": "A significant concern for Internet-based service providers is the continued operation and availability of services in the face of outages, whether planned or unplanned. In this paper we advocate a cooperative, context-aware approach to data center migration across WANs to deal with outages in a non-disruptive manner. We specifically seek to achieve high availability of data center services in the face of both planned and unanticipated outages of data center facilities. We make use of server virtualization technologies to enable the replication and migration of server functions. We propose new network functions to enable server migration and replication across wide area networks (e.g., the Internet), and finally show the utility of intelligent and dynamic storage replication technology to ensure applications have access to data in the face of outages with very tight recovery point objectives.", "references": ["Ursa minor: versatile cluster-based storage", "Routerfarm: Towards a dynamic, manageable network edge", "Understanding the Message Logging Paradigm for Masking Process Crashes", "Message logging: Pessimistic, optimistic, and causal", "Xen and the art of virtualization", "Embracing failure: A case for recovery-oriented computing (roc)", "Snapmirror and snaprestore: Advances in snapshot technology", "Live migration of virtual machines", "Virtual machine hosting for networked clusters: Building the foundations for autonomic orchestration", "Seneca: Remote mirroring done write", "Internet suspend and resume", "The case for semantic aware remote replication", "Fast Transparent Migration for Virtual Machines", "Virtual machine monitors: Current technology and future trends", "Unix disk access patterns", "Autonomic Live Adaptation of Virtual Computational Environments in a Multi-Domain Infrastructure", "Optimizing the migration of virtual computers", "Increasing Application Performance in Virtual Environments through Run-time Inference and Adaptation", "Symantec Corporation", "Seamless live migration of virtual machines over the man/wan", "Black-box and gray-box strategies for virtual machine migration", "A xen way to iscsi virtualization?"], "title": "Live Data Center Migration across WANs: A Robust Cooperative Context Aware Approach"}
{"id": "C-22", "keywords": "metric collect;adapt;mobil object;mobjex", "abstract": "This paper proposes, implements, and evaluates in terms of worst case performance, an online metrics collection strategy to facilitate application adaptation via object mobility using a mobile object framework and supporting middleware. The solution is based upon an abstract representation of the mobile object system, which holds containers aggregating metrics for each specific component including host managers, runtimes and mobile objects. A key feature of the solution is the specification of multiple configurable criteria to control the measurement and propagation of metrics through the system. The MobJeX platform was used as the basis for implementation and testing with a number of laboratory tests conducted to measure scalability, efficiency and the application of simple measurement and propagation criteria to reduce collection overhead.", "references": ["Adaptation and Mobility in Wireless Information Systems", "Dynamic Service Adaptation", "Context-Aware Adaptation for Mobile Devices", "Agile Application-Aware Adaptation for Mobility", "An Empirical Evaluation of Dynamic Local Adaptation for Distributed Mobile Applications", "Application Adaptation through Transparent and Portable Object Mobility in Java", "Developing Adaptive Distributed Applications: A Framework Overview and Experimental Results", "Definition and validation of design metrics for distributed applications", "Software, Performance and Resource Utilisation Metrics for Context Aware Mobile Applications", "Recursion Software Inc", "System Support for Dynamic Layout of Distributed Applications", "Dynamic Layout of Distributed Applications in FarGo", "JavaParty - Transparent Remote Objects in Java. Concurrency: Practice and Experience", "Structure and Encapsulation in Distributed Systems: the Proxy Principle", "Monitoring-Based Dynamic Relocation of Components in Fargo", "The Java Virtual Machine Specification 2nd Edition", "Incremental System Development of Large Discrete-Event Simulation Models", "Remote Procedure Calls and Java Remote Method Invocation", "Consistency Issues in Distributed Application Performance Metrics", "A software engineering framework for context-aware pervasive computing"], "title": "Runtime Metrics Collection for Middleware Supported Adaptation of Mobile Applications"}
{"id": "C-23", "keywords": "grid comput;data grid;replica select;co-alloc;data transfer;globu;gridftp", "abstract": "The co-allocation architecture was developed in order to enable parallel downloading of datasets from multiple servers. Several co-allocation strategies have been coupled and used to exploit rate differences among various client-server links and to address dynamic rate fluctuations by dividing files into multiple blocks of equal sizes. However, a major obstacle, the idle time of faster servers having to wait for the slowest server to deliver the final block, makes it important to reduce differences in finishing time among replica servers. In this paper, we propose a dynamic co-allocation scheme, namely Recursive-Adjustment Co-Allocation scheme, to improve the performance of data transfer in Data Grids. Our approach reduces the idle time spent waiting for the slowest server and decreases data transfer completion time. We also provide an effective scheme for reducing the cost of reassembling data blocks.", "references": ["Data Management and Transfer in HighPerformance Computational Grid Environments", "Secure, Efficient Data Transport and Replica Management for High-Performance Data-Intensive Computing", "Protocols and Services for Distributed DataIntensive Science", "Giggle: A Framework for Constructing Scalable Replica Location Services", "The Data Grid: Towards an Architecture for the Distributed Management and Analysis of Large Scientific Datasets", "Grid Information Services for Distributed Resource Sharing", "Resource CoAllocation in Computational Grids", "DataGrid Prototype 1", "The Anatomy of the Grid: Enabling Scalable Virtual Organizations", "Globus: A Metacomputing Infrastructure Toolkit", "Global Grid Forum", "Data Management in an International Data Grid Project", " Introduction to Grid Computing with Globus", "File and Object Replication in Data Grids", "SYSSTAT utilities home page", "The Globus Alliance", "Enabling the Co-Allocation of Grid Data Transfers", "Using Regression Techniques to Predict Large Data Transfers", "Replica Selection in the Globus Data Grid", "Predicting Sporadic Grid Data Transfers", "Predicting the Performance of Wide Area Data Transfers", "The Network Weather Service: A Distributed Resource Performance Forecasting Service for Metacomputing", "Performance Analysis of Applying Replica Selection Technology for Data Grid Environments", "A Recursive-Adjustment Co-Allocation Scheme in Data Grid Environments", "A Performance Study of Monitoring and Information Services for Distributed Systems"], "title": "Implementation of a Dynamic Adjustment Mechanism with Efficient Replica Selection in Data Grid Environments"}
{"id": "C-27", "keywords": "wireless sensor network;local;event distribut;laser", "abstract": "The problem of localization of wireless sensor nodes has long been regarded as very difficult to solve, when considering the realities of real world environments. In this paper, we formally describe, design, implement and evaluate a novel localization system, called Spotlight. Our system uses the spatio-temporal properties of well controlled events in the network (e.g., light), to obtain the locations of sensor nodes. We demonstrate that a high accuracy in localization can be achieved without the aid of expensive hardware on the sensor nodes, as required by other localization systems. We evaluate the performance of our system in deployments of Mica2 and XSM motes. Through performance evaluations of a real system deployed outdoors, we obtain a 20cm localization error. A sensor network, with any number of nodes, deployed in a 2500m2 area, can be localized in under 10 minutes, using a device that costs less than $1000. To the best of our knowledge, this is the first report of a sub-meter localization error, obtained in an outdoor environment, without equipping the wireless sensor nodes with specialized ranging hardware.", "references": ["A Line in the Sand: A Wireless Sensor Network for Target Detection, Classification and Tracking", "RADAR: An In-Building RFbased User Location and Tracking System", "Localizing a Sensor Network via Collaborative Processing of Global Stimuli", "GPS-less Low Cost Outdoor Localization for Very Small Devices", "Networked Robots: Flying Robot Navigation Using a Sensor Net", "Convex Position Estimation in Wireless Sensor Networks", "Design of a Wireless Sensor Network Platform for Detecting Rare, Random, and Ephemeral Events", "The Limits of Localization using RSSI", "Markov Localization for Mobile Robots in Dynamic Environments", "Monte Carlo Localization: Efficient Position Estimation for Mobile Robots", "Complex Behaviour at Scale: An Experimental Study of Low Power Wireless Sensor Networks", "An Energy-Efficient Surveillance System Using Wireless Sensor Networks", "Range-Free Localization Schemes for Large Scale Sensor Networks", "Localization for Mobile Sensor Networks", "Resilient Localization for Sensor Networks in Outdoor Environments", "Distributed Localization in Wireless Sensor Networks, A Comparative Study", "MoteTrack: A Robust, Decentralized Approach to RF-Based Location Tracking", "The Flooding Time Synchronization Protocol", "Robust Distributed Network Localization with Noisy Range Measurements", "Organizing a Global Coordinate System for Local Information on an Adhoc Sensor Network", "DV-based Positioning in Adhoc Networks in Telecommunication Systems", "The Extensible Sensing System", "Global Positioning System: theory and applications", "Node Localization Using Mobile Robots in Delay-Tolerant Sensor Networks", "The Cricket Location-support System", "Mobile-Assisted Topology Generation for Auto-Localization in Sensor Networks", "Dynamic Fine-grained localization in Adhoc Networks of Sensors", "Improved MDS-Based Localization", "Localization of Wireless Sensor Networks with a Mobile Beacon", "Sensor Network-Base Countersniper System", "Walking GPS: A Practical Solution for Localization in Manually Deployed Wireless Sensor Networks", "Probability Grid: A Location Estimation Scheme for Wireless Sensor Networks", "An Analysis of a Large Scale Habitat Monitoring Application", "The Effects of Ranging Noise on Multi-hop Localization: An Empirical Study", "Calibration as Parameter Estimation in Sensor Networks", "Hardware Design Experiences in ZebraNet", "Construction and Characteristics of CdS Cells"], "title": "A High-Accuracy, Low-Cost Localization System for Wireless Sensor Networks"}
{"id": "C-28", "keywords": "bioinformat;grid comput;task alloc", "abstract": "In this paper, we propose an adaptive task allocation framework to perform BLAST searches in a grid environment against sequence database segments. The framework, called PackageBLAST, provides an infrastructure to choose or incorporate task allocation strategies. Furthermore, we propose a mechanism to compute grid nodes execution weight, adapting the chosen allocation policy to the current computational power of the nodes. Our results present very good speedups and also show that no single allocation strategy is able to achieve the lowest execution times for all scenarios.", "references": ["A basic local alignment search tool", "The design, implementation, and evaluation of mpiblast", "Gapped blast and psi-blast: a new generation of protein database search programs", "Globus: A metacomputing infrastructure toolkit", "The Grid: Blueprint of a Future Computing Infrastructure", "Washington university blast", "Algorithms on Strings, Trees and Sequences", "Factoring: A method for scheduling parallel loops", "Gridblast: High throughput blast on the grid", "Guided self-scheduling: A practical scheduling scheme for parallel supercomputers", "Gbtk: A toolkit for grid implementation of blast", "Adaptive Scheduling of Master/Worker Applications on Distributed Computational Resources", "Identification of common molecular subsequences", "Processor self-scheduling for multiple nested parallel loops", "Trapezoidal self-scheduling: A practical scheme for parallel compilers"], "title": "PackageBLAST: An Adaptive Multi-Policy Grid Service for Biological Sequence Comparison"}
{"id": "C-29", "keywords": "comput chemistri;grid comput;omnirpc;grid rpc system;conflex-g;conform space search", "abstract": "CONFLEX-G is the grid-enabled version of a molecular conformational space search program called CONFLEX. We have implemented CONFLEX-G using a grid RPC system called OmniRPC. In this paper, we report the performance of CONFLEX-G in a grid testbed of several geographically distributed PC clusters. In order to explore many conformation of large bio-molecules, CONFLEX-G generates trial structures of the molecules and allocates jobs to optimize a trial structure with a reliable molecular mechanics method in the grid. OmniRPC provides a restricted persistence model to support the parametric search applications. In this model, when the initialization procedure is defined in the RPC module, the module is automatically initialized at the time of invocation by calling the initialization procedure. This can eliminate unnecessary communication and initialization at each call in CONFLEX-G. CONFLEX-G can achieve performance comparable to CONFLEX MPI and can exploit more computing resources by allowing the use of a cluster of multiple clusters in the grid. The experimental result shows that CONFLEX-G achieved a speedup of 56.5 times in the case of the 1BL1 molecule, where the molecule consists of a large number of atoms, and each trial structure optimization requires significant time. The load imbalance of the optimization time of the trial structure may also cause performance degradation.", "references": ["An efficient algorithm for searching low-energy conformers of cyclic and acyclic molecules", "OmniRPC: a Grid RPC System for Parallel Programming in Cluster and Grid Environment", "OmniRPC: a Grid RPC facility for Cluster and Global Computing in OpenMP", "OmniRPC Project", "Ninf: A Network Based Information Library for Global World-Wide Computing Infrastructure", "Ninf Project", "Ninf-G: A Reference Implementation of RPC-based Programming Middleware for Grid Computing", "Object management group", "GridRPC: A Remote Procedure Call API for Grid Computing", "Conflex: Conformational behaviors of polypeptides as predicted by a conformational space search", "Globus: A metacomputing infrastructure toolkit", "Using distributed computing to tackle prev iously intractable problems in computational biology", "The virtual laboratory: a toolset to enable distributed molecular modelling for drug design on the world-wide grid", "Design issues of Network Enabled Server Systems for the Grid"], "title": "Implementation and Performance Evaluation of CONFLEX-G: Grid-enabled Molecular Conformational Space Search Program with OmniRPC"}
{"id": "C-30", "keywords": "bandwidth;overlai;peer-to-peer", "abstract": "In recent years, overlay networks have become an effective alternative to IP multicast for efficient point to multipoint communication across the Internet. Typically, nodes self-organize with the goal of forming an efficient overlay tree, one that meets performance targets without placing undue burden on the underlying network. In this paper, we target high-bandwidth data distribution from a single source to a large number of receivers. Applications include large-file transfers and real-time multimedia streaming. For these applications, we argue that an overlay mesh, rather than a tree, can deliver fundamentally higher bandwidth and reliability relative to typical tree structures. This paper presents Bullet, a scalable and distributed algorithm that enables nodes spread across the Internet to self-organize into a high bandwidth overlay mesh. We construct Bullet around the insight that data should be distributed in a disjoint manner to strategic points in the network. Individual Bullet receivers are then responsible for locating and retrieving the data from multiple points in parallel.Key contributions of this work include: i) an algorithm that sends data to different points in the overlay such that any data object is equally likely to appear at any node, ii) a scalable and decentralized algorithm that allows nodes to locate and recover missing data items, and iii) a complete implementation and evaluation of Bullet running across the Internet and in a large-scale emulation environment reveals up to a factor two bandwidth improvements under a variety of circumstances. In addition, we find that, relative to tree-based solutions, Bullet reduces the need to perform expensive bandwidth probing. In a tree, it is critical that a node's parent delivers a high rate of application data to each child. In Bullet however, nodes simultaneously receive data from multiple sources in parallel, making it less important to locate any single source capable of sustaining a high transmission rate.", "references": ["Scalable Application Layer Multicast", "Bimodal Multicast", "Bittorrent", "Space/Time Trade-offs in Hash Coding with Allowable Errors", "On the Resemblance and Containment of Documents", "Informed Content Delivery Across Adaptive Overlay Networks", "A Digital Fountain Approach to Reliable Distribution of Bulk Data", "Modeling Internet Topology", "Splitstream: High-bandwidth Content Distribution in Cooperative Environments", "Towards Capturing Representative AS-Level Internet Topologies", "FastReplica: Efficient Large File Distribution within Content Delivery Networks", "A Unicast-based Approach for Streaming Multicast", "Lightweight Probabilistic Broadcast", "Lightweight Probabilistic Broadcast", "Equation-based congestion control for unicast applications", "A Reliable Multicast Framework for Light-weight Sessions and Application Level Framing", "Multiple Description Coding: Compression Meets the Network", "A Case For End System Multicast", "Enabling Conferencing Applications on the Internet using an Overlay Multicast Architecture", "End-to-end Available Bandwidth: Measurement Methodology, Dynamics, and Relation with TCP Throughput", "Overcast: Reliable Multicasting with an Overlay Network", " Kazaa media desktop", "Optimal Distribution Tree for Internet Streaming Media", "Using Random Subsets to Build Scalable Network Services", "LT Codes", "Practical Loss-Resilient Codes", "Modeling TCP Throughput: A Simple Model and its Empirical Validation", "Server-based Inference of Internet Link Lossiness", "Resilient Peer-to-Peer Streaming", "Distributing Streaming Media Content Using Cooperative Networking", "A Blueprint for Introducing Disruptive Technology into the Internet", "Shortest Connection Networks and Some Generalizations", "MACEDON: Methodology for Automatically Creating, Evaluating, and Designing Overlay Networks", "SCRIBE: The Design of a Large-scale Event Notification Infrastructure", "Sting: A TCP-based Network Measurement Tool", "Mesh-Based Content Routing Using XML", "Scalability and Accuracy in a Large-Scale Network Emulator"], "title": "Bullet: High Bandwidth Data Dissemination Using an Overlay Mesh"}
{"id": "C-31", "keywords": "peer-to-peer;p2p;file share;jxta;apocrita", "abstract": "Many organizations are required to author documents for various purposes, and such documents may need to be accessible by all member of the organization. This access may be needed for editing or simply viewing a document. In some cases these documents are shared between authors, via email, to be edited. This can easily cause incorrect version to be sent or conflicts created between multiple users trying to make amendments to a document. There may even be multiple different documents in the process of being edited. The user may be required to search for a particular document, which some search tools such as Google Desktop may be a solution for local documents but will not find a document on another user's machine. Another problem arises when a document is made available on a user's machine and that user is offline, in which case the document is no longer accessible. In this paper we present Apocrita, a revolutionary distributed P2P file sharing system for Intranets.", "references": ["The Design of a Robust Peer-to-Peer System", "Making Gnutella-like P2P Systems Scalable", "Harvest: A Distributed Search System", "Majestic-12: Distributed Search Engine", "YaCy: Distributed P2P-based Web Indexing", "Lucene Search Engine Library", "Test Collections (Time Magazine and NPL)"], "title": "Apocrita: A Distributed Peer-to-Peer File Sharing System for Intranets"}
{"id": "C-32", "keywords": "cooper cach;object storag system;fine-grain share;transact;wide-area network;fault-toler", "abstract": "Collaborative applications provide a shared work environment for groups of networked clients collaborating on a common task. They require strong consistency for shared persistent data and efficient access to fine-grained objects. These properties are difficult to provide in wide area networks because of high network latency.BuddyCache is a new transactional caching approach that improves the latency of access to shared persistent objects for collaborative strong-consistency applications in high-latency network environments. The challenge is to improve performance while providing the correctness and availability properties of a transactional caching protocol in the presence of node failures and slow peers.We have implemented a BuddyCache prototype and evaluated its performance. Analytical results, confirmed by measurements of the BuddyCache prototype using the multi-user 007 benchmark indicate that for typical Internet latencies, e.g. ranging from 40 to 80 milliseconds round trip time to the storage server, peers using BuddyCache can reduce by up to 50% the latency of access to shared objects compared to accessing the remote servers directly.", "references": ["the Utah Network Emulation Facility", "Fragment Reconstruction: Providing Global Cache Coherence in a Transactional Storage System", "Efficient optimistic concurrencty control using loosely synchronized clocks", "Treadmarks: Shared memory computing on networks of workstations", "Two Adaptive Hybrid Cache Coherency Protocols", "Fast Crash Recovery in Distributed File Systems", "Maintaining Strong Cache Consistency in the World Wide Web", "A Status Report on the OO7 OODBMS Benchmarking Effort", "A Hierarchical Internet Object Cache", "Directory Structures for Scalable Internet Caches", "Not All Hits Are Created Equal: Cooperative Proxy Caching Over a Wide-Area Network", "Scalable Web Caching of Frequently Updated Objects using Reliable Multicast", "Cooperative caching: Using remote client memory to improve file system performance", "Zwaenepoel. Combining Compile-Time and Run-Time Support for Efficient Software Distributed Shared Memory", "Summary Cache: A Scalable Wide-Area Web Cache Sharing Protocol", "Implementing Global Memory Management in a Workstation Cluster", "Integrating Coherency and Recoverablity in Distributed Systems", "PerDiS: Design, Implementation, and Use of a PERsistent DIstributed Store", "Transactional Client-Server Cache Consistency: Alternatives and Performance", "Global Memory Management for Client-Server DBMS Architectures", "The Modified Object Buffer: A Storage Management Technique for Object-Oriented Databases", "Replicated document management in a group communication system", "Providing Persistent Objects in Distributed Systems", "A Low-bandwidth Network File System", "Viewstamped Replication: A New Primary Copy Method to Support Highly-Available Distributed Systems", "Opportunistic Log: Efficient Installation Reads in a Reliable Object Server", "ALMI: An Application Level Multicast Infrastructure", "Efficient Cooperative Caching Using Hints", "WebFS: A Global Cache Coherent File System", "On the Scale and Performance of Cooperative Web Proxy Caching", "Hierarchical Cache Consistency in a WAN", "Volume Leases for Consistency in Large-Scale Systems", "Adaptive, Fine-Grained Sharing in a Client-Server OODBMS: A Callback-Based Approach"], "title": "BuddyCache: High-Performance Object Storage for Collaborative Strong-Consistency Applications in a WAN"}
{"id": "C-33", "keywords": "context-awar;context provid;negoti", "abstract": "How to provide appropriate context information is a challenging problem in context-aware computing. Most existing approaches use a centralized selection mechanism to decide which context information is appropriate. In this paper, we propose a novel approach based on negotiation with rewards to solving such problem. Distributed context providers negotiate with each other to decide who can provide context and how they allocate proceeds. In order to support our approach, we have designed a concrete negotiation model with rewards. We also evaluate our approach and show that it indeed can choose an appropriate context provider and allocate the proceeds fairly.", "references": ["Context is key", "Negotiation behavior", "Modelling and using imperfect context information", "Adaptive middleware for context-aware applications in smart-homes", "Providing contextual information to pervasive computing applications", "The design and applications of a context service", "Enhanced reputation mechanism for mobile ad-hoc networks", "The Art and Science of Negotiation", "Persuasive negotiation for autonomous agents: A rhetorical approach", "Inconsistency detection and resolution for context-aware middleware support"], "title": "Rewards-Based Negotiation for Providing Context Information"}
{"id": "C-34", "keywords": "pairwis kei;sensor network;kei pool;kei predistribut;hierarch hypercub model", "abstract": "Security schemes of pairwise key establishment, which enable sensors to communicate with each other securely, play a fundamental role in research on security issue in wireless sensor networks. A new kind of cluster deployed sensor networks distribution model is presented, and based on which, an innovative Hierarchical Hypercube model - H(k,u,m,v,n) and the mapping relationship between cluster deployed sensor networks and the H(k,u,m,v,n) are proposed. By utilizing nice properties of H(k,u,m,v,n) model, a new general framework for pairwise key predistribution and a new pairwise key establishment algorithm are designed, which combines the idea of KDC(Key Distribution Center) and polynomial pool schemes. Furthermore, the working performance of the newly proposed pairwise key establishment algorithm is seriously inspected. Theoretic analysis and experimental figures show that the new algorithm has better performance and provides higher possibilities for sensor to establish pairwise key, compared with previous related works.", "references": ["A key-management scheme for distribute sensor networks", "Random key predistribution schemes for sensor networks", "Perfectly-secure key distribution for dynamic conferences", "Establishing pairwise keys in distributed sensor networks", "A pairwise key pre-distribution scheme for wireless sensor networks", "An optimal class of symmetric key generation systems", "Establishing Pairwise Keys in Distributed Sensor Networks", "Maximum safety path matrix based fault-tolerant routing algorithm for hypercube interconnection network", "Maximum safety path vector based fault-tolerant routing algorithm for hypercube interconnection network", "Location information based hierarchical data congregation routing algorithm for sensor networks", "Balakrishnan, Negotiation Based Protocols for Disseminating Information in Wireless Sensor Networks", "TEEN: a routing protocol for enhanced efficiency in wireless sensor networks", "Modelling Data-Centric Routing in Wireless Sensor Networks"], "title": "Researches on Scheme of Pairwise Key Establishment for DistributedSensor Networks"}
{"id": "C-36", "keywords": "secur publish/subscrib system;distribut access control;administr domain;attribut encrypt", "abstract": "Publish/subscribe systems provide an ecien t, event-based, wide-area distributed communications infrastructure. Large scale publish/subscribe systems are likely to employ compo- nents of the event transport network owned by cooperating, but independent organisations. As the number of partici- pants in the network increases, security becomes an increas- ing concern. This paper extends previous work to present and evaluate a secure multi-domain publish/subscribe in- frastructure that supports and enforces ne-grained access control over the individual attributes of event types. Key refresh allows us to ensure forward and backward security when event brokers join and leave the network. We demon- strate that the time and space overheads can be minimised by careful consideration of encryption techniques, and by the use of caching to decrease unnecessary decryptions. We show that our approach has a smaller overall communica- tion overhead than existing approaches for achieving the same degree of control over security in publish/subscribe networks.", "references": ["Securing publish/subscribe for multi-domain systems", "Eax: A conventional authenticated-encryption mode", "Design and evaluation of a wide-area event notification service", "SCRIBE: A large-scale and decentralized application-level multicast infrastructure", "The TLS protocol", "Engineering event-based systems with scopes", "OMAC: One-key CBC MAC", "Key establishment in large dynamic groups using one-way function trees", "Advanced Encryption Standard (AES)", "Secure distribution of events in content-based publish subscribe systems", "Secure event types in content-based, multi-domain publish/subscribe systems", "A capabilities-based access control architecture for multi-domain publish/subscribe systems", "Hermes: A distributed event-based middleware architecture", "Congestion control in a reliable scalable message-oriented middleware", "A survey of key management for secure group communication", "Enabling confidentiality in content-based publish/subscribe infrastructures", "OCB: a block-cipher mode of operation for efficient authenticated encryption", "A critique of CCM", "Securing publish-subscribe overlay services with eventguard", "Security issues and requirements in internet-scale publish-subscribe systems", "Privacy and authentication: An introduction to cryptography", "Counter with CBC-MAC (CCM)"], "title": "Encryption-Enforced Access Control in Dynamic Multi-Domain Publish/Subscribe Networks"}
{"id": "C-38", "keywords": "peer-to-peer stream;congest control", "abstract": "This paper presents a simple and scalable framework for architecting peer-to-peer overlays called Peer-to-peer Receiver-driven Overlay (or PRO). PRO is designed for non-interactive streaming applications and its primary design goal is to maximize delivered bandwidth (and thus delivered quality) to peers with heterogeneous and asymmetric bandwidth. To achieve this goal, PRO adopts a receiver-driven approach where each receiver (or participating peer) (i) independently discovers other peers in the overlay through gossiping, and (ii) selfishly determines the best subset of parent peers through which to connect to the overlay to maximize its own delivered bandwidth. Participating peers form an unstructured overlay which is inherently robust to high churn rate. than structured overlay networks. Furthermore, each receiver leverages congestion controlled bandwidth from its parents as implicit signal to detect and react to long-term changes in network or overlay condition without any explicit coordination with other participating peers. Independent parent selection by individual peers dynamically converge to an efficient overlay structure.", "references": ["An empirical evaluation of wide-area internet bottlenecks", "Scalable application layer multicast", "Informed Content Delivery Across Adaptive Overlay Networks", "SplitStream: High-bandwidth content distribution in a cooperative environment", "Making gnutella-like p2p systems scalable", "Enabling conferencing applications on the internet using an overlay multicast architecture", "A case for end-system multicast", "Reliable Multicast Framework for Light-Weight Sessions and Application Level Framing", "Resource discovery in distributed networks", "A Survey of Gossiping and Broadcasting in Communication Networks", "Bullet: High bandwidth data dissemination using an overlay mesh", "Some findings on the network performance of broadband hosts", "Receiver-driven layered multicast", "Measurement-based optimization techniques for bandwidth-demanding peer-to-peer systems", "Predicting internet network distance with coordinates-based approaches", "Resilient peer-to-peer streaming", "RAP: An end-to-end rate-based congestion control mechanism for realtime streams in the internet", "PALS: Peer-to-Peer Adaptive Layered Streaming", "Measurement study of peer-to-peer file system sharing", "Zigzag: An efficient peer-to-peer scheme for media streaming"], "title": "A Framework for Architecting Peer-to-Peer Receiver-driven Overlays"}
{"id": "C-40", "keywords": "3d object stream;object pop problem;spatial index;visibl model", "abstract": "Newly emerging game--based application systems such as Second Life1 provide 3D virtual environments where multiple users interact with each other in real--time. They are filled with autonomous, mutable virtual content which is continuously augmented by the users. To make the systems highly scalable and dynamically extensible, they are usually built on a client--server based grid subspace division where the virtual worlds are partitioned into manageable sub--worlds. In each sub--world, the user continuously receives relevant geometry updates of moving objects from remotely connected servers and renders them according to her viewpoint, rather than retrieving them from a local storage medium. In such systems, the determination of the set of objects that are visible from a user's viewpoint is one of the primary factors that affect server throughput and scalability. Specifically, performing real--time visibility tests in extremely dynamic virtual environments is a very challenging task as millions of objects and sub-millions of active users are moving and interacting. We recognize that the described challenges are closely related to a spatial database problem, and hence we map the moving geometry objects in the virtual space to a set of multi-dimensional objects in a spatial database while modeling each avatar both as a spatial object and a moving query. Unfortunately, existing spatial indexing methods are unsuitable for this kind of new environments.The main goal of this paper is to present an efficient spatial index structure that minimizes unexpected object popping and supports highly scalable real--time visibility determination. We then uncover many useful properties of this structure and compare the index structure with various spatial indexing methods in terms of query quality, system throughput, and resource utilization. We expect our approach to lay the groundwork for next--generation virtual frameworks that may merge into existing web--based services in the near future.", "references": ["Challeges in modern distributed interactive application design", "Game traffic analysis: An MMORPG perspective", "Traffic charateristics of a massively multiplayer online role playing game and its implications", "Enabling player-created online worlds with grid computing and streaming", "Streaming of complex 3d scenes for remote walkthroughs", "A network architecture for remote rendering", "Adaptive display algorithm for interactive frame rates during visualization of complex virtual environments", "High-performance spatial indexing for location-based services", "Supporting frequent updates in r-trees: A bottom-up approach", "Sina: scalable incremental processing of continuous queries in spatio-temporal databases", "Main memory evaluation of monitoring queries over moving objects", "A generic framework for monitoring continuous spatial queries over moving objects", "Query indexing and velocity constrained indexing: Scalable techniques for continuous queries on moving objects", "Q+rtree: Efficient indexing for moving object databases", "Information Retrieval"], "title": "Edge Indexing in a Grid for Highly Dynamic Virtual Environments"}
{"id": "C-86", "keywords": "strateg behavior;resourc alloc;market-base system", "abstract": "While market-based systems have long been proposed as solutions for distributed resource allocation, few have been deployed for production use in real computer systems. Towards this end, we present our initial experience using Mirage, a microeconomic resource allocation system based on a repeated combinatorial auction. Mirage allocates time on a heavily-used 148-node wireless sensor network testbed. In particular, we focus on observed strategic user behavior over a four-month period in which 312,148 node hours were allocated across 11 research projects. Based on these results, we present a set of key challenges for market-based resource allocation systems based on repeated combinatorial auctions. Finally, we propose refinements to the system's current auction scheme to mitigate the strategies observed to date and also comment on some initial steps toward building an approximately strategyproof repeated combinatorial auction.", "references": ["Crossbow corporation", "NimrodG: An Architecture of a Resource Management and Scheduling System in a Global Computational Grid", "A Microeconomic Resource Allocation System for SensorNet Testbeds", "Multipart pricing of public goods", "Combinatorial Auctions: A Survey", "Incentives in Teams", "Adaptive Limited-Supply Online Auctions", "Mechanism Theory", "Tycoon: A Distributed Market-based Resource Allocation System", "Towards a Characterization of Truthful Combinatorial Auctions", "Competitive Analysis of Incentive Compatible On-line Auctions", "Truth Revelation in Approximately Efficient Combinatorial Auctions", "Virtual Worlds: Fast and Strategyproof Auctions for Dynamic Resou rce Allocation", "Bidding and Allocation in Combinatorial Auctions", "Algorithmic Mechanism Design", "Computationally Feasible VCG Mechanisms", "The POPCORN Market - an Online Market for Computational Resources", "Generalized Vickrey auctions", "Counterspeculation, Auctions and Competitive Sealed Tenders", "Analyzing Market-based Resource Allocation Strategies for the Computational Grid"], "title": "Addressing Strategic Behavior in a Deployed Microeconomic Resource Allocator"}
{"id": "H-2", "keywords": "person web search;queri expans;desktop profil;keyword extract;keyword co-occurr", "abstract": "The inherent ambiguity of short keyword queries demands for enhanced methods for Web retrieval. In this paper we propose to improve such Web queries by expanding them with terms collected from each user's Personal Information Repository, thus implicitly personalizing the search output. We introduce five broad techniques for generating the additional query keywords by analyzing user data at increasing granularity levels, ranging from term and compound level analysis up to global co-occurrence statistics, as well as to using external thesauri. Our extensive empirical analysis under four different scenarios shows some of these approaches to perform very well, especially on ambiguous queries, producing a very strong increase in the quality of the output rankings. Subsequently, we move this personalized search framework one step further and propose to make the expansion process adaptive to various features of each query. A separate set of experiments indicates the adaptive algorithms to bring an additional statistically significant improvement over the best static expansion approach.", "references": ["Using part-of-speech patterns to reduce query ambiguity", "The paraphrase search assistant: Terminological feedback for iterative information seeking", "Automatic query wefinement using lexical affinities with maximal information gain", "An information-theoretic approach to automatic query expansion", "Integrating query expansion and conceptual relevance feedback for personalized web information retrieval", "Summarizing local context to personalize global web search", "Predicting query performance", "Probabilistic query expansion using query logs", "Accurate methods for the statistics of surprise and coincidence", "New methods in automatic extracting", "User choices: A new yardstick for the evaluation of ranking algorithms for interactive query expansion", "Scaling link based similarity search", "Topic-sensitive pagerank", "Inferring query performance using pre-retrieval predictors", "Ir evaluation methods for retrieving highly relevant documents", "Scaling personalized web search", "A comparison of collocation-based similarity measures in query expansion", "Information retrieval using word senses: root sense tagging approach", "Mining anchor text for query refinement", "Lexical ambiguity and information retrieval", "Applying summarization techniques for term selection in relevance feedback", "An effective approach to document retrieval via utilizing wordnet and recognizing phrases", "Wordnet: An electronic lexical database", "Topical link analysis for web search", "The PageRank citation ranking: Bringing order to the web", "Automatic indentification of user interest for personalized search", "Concept based query expansion", "Relevance feedback in information retrieval", "Re-examining the potential effectiveness of interactive query expansion", "To randomize or not to randomize: Space optimal summaries for hyperlink analysis", "Evaluating high accuracy retrieval techniques", "Adaptive web search based on user profile constructed without any effort from users", "The older you are, the more you want personalized search", "Personalizing search via automated analysis of interests and activities", "Personalization and privacy", "Query expansion using lexical-semantic relations", "Query expansion using local and global document analysis", "Improving pseudo-relevance feedback in web information retrieval using web page segmentation"], "title": "Personalized Query Expansion for the Web"}
{"id": "H-3", "keywords": "queri context;domain model;term relat;languag model", "abstract": "User query is an element that specifies an information need, but it is not the only one. Studies in literature have found many contextual factors that strongly influence the interpretation of a query. Recent studies have tried to consider the user's interests by creating a user profile. However, a single profile for a user may not be sufficient for a variety of queries of the user. In this study, we propose to use query-specific contexts instead of user-centric ones, including context around query and context within query . The former specifies the environment of a query such as the domain of interest, while the latter refers to context words within the query, which is particularly useful for the selection of relevant term relations. In this paper, both types of context are integrated in an IR model based on language modeling. Our experiments on several TREC collections show that each of the context factors brings significant improvements in retrieval effectiveness.", "references": ["Context-dependent term relations for information retrieval", "Interaction with texts: Information retrieval as information seeking behavior", "Information retrieval as statistical translation", "Modles de langue appliqus  la recherche", "Using ODP metadata to personalize search", "Word association norms, mutual information, and lexicography", "Relevance feedback and personalization: A language modeling perspective", "Context-based topic models for query modification", "Stuff I've seen: a system for personal information retrieval and re-use", "Semantic term matching in axiomatic approaches to information retrieval", "Linear discriminative model for information retrieval", "Goole Personalized Search", "Algorithms for association rule mining - a general survey and comparison", "Information retrieval in context: IRiX", "Personalized ranking of search results with learned user interest hierarchies from bookmarks", "Relevance-based language models", "Belief revision for adaptive information retrieval", "Personalized web search by mapping user queries to categories", "Cluster-based retrieval using language models", "Toward a user-centered information service", "Toward a theory of user-based relevance: A call for a new paradigm of inquiry", "Augmenting Naive Bayes Classifiers with Statistical Language Models", "Personalized Search", "Concept based query expansion", "Retrieving with good sense", " A reexamination of relevance: Towards a dynamic, situational definition", "A cooccurrence-based thesaurus and two applications to information retrieval", "Query enrichment for web-query classification", "Context-sensitive information retrieval using implicit feedback", "Personalizing search via automated analysis of interests and activities", "Query expansion using lexical-semantic relations", "Query expansion using local and global document analysis", "Unsupervised word sense disambiguation rivaling supervised methods", "Contextsensitive semantic smoothing for the language modeling approach to genomic IR", "Model-based feedback in the language modeling approach to information retrieval", "A study of smoothing methods for language models applied to ad-hoc information retrieval"], "title": "Using Query Contexts in Information Retrieval"}
{"id": "H-4", "keywords": "person inform manag;user evalu", "abstract": "Personal Information Management (PIM) is a rapidly grow- ing area of research concerned with how people store, man- age and re-find information. A feature of PIM research is that many systems have been designed to assist users man- age and re-find information, but very few have been evalu- ated. This has been noted by several scholars and explained by the diculties involved in performing PIM evaluations. The diculties include that people re-find information from within unique personal collections; researchers know little about the tasks that cause people to re-find information; and numerous privacy issues concerning personal informa- tion. In this paper we aim to facilitate PIM evaluations by addressing each of these diculties. In the first part, we present a diary study of information re-finding tasks. The study examines the kind of tasks that require users to re-find information and produces a taxonomy of re-finding tasks for email messages and web pages. In the second part, we propose a task-based evaluation methodology based on our findings and examine the feasibility of the approach using two dierent methods of task creation.", "references": ["Improving tool support for personal information management", "The iir evaluation model: A framework for evaluation of interactive information retrieval systems", "Task complexity affects information seeking and use", "Re-finding found things: An exploratory study of how users re-find information", "Using web search engines to find and refind information", "Factors and evaluation of refinding behaviors", "Fast, flexible filtering with phlat", "A diary study of task switching and interruptions", "Stuff i"ve seen: a system for personal information retrieval and re-use", "Memory and email re-finding, In preparation for ACM TOIS CFP special issue on Keeping, Re-finding, and Sharing Personal Information", "Dealing with fragmented recollection of context in information management", "Towards memory supporting personal information management tools", "What we have learned, and not learned, from trec", "Information retrieval interaction", "Measurement and design", "Understanding what works: Evaluating personal information management tools", "How a personal document"s intended use or purpose affects its classification in an office", "The psychology of personal information management", "Voice-mail diary studies for naturalistic data capture under mobile conditions", "Milestones in time: The value of landmarks in retrieving information from personal stores", "Data mountain: using spatial memory for document management", "How do people organise their photographs", "One hundred years of forgetting: A quantitative description of retention", "The myth of the paperless office", "Task complexity, problem structure and information actions: Integrating studies in on information seeking and retrieval", "A theory of task-based information retrieval"], "title": "Towards Task-based Personal Information Management Evaluations"}
{"id": "H-5", "keywords": "util-base distil;unifi framework;adapt filter;novelti detect;passag rank;exibl user feedback;evalu methodolog", "abstract": "This paper examines a new approach to information distil- lation over temporally ordered documents, and proposes a novel evaluation scheme for such a framework. It combines the strengths of and extends beyond conventional adaptive filtering, novelty detection and non-redundant passage ranking with respect to long-lasting information needs ('tasks' with multiple queries). Our approach supports fine-grained user feedback via highlighting of arbitrary spans of text, and leverages such information for utility optimization in adaptive settings. For our experiments, we defined hypothetical tasks based on news events in the TDT4 corpus, with multiple queries per task. Answer keys (nuggets) were generated for each query and a semi- automatic procedure was used for acquiring rules that allow automatically matching nuggets against system responses. We also propose an extension of the NDCG metric for assessing the utility of ranked passages as a combination of relevance and novelty. Our results show encouraging utility enhancements using the new approach, compared to the baseline systems without incremental learning or the novelty detection components.", "references": ["Incremental Relevance Feedback for Information Filtering", "Retrieval and Novelty Detection at the Sentence Level", "Automatic Retrieval with Locality Information using SMART", "Learning While Filtering Documents", "The use of MMR, Diversity-based Reranking for Reordering Documents and Producing Summaries", "Annual Review of Information Science and Technology (ARIST)", "Topic Detection and Tracking Overview", "A Statistical Model for Multilingual Entity Detection and Tracking", "Cumulated Gain-based Evaluation of IR Techniques", "Automatically Evaluating Answers to Definition Questions", "Will Pyramids Built of nUggets Topple Over", "A Mention-synchronous Coreference Resolution Algorithm based on the Bell Tree", "Nuggeteer: Automatic Nugget-Based Evaluation Using Descriptions and Judgments", "Automatically Constructing a Dictionary for Information Extraction Tasks", "Microsoft Cambridge at TREC-9: Filtering track", "Boosting and Rocchio Applied to Text Filtering", "Indri: A Language Model-based Search Engine for Complex Queries", "The Linguistic Data Consortium", "Overview of the TREC 2003 Question Answering Track", "Margin-based Local Regression for Adaptive Filtering", "Robustness of Adaptive Filtering Methods in a Cross-benchmark Evaluation", "Beyond Independent Relevance: Methods and Evaluation Metrics for Subtopic Retrieval", "Robustness of Regularized Linear Classification Methods in Text Categorization", "Using Bayesian Priors to Combine Classifiers for Adaptive Filtering", "Novelty and Redundancy Detection in Adaptive Filtering"], "title": "Utility-based Information Distillation Over Temporally Sequenced Documents"}
{"id": "H-7", "keywords": "recommend system;inform filter;person;em algorithm;bayesian hierarch model", "abstract": "A content-based personalized recommendation system learns user specific profiles from user feedback so that it can deliver information tailored to each individual user's interest. A system serving millions of users can learn a better user profile for a new user, or a user with little feedback, by borrowing information from other users through the use of a Bayesian hierarchical model. Learning the model parameters to optimize the joint data likelihood from millions of users is very computationally expensive. The commonly used EM algorithm converges very slowly due to the sparseness of the data in IR applications. This paper proposes a new fast learning technique to learn a large number of individual user profiles. The efficacy and efficiency of the proposed algorithm are justified by theory and demonstrated on actual user data from Netflix and MovieLens.", "references": ["Recommendation as classification: Using social and content-based information in recommendation", "Empirical analysis of predictive algorithms for collaborative filtering", "Document filtering with inference networks", "Kernel method for document filtering", "Adaptation of maximum entropy capitalizer: Little data can help a lot", "Language Modeling for Information Retrieval", "Constructing informative prior distributions from domain knowledge in text classification", "Memory-based weightedmajority prediction for recommender systems", "A tutorial on learning with bayesian networks", "An algorithmic framework for performing collaborative filtering", "Latent class models for collaborative filtering", "Internet movie database", "An automatic weighting scheme for collaborative filtering", "GroupLens: Applying collaborative filtering to Usenet news", "Applying support vector machines to the TREC-2001 batch filtering and routing tasks", "Text classification by labeling words", "Content-boosted collaborative filtering for improved recommendations", "Relevance weighting of search terms", "Unifying user-based and item-based collaborative filtering approaches by similarity fusion", "Incorporating prior knowledge with weighted margin support vector machines", "Robustness of adaptive filtering methods in a cross-benchmark evaluation", "Learning gaussian processes from multiple tasks", "A nonparametric hierarchical bayesian framework for information filtering", "Semi-supervised learning literature survey", "Bayesian adaptive user profiling with explicit & implicit feedback"], "title": "Efficient Bayesian Hierarchical User Modeling for Recommendation Systems"}
{"id": "H-8", "keywords": "inform retriev;evalu;test collect;reusabl", "abstract": "Low-cost methods for acquiring relevance judgments can be a boon to researchers who need to evaluate new retrieval tasks or topics but do not have the resources to make thou- sands of judgments. While these judgments are very use- ful for a one-time evaluation, it is not clear that they can be trusted when re-used to evaluate new systems. In this work, we formally define what it means for judgments to be reusable: the confidence in an evaluation of new systems can be accurately assessed from an existing set of relevance judg- ments. We then present a method for augmenting a set of relevance judgments with relevance estimates that requires no additional assessor eort. Using this method practically guarantees reusability: with as few as five judgments per topic taken from only two systems, we can reliably evaluate a larger set of ten systems. This makes even the smallest sets of judgments useful for evaluation of new systems.", "references": ["Models for Metasearch", "On the effectiveness of evaluating retrieval systems in the absence of relevance judgments", "A unified model for metasearch, pooling, and system evaluation", "A statistical method for system evaluation using incomplete judgments", "A maximum entropy approach to natural language processing", "An easy derivation of logistic regression from the bayesian and maximum entropy perspective", "Research methodology in studies of assessor effort for retrieval evaluation", "Minimal test collections for retrieval evaluation", "Learning a ranking from pairwise preferences", "Unanimity and compromise among probability forecasters", "Efficient Construction of Large Test Collections", "Bayesian Data Analysis", "Probability Theory: The Logic of Science", "A Formal Approach to Score Normalization for Metasearch", "Maximum entropy aggregation of expert predictions", "Probabilistic outputs for support vector machines and comparison to regularized likelihood methods", "Forming test collections with no system pooling", "Dynamic test collections: measuring search effectiveness on the live web", "Information Retrieval Test Collections", "TREC: Experiment and Evaluation in Information Retrieval", "How Reliable are the Results of Large-Scale Information Retrieval Experiments?"], "title": "Robust Test Collections for Retrieval Evaluation Ben Carterette"}
{"id": "H-9", "keywords": "search result organ;search engin log;interest aspect", "abstract": "Eectiv e organization of search results is critical for improv- ing the utility of any search engine. Clustering search results is an eectiv e way to organize search results, which allows a user to navigate into relevant documents quickly. How- ever, two deciencies of this approach make it not always work well: (1) the clusters discovered do not necessarily correspond to the interesting aspects of a topic from the user's perspective; and (2) the cluster labels generated are not informative enough to allow a user to identify the right cluster. In this paper, we propose to address these two de- ciencies by (1) learning \interesting aspects" of a topic from Web search logs and organizing search results accordingly; and (2) generating more meaningful cluster labels using past query words entered by users. We evaluate our proposed method on a commercial search engine log data. Compared with the traditional methods of clustering search results, our method can give better result organization and more mean- ingful labels.", "references": ["Improving web search ranking by incorporating user behavior information", "The star clustering algorithm for static and dynamic information organization", "Applications of web query mining", "Agglomerative clustering of a search engine query log", "What makes a query difficult?", "Bringing order to the web: automatically categorizing search results", "Predicting query performance", "Optimizing search by showing results in context", "Reexamining the cluster hypothesis: Scatter/gather on retrieval results", "Optimizing search engines using clickthrough data", "Evaluating Retrieval Performance Using Clickthrough Data", "Generating query substitutions", "A hierarchical monothetic document clustering algorithm for summarization and browsing search results", "Accelerating search in academic research", "Scatter/gather browsing communicates the topic structure of a very large text collection", "Query chains: learning to rank from implicit feedback", "Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval", "A vector space model for automatic indexing", "Context-sensitive information retrieval using implicit feedback", "Information Retrieval", "The Nature of Statistical Learning Theory", "Latent semantic analysis for multiple-type interrelated data objects", "Clustering user queries of a search engine", "Learning to estimate query difficulty: including applications to missing content detection and distributed information retrieval", "Web document clustering: A feasibility demonstration", "Grouper: A dynamic clustering interface to web search results", "Learning to cluster web search results"], "title": "Learn from Web Search Logs to Organize Search Results"}
{"id": "H-10", "keywords": "document cluster;regular", "abstract": "In recent years, document clustering has been receiving more and more attentions as an important and fundamental tech- nique for unsupervised document organization, automatic topic extraction, and fast information retrieval or flltering. In this paper, we propose a novel method for clustering doc- uments using regularization. Unlike traditional globally reg- ularized clustering methods, our method flrst construct a local regularized linear label predictor for each document vector, and then combine all those local regularizers with a global smoothness regularizer. So we call our algorithm Clustering with Local and Global Regularization (CLGR). We will show that the cluster memberships of the docu- ments can be achieved by eigenvalue decomposition of a sparse symmetric matrix, which can be e-ciently solved by iterative methods. Finally our experimental evaluations on several datasets are presented to show the superiorities of CLGR over traditional document clustering methods.", "references": ["Distributional Clustering of Words for Text Classification", "Laplacian Eigenmaps for Dimensionality Reduction and Data Representation", "Towards a Theoretical Foundation for Laplacian-Based Manifold Methods", "Manifold Regularization: a Geometric Framework for Learning from Examples", "Principal Direction Divisive Partitioning", "Local learning algorithms", "Spectral K-way Ratio-Cut Partitioning and Clustering", "Scatter/Gather: A Cluster-Based Approach to Browsing Large Document Collections", "Concept Decompositions for Large Sparse Text Data using Clustering", "On the equivalence of nonnegative matrix factorization and spectral clustering", "A min-max cut algorithm for graph partitioning and data clustering", "Orthogonal Nonnegative Matrix Tri-Factorizations for Clustering", "Pattern Classification", "Document Clustering via Adaptive Subspace Iteration", "The Relationships Among Various Nonnegative Matrix Factorization Methods for Clustering", "Document Clustering with Cluster Refinement and Model Selection Capabilities", "WebACE: A Web Agent for Document Categorization and Exploration", "From Graphs to Manifolds - Weak and Strong Pointwise Consistency of Graph Laplacians", "Initialization of Cluster Refinement Algorithms: A Review and Comparative Study", "On Spectral Clustering: Analysis and an algorithm", "Learning with Kernels", "Normalized Cuts and Image Segmentation", "Cluster Ensembles - A Knowledge Reuse Framework for Combining Multiple Partitions", "The Nature of Statistical Learning Theory", "A Local Learning Approach for Clustering", "Multiclass Spectral Clustering", "Document Clustering Based On Non-Negative Matrix Factorization", "Spectral Relaxation for K-means Clustering", "Text Categorization Based on Regularized Linear Classification Methods", "Self-Tuning Spectral Clustering", "Learning with Local and Global Consistency"], "title": "Regularized Clustering for Documents"}
{"id": "H-11", "keywords": "imag retriev;activ learn;relev feedback", "abstract": "Relevance feedback is a powerful technique to enhance ContentBased Image Retrieval (CBIR) performance. It solicits the user's relevance judgments on the retrieved images returned by the CBIR systems. The user's labeling is then used to learn a classifier to distinguish between relevant and irrelevant images. However, the top returned images may not be the most informative ones. The challenge is thus to determine which unlabeled images would be the most informative (i.e., improve the classifier the most) if they were labeled and used as training samples. In this paper, we propose a novel active learning algorithm, called Laplacian Optimal Design (LOD), for relevance feedback image retrieval. Our algorithm is based on a regression model which minimizes the least square error on the measured (or, labeled) images and simultaneously preserves the local geometrical structure of the image space. Specifically, we assume that if two images are sufficiently close to each other, then their measurements (or, labels) are close as well. By constructing a nearest neighbor graph, the geometrical structure of the image space can be described by the graph Laplacian. We discuss how results from the field of optimal experimental design may be used to guide our selection of a subset of images, which gives us the most amount of information. Experimental results on Corel database suggest that the proposed approach achieves higher precision in relevance feedback image retrieval.", "references": ["Optimum Experimental Designs", "Manifold regularization: A geometric framework for learning from examples", "Spectral Graph Theory", "Active learning with statistical models", "A new semi-supervised em algorithm for image retrieval", "Robust design of biological experiments", "Multimodal concept-dependent active learning for image retrieval", "Incremental semi-supervised subspace learning for image retrieval", "A semi-supervised active learning framework for image retrieval", "How to complete performance graphs in content-based image retrieval: Add generality and normalize scope", "Semantic manifold learning for image retrieval", "Relevance feedback: A power tool for interative content-based image retrieval", "Content-based image retrieval at the end of the early years", "Support vector machine active learning for image retrieval", "Color texture moments for content-based image retrieval", "Active learning via transductive experimental design"], "title": "Laplacian Optimal Design for Image Retrieval"}
{"id": "H-12", "keywords": "web summari;snippet gener;document cach", "abstract": "The presentation of query biased document snippets as part of results pages presented by search engines has become an expectation of search engine users. In this paper we explore the algorithms and data structures required as part of a search engine to allow efficient generation of query biased snippets. We begin by proposing and analysing a document compression method that reduces snippet generation time by 58% over a baseline using the zlib compression library. These experiments reveal that finding documents on secondary storage dominates the total cost of generating snippets, and so caching documents in RAM is essential for a fast snippet generation process. Using simulation, we examine snippet generation performance for different size RAM caches. Finally we propose and analyse document reordering and compaction, revealing a scheme that increases the number of document cache hits with only a marginal affect on snippet quality. This scheme effectively doubles the number of documents that can fit in a fixed size cache.", "references": ["The anatomy of a large-scale hypertextual Web search engine", "Searching the workplace web", "Boosting the performance of web search engines: Caching and prefetching query results by exploiting historical usage data", "Zlib Compression Library", "Access-ordered indexes", "The google file system", "Summarizing text documents: sentence selection and evaluation metrics", "Overview of TREC-7 Very Large Collection Track", "A temporal comparison of altavista web searching", "A trainable document summarizer", "Accessibility of information on the web", "The automatic creation of literature abstracts", "Automatic Summarization", "Text compression for dynamic document databases", "Compressed full text indexes", "Introduction to the special issue on summarization", "Beyond pagerank: machine learning for static ranking", "Generic summaries for indexing in information retrieval", "Efficiently computed lexical chains as an intermediate representation for automatic text summarization", "Advantages of query biased summaries in information retrieval", "Finding relevant documents using top ranking sentences: an evaluation of two alternative schemes", "Compressing integers for fast file access", "Searchable words on the Web", "Managing Gigabytes: Compressing and Indexing Documents and Images", "The Zettair Search Engine"], "title": "Fast Generation of Result Snippets in Web Search"}
{"id": "H-13", "keywords": "web search;summar;snippet;queri log", "abstract": "Web search engines present lists of captions, comprising ti- tle, snippet, and URL, to help users decide which search results to visit. Understanding the influence of features of these captions on Web search behavior may help validate algorithms and guidelines for their improved generation. In this paper we develop a methodology to use clickthrough logs from a commercial search engine to study user behavior when interacting with search result captions. The findings of our study suggest that relatively simple caption features such as the presence of all terms query terms, the readabil- ity of the snippet, and the length of the URL shown in the caption, can significantly influence users' Web search behav- ior.", "references": ["Improving web search ranking by incorporating user behavior information", "Learning user interaction models for predicting Web search result preferences", "A taxonomy of Web search", "What are you looking for? An eye-tracking study of information usage in Web search", "Optimizing search by showing results in context", "Summarizing text documents: Sentence selection and evaluation metrics", "Eye-tracking analysis of user behavior in WWW search", "Title extraction from bodies of HTML documents and its application to Web page retrieval", "Accurately interpreting clickthrough data as implicit feedback", "Automatic identification of user goals in Web search", "The automatic creation of literature abstracts", "WaveLens: A new view onto Internet search results", "Understanding user goals in Web search", "Web-page summarization using clickthrough data", "Advantages of query biased summaries in information retrieval", "A system for query-specific document summarization", "Finding relevant documents using top ranking sentences: An evaluation of two alternative schemes", "Optimizing web search using Web click-through data"], "title": "The Influence of Caption Features on Clickthrough Patterns in Web Search"}
{"id": "H-14", "keywords": "user studi;search destin;enhanc web search", "abstract": "We present a novel Web search interaction feature which, for a given query, provides links to websites frequently visited by other users with similar information needs. These popular destinations complement traditional search results, allowing direct navigation to authoritative resources for the query topic. Destinations are identified using the history of search and browsing behavior of many users over an extended time period, whose collective behavior provides a basis for computing source authority. We describe a user study which compared the suggestion of destinations with the previously proposed suggestion of related queries, as well as with traditional, unaided Web search. Results show that search enhanced by destination suggestions outperforms other systems for exploratory tasks, with best performance obtained from mining past user behavior at query-level granularity.", "references": ["Improving Web search ranking by incorporating user behavior information", "Adaptive Web navigation for wireless devices", "Using terminological feedback for Web search refinement: A log-based study", "Experiments with interfaces to support query expansion", "Experimental components for the evaluation of interactive information retrieval systems", "Models of searching and browsing: languages, studies and applications", "The TREC interactive tracks: putting the user into search", "Experience with an adaptive indexing scheme", "FERRET: Interactive questionanswering for real-world environments", "Generating query substitutions", "A case for interaction: a study of interactive information retrieval behavior and effectiveness", "Orienteering in an information landscape: how information seekers get from here to there", "Query chains: Learning to rank from implicit feedback", "Term-weighting approaches in automatic text retrieval", "Analysis of a very large Web search engine query log", "Exploiting query repetition and regularity in an adaptive community-based Web search engine", "versus European Web searching trends", "Multitasking during Web search sessions", "Footprints: history-rich tools for information foraging", "Investigating behavioral variability in Web search", "Examining the effectiveness of real-time query expansion"], "title": "Studying the Use of Popular Destinations to Enhance Web Search Interaction"}
{"id": "H-16", "keywords": "cach;web search;inform retriev system;queri log", "abstract": "In this paper we study the trade-offs in designing efficient caching systems for Web search engines. We explore the impact of different approaches, such as static vs. dynamic caching, and caching query results vs. caching posting lists. Using a query log spanning a whole year we explore the limi- tations of caching and we demonstrate that caching posting lists can achieve higher hit rates than caching query an- swers. We propose a new algorithm for static caching of posting lists, which outperforms previous methods. We also study the problem of finding the optimal way to split the static cache between answers and posting lists. Finally, we measure how the changes in the query log affect the effec- tiveness of static caching, given our observation that the distribution of the queries changes slowly over time. Our results and observations are applicable to different levels of the data-access hierarchy, for instance, for a memory/disk layer or a broker/remote server layer.", "references": ["Pruned query evaluation using pre-computed impacts", "A three level search engine index based in query log distribution", "Optimization of inverted vector searches", "A document-centric approach to static index pruning in text retrieval systems", "Cost-aware WWW proxy caching Working sets past and presentalgorithms", "Boosting the performance of web search engines: Caching and prefetching query results by exploiting historical usage data", "Predictive caching and prefetching of query results in search engines", "Three-level caching for efficient query processing in large web search engines", "On caching search engine query results", "Terrier: A High Performance and Scalable Information Retrieval Platform", "On the reuse of past optimal queries", "Rank-preserving two-level caching for scalable search engines", "A note on the calculation of average working set size", "Optimization strategies for complex queries", "Managing Gigabytes: Compressing and Indexing Documents and Images", "On-line file caching"], "title": "The Impact of Caching on Search Engines"}
{"id": "H-17", "keywords": "invert index;prune;correct guarante;web search engin", "abstract": "The Web search engines maintain large-scale inverted indexes which are queried thousands of times per second by users eager for information. In order to cope with the vast amounts of query loads, search engines prune their index to keep documents that are likely to be returned as top results, and use this pruned index to compute the first batches of results. While this approach can im- prove performance by reducing the size of the index, if we compute the top results only from the pruned index we may notice a signif- icant degradation in the result quality: if a document should be in the top results but was not included in the pruned index, it will be placed behind the results computed from the pruned index. Given the fierce competition in the online search market, this phenomenon is clearly undesirable. In this paper, we study how we can avoid any degradation of result quality due to the pruning-based performance optimization, while still realizing most of its benefit. Our contribution is a num- ber of modifications in the pruning techniques for creating the pruned index and a new result computation algorithm that guar- antees that the top-matching pages are always placed at the top search results, even though we are computing the first batch from the pruned index most of the time. We also show how to determine the optimal size of a pruned index and we experimentally evaluate our algorithms on a collection of 130 million Web pages.", "references": ["Vector-space ranking with effective early termination", "Pruning strategies for mixed-mode querying", "Modern Information Retrieval", "Evaluating top-k queries over web-accessible databases", "A document-centric approach to static index pruning in text retrieval systems", "Evaluating the performance of distributed architectures for information retrieval using a variety of workloads", "Static index pruning for information retrieval systems", "Optimizing queries over multimedia repositories", "Introduction to Algorithm", "Open directory", "Combining fuzzy information: an overview", "Optimal aggregation algorithms for middleware", "The indexable web is more than 11.5 billion pages", "Towards efficient multi-feature queries in heterogeneous environments", "Combating web spam with trustrank", "An analysis of web documents retrieved and viewed", "Authoritative sources in a hyperlinked environment", "Predictive caching and prefetching of query results in search engines", "Optimizing result prefetching in web search engines with segmented indices", "Optimized query execution in large search engines with global page ordering", "Three-level caching for efficient query processing in large web search engines", "Building a distributed full-text index for the web", "What"s new on the web? The evolution of the web from a search engine perspective", "Detecting spam web pages through content analysis", "The pagerank citation ranking: Bringing order to the web", "Filtered document retrieval with frequency-sorted indexes", "The intelligent surfer: Probabilistic combination of link and content information in pagerank", "Relevance weighting of search terms", "Introduction to modern information retrieval", "Rank-preserving two-level caching for scalable search engines", "Top-k query evaluation with probabilistic guarantees", "Performance of inverted indices in shared-nothing distributed text document information retrieval systems"], "title": "Pruning Policies for Two-Tiered Inverted Index with Correctness Guarantee"}
{"id": "H-18", "keywords": "topic segment;share topic detect;topic align;mutual inform;multipl document;term weight", "abstract": "Topic detection and tracking (26) and topic segmentation (15) play an important role in capturing the local and se- quential information of documents. Previous work in this area usually focuses on single documents, although similar multiple documents are available in many domains. In this paper, we introduce a novel unsupervised method for shared topic detection and topic segmentation of multiple similar documents based on mutual information (MI) and weighted mutual information (WMI) that is a combination of MI and term weights. The basic idea is that the optimal segmen- tation maximizes MI(or WMI). Our approach can detect shared topics among documents. It can flnd the optimal boundaries in a document, and align segments among docu- ments at the same time. It also can handle single-document segmentation as a special case of the multi-document seg- mentation and alignment. Our methods can identify and strengthen cue terms that can be used for segmentation and partially remove stop words by using term weights based on entropy learned from multiple documents. Our experimental results show that our algorithm works well for the tasks of single-document segmentation, shared topic detection, and multi-document segmentation. Utilizing information from multiple documents can tremendously improve the perfor- mance of topic segmentation, and using WMI is even better than using MI for the multi-document segmentation.", "references": ["A generalized maximum entropy approach to bregman co-clustering and matrix approximation", "Multi-way distributional clustering via pairwise interactions", "Topic segmentation with an aspect hidden markov model", "Latent dirichlet allocation", "Topic-based document segmentation with probabilistic latent semantic analysis", "Advances in domain indepedent linear text segmentation", "Maximum entropy segmentation of broadcast news", "Elements of Information Theory", "Indexing by latent semantic analysis", "Information-theoretic co-clustering", "Text segmentation with multiple surface linguistic cues", "Stop word location and identification for adaptive text recognition", "Probabilistic latent semantic analysis", "Correlating summarization of a pair of multilingual documents", "Domain-independent text segmentation using anisotropic diffusion and dynamic programming", "Extracting shared topics of multiple documents", "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "Entropy-based criterion in categorical clustering", "Maximum entropy markov models for information extraction and segmentation", "A critique and improvement of an evaluation metric for text segmentation", "Statistical models for topic segmentation", "Introduction to Modern Information Retrieval", "Extraction and search of chemical formulae in text documents on the web", "Multi-task text segmentation and alignment based on weighted mutual information", "A statistical model for domain-independent text segmentation", "Multilingual topic detection and tracking: Successful research enabled by corpora and evaluation", "A hidden markov model approach to text segmentation and event tracking", "Correlating multilingual documents via bipartite graph modeling"], "title": "Topic Segmentation with Shared Topic Detection andAlignment of Multiple Documents"}
{"id": "H-19", "keywords": "featur categor;event detect;dft;gaussian", "abstract": "We consider the problem of analyzing word trajectories in both time and frequency domains, with the specific goal of identifying important and less-reported, periodic and aperiodic words. A set of words with identical trends can be grouped together to reconstruct an event in a completely un-supervised manner. The document frequency of each word across time is treated like a time series, where each element is the document frequency - inverse document frequency (DFIDF) score at one time point. In this paper, we 1) first applied spectral analysis to categorize features for different event characteristics: important and less-reported, periodic and aperiodic; 2) modeled aperiodic features with Gaussian density and periodic features with Gaussian mixture densities, and subsequently detected each feature's burst by the truncated Gaussian approach; 3) proposed an unsupervised greedy event detection algorithm to detect both aperiodic and periodic events. All of the above methods can be applied to time series data in general. We extensively evaluated our methods on the 1-year Reuters News Corpus [3] and showed that they were able to uncover meaningful aperiodic and periodic events.", "references": ["Google news alerts", "Reuters corpus", "Topic Detection and Tracking", "First story detection in tdt is hard", "Retrieval and novelty detection at the sentence level", "A system for new event detection", "Maximum likelihood from incomplete data via the EM algorithm", "Parameter free bursty events detection in text streams", "A model for anticipatory event detection", "Bursty feature reprensentation for clustering text streams", "Bursty and hierarchical structure in streams", "On the bursty evolution of blogspace", "Text classification and named entities for new event detection", "Discovering evolutionary theme patterns from text: an exploration of temporal text mining", "Kullback-liebler divergences of normal, gamma, dirichlet and wishart densities", "Combining semantic and syntactic document classifiers to improve first story detection", "Automatic generation of overview timelines", "Identifying similarities, periodicities and bursts for online search queries", "A study of retrospective and on-line event detection", "Topic-conditioned novelty detection"], "title": "Analyzing Feature Trajectories for Event Detection"}
{"id": "H-20", "keywords": "topic detect and track;new event detect;name entiti;real-time index", "abstract": "New Event Detection (NED) aims at detecting from one or multiple streams of news stories that which one is reported on a new event (i.e. not reported previously). With the overwhelming volume of news available today, there is an increasing need for a NED system which is able to detect new events more efficiently and accurately. In this paper we propose a new NED model to speed up the NED task by using news indexing-tree dynamically. Moreover, based on the observation that terms of different types have different effects for NED task, two term reweighting approaches are proposed to improve NED accuracy. In the first approach, we propose to adjust term weights dynamically based on previous story clusters and in the second approach, we propose to employ statistics on training data to learn the named entity reweighting model for each class of stories. Experimental results on two Linguistic Data Consortium (LDC) datasets TDT2 and TDT3 show that the proposed model can improve both efficiency and accuracy of NED task significantly, compared to the baseline system and other existing systems.", "references": ["In Topic Detection and Tracking. Event-based Information Organization", "Learning Approaches for Detecting and Tracking News Events", "A Study on Retrospective and On-line Event Detection", "Detections, Bounds, and Timelines: Umass and tdt-3", "On-line New Event Detection Using Single Pass Clustering TITLE2", "Using Contextual Analysis for News Event Detection", "A System for New Event Detection", "Combining Semantic and Syntactic Document Classifiers to Improve First Story Detection", "Topic conditioned Novelty Detection", "Applying Semantic Classes in Event Detection and Tracking", "Simple Semantics in Topic Detection and Tracking", "Text Classification and Named Entities for New Event Detection", "The INQUERY Retrieval System", "Viewing Morphology as An Inference Process", "A Comparative Study on Feature Selection in Text Categorization", "Elements of Information Theory", "The linguistic data consortium", "Boostexter: A Boosting-based System for Text Categorization", "Using Names and Topics for New Event Detection"], "title": "New Event Detection Based on Indexing-tree and Named Entity"}
{"id": "H-21", "keywords": "queri classif;web search;blind relev feedback", "abstract": "We propose a methodology for building a practical robust query classiflcation system that can identify thousands of query classes with reasonable accuracy, while dealing in real- time with the query volume of a commercial web search en- gine. We use a blind feedback technique: given a query, we determine its topic by classifying the web search results retrieved by the query. Motivated by the needs of search ad- vertising, we primarily focus on rare queries, which are the hardest from the point of view of machine learning, yet in ag- gregation account for a considerable fraction of search engine tra-c. Empirical evaluation conflrms that our methodology yields a considerably higher classiflcation accuracy than pre- viously reported. We believe that the proposed methodology will lead to better matching of online ads to rare queries and overall to a better user experience.", "references": ["Automatic web query classification using labeled and unlabeled training data", "Improving automatic query classification via semi-supervised learning", "Pattern Classification and Scene Analysis", "UCLA-Okapi at TREC-2: Query expansion experiments", "Feature generation for text categorization using world knowledge", "Categorizing web queries according to geographical locality", "Centroid-based document classification: Analysis and experimental results", "IR evaluation methods for retrieving highly relevant documents", "The ferrety algorithm for the KDD Cup 2005 problem", "Analyzing the effect of query class on document retrieval performance", "Facing a great challenge", "Improving automatic query expansion", "Search Engine Marketing", "Okapi at TREC-3", "Relevance feedback in information retrieval", "Improving retrieval performance by relevance feedback", "The Statistical Analysis of Discrete Data", "Our winning solution to query classification in KDDCUP 2005", "Query enrichment for web-query classification", "Building bridges for web query classification", "Classifying search engine queries using the web as background knowledge", "Query expansion using lexical-semantic relations", "Improving the effectiveness of information retrieval with local context analysis"], "title": "Robust Classification of Rare Queries Using Web Knowledge"}
{"id": "H-24", "keywords": "queri syntax;advanc search featur;expert search", "abstract": "One way to help all users of commercial Web search engines be more successful in their searches is to better understand what those users with greater search expertise are doing, and use this knowledge to benefit everyone.  In this paper we study the interaction logs of advanced search engine users (and those not so advanced) to better understand how these user groups search.  The results show that there are marked differences in the queries, result clicks, post-query browsing, and search success of users we classify as advanced (based on their use of query operators), relative to those classified as non-advanced.  Our findings have implications for how advanced users should be supported during their searches, and how their interactions could be used to help searchers of all experience levels find more relevant information and learn improved searching strategies.", "references": ["Using terminological feedback for Web search refinement: A log-based study", "Where should the person stop and the information search interface start?", "Helping people find what they don"t know", "Query length in interactive information retrieval", "Domain-specific search strategies for the effective retrieval of healthcare and shopping information", "Using information scent to model user information needs and actions and the Web", "Phrase recognition and expansion for short, precision-biased queries based on a query log", "Coverage, relevance, and ranking: The impact of query operators on Web search engine results", "Query expansion", "Experience with an adaptive indexing scheme", "The vocabulary problem in human-system communication: An analysis and a solution", "Eye-tracking analysis of user behavior in WWW search", "Web search behavior of internet experts and newbies", "An investigation into the use of simple queries on Web IR systems", "Real life, real users, and real needs: A study and analysis of user queries on the Web", "Generating query substitutions", "User models from implicit feedback for proactive information retrieval", "The loquacious user: a document-independent source of terms for query expansion", "Differences between novice and experienced users in searching for information on the World Wide Web", "Information filtering based on user behavior analysis and best match text retrieval", "Information retrieval through man-machine dialogue", "Understanding user goals Improving retrieval performance by relevance feedbackin Web search", "Analysis of a very large web search engine query log", "General intelligence, objectively determined and measured", "Searching heterogeneous collections on the Web: Behavior of Excite users", "From highly relevant to not relevant: examining different regions of relevance", "The perfect search engine is not enough: A study of orienteering behavior in directed search", "Personalizing search via automated analysis of interests and activities", "Mining longitudinal Web queries: Trends and patterns", "Studying the use of popular destinations to enhance Web search interaction", "Investigating behavioral variability in Web search", "Finding relevant documents using top-ranking sentences: An evaluation of two alternative schemes", "Medical students" personal knowledge. Search proficiency, and database use in problem solving"], "title": "Investigating the Querying and Browsing Behavior of Advanced Search Engine Users"}
{"id": "H-25", "keywords": "queri expans;interact retriev", "abstract": "In this paper we study term-based feedback for information re- trieval in the language modeling approach. With term feedback a user directly judges the relevance of individual terms without in- teraction with feedback documents, taking full control of the query expansion process. We propose a cluster-based method for select- ing terms to present to the user for judgment, as well as effective algorithms for constructing refined query language models from user term feedback. Our algorithms are shown to bring significant improvement in retrieval accuracy over a non-feedback baseline, and achieve comparable performance to relevance feedback. They are helpful even when there are no relevant documents in the top.", "references": ["Relevance feedback with too much data", "HARD track overview in TREC 2005 - High Accuracy Retrieval from Documents", "Using terminological feedback for web search refinement: a log-based study", "The paraphrase search assistant: terminological feedback for iterative information seeking", "Automatic query expansion using SMART", "Towards interactive query expansion", "UMass at TREC 2003: HARD and QA", "Hierarchical presentation of expansion terms", "A probabilistic model of information retrieval: development and status", "The loquacious user: a document-independent source of terms for query expansion", "Elicitation of term relevance feedback: an investigation of term source and context", "A case for interaction: A study of interactive information retrieval behavior and effectiveness", "Relevance-based language models", "Evaluation of the real and perceived value of automatic and interactive query expansion", "A Language Modeling Approach to Information Retrieval", "Okapi at TREC-3", "Relevance feedback in information retrieval", "Re-examining the potential effectiveness of interactive query expansion", "Improving retrieval performance by relevance feedback", "Implicit user modeling for personalized search", "Active feedback in ad-hoc information retrieval", "Term relevance feedback and query expansion: relation to design", "Query expansion using local and global document analysis", "Microsoft cambridge at TREC-13: Web and HARD tracks", "Model-based feedback in the language modeling approach to information retrieval", "A cross-collection mixture model for comparative text mining"], "title": "Term Feedback for Information Retrieval with Language Models"}
{"id": "H-26", "keywords": "machin learn for inform retriev;support vector machin;rank", "abstract": "Machine learning is commonly used to improve ranked re- trieval systems. Due to computational diculties, few learn- ing techniques have been developed to directly optimize for mean average precision (MAP), despite its widespread use in evaluating such systems. Existing approaches optimiz- ing MAP either do not find a globally optimal solution, or are computationally expensive. In contrast, we present a general SVM learning algorithm that eciently finds a globally optimal solution to a straightforward relaxation of MAP. We evaluate our approach using the TREC 9 and TREC 10 Web Track corpora (WT10g), comparing against SVMs optimized for accuracy and ROCArea. In most cases we show our method to produce statistically significant im- provements in MAP scores.", "references": ["Automatic combination of multiple ranked retrieval systems", "Learning to rank using gradient descent", "Learning to rank with non-smooth cost functions", "Adapting ranking SVM to document retrieval", "Learning a ranking from pairwise preferences", "Ensemble selection from libraries of models", "The relationship between precision-recall and ROC curves", "Overview of the TREC-9 web track", "Overview of the TREC-2001 web track", "Large margin rank boundaries for ordinal regression", "Optimising area under the ROC curve using gradient descent", "Ir evaluation methods for retrieving highly relevant documents", "A support vector method for multivariate performance measures", "Document language models, query models, and risk minimization for information retrieval", "Support vector machines for classification in nonstandard situations", "A markov random field model for term dependencies", "Combining statistical learning with a knowledge-based approach", "The probability ranking principle in ir. journal of documentation", "Large margin methods for structured and interdependent output variables", "Statistical Learning Theory", "Optimizing classifier performance via approximation to the Wilcoxon-Mann-Witney statistic"], "title": "A Support Vector Method for Optimizing Average Precision"}
{"id": "H-29", "keywords": "queri expans;pseudo-relev feedback", "abstract": "Existing pseudo-relevance feedback methods typically per- form averaging over the top-retrieved documents, but ig- nore an important statistical dimension: the risk or variance associated with either the individual document models, or their combination. Treating the baseline feedback method as a black box, and the output feedback model as a random variable, we estimate a posterior distribution for the feed- back model by resampling a given query's top-retrieved doc- uments, using the posterior mean or mode as the enhanced feedback model. We then perform model combination over several enhanced models, each based on a slightly modified query sampled from the original query. We find that resam- pling documents helps increase individual feedback model precision by removing noise terms, while sampling from the query improves robustness (worst-case performance) by em- phasizing terms related to multiple query aspects. The re- sult is a meta-feedback algorithm that is both more robust and more precise than the original strong baseline method.", "references": ["The Lemur toolkit for language modeling and retrieval", "Query difficulty, robustness, and selective application of query expansion", "A high-performance semi-supervised learning method for text chunking", "Improving retrieval feedback with multiple term-ranking function combination", "Initial results with structured queries and language models on half a terabyte of text", "Pattern Classification", "The role of variance in term weighting for probabilistic information retrieval", "SOMPAK: The self-organizing map program package", "A Generative Theory of Relevance", "Using query-specific variance estimates to combine Bayesian classifiers", "Combining the language model and inference network approaches to retrieval", "Estimating a Dirichlet distribution", "Advances in Information Retrieval, chapter Language models for relevance feedback", "A language modeling approach to information retrieval", "The SMART Retrieval System, chapter Relevance Feedback in Information Retrieval", "Flexible pseudo-relevance feedback via selective sampling", "Regularized estimation of mixture models for robust pseudo-relevance feedback", "Improving the effectiveness of information retrieval with local context analysis", "Learning to estimate query difficulty", "Ranking robustness: a novel framework to predict query performance"], "title": "Estimation and Use of Uncertainty in Pseudo-relevance Feedback"}
{"id": "H-30", "keywords": "inform retriev;queri expans;markov random field", "abstract": "Query expansion, in the form of pseudo-relevance feedback or relevance feedback, is a common technique used to im- prove retrieval effectiveness. Most previous approaches have ignored important issues, such as the role of features and the importance of modeling term dependencies. In this paper, we propose a robust query expansion technique based on the Markov random field model for information retrieval. The technique, called latent concept expansion, provides a mechanism for modeling term dependencies during expan- sion. Furthermore, the use of arbitrary features within the model provides a powerful framework for going beyond sim- ple term occurrence features that are implicitly used by most other expansion techniques. We evaluate our tech- nique against relevance models, a state-of-the-art language modeling query expansion technique. Our model demon- strates consistent and significant improvements in retrieval effectiveness across several TREC data sets. We also de- scribe how our technique can be used to generate mean- ingful multi-term concepts for tasks such as query sugges- tion/reformulation.", "references": ["UMass at TREC 2004: Novelty and HARD", "Shortest-substring retrieval and ranking", "Query expansion using random walk models", "Boolean queries and term dependencies in probabilistic retrieval models", "The use of phrases and structured queries in information retrieval", "NTCIR-5 query expansion experiments using term dependence models", "Automatic phrase indexing for document retrieval: An examination of syntactic and non-syntactic methods", "Dependence language model for information retrieval", "An evaluation of feedback in document retrieval using co-occurrence data", "A support vector method for multivariate performance measures", "Corpus structure, language models, and ad-hoc information retrieval", "Relevance-based language models", "Cluster-based retrieval using language models", "A Markov random field model for term dependencies", "Linear feature based models for information retrieval", "Indri at terabyte track 2005", "Direct maximization of average precision by hill-climbing with a comparison to a maximum entropy approach", "Experiments using the lemur toolkit", "Why bigger windows are better than smaller ones", "Okapi at trec-3", "Relevance Feedback in Information Retrieval", "A general language model for information retrieval", "Indri: A language model-based serach engine for complex queries", "Language model information retrieval with document expansion", "Max-margin markov networks", "A theoretical basis for the use of cooccurrence data in information retrieval", "LDA-based document models for ad-hoc retrieval", "Improving the effectiveness of information retrieval with local context analysis", "Model-based feedback in the language modeling approach to information retrieval"], "title": "Latent Concept Expansion Using Markov Random Fields"}
{"id": "H-31", "keywords": "languag model;poisson process;queri gener;formal model;term depend smooth", "abstract": "Many variants of language models have been proposed for information retrieval. Most existing models are based on multinomial distribution and would score documents based on query likelihood computed based on a query generation probabilistic model. In this paper, we propose and study a new family of query generation models based on Poisson dis- tribution. We show that while in their simplest forms, the new family of models and the existing multinomial models are equivalent, they behave difierently for many smoothing methods. We show that the Poisson model has several ad- vantages over the multinomial model, including naturally ac- commodating per-term smoothing and allowing for more ac- curate background modeling. We present several variants of the new model corresponding to difierent smoothing meth- ods, and evaluate them on four representative TREC test collections. The results show that while their basic mod- els perform comparably, the Poisson model can outperform multinomial model with per-term smoothing. The perfor- mance can be further improved with two-stage smoothing. Categories and Subject Descriptors: H.3.3 (Informa- tion Search and Retrieval): Retrieval Models General Terms: Algorithms", "references": ["Latent dirichlet allocation", "An empirical study of smoothing techniques for language modeling", "Poisson mixtures", "Language Modeling and Information Retrieval", "A formal study of information retrieval heuristics", "Using Language Models for Information Retrieval", "Term-specific smoothing for the language modeling approach to information retrieval: the importance of a query term", "Probabilistic latent semantic indexing", "Distribution of content words and phrases in text and language modelling", "Corpus structure, language models, and ad-hoc information retrieval", "Document language models, query models, and risk minimization for information retrieval", "Probabilistic IR models based on query and document generation", "Probabilistic relevance models based on document and query generation", "Relevance-based language models", "Cluster-based retrieval using language models", "Modelling documents with multiple poisson distributions", "A comparison of event models for naive bayes text classification", "Formal multiple-bernoulli models for language modeling", "A hidden Markov model information retrieval system", "Probability, random variables and stochastic processes", "A language modeling approach to information retrieval", "Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval", "Okapi at TREC-3", "A parallel derivation of probabilistic information retrieval models", "Language model information retrieval with document expansion", "Empirical development of an exponential probabilistic model for text retrieval: using textual analysis to build a better model", "Lda-based document models for ad-hoc retrieval", "A study of smoothing methods for language models applied to ad-hoc information retrieval", "Two-stage language models for information retrieval"], "title": "A Study of Poisson Query Generation Model for Information Retrieval"}
{"id": "H-32", "keywords": "definit question answer;human interest", "abstract": "Current approaches to identifying definitional sentences in the context of Question Answering mainly involve the use of linguistic or syntactic patterns to identify informative nuggets. This is insufficient as they do not address the novelty factor that a definitional nugget must also possess. This paper proposes to address the deficiency by building a "Human Interest Model" from external knowledge. It is hoped that such a model will allow the computation of human interest in the sentence with respect to the topic. We compare and contrast our model with current definitional question answering models to show that interestingness plays an important factor in definitional question answering.", "references": ["A hybrid approach for qa track definitional questions", "The use of MMR, diversity-based reranking for reordering documents and producing summaries", "Reranking answers for definitional qa using language modeling", "Generic soft pattern models for definitional question answering", "Ensemble methods in machine learning", "Employing two question answering systems at trec 2005", "Experiments at the university of edinburgh for the trec 2006 qa track", "Divergence measures based on the shannon entropy", "A menagerie of tracks at maryland: Hard, enterprise, qa, and genomics, oh my!", "Automatically evaluating answers to definition questions", "Using syntactic and semantic relation analysis in question answering", "Overview of the trec 2003 question answering track", "Overview of the trec 2005 question answering track", "TREC 2003 QA at BBN: Answering definitional questions", "A language modeling approach to passage question answering"], "title": "Interesting Nuggets and Their Impact on Definitional Question Answering"}
{"id": "I-1", "keywords": "agent;reactiv and delib architectur;formal model of agenc+agenc formal model", "abstract": "Intelligent agents that are intended to work in dynamic environments must be able to gracefully handle unsuccessful tasks and plans. In addition, such agents should be able to make rational decisions about an appropriate course of action, which may include aborting a task or plan, either as a result of the agent's own deliberations, or potentially at the request of another agent. In this paper we investigate the incorporation of aborts into a BDI-style architecture. We discuss some conditions under which aborting a task or plan is appropriate, and how to determine the consequences of such a decision. We augment each plan with an optional abort-method, analogous to the failure method found in some agent programming languages. We provide an operational semantics for the execution cycle in the presence of aborts in the abstract agent language CAN, which enables us to specify a BDI-based execution model without limiting our attention to a particular agent system (such as JACK, Jadex, Jason, or SPARK). A key technical challenge we address is the presence of parallel execution threads and of sub-tasks, which require the agent to ensure that the abort methods for each plan are carried out in an appropriate sequence.", "references": ["Goal representation for BDI Agent systems", "JACK intelligent agents - components for intelligent agents in Java", "Extending the concept of transaction compensation", "Goal types in agent programming", "Aoex: An agent-based exception handling framework for building reliable, distributed, open software systems", "Programming declarative goals using plan patterns", "The Psi calculus: an algebraic agent language", "Using domain-independent exception handling services to enable robust open multi-agent systems: The case of agent death", "The SPARK agent framework", "Continuous refinement of agent resource estimates", "An intelligent personal assistant for task and time management", "A cognitive framework for delegation to an assistive user agent", "Developing Intelligent Agent Systems: A Practical Guide", "Jadex: A BDI reasoning engine", "AgentSpeak(L): BDI agents speak out in a logical computable language", "An abstract architecture for rational agents", "Hierarchical planning in BDI agent programming languages: a formal approach", "Goals in the context of bdi plan failure and planning", "Detecting and exploiting positive goal interaction in intelligent agents", "Avoiding resource conflicts in intelligent agents", "A framework for goal-based semantic compensation in agent systems", "Semantic-compensation-based recovery management in multi-agent systems", "Declarative and procedural goals in intelligent agent systems"], "title": "Aborting Tasks in BDI Agents"}
{"id": "I-4", "keywords": "negoti chain;flexibl;multi-link negoti", "abstract": "A negotiation chain is formed when multiple related negotiations are spread over multiple agents. In order to appropriately order and structure the negotiations occurring in the chain so as to op- timize the expected utility, we present an extension to a single- agent concurrent negotiation framework. This work is aimed at semi-cooperative multi-agent systems, where each agent has its own goals and works to maximize its local utility; however, the per- formance of each individual agent is tightly related to other agent's cooperation and the system's overall performance. We introduce a pre-negotiation phase that allows agents to transfer meta-level information. Using this information, the agent can build a more accurate model of the negotiation in terms of modeling the rela- tionship of flexibility and success probability. This more accurate model helps the agent in choosing a better negotiation solution in the global negotiation chain context. The agent can also use this in- formation to allocate appropriate time for each negotiation, hence to find a good ordering of all related negotiations. The experimen- tal data shows that these mechanisms improve the agents' and the system's overall performance significantly.", "references": ["Optimal negotiation strategies for agents with incomplete information", "A combinatorial auction for collaborative planning", "Implementing a business process management system using adept: A real-world case study", "A Cooperative Mediation-Based Protocol for Dynamic, Distributed Resource Allocation", "Agent-based formation of virtual organisations", "Issues in automated negotiation and electronic commerce: Extending the contract net framework", "Degree of Local Cooperation and its Implication on Global Utility", "Lateral and Hierarchical Partial Centralization for Distributed Coordination and Scheduling of Complex Hierarchical Task Networks", "Combinatorial auctions for supply chain formation", "Efficient management of multi-linked negotiation based on a formalized model", "Integrative negotiation among agents situated in organizations", "Automatic formation and analysis of multi-agent virtual organization"], "title": "Meta-Level Coordination for Solving Negotiation Chains in Semi-Cooperative Multi-Agent Systems"}
{"id": "I-5", "keywords": "resourc alloc;distribut control;self-organis", "abstract": "Distributed applications require distributed techniques for efficient resource allocation. These techniques need to take into account the heterogeneity and potential unreliability of resources and resource consumers in a distributed environments. In this paper we propose a distributed algorithm that solves the resource allocation problem in distributed multi-agent systems. Our solution is based on the self-organisation of agents, which does not require any facilitator or management layer. The resource allocation in the system is a purely emergent effect. We present results of the proposed resource allocation mechanism in the simulated static and dynamic multi-server environment.", "references": ["Cactus Tools for Grid Applications", "Inductive Reasoning and Bounded Rationality", "Server Load Balancing", "Economic-based Distributed Resource Management and Scheduling for Grid Computing", "Economic Models for Resource Management and Scheduling in Grid Computing", "Architectural Models for Resource Management in the Grid", "A taxonomy of scheduling in general-purpose distributed computing systems", "Emergence of Cooperation and Organization in an Evolutionary Game", "On load balancing for distributed multiagent computing", "Market-based control. A Paradigm for Distributed Resource Allocation", "Capacity Planning of Mobile Agent Systems Designing Efficient Intranet Applications", "Globus: A Metacomputing Infrastructure Toolkit", "Condor-G: A Computation Management Agent for Multi-Institutional Grids", "Resource allocation games with changing resource capacities", "Combining state and model-based approaches for mobile agent load balancing", "Decentralized Adaptive Resource Allocation for Sensor Networks", "An Agent-based Resource Allocation Model for Computational Grids", "Adaptive Load Balancing: A Study in Multi-Agent Learning", "Towards Autonomous Mobile Agents with Emergent Migration Behaviour", "W3C. Web services activity", "Complexity: The Emerging Science at the Edge of Order and Chaos", "Analyzing Market-Based Resource Allocation Strategies for the Computational Grid"], "title": "Towards Self-organising Agent-based Resource Allocation in a Multi-Server Environment"}
{"id": "I-6", "keywords": "agent commun languag;social reason", "abstract": "This paper proposes dynamic semantics for agent commu- nication languages (ACLs) as a method for tackling some of the fundamental problems associated with agent commu- nication in open multiagent systems. Based on the idea of providing alternative semantic "variants" for speech acts and transition rules between them that are contingent on previous agent behaviour, our framework provides an im- proved notion of grounding semantics in ongoing interaction, a simple mechanism for distinguishing between compliant and expected behaviour, and a way to specify sanction and reward mechanisms as part of the ACL itself. We extend a common framework for commitment-based ACL semantics to obtain these properties, discuss desiderata for the design of concrete dynamic semantics together with examples, and analyse their properties.", "references": ["ACL Semantics between Social Commitments and Mental Attitudes", "Expectation-Oriented Analysis and Design", "Communicative actions for artificial agents", "Elements of a Plan-Based Theory of Speech Acts", "Formalising Agent Mediated Electronic Institutions", "Operational specification of a commitment-based agent communication language", "A New Semantics for the FIPA Agent Communication Language based on Social Attitudes", "Denotational Semantics for Agent Communication Languages", "EmpiricalRational Semantics of Agent Communication", "Some Remarks on the Semantics of FIPA"s Agent Communication Language", "Interaction is Meaning: A New Model for Communication in Open Systems", "Dialogue acts are rational plans", "Agent communication languages: Rethinking the principles", "A social semantics for agent communication languages", "A semantics for speech acts", "Specifying the Intertwining of Cooperation and Autonomy in Agent-based Systems", "Verifiable semantics for agent communication languages"], "title": "Dynamic Semantics for Agent Communication Languages"}
{"id": "I-7", "keywords": "multiag system;game theori;commit;extort", "abstract": "Making commitments, e.g., through promises and threats, enables a player to exploit the strengths of his own strategic position as well as the weaknesses of that of his opponents. Which commitments a player can make with credibility depends on the circumstances. In some, a player can only commit to the performance of an ac-tion, in others, he can commit himself conditionally on the actions of the other players. Some situations even allow for commitments on commitments or for commitments to randomized actions. We explore the formal properties of these types of (conditional) com-mitment and their interrelationships. So as to preclude inconsis-tencies among conditional commitments, we assume an order in which the players make their commitments. Central to our analy-ses is the notion of an extortion, which we define, for a given order of the players, as a profile that contains, for each player, an optimal commitment given the commitments of the players that committed earlier. On this basis, we investigate for different commitment types whether it is advantageous to commit earlier rather than later, and how the outcomes obtained through extortions relate to backward induction and Pareto efficiency.", "references": ["Contextualizing commitment protocols", "Computing the optimal strategy to commit to", "A simplified bargaining model for the n-person cooperative game", "Games and Decisions: Introduction and Critical Survey", "Two-person cooperative games", "A Course in Game Theory", "How to commit to cooperation", "Leveled-commitment contracting. A backtracking instrument for multiagent systems", "The Strategy of Conflict", "Spieltheoretische Behandlung eines Oligopolmodells mit Nachfragetragheit", "An ontology for commitments in multiagent systems: Toward a unification of normative concepts", "Program equilibrium", "Commitment robust equilibria and endogenous timing", "The Theory of Games and Economic Behavior", "Marktform und Gleichgewicht", "Leadership with commitment to mixed strategies"], "title": "Commitment and Extortion"}
{"id": "I-9", "keywords": "agent commun languag and protocol;logic and formal model of agenc and multi-agent system", "abstract": "Interactions between agents in an open system such as the Internet require a significant degree of flexibility. A crucial aspect of the development of such methods is the notion of commitments, which provides a mechanism for coordinating interactive behaviors among agents. In this paper, we investigate an approach to model commitments with tight integration with protocol actions. This means that there is no need to have an explicit mapping from protocols actions to operations on commitments and an external mechanism to process and enforce commitments. We show how agents can reason about commitments and protocol actions to achieve the end results of protocols using a reasoning system based on temporal linear logic, which incorporates both temporal and resourcesensitive reasoning. We also discuss the application of this framework to scenarios such as online commerce.", "references": ["Contextualizing commitment protocol", "Temporal and modal logic", "Theoretical Computer Science", "Communication and Cooperation in Agent Systems: a pragmatic theory", "Agent negotiation as proof search in linear logic", "Temporal Linear Logic and Its Applications", "Commitments and conventions: The foundation of coordination in multi-agent systems", "Linear logic, partial deduction and cooperative problem solving", "Temporal linear logic for symbolic agent negotiation", "Verifying compliance with commitment protocols", "Commitment machines", "Flexible protocol specification and execution: applying event calculus planning using commitments"], "title": "Temporal Linear Logic as a Basis for Flexible Agent Interactions"}
{"id": "I-10", "keywords": "multi-agent learn", "abstract": "This article deals with the problem of collaborative learning in a multi-agent system. Here each agent can update incrementally its beliefs B (the concept representation) so that it is in a way kept consistent with the whole set of information K (the examples) that he has received from the environment or other agents. We extend this notion of consistency (or soundness) to the whole MAS and discuss how to obtain that, at any moment, a same consistent concept representation is present in each agent. The corresponding protocol is applied to supervised concept learning. The resulting method SMILE (standing for Sound Multiagent Incremental LEarning) is described and experimented here. Surprisingly some difficult boolean formulas are better learned, given the same learning set, by a Multi agent system than by a single agent. oui", "references": ["When agents communicate hypotheses in critical situations", "Fast effective rule induction", "UCI repository of machine learning databases", "Lookahead-based algorithms for anytime induction of decision trees", "A pathology of bottom-up hill-climbing in inductive rule learning", "Learning in BDI multi-agent systems", "Mgi: an incremental bottom-up algorithm", "A comparison of prediction accuracy, complexity, and training time of thirty-three old and new classification algorithms", "Incremental learning with partial instance memory", "Collaborative multiagent learning for classification tasks", "Recycling data for multi-agent learning", "Induction of decision trees", "Towards tight bounds for rule learning", "Mutual online concept learning for multiple agents", "Adaption and Learning in Multi-Agent Systems", "Data Mining: Practical Machine Learning Tools and Techniques with Java Implementations"], "title": "SMILE: Sound Multi-agent Incremental LEarning"}
{"id": "I-11", "keywords": "plan recognit;plan infer;evolut;predict;emot;bdi;swarm intellig;digit pheromon;dynam", "abstract": "Reasoning about agents that we observe in the world is challenging. Our available information is often limited to observations of the agent's external behavior in the past and present. To understand these actions, we need to deduce the agent's internal state, which includes not only rational elements (such as intentions and plans), but also emotive ones (such as fear). In addition, we often want to predict the agent's future actions, which are constrained not only by these inward characteristics, but also by the dynamics of the agent's interaction with its environment. BEE (Behavior Evolution and Extrapolation) uses a faster-than-real-time agent-based model of the environment to characterize agents' internal state by evolution against observed behavior, and then predict their future behavior, taking into account the dynamics of their interaction with the environment.", "references": ["A maximization technique occurring in the statistical analysis of probabilistic functions of Markov chains", "Return from the Ant: Synthetic Ecosystems for Manufacturing Control", "Techniques for Plan Recognition", "Influences and Reactions: a Model of Situated Multiagent Systems", "Belief Desire Intention Agent Architectures", "Artificial War: Multiagent-based Simulation of Combat", "Nonlinear Time Series Analysis", "Real-Time Adversarial Intelligence & Decision Making (RAID)", "Map-Aware Non-uniform Automata (MANA)-A New Zealand Approach to Scenario Modelling", "mthodologie et outils pour la modlisation et la simulation de systmes multi-agents", "The cognitive structure of emotions", "Representing Dispositions and Emotions in Simulated Combat", "Ant-Like Missionaries and Cannibals: Synthetic Pheromones for Distributed Motion Control", "Modeling Uncertain Domains with Polyagents", "A Design Taxonomy of Multi-Agent Interactions", "Characterizing and Predicting Agents via Multi-Agent Evolution", "Digital Pheromone Mechanisms for Coordination of Unmanned Vehicles", "A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition", "Modeling Rational Agents within a BDI Architecture", "Evolving Adaptive Pheromone Path Planning Mechanisms", "Linguistic Geometry: From Search to Construction"], "title": "Real-Time Agent Characterization and Prediction"}
{"id": "I-12", "keywords": "adjust autonomi;interrupt manag", "abstract": "This paper investigates the problem of estimating the value of probabilistic parameters needed for decision making in environments in which an agent, operating within a multi-agent system, has no a priori information about the structure of the distribution of parameter values. The agent must be able to produce estimations even when it may have made only a small number of direct observations, and thus it must be able to operate with sparse data. The paper describes a mechanism that enables the agent to significantly improve its estimation by augmenting its direct observations with those obtained by other agents with which it is coordinating. To avoid undesirable bias in relatively heterogeneous environments while effectively using relevant data to improve its estimations, the mechanism weighs the contributions of other agents' observations based on a real-time estimation of the level of similarity between each of these agents and itself. The "coordination autonomy" module of a coordination-manager system provided an empirical setting for evaluation. Simulation-based evaluations demonstrated that the proposed mechanism outperforms estimations based exclusively on an agent's own observations as well as estimations based on an unweighted aggregate of all other agents' observations. Engineering and Applied Sciences", "references": ["A method, system, and tools for intelligent interruption management", "Survey of clustering data mining techniques", "Incremental clustering for mining in a data warehousing environment", "A density-based algorithm for discovering clusters in large spatial databases with noise", "A decision procedure for autonomous agents to reason about interaction with humans", "Bayesian network classifiers", "Combining collaborative filtering with personal agents for better recommendations", "Sensing techniques for mobile interaction", "Models of attention in computing and communication: from principles to applications", "Who"s asking for help?: a bayesian approach to intelligent assistance", "Neuro-Fuzzy and Soft Computing A Computational Approach to Learning and Machine Intelligence", "On information and sufficiency", "Gaze and speech in attentive user interfaces", "On a test of whether one of 2 random variables is stochastically larger than the other", "Technology and command: Implications for military operations in the twenty-first century", "Handbook of Game Theory with Economic Applications", "Machine learning methods for predicting failures in hard drives: A multiple-instance application", "Estimating information value in collaborative multi-agent planning systems", "Timing interruptions for better human-computer coordinated planning", "The GAZE groupware system: Mediating joint attention in multiparty communication and collaboration", "An application view of coordinators: Coordination managers for first responders", "Individual comparisons by ranking methods", "Bayesian learning in negotiation", "A distributed intelligent agent architecture for simulating aggregate-level behavior and interactions on the battlefield"], "title": "Sharing Experiences to Learn User Characteristics in Dynamic Environments with Sparse Data"}
{"id": "I-14", "keywords": "peer-to-peer inform retriev;multi-agent learn;distribut search control", "abstract": "The dominant existing routing strategies employed in peer- to-peer(P2P) based information retrieval(IR) systems are similarity-based approaches. In these approaches, agents depend on the content similarity between incoming queries and their direct neighboring agents to direct the distributed search sessions. However, such a heuristic is myopic in that the neighboring agents may not be connected to more rele- vant agents. In this paper, an online reinforcement-learning based approach is developed to take advantage of the dy- namic run-time characteristics of P2P IR systems as repre- sented by information about past search sessions. Speci- cally, agents maintain estimates on the downstream agents' abilities to provide relevant documents for incoming queries. These estimates are updated gradually by learning from the feedback information returned from previous search sessions. Based on this information, the agents derive corresponding routing policies. Thereafter, these agents route the queries based on the learned policies and update the estimates based on the new routing policies. Experimental results demon- strate that the learning algorithm improves considerably the routing performance on two test collection sets that have been used in a variety of distributed IR studies. Categories and Subject Descriptors", "references": ["Learning the task allocation game", "Packet routing in dynamically changing networks: A reinforcement learning approach", "Comparing the performance of database selection algorithms", "Farm: A scalable environment for multi-agent development and evaluation", "A distributed reinforcement learning scheme for network routing", "Federated search of text-based digital libraries in hierarchical peer-to-peer networks", "User modeling for full-text federated search in peer-to-peer networks", "Foundations of Statistical Natural Language Processing", "Generating network topologies that obey power laws", "Efficient content location using interest-based locality in peer-topeer systems", "Ants and reinforcement learning: A case study in routing in dynamic networks", "A multi-agent, policy gradient approach to network routing", "A multi-agent approach for peer-to-peer information retrieval", "Multi-agent based peer-to-peer information retrieval systems with concurrent search sessions", "A dynamically formed hierarchical agent organization for a distributed content sharing system"], "title": "A Reinforcement Learning based Distributed Search Algorithm For Hierarchical Peer-to-Peer Information Retrieval Systems"}
{"id": "I-15", "keywords": "artifici social system;perform;scalabl;robust;depend", "abstract": "Finding the right agents in a large and,dynamic,network,to provide the needed resources in a timely fashion, is a long standing problem. This paper presents a method for information searching and sharing that combines,routing indiceswith,token- based methods. The proposed,method,enables agents to search effectively by acquiring their neighbors interests, advertising their information provision abilities and maintaining indices for routing queries, in an integrated way. Specifically, the paper demonstrates,through,performance,experiments,how,static and dynamic,networks,of agents can be tuned to answer queries effectively as,they,gather,evidence,for,the,interests and information provision abilities of others, without altering the topology,or imposing,an overlay structure to the network,of acquaintances.", "references": ["self-supervising peer-to-peer search networks", "Routing indices for peer-topeer systems", "Decentralized Control of Cooperative Systems: Categorization and Complexity Analysis", "Optimizing Information Exchange in Cooperative Multi-agent Systems", "Peer Selection in Peer-to-Peer Networks with Semantic Topologies", "A Semantics-Based Bibliographic Peer-to-Peer System", "An approach for measuring semantic similarity between words using semantic multiple information sources", "Semantic Social Overlay Networks", "Loser, Super-Peer-Based Routing and Clustering Strategies for RDF-Based Peer-to-Peer Networks", "REMINDIN': Semantic Query Routing in Peer-to-Peer Networks Based on Social Metaphors", "A POMDP Approach to Token-Based Team Coordination", "Information Sharing in Large Scale Teams", "Towards Flexible Coordination of Large Scale MultiAgent Systems", "An Integrated Token Based Algorithm for Scalable Coordination", "Communication Decisions in Multi-agent Cooperation: Model and Experiments", "Searching Social Networks", "A Decision Theoretic Approach for Designing Proactive Communication in Multi-Agent Teamwork", "A MultiAgent Approach for Peer-to-Peer-based Information Retrieval Systems", "Multi-Agent Based Peer-to-Peer Information Retrieval Systems with Concurrent Search Sessions"], "title": "Information Searching and Sharing in Large-Scale Dynamic Networks"}
{"id": "I-16", "keywords": "bid agent;auction;public displai", "abstract": "In this paper we present an advanced bidding agent that partici- pates in first-price sealed bid auctions to allocate advertising space on BluScreen - an experimental public advertisement system that detects users through the presence of their Bluetooth enabled de- vices. Our bidding agent is able to build probabilistic models of both the behaviour of users who view the adverts, and the auctions that it participates within. It then uses these models to maximise the exposure that its adverts receive. We evaluate the effectiveness of this bidding agent through simulation against a range of alternative selection mechanisms including a simple bidding strategy, random allocation, and a centralised optimal allocation with perfect fore- sight. Our bidding agent significantly outperforms both the simple bidding strategy and the random allocation, and in a mixed popula- tion of agents it is able to expose its adverts to 25% more users than the simple bidding strategy. Moreover, its performance is within 7.5% of that of the centralised optimal allocation despite the highly uncertain environment in which it must operate.", "references": ["Efficient scheduling of internet banner advertisements", "Market-based recommendation: Agents that compete for consumer attention", "Exploring bluetooth based mobile phone interaction with the hermes photo display", "Price formation in double auctions", "Fundamentals of Queueing Theory", "Location systems for ubiquitous computing", "Unicast, outcast & groupcast: Three steps toward ubiquitous, peripheral displays", "Auction mechanisms for efficient advertisment selection on public display", "Advertising in a pervasive computing environment", "Why are vickrey auctions rare?", "The active badge location system"], "title": "An Advanced Bidding Agent for Advertisement Selection on Public Displays"}
{"id": "I-18", "keywords": "multiag system;cooper distribut problem solv;task and resourc alloc;coordin;cooper and teamwork", "abstract": "The paper deals with on-board planning for a satellite swarm via communication and negotiation. We aim at defining individual behaviours that result in a global behaviour that meets the mission requirements. We will present the formalization of the problem, a communication protocol, a solving method based on reactive decision rules, and first results.", "references": ["Communication in reactive multiagent robotic systems", "JADE - a FIPA-compliant agent framework", "Fast planning through planning graph analysis", "Constraint-based layered planning and distributed control for an autonomous spacecraft formation flying", "Automatic planning for autonomous spacecraft constellation", "A robust layered control system for a mobile robot", "Nonmonotonic commitment machines", "Contextualizing commitment protocols", "Continual coordination through shared activites", "Efficient mechanisms for multiagent plan merging", "Use of swarm intelligence in spacecraft constellations for the resource exploration of the asteroid belt", "An Earth watching satellite constellation : How to manage a team of watching agents with limited communications", "Multi-agent planning and scheduling environment for enhanced spacecraft autonomy", "Incremental contingency planning", "Autonomous agents with norms", "Scaling up agent coordination strategies", "HTN planning : Complexity and expressivity", "Fuego : a dedicated constellation of small satellites to detect and monitor forest fires", "A formal analysis and taxonomy of task allocation in multi-robot systems", "Multi-level planning for spacecraft autonomy", "Efficient epidemic-style protocols for reliable and scalable multicast", "Representing conversations for scalable overhearing", "A gossip protocol for subgroup multicast", "On augumentation-based negotiation", "A semi-formal specification language dedicated to interaction protocols", "LOTTO: group formation by overhearing in large teams", "Systematic nonlinear planning", "Optimal limited contingency planning", "Bumping strategies for the multiagent agreement problem", "Agent-based control of multiple satellite formation flying", "Extending UML for agents", "On spreading a rumor", "Autonomy requirement and technologies for future constellation", "Contract types for satisficing task allocation", "Multiple agent-based autonomy for satellite constellation", "Methods for task allocation via agent coalition formation", "ObjectAgent for robust autonomous control", "Autonomy for constellations", "Plan repair as an extension of planning", "Cooperation without deliberation : A minimal behavior-based approach to multi-robot teams", "Satellite cluster command and control"], "title": "Collaboration Among a Satellite Swarm"}
{"id": "I-19", "keywords": "vickrei auction;simultan auction;market effici", "abstract": "We derive optimal bidding strategies for a global bidding agent that participates in multiple, simultaneous second-price auctions with perfect substitutes. We first consider a model where all other bidders are local and participate in a single auction. For this case, we prove that, assuming free dis- posal, the global bidder should always place non-zero bids in all available auctions, irrespective of the local bidders' valua- tion distribution. Furthermore, for non-decreasing valuation distributions, we prove that the problem of finding the opti- mal bids reduces to two dimensions. These results hold both in the case where the number of local bidders is known and when this number is determined by a Poisson distribution. This analysis extends to online markets where, typically, auc- tions occur both concurrently and sequentially. In addition, by combining analytical and simulation results, we demon- strate that similar results hold in the case of several global bidders, provided that the market consists of both global and local bidders. Finally, we address the efficiency of the overall market, and show that information about the number of lo- cal bidders is an important determinant for the way in which a global bidder affects efficiency.", "references": ["Strategic bidding for multiple units in simultaneous and sequential auctions", "Combinatorial Auctions", "An example of a multiobject auction game", "The Theory of Learning in Games", "Bid determination in simultaneous auctions: A case study", "Auction Theory", "Simultaneous auctions with synergies", "The contractor"s game", "Simultaneous auctions with synergies and common values", "Last-minute bidding and the rules for ending second-price auctions: Evidence from ebay and amazon auctions on the internet", "Optimal bidding in multiple concurrent auctions", "Three-object two-bidder simultaeous auctions:chopsticks and tetrahedra", "Heuristic bidding strategies for multiple heterogeneous auctions"], "title": "Bidding Optimally in Concurrent Second-Price Auctions of Perfectly Substitutable Goods"}
{"id": "I-20", "keywords": "comput complex;vote;power index", "abstract": "Preference aggregation is used in a variety of multiagent applica- tions, and as a result, voting theory has become an important topic in multiagent system research. However, power indices (which reflect how much "real power" a voter has in a weighted voting system) have received relatively little attention, although they have long been studied in political science and economics. The Banzhaf power index is one of the most popular; it is also well-defined for any simple coalitional game. In this paper, we examine the computational complexity of cal- culating the Banzhaf power index within a particular multiagent do- main, a network flow game. Agents control the edges of a graph; a coalition wins if it can send a flow of a given size from a source ver- tex to a target vertex. The relative power of each edge/agent reflects its significance in enabling such a flow, and in real-world networks could be used, for example, to allocate resources for maintaining parts of the network. We show that calculating the Banzhaf power index of each agent in this network flow domain is #P-complete. We also show that for some restricted network flow domains there exists a polynomial algorithm to calculate agents' Banzhaf power indices.", "references": ["Weighted voting doesn"t work: a mathematical analysis", "On the complexity of cooperative solution concepts", "Mathematical properties of the Banzhaf power index", "The Clarke Tax as a consensus mechanism among automated agents", "A heuristic technique for multiagent planning", "Voting for movies: the anatomy of a recommender system", "An automated meeting scheduling system that utilizes user preferences", "Anyone but him: The complexity of precluding an alternative", "On totally balanced games and games of flow", "Generalized network problems yielding totally balanced games", "On the choice of a power index", "A survey of algorithms for calculating power indices of weighted majority games", "NP-completeness for calculating power indices of weighted majority games", "Algorithmic mechanism design", "Junta distributions and the average-case complexity of manipulating elections", "Deals among rational agents", "Issues in automated negotiation and electronic commerce: Extending the contract net framework", "A value for n-person games", "A method for evaluating the distribution of power in a committee system", "Homogeneity, independence and power indices", "On the axiomatic The Sixth Intl"], "title": "Computing the Banzhaf Power Index in Network Flow Games"}
{"id": "I-21", "keywords": "social network;cognit model;artifici social system", "abstract": "We investigate a framework where agents search for satis- fying products by using referrals from other agents. Our model of a mechanism for transmitting word-of-mouth and the resulting behavioural effects is based on integrating a module governing the local behaviour of agents with a mod- ule governing the structure and function of the underlying network of agents. Local behaviour incorporates a satis- ficing model of choice, a set of rules governing the interac- tions between agents, including learning about the trustwor- thiness of other agents over time, and external constraints on behaviour that may be imposed by market barriers or switching costs. Local behaviour takes place on a network substrate across which agents exchange positive and neg- ative information about products. We use various degree distributions dictating the extent of connectivity, and incor- porate both small-world effects and the notion of preferential attachment in our network models. We compare the effec- tiveness of referral systems over various network structures for easy and hard choice tasks, and evaluate how this effec- tiveness changes with the imposition of market barriers.", "references": ["Customer satisfaction and word-of-mouth", "Emergence of scaling in random networks", "Social ties and word-of-mouth referral behaviour", "Spreading the word: investigating positive word-of-mouth intentions and behaviours in a retailing context", "Consumer switching costs: A typology, antecedents, and consequences", "Effect of referrals on convergence to satisficing distributions", "Influences on consumer use of word-of-mouth recommendation sources", "Word-of-mouth communication and social learning", "Belief, Attitude, Intention, and Behaviour: An Introduction to the Theory and Research", "An investigation into the social context of early adoption behaviour", "Customer switching behaviour in service industries: an exploratory study", "Markets with consumer switching costs", "Recommending collaboration with social networks: a comparative evaluation", "A cognitive model of the antecedents and consequences of satisfaction decisions", "Administrative Behaviour", "Improving user satisfaction in agent-based electronic marketplaces by reputation modelling and adjustable product quality", "A New Model for Predicting Behavioural Intentions: An Alternative to Fishbein", "Networks, dynamics, and the small world phenomenon"], "title": "Interactions between Market Barriers and Communication Networks in Marketing Systems"}
{"id": "I-22", "keywords": "cognit model;human-center teamwork;share belief map;multi-parti commun", "abstract": "Human team members often develop shared expectations to predict each other's needs and coordinate their behaviors. In this paper the concept \Shared Belief Map" is proposed as a basis for developing realistic shared expectations among a team of Human-Agent-Pairs (HAPs). The establishment of shared belief maps relies on inter-agent information shar- ing, the efiectiveness of which highly depends on agents' pro- cessing loads and the instantaneous cognitive loads of their human partners. We investigate HMM-based cognitive load models to facilitate team members to \share the right infor- mation with the right party at the right time". The shared belief map concept and the cognitive/processing load models have been implemented in a cognitive agent architecture| SMMall. A series of experiments were conducted to evaluate the concept, the models, and their impacts on the evolving of shared mental models of HAP teams.", "references": ["Working memory", "What we can learn about human-agent teamwork from practice", "Teamwork", "RPD-enabled agents teaming with humans for multi-context decision making", "Collaborative plans for complex group actions", "Team mental model: Construct or metaphor?", "The limited capacity model of mediated message processing", "Alternative information processing models and their implications for theory, research, and practice", "The magical number seven, plus or minus two: some limits on our capacity for processing information", "Folk psychology for human modelling: Extending the BDI paradigm", "The efficiency of instructional conditions: an approach to combine mental-effort and performance measures", "Cognitive load measurement as a means to advance cognitive load theory", "A tutorial on hidden markov models and selected applications in speech recognition", "The role of mental models in team performance in complex systems", "Cognitive load during problem solving: effects on learning", "Prediction of mental workload in single and multiple task environments"], "title": "Realistic Cognitive Load Modeling for Enhancing Shared Mental Models in Human-Agent Collaboration"}
{"id": "I-26", "keywords": "two-side search;match", "abstract": "This paper presents a two-sided economic search model in which agents are searching for beneficial pairwise partnerships. In each search stage, each of the agents is randomly matched with several other agents in parallel, and makes a decision whether to accept a potential partnership with one of them. The distinguishing feature of the proposed model is that the agents are not restricted to maintaining a synchronized (instantaneous) decision protocol and can sequentially accept and reject partnerships within the same search stage. We analyze the dynamics which drive the agents' strategies towards a stable equilibrium in the new model and show that the proposed search strategy weakly dominates the one currently in use for the two-sided parallel economic search model. By identifying several unique characteristics of the equilibrium we manage to efficiently bound the strategy space that needs to be explored by the agents and propose an efficient means for extracting the distributed equilibrium strategies in common environments.", "references": ["Reducing buyer search costs: Implications for electronic marketplaces", "A theory of marriage", "Non-cooperative models of bargaining", "Sequential formation of coalitions in games with externalities and fixed payoff division", "Two-sided search with nontransferable utility", "Middle-agents for the internet", "A compound strategy for search in the labor market", "College admissions and the stability of marriage", "Sharedplans in electronic commerce", "Efficiency and voluntary implementation in markets with repeated pairwise bargaining", "Shopbot economics", "Agent-mediated trading: Intelligent agents and e-business", "Coalition formation with uncertain heterogeneous information", "Coalition formation for large scale electronic markets", "The economics of job search: A survey", "Integrating parallel interactions into cooperative search", "Handbook of Game Theory with Economic Applications", "The job search problem as an employer-candidate game", "Search and optimal sample size", "Perfect equilibrium in a bargaining model", "Agents strategies for the dual parallel search in partnership formation applications", "Assortative matching and search", "Larks: Dynamic matchmaking among heterogeneous software agents in cyberspace", "Customer coalitions in electronic markets"], "title": "Sequential Decision Making in Parallel Two-Sided Economic Search"}
{"id": "I-29", "keywords": "multi-agent schedul;agent architectur", "abstract": "We consider the problem of managing schedules in an un- certain, distributed environment. We assume a team of col- laborative agents, each responsible for executing a portion of a globally pre-established schedule, but none possessing a global view of either the problem or solution. The goal is to maximize the joint quality obtained from the activities executed by all agents, given that, during execution, unex- pected events will force changes to some prescribed activi- ties and reduce the utility of executing others. We describe an agent architecture for solving this problem that couples two basic mechanisms: (1) a "flexible times" representation of the agent's schedule (using a Simple Temporal Network) and (2) an incremental rescheduling procedure. The former hedges against temporal uncertainty by allowing execution to proceed from a set of feasible solutions, and the latter acts to revise the agent's schedule when execution is forced out- side of this set of solutions or when execution events reduce the expected value of this feasible solution set. Basic coordi- nation with other agents is achieved simply by communicat- ing schedule changes to those agents with inter-dependent activities. Then, as time permits, the core local problem solving infra-structure is used to drive an inter-agent option generation and query process, aimed at identifying opportu- nities for solution improvement through joint change. Using a simulator to model the environment, we compare the per- formance of our multi-agent system with that of an expected optimal (but non-scalable) centralized MDP solver.", "references": ["teams language specification v. 1.06", "Gaining efficiency and flexibility in the simple temporal problem", "Temporal constraint networks", "TEAMS: A framework for environment centered analysis & design of coordination mechanisms", "Designing a family of coordination algorithms", "Design-To-Time Real-Time Scheduling", "Algorithms for a temporal decoupling problem in multi-agent planning", "Interleaving temporal planning and execution in robotics domains", "Remote agent: To boldly go where no AI system has gone before", "On-line planning and scheduling of high-speed manufacturing", "Enabling fast flexible planning through incremental temporal reasoning with conflict extraction", "Slack-based heuristics for constraint satisfaction scheduling", "Criteria-directed heuristic task scheduling"], "title": "Distributed Management of Flexible Times Schedules"}
{"id": "I-30", "keywords": "task alloc;social network;agent;resourc;comput complex", "abstract": "This paper proposes a new variant of the task allocation problem, where the agents are connected in a social network and tasks arrive at the agents distributed over the network. We show that the complexity of this problem remains NP- hard. Moreover, it is not approximable within some factor. We develop an algorithm based on the contract-net proto- col. Our algorithm is completely distributed, and it assumes that agents have only local knowledge about tasks and re- sources. We conduct a set of experiments to evaluate the performance and scalability of the proposed algorithm in terms of solution quality and computation time. Three dif- ferent types of networks, namely small-world, random and scale-free networks, are used to represent various social rela- tionships among agents in realistic applications. The results demonstrate that our algorithm works well and that it scales well to large-scale applications.", "references": ["Modeling Task Allocation Using a Decision Theoretic Model", "Derandomized Graph Products", "Emergence of scaling in random networks", "The Nature of the Firm", "My Evolution as an Economist", "Supply Chain Formation in Open, Market-Based Multi-Agent Systems", "Brain Meets Brawn: Why Grid and Agents Need Each Other", "Agent-organized networks for dynamic team formation", "An Efficient Implementation of a Scaling Minimum-Cost Flow Algorithm", "Does Familiarity Breed Trust? The Implications of Repeated Ties for Contractual Choice in Alliances", "Agent-based Computational Transaction Cost Economics", "Coalition formation with uncertain heterogeneous information", "Coalition formation for large-scale electronic markets", "Forming Efficient Agent Groups for Completing Complex Tasks", "Agent-Based Virtual Organizations for the Grid", "A scalable, distributed algorithm for efficient task allocation", "A scalable agent location mechanism", "Methods for Task Allocation via Agent Coalition Formation", "Agent-based service selection", "Maximal Clique Based Distributed Coalition Formation for Task Allocation in Large-Scale Multi-Agent Systems", "Modeling Supply Chain Formation in Multiagent Systems", "Collective dynamics of small world networks"], "title": "Distributed Task Allocation in Social Networks"}
{"id": "I-31", "keywords": "judgment aggreg;prefer aggreg;modal logic", "abstract": "Agents that must reach agreements with other agents need to rea- son about how their preferences, judgments, and beliefs might be aggregated with those of others by the social choice mechanisms that govern their interactions. The recently emerging field of judg- ment aggregation studies aggregation from a logical perspective, and considers how multiple sets of logical formulae can be ag- gregated to a single consistent set. As a special case, judgment aggregation can be seen to subsume classical preference aggrega- tion. We present a modal logic that is intended to support reasoning about judgment aggregation scenarios (and hence, as a special case, about preference aggregation): the logical language is interpreted directly in judgment aggregation rules. We present a sound and complete axiomatisation of such rules. We show that the logic can express aggregation rules such as majority voting; rule properties such as independence; and results such as the discursive paradox, Arrow's theorem and Condorcet's paradox - which are derivable as formal theorems of the logic. The logic is parameterised in such a way that it can be used as a general framework for comparing the logical properties of di erent types of aggregation - including classical preference aggregation.", "references": ["Social Choice and Individual Values", "Handbook of Social Choice and Welfare", "Modal Logic", "Model Checking", "Arrow"s theorem in judgment aggregation", "Logical representation of preferences for group decision making", "From preference representation to combinatorial vote", "Axiomatizing collective judgment sets in a minimal logical language", "A crash course in arrow logic"], "title": "Reasoning about Judgment and Preference Aggregation"}
{"id": "I-32", "keywords": "agent;multiag system;modal logic", "abstract": "Multiagent environments are often not cooperative nor col- laborative; in many cases, agents have conflicting interests, leading to adversarial interactions. This paper presents a formal Adversarial Environment model for bounded ratio- nal agents operating in a zero-sum environment. In such environments, attempts to use classical utility-based search methods can raise a variety of difficulties (e.g., implicitly modeling the opponent as an omniscient utility maximizer, rather than leveraging a more nuanced, explicit opponent model). We define an Adversarial Environment by describing the mental states of an agent in such an environment. We then present behavioral axioms that are intended to serve as de- sign principles for building such adversarial agents. We ex- plore the application of our approach by analyzing log files of completed Connect-Four games, and present an empirical analysis of the axioms' appropriateness.", "references": ["A knowledge-based approach of Connect-Four - the game is solved: White wins", "Incorporating opponent models into adversary search", "Opponent modeling in multi-agent systems", "Collaborative plans for complex group action", "Supporting collaborative activity", "Designing and building a negotiating automated agent", "On acting together", "Learning and exploiting relative weaknesses of opponent agents", "Reasoning about knowledge", "Adversarial problem solving: Modeling an oponent using explanatory coherence", "An Introduction to Group Work Practice", "An adversarial planning approach to Go"], "title": "An adversarial environment model for bounded rational agents in zero-sum interactions"}
{"id": "I-33", "keywords": "institut;norm;logic;organiz structur", "abstract": "Up to now, the way institutions and organizations have been used in the development of open systems has not often gone further than a useful heuristics. In order to develop systems actually implementing institutions and organizations, formal methods should take the place of heuristic ones. The paper presents a formal semantics for the notion of institution and its components (abstract and concrete norms, empowerment of agents, roles) and defines a formal relation between institutions and organizational structures. As a result, it is shown how institutional norms can be refined to constructs---organizational structures---which are closer to an implemented system. It is also shown how such a refinement process can be fully formalized and it is therefore amenable to rigorous verification.", "references": ["In Ontologia sociale potere deontico e regole costitutive", "Pushing the EL envelope", "The Description Logic Handbook", "The micro-macro constitution of power", "Dynamic logic", "ISLANDER: an electronic institutions editor", "Ameli: An agent-based middleware for electronic institutions", "An organizational view of distributed systems", "A semantics for abstraction", "Ontological aspects of the implementation of norms in agent-based electronic institutions", "Structural evaluation of agent organizations", "Context in categorization", "Classificatory aspects of counts-as: An analysis in modal logic", "Moise+: Towards a structural functional and deontic model for mas organization", "On the characterization of law and computer systems: The normative systems perspective", "Contextual deontic logics", "On programming karo agents", "Institutions, Institutional Change and Economic Performance", "Coordination artifacts: Environment-based coordination for intelligent agents", "Action theory and social science", "De Jure Naturae et Gentium", "Modeling rational agents within a BDI-architecture", "A basic classification of legal institutions", "Tractable reasoning via approximation", "The Construction of Social Reality", "The role of Norms and Electronic Institutions in Multi-Agent Systems"], "title": "A Formal Road from Institutional Norms to Organizational Structures"}
{"id": "I-34", "keywords": "artifici social system", "abstract": "Norm-governed virtual organizations dene, govern and facilitate coordinated resource sharing and problem solving in societies of agents. With an explicit account of norms, openness in virtual or- ganizations can be achieved: new components, designed by vari- ous parties, can be seamlessly accommodated. We focus on vir- tual organizations realised as multi-agent systems, in which human and software agents interact to achieve individual and global goals. However, any realistic account of norms should address their dy- namic nature: norms will change as agents interact with each other and their environment. Due to the changing nature of norms or due to norms stemming from different virtual organizations, there will be situations when an action is simultaneously permitted and pro- hibited, that is, a conict arises. Likewise, there will be situations when an action is both obliged and prohibited, that is, an inconsis- tency arises. We introduce an approach, based on rst-order uni- cation, to detect and resolve such conicts and inconsistencies. In our proposed solution, we annotate a norm with the set of val- ues their variables should not have in order to avoid a conict or an inconsistency with another norm. Our approach neatly accom- modates the domain-dependent interrelations among actions and the indirect conicts/inconsistencies these may cause. More gener- ally, we can capture a useful notion of inter-agent (and inter-role) delegation of actions and norms associated to them, and use it to address conicts/inconsistencies caused by action delegation. We illustrate our approach with an e-Science example in which agents support Grid services.", "references": ["First-Order Logic and Automated Theorem Proving", "Understanding the Functions of Norms in Social Groups through Simulation", "Norm Refinement - Informing the ReNegotiation of Contracts", "On the Design and Construction of Agent-mediated Electronic Institutions", "Executable Specification of Open Norm-Governed Computational Systems", "A role based model for the normative speci cation of organized collective agency and agents interaction", "On the Formal Analysis of Normative Conflicts", "A rule-based approach to norm-oriented programming of electronic institutions", "An Algorithm for Conflict Resolution in Regulated Compound Activities", "Autonomous agents and norms", "CONOISE: Agent-Based Formation of Virtual Organisations", "Multi-dimensional Dynamic Knowledge Representation", "An Algorithm for Conflict Resolution in Regulated Compound Activities", "A Protocol for Resource Sharing in Norm-Governed Ad Hoc Networks", "A Distributed Architecture for Norm-Aware Agent Societies", "Defeasible Reasoning with Legal Rules", "Designing a Deontic Logic of Deadlines", "From logic programming to Prolog", "A computational theory of normative positions", "Normative Conflicts in Legal Reasoning", "Artificial Intelligence and Virtual Organizations", "Agent-based formation of virtual organisations", "The Anatomy of the Grid: Enabling Scalable Virtual Organizations", "A Dynamic Virtual Organization Solution for Web-Services Based Grid Middleware", "Representing Social Structures in UML", "On Social Laws for Artificial Agent Societies: Off-Line Design", "Understanding the Emergence of Conventions in Multi-Agent Systems"], "title": "Resolving Conflict and Inconsistency in Norm-Regulated Virtual Organizations"}
{"id": "I-35", "keywords": "regul multi-agent system;norm conflict;electron institut;organis;coordin", "abstract": "Norms are widely recognised as a means of coordinating multiagent systems. The distributed management of norms is a challenging issue and we observe a lack of truly distributed computational realisations of normative models. In order to regulate the behaviour of autonomous agents that take part in multiple, related activities, we propose a normative model, the Normative Structure (NS), an artifact that is based on the propagation of normative positions (obligations, prohibitions, permissions), as consequences of agents' actions. Within a NS, conflicts may arise due to the dynamic nature of the MAS and the concurrency of agents' actions. However, ensuring conflict-freedom of a NS at design time is computationally intractable. We show this by formalising the notion of conflict, providing a mapping of NSs into Coloured Petri Nets and borrowing well-known theoretical results from that field. Since online conflict resolution is required, we present a tractable algorithm to be employed distributedly. We then demonstrate that this algorithm is paramount for the distributed enactment of a NS.", "references": ["Engineering open environments with electronic institutions", "Using social power to enable agents to reason about being part of a group", "Solving normative conflicts by merging roles", "Design CPN - overview of CPN ML syntax", "Using colored petri nets for conversation modeling", "Autonomous Agents with Norms", "On the Formal Analysis of Normative Conflicts", "Norm consistency in electronic institutions", "First-Order Logic and Automated Theorem Proving", "An Event Driven Approach to Norms in Artificial Institutions", "Extending the BDI architecture with commitments", "An Algorithm for Conflict Resolution in Regulated Compound Activities", "A Distributed Architecture for Norm-Aware Agent Societies", "Multi-language hierarchical logics or: How we can do without modal logics", "The Theory of Communication Action", "Coloured Petri Nets: Basic Concepts, Analysis Methods and Practical Uses", "Strategies for resolving norm conflict in practical reasoning", "Formalization and pre-validation for interaction protocols in a multi agent systems", "Mylopoulos. Knowledge Representation", "A schema-based approach to specifying conversation policies", "Law Governed Interaction (LGI): A Distributed Coordination and Control Mechanism", "Petri nets: Properties, analysis and applications", "Agents that reason and negotiate by arguing", "Coordination Artifacts: A Unifying Abstraction for Engineering Environment-Mediated Coordination in MAS", "Normative conflicts in legal reasoning", "A Computational Theory of Normative Positions", "Resolving Conflict and Inconsistency in Norm-Regulated Virtual Organisations", "Norm and Action: A Logical Inquiry", "An Introduction to Multiagent Systems"], "title": "Distributed Norm Management in Regulated MultiAgent Systems"}
{"id": "J-1", "keywords": "trade reduct;budget balanc;intern competit;extern competit", "abstract": "When designing a mechanism there are several desirable properties tomaintain such as incentive compatibility (IC), individual rationality (IR), and budget balance (BB). It is well known [15] that it is impossible for a mechanism to maximize social welfare whilst also being IR, IC, and BB. There have been several attempts to circumvent [15] by trading welfare for BB, e.g.,in domains such as double-sided auctions [13], distributed markets [3] and supply chain problems [2, 4].In this paper we provide a procedure called a Generalized Trade Reduction (GTR) for single-value players, which given an IR and IC mechanism, outputs a mechanism which is IR, IC and BB with a loss of welfare. We bound the welfare achieved by our procedure for a wide range of domains. In particular, our resultsimprove on existing solutions for problems such as double sidedmarkets with homogenous goods, distributed markets and several kinds of supply chains. Furthermore, our solution provides budget balanced mechanisms for several open problems such as combinatorial double-sided auctions and distributed markets with strategic transportation edges.", "references": ["Frugal path mechanisms", " Concurrent Auctions Across the Supply Chain", "Mechanisms for a Spatially Distributed Market", "Incentive-Compatible, Budget-Balanced, yet Highly Efficient Auctions for Supply Chain Formation", "Negotiation-range mechanisms: exploring the limits of truthful efficient markets", "Trading Networks with Price-Setting Agents", "Optimal decision-making with minimal waste: Strategyproof redistribution of VCG payments", "Clarke Multipart Pricing of Public Goods", "Agent Competition Double Auction Mechanism", "Groves Incentives in teams", "Truth Revelation in Approximately Efficient Combinatorial Auctions", "Elicitation of Honest Preferences for the Assignment of Individuals to Positions", "A Dominant Strategy Double Auction", "Truthful Approximation Mechanisms for Restricted Combinatorial Auctions", "Efficient Mechanisms for Bilateral Trading", "Efficient Auction Mechanisms for Supply Chain Procurement", "Counterspeculation, Auctions and Competitive Sealed Tenders"], "title": "Generalized Trade Reduction Mechanisms"}
{"id": "J-2", "keywords": "mechan design;vickrei-clark-grove mechan;payment redistribut", "abstract": "For allocation problems with one or more items, the well-known Vickrey-Clarke-Groves (VCG) mechanism is efficient, strategy-proof, individually rational, and does not incur a deficit. However, the VCG mechanism is not (strongly) budget balanced: generally, the agents' payments will sum to more than 0. If there is an auctioneer who is selling the items, this may be desirable, because the surplus payment corresponds to revenue for the auctioneer. However, if the items do not have an owner and the agents are merely interested in allocating the items efficiently among themselves, any surplus payment is undesirable, because it will have to flow out of the system of agents. In 2006, Cavallo [3] proposed a mechanism that redistributes some of the VCG payment back to the agents, while maintaining efficiency, strategy-proofness, individual rationality, and the non-deficit property. In this paper, we extend this result in a restricted setting. We study allocation settings where there are multiple indistinguishable units of a single good, and agents have unit demand. (For this specific setting, Cavallo's mechanism coincides with a mechanism proposed by Bailey in 1997 [2].) Here we propose a family of mechanisms that redistribute some of the VCG payment back to the agents. All mechanisms in the family are efficient, strategy-proof, individually rational, and never incur a deficit. The family includes the Bailey-Cavallo mechanism as a special case. We then provide an optimization model for finding the optimal mechanism--that is, the mechanism that maximizes redistribution in the worst case--inside the family, and show how to cast this model as a linear program. We give both numerical and analytical solutions of this linear program, and the (unique) resulting mechanism shows significant improvement over the Bailey-Cavallo mechanism (in the worst case). Finally, we prove that the obtained mechanism is optimal among all anonymous deterministic mechanisms that satisfy the above properties.", "references": ["Derandomization of auctions", "The demand revealing process: to distribute the surplus", "Optimal decision-making with minimal waste: Strategyproof redistribution of VCG payments", "Multipart pricing of public goods", "A budget-balanced, incentive-compatible scheme for social choice", "Sharing the cost of muliticast transmissions", "Pricing WiFi at Starbucks - Issues in online mechanism design", "Competitive auctions", " Competitive auctions and digital goods", "Collusive bidder behavior at single-object second-price and English auctions", "Characterization of satisfactory mechanisms for the revelation of preferences for public goods", "Incentives in teams", "Online auctions with re-usable goods", "Adaptive limited-supply online auctions", "From optimal limited to unlimited supply auctions", "On the existence of allocation systems whose manipulative Nash equilibria are Pareto optimal", "Bidding clubs in first-price auctions", "Optimal multi-unit auctions", "Efficient and strategy-proof assignment with a cheap residual claimant", "Optimal auction design", "Efficient mechanisms for bilateral trading", "Achieving budget-balance with Vickrey-based payment schemes in exchanges", "Counterspeculation, auctions, and competitive sealed tenders"], "title": "Worst-Case Optimal Redistribution of VCG Payments"}
{"id": "J-3", "keywords": "sponsor search;optim;auction;bid", "abstract": "Internet search companies sell advertisement slots based on users' search queries via an auction. While there has been previous work onthe auction process and its game-theoretic aspects, most of it focuses on the Internet company. In this work, we focus on the advertisers, who must solve a complex optimization problem to decide how to place bids on keywords to maximize their return (the number of user clicks on their ads) for a given budget. We model the entire process and study this budget optimization problem. While most variants are NP-hard, we show, perhaps surprisingly, that simply randomizing between two uniform strategies that bid equally on all the keywordsworks well. More precisely, this strategy gets at least a 1-1/ fraction of the maximum clicks possible. As our preliminary experiments show, such uniform strategies are likely to be practical. We also present inapproximability results, and optimal algorithms for variants of the budget optimization problem.", "references": ["Truthful auctions for pricing search keywords", "Bidding to the Top: VCG and Equilibria of Position-Based Auctions", "Dynamics of bid optimization in online advertisement auctions", "Multipart pricing of public goods", "Internet Advertising and the Generalized Second Price Auction: Selling Billions of Dollars Worth of Keywords", "A threshold of ln n for approximating set cover", " Incentives in teams", "Greedy facility location algorithms analyzed using dual fitting with factor-revealing LP", "Internal Document", "Adwords and Generalized Online Matching", "Stochastic models for budget optimization in search-based advertising", "Computational Geometry: An Introduction", "An adaptive algorithm for selecting profitable keywords for search-based advertising services", "Counterspeculation, auctions and competitive-sealed tenders"], "title": "Budget Optimization in Search-Based Advertising Auctions"}
{"id": "J-4", "keywords": "sponsor search;search engin;keyword auction", "abstract": "Keyword auctions lie at the core of the business models of today's leading search engines. Advertisers bid for placement alongside search results, and are charged for clicks on their ads. Advertisers are typically ranked according to a score that takes into account their bids and potential click-through rates. We consider a family of ranking rules that contains those typically used to model Yahoo! and Google's auction designs as special cases. We find that in general neither of these is necessarily revenue-optimal in equilibrium, and that the choice of ranking rule can be guided by considering the correlation between bidders' values and click-through rates. We propose a simple approach to determine a revenue-optimal ranking rule within our family, taking into account effects on advertiser satisfaction and user experience. We illustrate the approach using Monte-Carlo simulations based on distributions fitted to Yahoo! bid and click-through rate data for a high-volume keyword.", "references": ["Truthful auctions for pricing search keywords", "Equilibrium bids in auctions of sponsored links: Theory and evidence", "Internet advertising and the Generalized Second Price auction: Selling billions of dollars worth of keywords", "Implementing sponsored search in Web search engines: Computational evaluation of alternative mechanisms", "Characterizing optimal keyword auctions", "The Art of Computer Programming", "Auction Theory", "An analysis of alternative slot auction designs for sponsored search", "Optimal auction design", "An Introduction to Copulas", "Position auctions"], "title": "Revenue Analysis of a Family of Ranking Rules for Keyword Auctions"}
{"id": "J-7", "keywords": "algorithm game theori;contagion on network;diffus of innov+innov diffus", "abstract": "In many settings, competing technologies -- for example, operating systems, instant messenger systems, or document formats -- can be seen adopting a limited amount of compatibility with one another; in other words, the difficulty in using multiple technologies is balanced somewhere between the two extremes of impossibility and effortless interoperability. There are a range of reasons why this phenomenon occurs, many of which -- based on legal, social, or business considerations -- seem to defy concise mathematical models. Despite this, we show that the advantages of limited compatibility can arise in a very simple model of diffusion in social networks, thus offering a basic explanation for this phenomenon in purely strategic terms. Our approach builds on work on the diffusion of innovations in the economics literature, which seeks to model how a new technology A might spread through a social network of individuals who are currently users of technology B. We consider several ways of capturing the compatibility of A and B, focusing primarily on a model in which users can choose to adopt A, adopt B, or -- at an extra cost -- adopt both A and B. We characterize how the ability of A to spread depends on both its quality relative to B, and also this additional cost of adopting both, and find some surprising non-monotonicity properties in the dependence on these parameters: in some cases, for one technology to survive the introduction of another, the cost of adopting both technologies must be balanced within a narrow, intermediate range. We also extend the framework to the case of multiple technologies, where we find that a simple model captures the phenomenon of two firms adopting a limited "strategic alliance" to defend against a new, third technology.", "references": ["The statistical mechanics of strategic interaction", "Language spread: Studies in diffusion and social change", "Desirability of Compatibility in the Absenceof Network Externalities", "Raising Rivals' Costs in Complementary Goods Markets: LECs Entering into Long Distance and Microsoft Bundling Internet Explorer", "Learning, local interaction, and coordination", "Network Effects and Merger Analysis: Instant Messaging and the AOL-Time Warner Case", "Diffusion on social networks", "Network Externalities, Competition and Compatibility", "Algorithms for Interdependent Security Games", "Strategic Incompatibility in ATM Markets", "Links Between Markets and Aftermarkets: Kodak", "Mix and Match: Product Compatibility without Network Externalities", "Review of Economic Studies", "Diffusion of innovations", "Micromotives and Macrobehavior", " Diffusion in organizations and social movements: From hybrid corn to poison pills", "Network Models of the Diffusion of Innovations", "Tying, Foreclosure, and Exclusion", "Individual Strategy and Social Structure: An Evolutionary Theory of Institutions"], "title": "The Role of Compatibility in the Diffusion of Technologies Through Social Networks"}
{"id": "J-8", "keywords": "game theori;nash equilibrium;strong equilibrium;coalit;price of anarchi+anarchi price;strong price of anarchi;network design;cost share game", "abstract": "In this work we study cost sharing connection games, where each player has a source and sink he would like to connect, and the cost of the edges is either shared equally (fair connection games) or in an arbitrary way (general connection games). We study the graph topologies that guarantee the existence of a strong equilibrium (where no coalition can improve the cost of each of its members) regardless of the specific costs on the edges. Our main existence results are the following: (1) For a single source and sink we show that there is always a strong equilibrium (both for fair and general connection games). (2) For a single source multiple sinks we show that for a series parallel graph a strong equilibrium always exists (both for fair and general connection games). (3) For multi source and sink we show that an extension parallel graph always admits a strong equilibrium in fair connection games. As for the quality of the strong equilibrium we show that in any fair connection games the cost of a strong equilibrium is (log n) from the optimal solution, where n is the number of players. (This should be contrasted with the (n) price of anarchy for the same setting.) For single source general connection games and single source single sink fair connection games, we show that a strong equilibrium is always an optimal solution.", "references": ["Strong Price of Anarchy", "The price of stability for network design with fair cost allocation", "Near-Optimal Network Design with Selfish Agents", "Acceptable Points in General Cooperative n-Person Games", "Tight bounds for worst-case equilibria", "On a network creation game", "Strong equilibrium in congestion games", "Network structure and strong equilibrium in route selection games", "Worst-case equilibria", "Topological conditions for uniqueness of equilibrium in networks", "Network topology and the efficiency of equilibrium", "The equilibrium existence problem in finite network congestion games", "Potential Games", "Strategyproof sharing of submodular costs: Budget balance versus efficiency", "Algorithms, Games, and the Internet", "A class of games possessing pure-strategy Nash equilibria", "The Price of Anarchy is Independent of the Network Topology", "How bad is selfish routing?", "Strong and correlated strong equilibria in monotone congestion games"], "title": "Strong Equilibrium in Cost Sharing Connection Games"}
{"id": "J-9", "keywords": "distribut inform market;market comput;inform aggreg;converg to equilibrium;ration expect;effici market hypothesi", "abstract": "According to economic theorysupported by empirical and laboratory evidencethe equilibrium price of a financial security reflects all of the information regarding the security's value. We investigate the computational process on the path toward equilibrium, where information distributed among traders is revealed step-by-step over time and incorporated into the market price. We develop a simplified model of an information market, along with trading strategies, in order to formalize the computational properties of the process. We show that securities whose payoffs cannot be expressed as weighted threshold functions of distributed input bits are not guaranteed to converge to the proper equilibrium predicted by economic theory. On the other hand, securities whose payoffs are threshold functions are guaranteed to converge, for all prior probability distributions. Moreover, these threshold securities converge in at most  rounds, where  is the number of bits of distributed information. We also prove a lower bound, showing a type of threshold security that requires at least n/2 rounds to converge in the worst case.", "references": ["The role of securities in the optimal allocation of risk-bearing", "A simple characterization of stochastically monotone functions", "Information incorporation in online in-game sports betting markets", "The revelation of information in strategic market games: a critique of rational expectations equilibrium", "Reasoning About Knowledge", "Computation in a distributed information market", "Anatomy of an experimental political stock market", "Wishes, expectations, and actions: A survey on price formation in election stock markets", "Informed traders and price variations in the betting market for professional basketball games", "We can't disagree forever", "An introduction to the theory of rational expectations under asymmetric information", "Combinatorial information market design", "Asymmetric information in a strategic market game: Reexamining the implications of rational expectations", "Recovering probability distributions from options prices", "Complexity results on learning by neural nets", "Expectations and the neutrality of money", "Theory of Incomplete Markets", "Microeconomic Theory", "Common knowledge, consensus, and aggregate information", "Information, trade, and common knowledge", "Common knowledge of an aggregate of expectations", "Modeling information incorporation in markets, with application to detecting and explaining events", "The real power of artificial markets", "Extracting collective probabilistic forecasts from web games", "Rational expectations and the aggregation of diverse information in laboratory security markets", "Parimutuel betting markets as information aggregation devices: Experimental results", "How accurate do markets predict the outcome of an event? the Euro 2000 soccer championships experiment", "Trade using one commodity as a means of payment", "Rational computation and the communication complexity of auctions", "Anomalies Parimutuel betting markets: Racetracks and lotteries", "The arbitrage principle in financial economics"], "title": "Computation in a Distributed Information Market"}
{"id": "J-10", "keywords": "onlin review;reput mechan", "abstract": "Online reviews have become increasingly popular as a way to judge the quality of various products and services. Previous work has demonstrated that contradictory reporting and underlying user biases make judging the true worth of a service difficult. In this paper, we investigate underlying factors that influence user behavior when reporting feedback. We look at two sources of information besides numerical ratings: linguistic evidence from the textual comment accompanying a review, and patterns in the time sequence of reports. We first show that groups of users who amply discuss a certain feature are more likely to agree on a common rating for that feature. Second, we show that a user's rating partly reflects the difference between true quality and prior expectation of quality as inferred from previous reviews. Both give us a less noisy way to produce rating estimates and reveal the reasons behind user bias.Our hypotheses were validated by statistical evidence from hotel reviews on the TripAdvisor website.", "references": ["Noisytalk.com: Broadcasting opinions in a noisy environment", "Thumbs up? sentiment classification using machine learning techniques", "Comparative Experiments on Sentiment Classification for Online Product Reviews", "Mining the peanut gallery:opinion extraction and semantic classification of product reviews", "Exploring the Value of Online Product Ratings in Revenue Forecasting: The Case of Motion Pictures", "A Multi-Level Examination of the Impact of Social Identities on Economic Transactions in Electronic Markets", "Reputation Premiums in Electronic Peer-to-Peer Markets: Analyzing Textual Feedback and Network Structure", "The Dimensions of Reputation in electronic Markets", "Amazon Glitch Unmasks War of Reviewers", "Reputation in Auctions: Theory and Evidence from eBay", "Mining and summarizing customer reviews", "Can Online Reviews Reveal a Product's True Quality?", "Return on reputation in online auction market", "Self-Selection, Slipping, Salvaging, Slacking, and Stoning: the Impacts of Negative Feedback at eBay", "Does a seller's reputation matter? evidence from ebay auctions", "Consumer Expectations, Product Performance and Perceived Product Quality", "A Conceptual Model of Service Quality and Its Implications for Future Research", "SERVQUAL: A Multiple-Item Scale for Measuring Consumer Perceptions of Service Quality", "The Nature and Role of Feedback Text Comments in Online Marketplaces: Implications for Trust Building, Price Premiums, and Seller Differentiation", "Extracting product features and opinions from reviews", "Expectations, Performance Evaluation, and Consumers' Perceptions of Quality", "Chatting a Singer Up the Pop Charts"], "title": "Understanding User Behavior in Online Feedback Reporting"}
{"id": "J-11", "keywords": "algorithm game theori;market;trade network", "abstract": "In a wide range of markets, individual buyers and sellers trade through intermediaries, who determine prices via strategic considerations. Typically, not all buyers and sellers have access to the same intermediaries, and they trade at correspondingly different prices that reflect their relative amounts of power in the market. We model this phenomenon using a game in which buyers, sellers, and traders engage in trade on a graph that represents the access each buyer and seller has to the traders. We show that the resulting game always has a subgame perfect Nash equilibrium, and that all equilibria lead to an efficient allocation of goods. Finally, we analyze trader profits in terms of the graph structure  roughly, a trader can command a positive profit if and only if it has an essential connection in the network, thus providing a graph-theoretic basis for quantifying the amount of competition among traders.", "references": ["Mechanisms for a Spatially Distributed Market", "Agricultural markets in developing countries", "Equilibrium Price Disperison", "Agent Competition Double Auction Mechanism", "Multi-item auctions", "A Network Formation Game for Bipartite Exchange Economies", "Dynamic Pricing by Software Agents", "Economic Properties of Social Networks", "A Theory of Buyer-Seller Networks", "Elicitation of Honest Preferences for the Assignment of Individuals to Positions", "The structure and function of complex networks", "Market Microstructure Theory", "The Assignment Game I: The Core", "The Theory of Industrial Organization"], "title": "Trading networks with price-setting agents"}
{"id": "J-13", "keywords": "hypergraph;combinatori auction;hypertre decomposit", "abstract": "The winner determination problem in combinatorial auctions is the problem of determining the allocation of the items among the bidders that maximizes the sum of the accepted bid prices. While this problem is in general NP-hard, it is known to be feasible in polynomial time on those instances whose associated item graphs have bounded treewidth (called structured item graphs). Formally, an item graph is a graph whose nodes are in one-to-one correspondence with items, and edges are such that for any bid, the items occurring in it induce a connected subgraph. Note that many item graphs might be associated with a given combinatorial auction, depending on the edges selected for guaranteeing the connectedness. In fact, the tractability of determining whether a structured item graph of a fixed treewidth exists (and if so, computing one) was left as a crucial open problem.In this paper, we solve this problem by proving that the existence of a structured item graph is computationally intractable, even for treewidth 3. Motivated by this bad news, we investigate different kinds of structural requirements that can be used to isolate tractable classes of combinatorial auctions. We show that the notion of hypertree decomposition, a recently introduced measure of hypergraph cyclicity, turns out to be most useful here. Indeed, we show that the winner determination problem is solvable in polynomial time on instances whose bidder interactions can be represented with (dual) hypergraphs having bounded hypertree width. Even more surprisingly, we show that the class of tractable instances identified by means of our approach properly contains the class of instances having a structured item graph.", "references": ["Hypertree-Width and Related Hypergraph Invariants", "Solving Concisely Expressed Combinatorial Auction Problems", "Combinatorial auctions with structured item graphs", "An o(n2) algorithm for circular-arc graph recognition", "Taming the computational complexity of combinatorial auctions: Optimal and approximate", "obbers, marshals, and guards: game theoretic and logical characterizations of hypertree width", "Hypertree decompositions and tractable queries", "Solving combinatorial auctions using stochastic local search", "A Catalog of Complexity Classes", "An incremental linear-time algorithm for recognizing interval graphs", "The Winner Determination Problem", "Truth revelation in approximately efficient combinatorial auctions", "Analyzing the airwaves auction", "Selling spectrum rights", "Bidding and allocation in combinatorial auctions", "Graph minors ii. algorithmic aspects of tree width", "Computationally manageable combinatorial auctions", "An implementation of the contract net protocol based on marginal cost calculations", "Algorithm for optimal winner determination in combinatorial auctions", "Winner determination algorithms", "Bob: Improved winner determination in combinatorial auctions and generalizations", "Some tractable combinatorial auctions", "An efficient approximate allocation algorithm for combinatorial auctions"], "title": "On The Complexity of Combinatorial Auctions: Structured Item Graphs and Hypertree Decompositions"}
{"id": "J-14", "keywords": "graphic game;nash equilibrium;approxim scheme", "abstract": "This paper addresses the problem of fair equilibrium selection in graphical games. Our approach is based on the data structure called the best response policy, which was proposed by Kearns et al. [13] as a way to represent all Nash equilibria of a graphical game. In [9], it was shown that the best response policy has polynomial size as long as the underlying graph is a path. In this paper, we show that if the underlying graph is abounded-degree tree and the best response policy has polynomial size then there is an efficient algorithm which constructs a Nash equilibrium that guarantees certain payoffs to all participants. Another attractive solution concept is a Nash equilibrium that maximizes the social welfare. We show that, while exactly computing the latter is infeasible (we prove that solving this problem may involve algebraic numbers of an arbitrarily high degree), there exists an FPTAS for finding such an equilibrium as long as the best response policy has polynomial size. These two algorithms can be combined to produce Nash equilibria that satisfy various fairness criteria.", "references": ["On the Value of Correlation", "Subjectivity and Correlation in Randomized Strategies", "A Continuation Method for Nash Equilibria in Structured Games", "Computing Nash Equilibria: Approximation and Smoothed Complexity", "Settling the Complexity of 2-Player Nash-Equilibrium", "Complexity Results about Nash Equilibria", "The Complexity of Computing a Nash Equilibrium", "Universality of Nash Equilibria", "Nash Equilibria in Graphical games on Trees Revisited", "Computing Good Nash Equilibria in Graphical Games", "Nash and Correlated Equilibria: Some Complexity Considerations", "Reducibility Among Equilibrium Problems", "Graphical Models for Game Theory", "An Efficient Exact Algorithm for Singly Connected Graphical Games", "Nash Equilibria via Polynomial Equations", "Nash Propagation for Loopy Graphical Games", "Computing Correlated Equilibria in Multi-Player Games", "Computing Equilibria in Multi-Player Games", "Multi-agent Algorithms for Solving Graphical Games"], "title": "Computing Good Nash Equilibria in Graphical Games"}
{"id": "J-15", "keywords": "auction;multiattribut auction;prefer handl", "abstract": "Multiattribute auction mechanisms generally either remain agnostic about traders' preferences, or presume highly restrictive forms, such as full additivity. Real preferences often exhibit dependencies among attributes, yet may possess some structure that can be usefully exploited to streamline communication and simplify operation of a multiattribute auction. We develop such a structure using the theory of measurable value functions, a cardinal utility representation based on an underlying order over preference differences. A set of local conditional independence relations over such differences supports a generalized additive preference representation, which decomposes utility across overlapping clusters of related attributes. We introduce an iterative auction mechanism that maintains prices on local clusters of attributes rather than the full space of joint configurations. When traders' preferencesare consistent with the auction's generalized additive structure, the mechanism produces approximately optimal allocations, atapproximate VCG prices.", "references": ["Graphical models for preference and utility", "An inverse-optimization-based auction for multiattribute RFQs", "The Future of e-Markets: Multi-Dimensional Market Mechanisms", "UCP-networks: A directed graphical representation of conditional utilities", "Compact value-function representations for qualitative preferences", "Local utility elicitation in GAI models", "Design competition through multidimensional auctions", "An English auction protocol for multi-attribute items", "Topological methods in cardinal utility theory", "An axiomatization of cardinal additive conjoint measurement theory", "Measurable multiattribute value functions", "Bid expressiveness and clearing algorithms in multiattribute double auctions", "Interdependence and additivity in multivariate, unidimensional expected utility theory", "GAI networks for utility elicitation", "GAI networks for decision making under certainty", "Regret-based incremental partial revelation mechanisms", "Decisions with Multiple Objectives: Preferences and Value Tradeoffs", "An efficient algorithm for finding the M most probable configurations in probabilistic expert systems", "Models for iterative multiattribute procurement auctions", "Graphoids: A graph based logic for reasoning about relevance relations", "Procurement auctions for differentiated goods", "Efficient mechanisms for the supply of services in multi-agent environments"], "title": "Generalized Value Decomposition and Structured Multiattribute Auctions"}
{"id": "J-17", "keywords": "mechan design;approxim algorithm;schedul", "abstract": "We consider the problem of makespan minimization on m unrelated machines in the context of algorithmic mechanism design, where the machines are the strategic players. This is a multidimensional scheduling domain, and the only known positive results for makespan minimization in such a domain are O(m)-approximation truthful mechanisms (22, 20). We study a well-motivated special case of this problem, where the processing time of a job on each machine may either be "low" or "high", and the low and high values are public and job-dependent. This preserves the multidimensional- ity of the domain, and generalizes the restricted-machines (i.e., {pj, }) setting in scheduling. We give a general tech- nique to convert any c-approximation algorithm to a 3c- approximation truthful-in-expectation mechanism. This is one of the few known results that shows how to export ap- proximation algorithms for a multidimensional problem into truthful mechanisms in a black-box fashion. When the low and high values are the same for all jobs, we devise a de- terministic 2-approximation truthful mechanism. These are the first truthful mechanisms with non-trivial performance guarantees for a multidimensional scheduling domain. Our constructions are novel in two respects. First, we do not utilize or rely on explicit price definitions to prove truthfulness; instead we design algorithms that satisfy cy- cle monotonicity. Cycle monotonicity (23) is a necessary and sufficient condition for truthfulness, is a generalization of value monotonicity for multidimensional domains. How- ever, whereas value monotonicity has been used extensively and successfully to design truthful mechanisms in single- dimensional domains, ours is the first work that leverages cycle monotonicity in the multidimensional setting. Second, our randomized mechanisms are obtained by first construct- ing a fractional truthful mechanism for a fractional relax- ation of the problem, and then converting it into a truthful- in-expectation mechanism. This builds upon a technique of (16), and shows the usefulness of fractional mechanisms in truthful mechanism design.", "references": ["Truthful approximation mechanisms for scheduling selfish related machines", "Mechanisms for discrete optimization with rational agents", "Truthful mechanisms for one-parameter agents", "Deterministic truthful approximation mechanisms for scheduling related machines", "Allocating indivisible goods", "Weak monotonicity characterizes deterministic dominant-strategy implementation", "Approximation techniques for utilitarian mechanism design", "A lower bound for scheduling mechanisms", "Multipart pricing of public goods", "Incentives in teams", "Characterizing dominant strategy mechanisms with multi-dimensional types", "Approximation algorithms for scheduling", "Fast monotone 3-approximation algorithm for scheduling related machines", "Approximation algorithms for scheduling on multiple machines", "Towards a characterization of truthful combinatorial auctions", "Truthful and near-optimal mechanism design via linear programming", "Truth revelation in approximately efficient combinatorial auctions", "Approximation algorithms for scheduling unrelated parallel machines", "On approximately fair allocations of indivisible goods", "Setting lower bounds on truthfulness", "Optimal auction design", "Algorithmic mechanism design", "A necessary and sufficient condition for rationalizability in a quasilinear context", "Weak monotonicity suffices for truthfulness on convex domains", "An approximation algorithm for the generalized assignment problem", "Counterspeculations, auctions, and competitive sealed tenders"], "title": "Truthful Mechanism Design for Multi-Dimensional Scheduling via Cycle Monotonicity"}
{"id": "J-18", "keywords": "auction;mediat;equilibrium", "abstract": "A mediator is a reliable entity, which can play on behalf of agents in a given game. A mediator however can not enforce the use of its services, and each agent is free to participate in the game directly. In this paper we introduce a study of mediators for games with incomplete information, and apply it to the context of position auctions, a central topic in electronic commerce. VCG position auctions, which are currently not used in practice, possess some nice theoretical properties, such as the optimization of social surplus and having dominant strategies. These properties may not be satisfied by current position auctions and their variants. We therefore concentrate on the search for mediators that will allow to transform current position auctions into VCG position auctions. We require that accepting the mediator services, and reporting honestly to the mediator, will form an ex post equilibrium, which satisfies the following rationality condition: an agent"s payoff can not be negative regardless of the actions taken by the agents who did not choose the mediator"s services, or by the agents who report false types to the mediator. We prove the existence of such desired mediators for the next-price (Google-like) position auctions, as well as for a richer class of position auctions, including all k-price position auctions, k > 1. For k=1, the self-price position auction, we show that the existence of such mediator depends on the tie breaking rule used in the auction.", "references": ["Subjectivity and correlation in randomized strategies", "Bidding Rings Revisited", "Internet advertising and the generalized second price auction: Selling billions of dollars worth of keywords", "Implementing sponsored search in web search engines: Computational evaluation of alternative mechanisms", "An approach to communication equilibria", "Collusive Bidder Behavior at Single-Object Second-Price and English Auctions", "Bundling equilibrium in combinatorial auctions", "Arbitration of Two-Party Disputes under Ignorance", "An analysis of alternative slot auction designs for sponsored search", "Towards a characterization of truthful combinatorial auctions", "Bidding Rings", "Adwords and Generalized Online Matching", "K-Implementation", "Strong mediated equilibrium", "Multistage games with communication", "Routing mediators", "Position auctions"], "title": "Mediators in Position Auctions"}
{"id": "J-20", "keywords": "barter;exchang;match;column gener;branch-and-price;kidnei;transplant", "abstract": "In barter-exchange markets, agents seek to swap their items with one another, in order to improve their own utilities. These swaps consist of cycles of agents, with each agent receiving the item of the next agent in the cycle. We focus mainly on the upcoming national kidney-exchange market, where patients with kidney disease can obtain compatible donors by swapping their own willing but incompatible donors. With over 70,000 patients already waiting for a cadaver kidney in the US, this market is seen as the only ethical way to significantly reduce the 4,000 deaths per year attributed to kidney diseas. The clearing problem involves finding a social welfare maximizing exchange when the maximum length of a cycle is fixed. Long cycles are forbidden, since, for incentive reasons, all transplants in a cycle must be performed simultaneously. Also, in barter-exchanges generally, more agents are affected if one drops out of a longer cycle. We prove that the clearing problem with this cycle-length constraint is NP-hard. Solving it exactly is one of the main challenges in establishing a national kidney exchange. We present the first algorithm capable of clearing these markets on a nationwide scale. The key is incremental problem formulation. We adapt two paradigms for the task: constraint generation and column generation. For each, we develop techniques that dramatically improve both runtime and memory usage. We conclude that column generation scales drastically better than constraint generation. Our algorithm also supports several generalizations, as demanded by real-world kidney exchanges. Our algorithm replaced CPLEX as the clearing algorithm of the Alliance for Paired Donation, one of the leading kidney exchanges. The match runs are conducted every two weeks and transplants based on our optimizations have already been conducted.", "references": ["Branch-and-price: Column generation for solving huge integer programs", "Generalized best-first search strategies and the optimality of A", " Exchanging kidneys - advances in living-donor transplantation", "Path, trees, and flowers", "Computers and Intractability; A Guide to the Theory of NP-Completeness", "A comparison of populations served by kidney paired donation and list paired donation", "A formal basis for the heuristic determination of minimum cost paths", "Solving airline crew-scheduling problems by branch-and-cut", "Intervac", "National odd shoe exchange", "Peerflix", "Read it swap it", "Kidney exchange", "A kidney exchange clearinghouse in New England", "Efficient kidney exchange: Coincidence of wants in a market with compatibility-based preferences", "Gabow's n3 maximum-weight matching algorithm: an implementation", "Increasing the opportunity of live kidney donation by matching for two and three way exchanges", "Optimal winner determination algorithms", "Side constraints and non-price attributes in markets", "Kidney paired donation and optimizing the use of live donor organs", "United Network for Organ Sharing", "Dynamic kidney exchange", "United States Renal Data System", "Optimal control of a paired-kidney exchange program"], "title": "Clearing Algorithms for Barter Exchange Markets: Enabling Nationwide Kidney Exchanges"}
{"id": "J-21", "keywords": "predict market;inform market;strateg analysi;dpm;msr;project game", "abstract": "Information markets, which are designed specifically to aggregate traders' information, are becoming increasingly popular as a means for predicting future events. Recent research in information markets has resulted in two new designs, market scoring rules and dynamic parimutuel markets. We develop an analytic method to guide the design and strategic analysis of information markets. Our central contribution is a new abstract betting game, the projection game, that serves as a useful model for information markets. We demonstrate that this game can serve as a strategic model of dynamic parimutuel markets, and also captures the essence of the strategies in market scoring rules. The projection game is tractable to analyze, and has an attractive geometric visualization that makes the strategic moves and interactions more transparent. We use it to prove several strategic properties about the dynamic parimutuel market. We also prove that a special form of the projection game is strategically equivalent to the spherical scoring rule, and it is strategically similar to other scoring rules. Finally, we illustrate two applications of the model to analysis of complex strategic scenarios: we analyze the precision of a market in which traders have inertia, and a market in which a trader can profit by manipulating another trader's beliefs.", "references": ["Information incorporation in online in-game sports betting markets", "Anatomy of an experimental political stock market", "Wishes, expectations, and actions: A survey on price formation in election stock markets", "Effective scoring rules for probabilistic forecasts", "Informed traders and price variations in the betting market for professional basketball games", "Combinatorial information market design", "Information aggregation and manipulation in an experimental market", "The tech buzz game", "Rational expectations and the theory of price movements", "A dynamic parimutuel market for information aggregation", "Computational aspects of prediction markets", "Modeling information incorporation in markets, with application to detecting and explaining events", "Rational expectations and the aggregation of diverse information in laboratory security markets", "Parimutuel betting markets as information aggregation devices: Experimental results", "Policy analysis market: An electronic commerce application of a combinatorial information market", "How accurate do markets predict the outcome of an event? the Euro 2000 soccer championships experiment"], "title": "A Strategic Model for Information Markets Evdokia Nikolova"}
{"id": "J-22", "keywords": "predict market;express bet;order match;comput complex", "abstract": "We consider a permutation betting scenario, where people wager on the final ordering of n candidates: for example, the outcome of a horse race. We examine the auctioneer problem of risklessly matching up wagers or, equivalently, finding arbitrage opportunities among the proposed wagers. Requiring bidders to explicitly list the orderings that they'd like to bet on is both unnatural and intractable, because the number of orderings is n! and the number of subsets of orderings is 2n!. We propose two expressive betting languages that seem natural for bidders, and examine the computational complexity of the auctioneer problem in each case. Subset betting allows traders to bet either that a candidate will end up ranked among some subset of positions in the final ordering, for example, "horse A will finish in positions 4, 9, or 13-21", or that a position will be taken by some subset of candidates, for example "horse A, B, or D will finish in position 2". For subset betting, we show that the auctioneer problem can be solved in polynomial time if orders are divisible. Pair betting allows traders to bet on whether one candidate will end up ranked higher than another candidate, for example "horse A will beat horse B". We prove that the auctioneer problem becomes NP-hard for pair betting. We identify a sufficient condition for the existence of a pair betting match that can be verified in polynomial time. We also show that a natural greedy algorithm gives a poor approximation for indivisible orders.", "references": ["The role of securities in the optimal allocation of risk-bearing", "Results from a dozen years of election futures markets research", "Introduction to Algorithms", "Combinatorial Auctions", "Wishes, expectations, and actions: A survey on price formation in election stock markets", "Betting boolean-style: A framework for trading in securities based on logical formulas", "The ellipsoid method and its consequences in combinatorial optimization", "Geometric Algorithms and Combinatorial Optimization", " Combinatorial information market design", "Reducibility among combinatorial problems", "Approximation algorithms for cycle packing problems", "The hungarian method for the assignment problem", "Algorithms for the assignment and transportation problems", "Bidding and allocation in combinatorial auctions", "The real power of artificial markets", "Efficiency of experimental security markets with insider information: An application of rational expectations models", "Rational expectations and the aggregation of diverse information in laboratory security markets", "Algorithm for optimal winner determination in combinatorial auctions", "Packing directed cycles efficiently"], "title": "Betting on Permutations"}
{"id": "J-23", "keywords": "auction;frugal;vertex cover", "abstract": "In set-system auctions, there are several overlapping teams of agents, and a task that can be completed by any of these teams. The auctioneer's goal is to hire a team and pay as little as possible. Examples of this setting include shortest-path auctions and vertex-cover auctions. Recently, Karlin, Kempe and Tamir introduced a new definition of frugality ratio for this problem. Informally, the "frugality ratio" is the ratio of the total payment of a mechanism to a desired payment bound. The ratio captures the extent to which the mechanism overpays, relative to perceived fair cost in a truthful auction. In this paper, we propose a new truthful polynomial-time auction for the vertex cover problem and bound its frugality ratio. We show that the solution quality is with a constant factor of optimal and the frugality ratio is within a constant factor of the best possible worst-case bound; this is the first auction for this problem to have these properties. Moreover, we show how to transform any truthful auction into a frugal one while preserving the approximation ratio. Also, we consider two natural modifications of the definition of Karlin et al., and we analyse the properties of the resulting payment bounds, such as monotonicity, computational hardness, and robustness with respect to the draw-resolution rule. We study the relationships between the different payment bounds, both for general set systems and for specific set-system auctions, such as path auctions and vertex-cover auctions. We use these new definitions in the proof of our main result for vertex-cover auctions via a boot-strapping technique, which may be of independent interest.", "references": ["Frugal path mechanisms", "Local ratio: A unified framework for approximation algorithms", "A local-ratio theorem for approximating the weighted vertex cover problem", "Multipart pricing of public goods", "Bounding the payment of approximate truthful mechanisms", "On the expected payment of mechanisms for task allocation", "True costs of cheap labor are hard to measure: edge deletion and VCG payments in graphs", "Frugality ratios and improved truthful mechanisms for vertex cover", "Frugality in path auctions", "A BGP-based mechanism for lowest-cost routing", "Competitive generalized auctions", "Coalitional games on graphs: core structures, substitutes and frugality", "Competitive auctions and digital goods", "Incentives in teams", "First-price path auctions", "Beyond VCG: frugality of truthful mechanisms", "Personal communication", "Cheap labor can be expensive", "Algorithmic mechanism design", "Towards generic low payment mechanisms for decentralized task allocation", "Towards generic low payment mechanisms for decentralized task allocation", "Counterspeculation, auctions, and competitive sealed tenders"], "title": "Frugality Ratios And Improved Truthful Mechanisms for Vertex Cover"}
{"id": "J-25", "keywords": "compound secur market;comput complex of match;combinatori bet;trade in financi instrument base on logic formula;risk alloc;inform aggreg;hedg;specul;bet;gambl", "abstract": "We develop a framework for trading in compound securities: financial instruments that pay off contingent on the outcomes of arbitrary statements in propositional logic. Buying or selling securities---which can be thought of as betting on or against a particular future outcome---allows agents both to hedge risk and to profit (in expectation) on subjective predictions. A compound securities market allows agents to place bets on arbitrary boolean combinations of events, enabling them to more closely achieve their optimal risk exposure, and enabling the market as a whole to more closely achieve the social optimum.The tradeoff for allowing such expressivity is in the complexity of the agents' and auctioneer's optimization problems.We develop and motivate the concept of a compound securities market, presenting the framework through a series of formal definitions and examples. We then analyze in detail the auctioneer's matching problem. We show that, with numevents events, the matching problem is co-NP-complete in the divisible case and complete in the indivisible case. We show that the latter hardness result holds even under severe language restrictions on bids. With events, and numevents securities, the problem is polynomial in the divisible case and NP-complete in the indivisible case. We briefly discuss matching algorithms and tractable special cases.", "references": ["The role of securities in the optimal allocation of risk-bearing", "Inducing liquidity in thin financial markets through combined-value trading mechanisms", "Decentralized computation procurement and computational robustness in a smart market", "Forecasting uncertain events with small groups", "Theory of Probability: A Critical Introductory Treatment", "Combinatorial auctions: A survey", "Information incorporation in online in-game sports betting markets", "Market allocation under uncertainty", "A logic for reasoning about probabilities", "Information aggregation in an experimental market", "Anatomy of an experimental political stock market", "Wishes, expectations, and actions: A survey on price formation in election stock markets", "Informed traders and price variations in the betting market for professional basketball games", "Decision markets", "Combinatorial information market design", "Could gambling save science? Encouraging an honest consensus", "Recovering probability distributions from options prices", "Separating probability elicitation from utilities", "Theory of Incomplete Markets", "Microeconomic Theory", "Bidding and allocation in combinatorial auctions", " Computationally feasible VCG mechanisms", "Probabilistic Reasoning in Intelligent Systems", "The real power of artificial markets", "Extracting collective probabilistic forecasts from web games", "Compact securities markets for Pareto optimal reallocation of risk", "Parimutuel betting markets as information aggregation devices: Experimental results", "Markets as information gathering tools", "Rational expectations and the aggregation of diverse information in laboratory security markets", "Orange juice and weather", "Winner determination in combinatorial auction generalizations", "How accurate do markets predict the outcome of an event? the Euro 2000 soccer championships experiment", "Anomalies: Parimutuel betting markets: Racetracks and lotteries", "The arbitrage principle in financial economics", "Good probability assessors"], "title": "Betting Boolean-Style: A Framework for Trading in Securities Based on Logical Formulas"}
{"id": "J-26", "keywords": "agenc theori;princip-agent model;incent;con", "abstract": "Much recent research concerns systems, such as the Internet, whose components are owned and operated by different parties, each with his own "selfish" goal. The field of Algorithmic Mechanism Design handles the issue of private information held by the different parties in such computational settings. This paper deals with a complementary problem in such settings: handling the "hidden actions" that are performed by the different parties.Our model is a combinatorial variant of the classical principalagent problem from economic theory. In our setting a principal must motivate a team of strategic agents to exert costly effort on his behalf, but their actions are hidden from him. Our focus is on cases where complex combinations of the efforts of the agents influence the outcome. The principal motivates the agents by offering to them a set of contracts, which together put the agents in an equilibrium point of the induced game. We present formal models for this setting, suggest and embark on an analysis of some basic issues, but leave many questions open.", "references": ["The Price of Purity and Free-Labor in Combinatorial Agency", "Combinatorial agency", "Hidden-action in multi-hop routing", "Moral Hazard in Teams", "Microeconomic Theory", "Algorithmic mechanism design", "Algorithms, Games, and the Internet", "The complexity of counting cuts and of computing the probability that a graph is connected", "Prediction Games", "Sequential Information Elicitation in Multi-Agent Systems", "Overcoming Free-Riding in Multi-Party Computations - The Anonymous Case", "Incentives and Discrimination"], "title": "Combinatorial Agency"}
{"id": "J-27", "keywords": "reveal prefer;machin learn;fat shatter", "abstract": "A sequence of prices and demands are rationalizable if there exists a concave, continuous and monotone utility function such that the demands are the maximizers of the utility function over the budget set corresponding to the price. Afriat [1] presented necessary and sufficient conditions for a finite sequence to be rationalizable. Varian [20] and later Blundell et al. [3, 4] continued this line of work studying nonparametric methods to forecasts demand. Their results essentially characterize learnability of degenerate classes of demand functions and therefore fall short of giving a general degree of confidence in the forecast. The present paper complements this line of research by introducing a statistical model and a measure of complexity through which we are able to study the learnability of classes of demand functions and derive a degree of confidence in the forecasts.Our results show that the class of all demand functions has unbounded complexity and therefore is not learnable, but that there exist interesting and potentially useful classes that are learnable from finite samples. We also present a learning algorithm that is an adaptation of a new proof of Afriat's theorem due to Teo and Vohra [17].", "references": ["The Construction of a Utility Function from Expenditure Data International Economic Review", "Neural Network Learning: Theoretical Foundations Cambridge University Press", "Nonparametric Engel curves and revealed preference", "How revealing is revealed preference? ", "Afriat and Revealed Preference Theory Review of Economic Studies", "ber die Theorie der Einfachen Ungleichungen Journal fr die Reine und Angewandte Mathematik", "Revealed Preference and the Utility Function Economica", "An Introduction to Computational Learning Theory The MIT PressCambridge MA", "A Tight Upper Bound on the Money Metric Utility Function", "The Recoverability of Consumers' Preferences from Market Demand", "On Revealed Preference Analysis", "Microeconomic Theory Oxford University Press", "Testing Strictly Concave Rationality", "Combinatorial Optimization Dover Publications", "Revealed Preference Theory", "Preference and rational choice in the theory of consumption", "Afriat's Theorem and Negative Cycles Working Paper", "Consumption Theory in Terms of Revealed Preference", "Statistical Learning Theory", "The Non-Parametric Approach to Demand Analysis", " Revealed Preference", "Lectures on Polytopes"], "title": "Learning From Revealed Preference"}
{"id": "J-28", "keywords": "approxim algorithm;multi-unit auction;strategyproof", "abstract": "We present an approximately-efficient and approximately-strategyproof auction mechanism for a single-good multiunit allocation problem. The bidding language allows marginal-decreasing piecewise-constant curves and quantity-based side constraints. We develop a fully polynomial-time approximation scheme for the multiunit allocation problem, which computes a (1+) approximation in worst-case time T=O(n3/), given n bids each with a constant number of pieces. We integrate this approximation scheme within a VickreyClarkeGroves (VCG) mechanism and compute payments for an asymptotic cost of O(T log n). The maximal possible gain from manipulation to a bidder in the combined scheme is bounded by V/(1+), where V is the total surplus in the efficient outcome.", "references": ["Ascending auctions with package bidding", "Linear programming and Vickrey auctions", "The package assignment model", "uelson. Bargaining under incomplete information", "Multipart pricing of public goods", "Combinatorial auctions: A survey", "Bid evaluation in procurement auctions with piece-wise linear supply curves", "Distributed Algorithmic Mechanism Design: Recent Results and Future Directions", "Computers and Intractability: A Guide to the Theory of NP-Completeness", "Computational complexity of approximation algorithms for combinatorial problems", "Incentives in teams", "Computational aspects of clearing continuous call double auctions with assignment constraints and indivisible demand", "Auction Theory", "Efficient mechanism design", "Truth revelation in approximately efficient combinatorial auctions", "Optimal auction design", "Efficient mechanisms for bilateral trading", "Computationally feasible VCG mechanisms", "Achieving budget-balance with Vickrey-based payment schemes in exchanges", "Computationally manageable combinatorial auctions", "Almost dominant strategy implementation", "Counterspeculation, auctions, and competitive sealed tenders"], "title": "Approximately-Strategyproof and Tractable Multi-Unit Auctions"}
{"id": "J-30", "keywords": "implement;mechansm design;singl-cross condit;commun complex", "abstract": "While traditional mechanism design typically assumes isomorphism between the agents' type- and action spaces, in many situations the agents face strict restrictions on their action space due to, e.g., technical, behavioral or regulatory reasons. We devise a general framework for the study of mechanism design in single-parameter environments with restricted action spaces. Our contribution is threefold. First, we characterize sufficient conditions under which the information-theoretically optimal social-choice rule can be implemented in dominant strategies, and prove that any multi-linear social-choice rule is dominant-strategy implementable with no additional cost. Second, we identify necessary conditions for the optimality of action-bounded mechanisms, and fully characterize the optimal mechanisms and strategies in games with two players and two alternatives. Finally, we prove that for any multilinear social-choice rule, the optimal mechanism with k actions incurs an expected loss of O( 1k2 ) compared to the optimal mechanisms with unrestricted action spaces. Our results apply to various economic and computational settings, and we demonstrate their applicability to signaling games, public-good models and routing in networks.", "references": ["Single crossing properties and the existence of pure strategy equilibria in games of incomplete information", "Computationally-feasible auctions for convex bundles", "Mechanism design for single-value domains", "Auctions with severely bounded communications", "Auctions with severely bounded communications", "Multi-player and multi-round auctions with severely bounded communication", "The discrete bid first price auction", "Optimal design of english auctions with discrete bid levels", "On the role of discrete bid levels in oral auctions", "Lecture notes in economics", "A study of limited precision, incremental elicitation in auctions", "Costly valuation computation in auctions", "Towards a characterization of truthful combinatorial auctions", "Coarse matching", "Optimal auction design", "Privacy preserving auctions and mechanism design", "Sequences of take-it-or-leave-it offers: Near-optimal auctions without full-valuation revelation", "The optimality of a simple market mechanism", "Efficient and competitive rationing"], "title": "Implementation with a Bounded Action Space"}
{"id": "J-31", "keywords": "game theori;commit;leadership;stackelberg;normal-form game;bayesian game;nash equilibrium", "abstract": "In multiagent systems, strategic settings are often analyzed under the assumption that the players choose their strategies simultaneously. However, this model is not always realistic. In many settings, one player is able to commit to a strategy before the other player makes a decision. Such models are synonymously referred to as leadership, commitment, or Stackelberg models, and optimal play in such models is often significantly different from optimal play in the model where strategies are selected simultaneously.The recent surge in interest in computing game-theoretic solutions has so far ignored leadership models (with the exception of the interest in mechanism design, where the designer is implicitly in a leadership position). In this paper, we study how to compute optimal strategies to commit to under both commitment to pure strategies and commitment to mixed strategies, in both normal-form and Bayesian games. We give both positive results (efficient algorithms) and negative results (NP-hardness results).", "references": ["Computing Nash equilibria of action-graph games", "Complexity results about Nash equilibria", "Complexity of (iterated) dominance", "A generalized strategy eliminability criterion and computational methods for applying it", "Recherches sur les principes mathmatiques de la thorie des richesses (Researches 1Bayesian games are one potentially concise representation of normal-form games", "A proof of the equivalence of the programming problem and the game problem", "The complexity of eliminating dominated strategies", "Nash and correlated equilibria: Some complexity considerations", "Reducibility among combinatorial problems", "Graphical models for game theory", "A note on strategy elimination in bimatrix games", "The complexity of two-person zero-sum games in extensive form", "Efficient computation of equilibria for extensive two-person games", "Local-effect games", "Playing large games using simple strategies", "A polynomial-time Nash equilibrium algorithm for repeated games", "Games and Decisions", "Equilibrium points in n-person games", "Run the GAMUT: A comprehensive approach to evaluating game-theoretic algorithms", "A Course in Game Theory", "Algorithms, games and the Internet", "Simple search methods for finding a Nash equilibrium", "Mixed-integer programming methods for finding Nash equilibria", "Zur Theorie der Gesellschaftsspiele", "Marktform und Gleichgewicht", "Leadership with commitment to mixed strategies"], "title": "Computing the Optimal Strategy to Commit to"}
{"id": "J-32", "keywords": "graphic game;nash equilibrium;ppad-complet", "abstract": "Graphical games have been proposed as a game-theoretic model of large-scale distributed networks of non-cooperative agents. When the number of players is large, and the underlying graph has low degree, they provide a concise way to represent the players' payoffs. It has recently been shown that the problem of finding Nash equilibria in a general degree-3 graphical game with two actions per player is complete for the complexity class PPAD, indicating that it is unlikely that there is any polynomial-time algorithm for this problem. In this paper, we study the complexity of graphical games with two actions per player on bounded-degree trees. This setting was first considered by Kearns, Littman and Singh, who proposed a dynamic programming-based algorithm that computes all Nash equilibria of such games. The running time of their algorithm is exponential, though approximate equilibria can be computed efficiently. Later, Littman, Kearns and Singh proposed a modification to this algorithm that can find a single Nash equilibrium in polynomial time. We show that this modified algorithm is incorrect-the output is not always a Nash equilibrium. We then propose a new algorithm that is based on the ideas of Kearns et al. and computes all Nash equilibria in quadratic time if the input graph is a path, and in polynomial time if it is an arbitrary graph of maximum degree 2. Moreover, our algorithm can be used to compute Nash equilibria of graphical games on arbitrary trees, but the running time can be exponential, even when the tree has bounded degree. We show that this is inevitable -- any algorithm of this type will take exponential time, even on bounded-degree trees with pathwidth 2. It is an open question whether our algorithm runs in polynomial time on graphs with pathwidth 1, but we show that finding a Nash equilibrium for a 2-action graphical game in which the underlying graph has maximum degree 3 and constant pathwidth is PPAD-complete (so is unlikely to be tractable).", "references": ["Efficient and constructive algorithms for the pathwidth and treewidth of graphs", "3-NASH is PPAD-complete", "Settling the complexity of 2-player Nash equilibrium", "The complexity of computing a Nash equilibrium", "Three-player games are hard", "Nash equilibria in graphical games on trees revisited", "Reducibility among equilibrium problems", "Graphical models for game theory", "An efficient exact algorithm for singly connected graphical games", "Nash propagation for loopy graphical games", "On the complexity of the parity argument and other inefficient proofs of existence"], "title": "Nash Equilibria in Graphical Games on Trees Revisited"}
{"id": "C-41", "keywords": "distribut real-time emb system;hybrid system;qualiti of servic+servic qualiti", "abstract": "A challenging problem faced by researchers and developers of distributed real-time and embedded (DRE) systems is devising and implementing effective adaptive resource management strategies that can meet end-to-end quality of service (QoS) requirements in varying operational conditions. This paper presents two contributions to research in adaptive resource management for DRE systems. First, we describe the structure and functionality of the Hybrid Adaptive Resource management Middleware (HyARM), which provides adaptive resource management using hybrid control techniques for adapting to workload fluctuations and resource availability. Second, we evaluate the adaptive behavior of HyARM via experiments on a DRE multimedia system that distributes video in real-time. Our results indicate that HyARM yields predictable, stable, and high system performance, even in the face of fluctuating workload and resource availability.", "references": ["Feddback Performance Control in Software Services", "Analysis of a reservation-based feedback scheduler", "An architecture for differentiated services", "Enabling Autonomic Workload Management in Linux", "Design Patterns: Elements of Reusable Object-Oriented Software", "Qos impact on user perception and understanding of multimedia video clips", "Differentiated Services Working Group", "Hybrid Supervisory Control of Real-Time Systems", "The Rate Monotonic Scheduling Algorithm: Exact Characterization and Average Case Behavior", "Comparing and Contrasting Adaptive Middleware Support in Wide-Area and Embedded Distributed Object Applications", "Feedback Control Real-Time Scheduling: Framework, Modeling, and Algorithms", "Real-time CORBA Specification, OMG Document formal", "The Design and Performance of Real-Time Object Request Brokers", "Trends and Perspectives in Image and Video Coding", "CAMRIT: Control-based Adaptive Middleware for Real-time Image Transmission"], "title": "Evaluating Adaptive Resource Management for Distributed Real-Time Embedded Systems"}
{"id": "C-42", "keywords": "reservoir model;energi explor;enkf;tigr", "abstract": "Ensemble Kalman filter data assimilation methodology is a popular approach for hydrocarbon reservoir simulations in energy exploration. In this approach, an ensemble of geological models and production data of oil fields is used to forecast the dynamic response of oil wells. The Schlumberger ECLIPSE software is used for these simulations. Since models in the ensemble do not communicate, message-passing implementation is a good choice. Each model checks out an ECLIPSE license and therefore, parallelizability of reservoir simulations depends on the number licenses available. We have Grid-enabled the ensemble Kalman filter data assimilation methodology for the TIGRE Grid computing environment. By pooling the licenses and computing resources across the collaborating institutions using GridWay metascheduler and TIGRE environment, the computational accuracy can be increased while reducing the simulation runtime. In this paper, we provide an account of our efforts in Grid-enabling the ensemble Kalman Filter data assimilation methodology. Potential benefits of this approach, observations and lessons learned will be discussed.", "references": ["The Grid: Blueprint for a new computing infrastructure", "TIGRE Portal", "Demonstration of TIGRE environment for Grid enabled/suitable applications", "The High Performance Computing across Texas Consortium", "The Open Science Grid", "the TeraGrid and Beyond", "Data Assimilation: The Ensemble Kalman Filter", "Scientific Programming", "The GriPhyN project: Towards petascale virtual data grids", "The PacMan documentation and installation guide", "Case studies in identify management for virtual organizations", "The Grid User Management System", "Building grid computing portals: The NPACI grid portal toolkit, Grid computing: making the global infrastructure a reality", "Open Ticket Request System", "The MoinMoin Wiki Engine", "Integrating dynamic data into high resolution reservoir models using streamline-based analytic sensitivity coefficients", "History matching finite difference models with 3D streamlines", "Reservoir monitoring and Continuous Model Updating using Ensemble Kalman Filter", "History matching with an ensemble Kalman filter and discrete cosine parameterization", "An iterative ensemble Kalman filter for data assimilation", "Streamline assisted ensemble Kalman filter for rapid and continuous reservoir model updating", "ECLIPSE Reservoir Engineering Software"], "title": "Demonstration of Grid-Enabled Ensemble Kalman Filter Data Assimilation Methodology for Reservoir Characterization"}
{"id": "C-44", "keywords": "wireless sensor network;local;node sequenc process", "abstract": "Wireless Sensor Networks have been proposed for use in many location-dependent applications. Most of these need to identify the locations of wireless sensor nodes, a challenging task because of the severe constraints on cost, energy and effective range of sensor devices. To overcome limitations in existing solutions, we present a Multi-Sequence Positioning (MSP) method for large-scale stationary sensor node localization in outdoor environments. The novel idea behind MSP is to reconstruct and estimate two-dimensional location information for each sensor node by processing multiple one-dimensional node sequences, easily obtained through loosely guided event distribution. Starting from a basic MSP design, we propose four optimizations, which work together to increase the localization accuracy. We address several interesting issues, such as incomplete (partial) node sequences and sequence flip, found in the Mirage test-bed we built. We have evaluated the MSP system through theoretical analysis, extensive simulation as well as two physical systems (an indoor version with 46 MICAz motes and an outdoor version with 20 MICAz motes). This evaluation demonstrates that MSP can achieve an accuracy within one foot, requiring neither additional costly hardware on sensor nodes nor precise event distribution. It also provides a nice tradeoff between physical cost (anchors) and soft cost (events), while maintaining localization accuracy.", "references": ["CC2420 Data Sheet", "Radar: An In-Building RF-Based User Location and Tracking System", "Localizing A Sensor Network via Collaborative Processing of Global Stimuli", "GPS-Less Low Cost Outdoor Localization for Very Small Devices", "Overview of Sensor Networks", "Fine-Grained Network Time Synchronization Using Reference Broadcasts", "Localization in Sparse Networks Using Sweeps", "Range-Free Localization Schemes in Large-Scale Sensor Networks", "Elapsed Time on Arrival: A Simple and Versatile Primitive for Canonical Time Synchronization Services", "SeRLoc: Secure Range-Independent Localization for Wireless Sensor Networks", "Radio Interferometric Geolocation", "The Flooding Time Synchronization Protocol", "Robust Distributed Network Localization with Noise Range Measurements", "An Algorithm for Group Formation in an Amorphous Computer", "Ad-Hoc Positioning System", "Ad hoc Positioning System (APS) Using AOA", "The Cricket Location-Support System", "The Lighthouse Location System for Smart Dust", "Dynamic Fine-Grained Localization in Ad-Hoc Networks of Sensors", "A High-Accuracy, Low-Cost Localization System for Wireless Sensor Networks", "StarDust: a Flexible Architecture for Passive Localization in Wireless Sensor Networks", "Plane Division by Lines", "Global Positions System: Theory and Practice", "The Design of Calamari: an Ad-hoc Localization System for Sensor Networks", "MSP Evaluation and Implementation Report", "Impact of Radio Irregularity on Wireless Sensor Networks"], "title": "MSP: Multi-Sequence Positioning of Wireless Sensor Nodes"}
{"id": "C-45", "keywords": "wireless sensor network;local", "abstract": "The problem of localization in wireless sensor networks where nodes do not use ranging hardware, remains a challenging problem, when considering the required location accuracy, energy expenditure and the duration of the localization phase. In this paper we propose a framework, called StarDust, for wireless sensor network localization based on passive optical components. In the StarDust framework, sensor nodes are equipped with optical retro-reflectors. An aerial device projects light towards the deployed sensor network, and records an image of the reflected light. An image processing algorithmis developed for obtaining the locations of sensor nodes. For matching a node ID to a location we propose a constraint-based label relaxation algorithm. We propose and develop localization techniques based on four types of constraints: node color, neighbor information, deployment time for a node and deployment location for a node. We evaluate the performance of a localization system based on our framework by localizing a network of 26 sensor nodes deployed in a 120-60ft2 area. The localization accuracy ranges from 2 ft to 5 ft while the localization time ranges from 10 milliseconds to 2 minutes.", "references": ["An energy-efficient surveillance system using wireless sensor networks", "Sensor network-based counter-sniper system", "A line in the sand: A wireless sensor network for trage detection, classification and tracking", "An analysis of a large scale habitat monitoring application", "A wireless sensor network for structural monitoring", "Dynamic fine-grained localization in ad-hoc networks of sensors", "The cricket location-support system", "Localizing a sensor network via collaborative processing of global stimuli", "Radar: An in-building rf-based user location and tracking system", "Mobile-assisted topology generation for autolocalization in sensor networks", "Node localization using mobile robots in delay-tolerant sensor networks", "Locationing in distribued ad-hoc wireless sensor networks", "Radio interferometric geolocation", "The effects of ranging noise on multi-hop localization: An empirical study", "Resilient localization for sensor networks in outdoor environment", "Probability grid: A location estimation scheme for wireless sensor networks", "GPS-less low cost outdoor localization for very small devices", "Range-Free localization schemes in large scale sensor networks", "Organizing a global coordinate system from local information on an ad hoc sensor network", "Ad-hoc positioning system", "A high-accuracy low-cost localization system for wireless sensor networks", "The lighthouse location system for smart dust", "A versatile camera calibration technique for high-accuracy 3d machine vision metrology using off-the-shelf tv cameras and lenses", "Spatial and temporal distributions of U.S. winds and wind power at 80m derived from measurements", "Team for advanced flow simulation and modeling", "3-D computation of parachute fluid-structure interactions - performance and control", "Technical manual for searchlight infrared"], "title": "StarDust: A Flexible Architecture for Passive Localization in Wireless Sensor Networks"}
{"id": "C-46", "keywords": "wireless sensor network;archiv storag;index method", "abstract": "Archival storage of sensor data is necessary for applications that query, mine, and analyze such data for interesting features and trends. We argue that existing storage systems are designed primarily for flat hierarchies of homogeneous sensor nodes and do not fully exploit the multi-tier nature of emerging sensor networks, where an application can comprise tens of tethered proxies, each managing tens to hundreds of untethered sensors. We present TSAR, a fundamentally different storage architecture that envisions separation of data from metadata by employing local archiving at the sensors and distributed indexing at the proxies. At the proxy tier, TSAR employs a novel multi-resolution ordered distributed index structure, the Interval Skip Graph, for efficiently supporting spatio-temporal and value queries. At the sensor tier,TSAR supports energy-aware adaptive summarization that can trade off the cost of transmitting metadata to the proxies against the overhead of false hits resulting from querying a coarse-grain index. We implement TSAR in a two-tier sensor testbed comprising Stargate-based proxies and Mote-based sensors. Our experiments demonstrate the benefits and feasibility of using our energy-efficient storage architecture in multi-tier sensor networks.", "references": ["Skip graphs", "Multidimensional binary search trees used for associative searching", "Towards sensor database systems", "ZigBee-ready RF transceiver", "Introduction to Algorithms", "Querying Peer-to-Peer Networks Using P-Trees", "ELF: an efficient log-structured flash file system for micro sensor nodes", "PRESTO: A predictive storage architecture for sensor networks", "An evaluation of multi-resolution storage in sensor networks", "A system for simulation, emulation, and deployment of heterogeneous sensor networks", "DIFS: A distributed index for features in sensor networks", "R-trees: a dynamic index structure for spatial searching", "Skipnet: A scalable overlay network with practical locality properties", "System architecture directions for networked sensors", "Samsung Semiconductor", "Directed diffusion: A scalable and robust communication paradigm for sensor networks", "Multi-dimensional range queries in sensor networks", "RP*: A family of order preserving scalable distributed data structures", "TAG: a tiny aggregation service for ad-hoc sensor networks", "High performance, low power sensor platforms featuring gigabyte scale storage", "Versatile low power media access for wireless sensor networks", "Skip lists: a probabilistic alternative to balanced trees", "Data-centric storage in sensornets", "A scalable content addressable network", "GHT - a geographic hash-table for data-centric storage"], "title": "TSAR: A Two Tier Sensor Storage Architecture Using Interval Skip Graphs"}
{"id": "C-48", "keywords": "dim;multi-dimension rang queri", "abstract": "In many sensor networks, data or events are named by attributes. Many of these attributes have scalar values, so one natural way to query events of interest is to use a multi-dimensional range query. An example is: "List all events whose temperature lies between 50 and 60, and whose light levels lie between 10 and 15." Such queries are useful for correlating events occurring within the network.In this paper, we describe the design of a distributed index that scalably supports multi-dimensional range queries. Our distributed index for multi-dimensional data (or DIM) uses a novel geographic embedding of a classical index data structure, and is built upon the GPSR geographic routing algorithm. Our analysis reveals that, under reasonable assumptions about query distributions, DIMs scale quite well with network size (both insertion and query costs scale as O(N)). In detailed simulations, we show that in practice, the insertion and query costs of other alternatives are sometimes an order of magnitude more than the costs of DIMs, even for moderately sized network. Finally, experiments on a small scale testbed validate the feasibility of DIMs.", "references": ["Skip Graphs", "Multidimensional Binary Search Trees Used for Associative Searching", "Towards Sensor Database Systems", "Freenet: A Distributed Anonymous Information Storage and Retrieval System", "The Ubiquitous B-tree", "Quad Trees: A Data Structure for Retrieval on Composite Keys", "DIMENSIONS: Why do we need a new Data Handling architecture for Sensor Networks?", "Similarity Search in High Dimensions via Hashing", "The Sensor Network as a Database", "DIFS: A Distributed Index for Features in Sensor Networks", "R-trees: A Dynamic Index Structure for Spatial Searching", "Complex Queries in DHT-based Peer-to-Peer Networks", "Approximate Nearest Neighbors: Towards Removing the Curse of Dimensionality", "Locality-preserving Hashing in Multidimensional Spaces", "Directed Diffusion: A Scalable and Robust Communication Paradigm for Sensor Networks", "GPSR: Greedy Perimeter Stateless Routing for Wireless Networks", "The Design of an Acquisitional Query Processor for Sensor Networks", "TAG: a Tiny AGregation Service for Ad-Hoc Sensor Networks", "A Scalable Content-Addressable Network", "GHT: A Geographic Hash Table for Data-Centric Storage", "Spatial Data Structures", "On the Effect of Localization Errors on Geographic Face Routing in Sensor Networks", "Data-Centric Storage in Sensornets", "Chord: A Scalable Peer-To-Peer Lookup Service for Internet Applications", "A Two-Tier Data Dissemination Model for Large-scale Wireless Sensor Networks"], "title": "Multi-dimensional Range Queries in Sensor Networks"}
{"id": "C-49", "keywords": "opportunist network;rout;simul", "abstract": "Traditional mobile ad hoc network (MANET) routing protocols assume that contemporaneous end-to-end communication paths exist between data senders and receivers. In some mobile ad hoc networks with a sparse node population, an end-to-end communication path may break frequently or may not exist at anytime. Many routing protocols have been proposed in the literature to address the problem, but few were evaluated in a realistic "opportunistic" network setting. We use simulation and contact traces (derived from logs in a production network) to evaluate and compare five existing protocols: direct-delivery, epidemic, random, PRoPHET, and Link-State, as well as our own proposed routing protocol. We show that the direct delivery and epidemic routing protocols suffer either low delivery ratio or high resource usage, and other protocols make tradeoffs between delivery ratio and resource usage.", "references": ["MaxProp: routing for vehicle-based disruption-tolerant networks", "Delay-tolerant networking: An approach to interplanetary Internet", "A survey of mobility models for ad hoc network research", "People-centric urban sensing", "Impact of human mobility on the design of opportunistic forwarding algorithms", "A delay-tolerant network architecture for challenged internets", "The changing usage of a mature campus-wide wireless network", "Pocket switched networks and human mobility in conference environments", "Model T: an empirical model for user registration patterns in a campus wireless LAN", "Using redundancy to cope with failures in a delay tolerant network", "Energy-efficient computing for wildlife tracking: Design tradeoffs and early experiences with ZebraNet", "Analysis of a campus-wide wireless network", "CRAWDAD data set dartmouth/campus", "Knowledge-based opportunistic forwarding in vehicular wireless ad hoc networks", "Evaluating mobility pattern space routing for DTNs", "Probabilistic routing in intermittently connected networks", "Adaptive routing for intermittently connected mobile ad hoc networks", "One laptop per child project", "Highly dynamic destination-sequenced distance-vector routing (DSDV) for mobile computers", "Ad-hoc on-demand distance vector routing", "Evaluating next-cell predictors with extensive Wi-Fi mobility data", "An empirical evaluation of the student-net delay tolerant network", "Epidemic routing for partially-connected ad hoc networks", "Erasure-coding based routing for opportunistic networks", "DFT-MSN: the delay fault tolerant mobile sensor network for pervasive information gathering"], "title": "Evaluating Opportunistic Routing Protocols with Large Realistic Contact Traces"}
{"id": "C-50", "keywords": "search and rescu;sensor network", "abstract": "This paper describes the design, implementation and evaluation of a search and rescue system called CenWits. CenWits uses several small, commonly-available RF-based sensors, and a small number of storage and processing devices. It is designed for search and rescue of people in emergency situations in wilderness areas. A key feature of CenWits is that it does not require a continuously connected sensor network for its operation. It is designed for an intermittently connected network that provides only occasional connectivity. It makes a judicious use of the combined storage capability of sensors to filter, organize and store important information, combined battery power of sensors to ensure that the system remains operational for longer time periods, and intermittent network connectivity to propagate information to a processing center. A prototype of CenWits has been implemented using Berkeley Mica2 motes. The paper describes this implementation and reports on the performance measured from it.", "references": ["based tracking system", "Brent geese 2002", "The onstar system", "Personal locator beacons with GPS receiver and satellite transmitter", "Personal tracking using GPS and GSM system", "Rf based kid tracking system", "Performance measurements with motes technology", "RADAR: An in-building RF-based user location and tracking system", "A delay-tolerant network architecture for challenged internets", "Radio triggered wake-up capability for sensor networks", "Location systems for ubiquitous computing", "Lifetch life saving system", "Energy-efficient computing for wildlife tracking: design tradeoffs and early experiences with ZebraNet", "Energy harvesting aware power management", "Embedding the internet: wireless integrated network sensors", "A study of low-level vibrations as a power source for wireless sensor networks", "Locationing in distributed ad-hoc wireless sensor networks", "Simulating the power consumption of large-scale sensor network applications", "Active badges and personal interactive computing objects", "Programming sensor networks using abstract regions"], "title": "CenWits: A Sensor-Based Loosely Coupled Search and Rescue System Using Witnesses"}
{"id": "C-52", "keywords": "distribut multi-player game;fair;dead-reckon;clock synchron;network delai", "abstract": "In a distributed multi-player game that uses dead-reckoning vectors to exchange movement information among players, there is inaccuracy in rendering the objects at the receiver due to network delay between the sender and the receiver. The object is placed at the receiver at the position indicated by the dead-reckoning vector, but by that time, the real position could have changed considerably at the sender. This inaccuracy would be tolerable if it is consistent among all players; that is, at the same physical time, all players see inaccurate (with respect to the real position of the object) but the same position and trajectory for an object. But due to varying network delays between the sender and different receivers, the inaccuracy is different at different players as well. This leads to unfairness in game playing. In this paper, we first introduce an "error" measure for estimating this inaccuracy. Then we develop an algorithm for scheduling the sending of dead-reckoning vectors at a sender that strives to make this error equal at different receivers over time. This algorithm makes the game very fair at the expense of increasing the overall mean error of all players. To mitigate this effect, we propose a budget based algorithm that provides improved fairness without increasing the mean error thereby maintaining the accuracy of game playing. We have implemented both the scheduling algorithm and the budget based algorithm as part of BZFlag, a popular distributed multi-player game. We show through experiments that these algorithms provide fairness among players in spite of widely varying network delays. An additional property of the proposed algorithms is that they require less number of DRs to be exchanged (compared to the current implementation of BZflag) to achieve the same level of accuracy in game playing.", "references": ["Accuracy in Dead-Reckoning based Distributed Multi-Player Games", "Design and Evaluation of MiMaze, a Multiplayer Game on the Internet", "Consistency in Replicated Continuous Interactive Media", "Exploiting Position History for Efficient Remote Rendering in Networked Virtual Reality", "A Distributed Architecture for Multiplayer Interactive Applications on the Internet", "On the Impact of Delay on Real-Time Multiplayer Games", "Sync-MS: Synchronized Messaging Service for Real-Time Multi-Player Distributed Games", "A Fair Message Exchange Framework for Distributed Multi-Player Games", "Cheat-Proof Playout for Centralized and Distributed Online Games", "On Estimating End-to-End Network Path Properties", "BZFlag Game", "NIST Net"], "title": "Fairness in Dead-Reckoning based Distributed Multi-Player Games"}
{"id": "C-53", "keywords": "distribut multi-player game;continu replic applic;consist;dead-reckon;local lag", "abstract": "Dead-Reckoning (DR) is an effective method to maintain consistency for Continuous Distributed Multiplayer Games (CDMG). Since DR can filter most unnecessary state updates and improve the scalability of a system, it is widely used in commercial CDMG. However, DR cannot maintain high consistency, and this constrains its application in highly interactive games. With the help of global synchronization, DR can achieve higher consistency, but it still cannot eliminate before inconsistency. In this paper, a method named Globally Synchronized DR with Local Lag (GS-DR-LL), which combines local lag and Globally Synchronized DR (GS-DR), is presented. Performance evaluation shows that GS-DR-LL can effectively decrease before inconsistency, and the effects increase with the lag.", "references": ["Local-Lag and Timewarp: Providing Consistency for Replicated Continuous Applications", "Supporting Continuous Consistency in Multiplayer Online Games", "On the Suitability of Dead Reckoning Schemes for Games", "An Experimental Study on the Effects of Network Delay in Cooperative Shared Haptic Virtual Environment", "On the Impact of Delay on Real-Time Multiplayer Games", "Effect of Latency on Presence in Stressful Virtual Environments", "Latency Compensation Methods in Client/Server In-Game Protocol Design and Optimization", "Accuracy in Dead-Reckoning based Distributed Multi-Player Games", "From Causal Consistency to Sequential Consistency in Shared Memory Systems", "Causal Memory", "Linearizability: a Correctness Condition for Concurrent Objects", "Axioms for Memory Access in Asynchronous Hardware Systems", "Is 100 Milliseconds too Fast", "Effects of Local-Lag Mechanism on Cooperation Performance in a Desktop CVE System", "Echo: a Method to Improve the Interaction Quality of CVEs"], "title": "Globally Synchronized Dead-Reckoning with Local Lag for Continuous Distributed Multiplayer Games"}
{"id": "C-54", "keywords": "gi;internet;client-server;peer-to-peer", "abstract": "Enterprises in the public and private sectors have been making their large spatial data archives available over the Internet. However, interactive work with such large volumes of online spatial data is a challenging task. We propose two efficient approaches to remote access to large spatial data. First, we introduce a client-server architecture where the work is distributed between the server and the individual clients for spatial query evaluation, data visualization, and data management. We enable the minimization of the requirements for system resources on the client side while maximizing system responsiveness as well as the number of connections one server can handle concurrently. Second, for prolonged periods of access to large online data, we introduce APPOINT (an Approach for Peer-to-Peer Offloading the INTernet). This is a centralized peer-to-peer approach that helps Internet users transfer large volumes of online data efficiently. In APPOINT, active clients of the client-server architecture act on the server's behalf and communicate with each other to decrease network latency, improve service bandwidth, and resolve server congestions.", "references": ["Fedstats: The gateway to statistics from over 100 U.S. federal agencies", "Arcinfo: Scalable system of software for geographic data creation, management, integration, analysis, and dissemination", "Extensible markup language", "Geography markup language", "Mapquest: Consumer-focused interactive mapping site on the web", "Mapsonus: Suite of online geographic services", "The Eternity Service", "Web caching and Zipf-like distributions: Evidence and implications", "Realtime visualization of large images over a thinwire", "Wide-area cooperative storage with CFS", "Web cache coherence", "Experience with SAND/Tcl: a scripting tool for spatial databases", "Squirrel: A decentralized peer-to-peer Web cache", "Web caching with consistent hashing", "OceanStore: An architecture for global-scale persistent store", "Maps alive: viewing geospatial information on the WWW", "Not all hits are created equal: Cooperative proxy caching over a wide-area network", "Storage management and caching in PAST, a large-scale, persistent peer-to-peer storage utility", "Applications of Spatial Data Structures: Computer Graphics", "The Design and Analysis of Spatial Data Structures", "Pyramidal parametrics"], "title": "Remote Access to Large Spatial Databases"}
{"id": "C-55", "keywords": "context awar;group interact;locat sens;sensor fusion", "abstract": "In this paper, we present an implemented system for supporting group interaction in mobile distributed computing environments. First, an introduction to context computing and a motivation for using contextual information to facilitate group interaction is given. We then present the architecture of our system, which consists of two parts: a subsystem for location sensing that acquires information about the location of users as well as spatial proximities between them, and one for the actual context-aware application, which provides services for group interaction.", "references": ["Modeling Context-aware Behavior by Interpreted ECA Rules", "Context-Aware Applications: From the Laboratory to the Marketplace", "A Survey of Context-Aware Mobile Computing Research", "Providing Architectural Support for Building Context-Aware Applications", "Location Modeling: State of the Art and Challenges", "Workspace Awareness in Mobile Virtual Teams", "Coordination in Pervasive Computing Environments", "Supporting Location Awareness in Open Distributed Systems", "Enhanced Reality Fieldwork: the Context-Aware Archaeological Assistant", "Disseminating Active Map Information to Mobile Hosts", "A System Architecture for Context-Aware Mobile Computing", "Supporting Persistent Social Groups in Ubiquitous Computing Environments Using Context-Aware Ephemeral Group Service", "The Stick-e Note Architecture: Extending the Interface Beyond the User", "CybreMinder: A Context-Aware System for Supporting Re-minders"], "title": "Context Awareness for Group Interaction Support"}
{"id": "C-56", "keywords": "grid comput;process support;distribut middlewar", "abstract": "Grid is an emerging infrastructure used to share resources among virtual organizations in a seamless manner and to provide breakthrough computing power at low cost. Nowadays there are dozens of academic and commercial products that allow execution of isolated tasks on grids, but few products support the enactment of long-running processes in a distributed fashion. In order to address such subject, this paper presents a programming model and an infrastructure that hierarchically schedules process activities using available nodes in a wide grid environment. Their advantages are automatic and structured distribution of activities and easy process monitoring and steering.", "references": ["WebFlow: High-Level Programming Environment and Visual Authoring Toolkit for High Performance Distributed Computing", "Specification: Business Process Execution Language for Web Services", "OPERA-G:A Microkernel for Computational Grids", "Extensible Markup Language", "GridFlow: Workflow Management for Grid Computing", "Globus: A Metacomputing Infrastructure Toolkit", "The Physiology of the Grid: An Open Grid Services Architecture for Distributed Systems Integration", "The Anatomy of the Grid: Enabling Scalable Virtual Organization", "Condor-G: A Computational Management Agent for Multi-institutional Grids", "Legion - A View from 50,000 Feet", "The Java Virtual Machine Specification", "Grid Computing with Active Services", "Enacting Business Processes in a Decentralised Environment with P2P-Based Workflow Support"], "title": "A Hierarchical Process Execution Support for Grid Computing"}
{"id": "C-57", "keywords": "congest game;load-depend resourc failur;pure strategi nash equilibrium", "abstract": "We define a new class of games, congestion games with loaddependent failures (CGLFs), which generalizes the well-known class of congestion games, by incorporating the issue of resource failures into congestion games. In a CGLF, agents share a common set of resources, where each resource has a cost and a probability of failure. Each agent chooses a subset of the resources for the execution of his task, in order to maximize his own utility. The utility of an agent is the difference between his benefit from successful task completion and the sum of the costs over the resources he uses. CGLFs possess two novel features. It is the first model to incorporate failures into congestion settings, which results in a strict generalization of congestion games. In addition, it is the first model to consider load-dependent failures in such framework, where the failure probability of each resource depends on the number of agents selecting this resource. Although, as we show, CGLFs do not admit a potential function, and in general do not have a pure strategy Nash equilibrium, our main theorem proves the existence of a pure strategy Nash equilibrium in every CGLF with identical resources and nondecreasing cost functions.", "references": ["Pure nash equilibria in player-specific and weighted congestion games", "The price of anarchy of finite congestion games", "The complexity of pure nash equilibria", "Worst-case equilibria", "Local-effect games", "Congestion games with player-specific payoff functions", "Solution-based congestion games", "Multipotential games", "Potential games", "Congestion games with failures", "A class of games possessing pure-strategy nash equilibria", "How bad is selfish routing"], "title": "Congestion Games with Load-Dependent Failures: Identical Resources"}
{"id": "C-58", "keywords": "inform manag system;distribut hash tabl;network system monitor", "abstract": "We present a Scalable Distributed Information Management System (SDIMS) that aggregates information about large-scale networked systems and that can serve as a basic building block for a broad range of large-scale distributed applications by providing detailed views of nearby information and summary views of global information. To serve as a basic building block, a SDIMS should have four properties: scalability to many nodes and attributes, flexibility to accommodate a broad range of applications, administrative isolation for security and availability, and robustness to node and network failures. We design, implement and evaluate a SDIMS that (1) leverages Distributed Hash Tables (DHT) to create scalable aggregation trees, (2) provides flexibility through a simple API that lets applications control propagation of reads and writes, (3) provides administrative isolation through simple extensions to current DHT algorithms, and (4) achieves robustness to node and network reconfigurations through lazy reaggregation, on-demand reaggregation, and tunable spatial replication. Through extensive simulations and micro-benchmark experiments, we observe that our system is an order of magnitude more scalable than existing approaches, achieves isolation properties at the cost of modestly increased read latency in comparison to flat DHTs, and gracefully handles failures.", "references": ["Join and Leave in Peer-to-Peer Systems: The DASIS approach", "Processes in KaffeOS: Isolation, Resource Management, and Sharing in Java", "Resource Containers: A New Facility for Resource Management in Server Systems", "Cone: A Distributed Heap-Based Approach to Resource Selection", "The Surprising Power of Epidemic Communication", "Space time tradeoffs in hash coding with allowable errors", "Exploiting Network Proximity in Peer-to-Peer Overlay Networks", "SplitStream: High-bandwidth Multicast in a Cooperative Environment", "SCRIBE: A Large-scale and Decentralised Application-level Multicast Infrastructure", "Serving DNS using a Peer-to-Peer Lookup Service", "PRACTI replication for large-scale systems", "Bitmap algorithms for counting active flows on high speed links", "SHARP: An architecture for secure resource peering", "Ganglia: Distributed Monitoring and Execution System", "What Can Peer-to-Peer Do for Databases, and Vice Versa?", "The Impact of DHT Routing Geometry on Resilience and Proximity", "SkipNet: A Scalable Overlay Network with Practical Locality Properties", "Querying the Internet with PIER", "Directed diffusion: a scalable and robust communication paradigm for sensor networks", "TAG: a Tiny AGgregation Service for Ad-Hoc Sensor Networks", "Dynamic Lookup Networks", "The ganglia distributed monitoring system: Design, implementation, and experience", "Kademlia: A Peer-to-peer Information System Based on the XOR Metric", "Offering a precision-performance tradeoff for aggregation queries over replicated data", "Flexible Update Propagation for Weakly Consistent Replication", "Planetlab", "Accessing Nearby Copies of Replicated Objects in a Distributed Environment", "A Scalable Content Addressable Network", "Routing Algorithms for DHTs: Some Open Questions", "InfoSpect: Using a Logic Language for System Health Monitoring in Distributed Systems", "Pastry: Scalable, Distributed Object Location and Routing for Large-scale Peer-to-peer Systems", "Application-level Multicast using Content-addressable Networks", "SNMP, SNMPv2, and CMIP", "Chord: A scalable Peer-To-Peer lookup service for internet applications", "Bayeux: An Architecture for Scalable and Fault-tolerant Wide-Area Data Dissemination", "IBM Tivoli Monitoring", "Astrolabe: A Robust and Scalable Technology for Distributed System Monitoring, Management, and Data Mining", "Willow: DHT, Aggregation, and Publish", "Bandwidth constrained placement in a wan", "Potential costs and benefits of long-term prefetching for content-distribution", "Sophia: An Information Plane for Networked Systems", "The network weather service: A distributed resource performance forecasting service for metacomputing", "SDIMS: A scalable distributed information management system", "SOMO: Self-Organized Metadata Overlay for Resource Management in P2P DHT", "Tapestry: An Infrastructure for Fault-tolerant Wide-area Location and Routing"], "title": "A Scalable Distributed Information Management System"}
{"id": "C-61", "keywords": "mmog;distribut multi-player game;author;commun proxi;latenc compens", "abstract": "We present a proxy-based gaming architecture and authority assignment within this architecture that can lead to better game playing experience in Massively Multi-player Online games. The proposed game architecture consists of distributed game clients that connect to game proxies (referred to as "communication proxies") which forward game related messages from the clients to one or more game servers. Unlike proxy-based architectures that have been proposed in the literature where the proxies replicate all of the game state, the communication proxies in the proposed architecture support clients that are in proximity to it in the physical network and maintain information about selected portions of the game space that are relevant only to the clients that they support. Using this architecture, we propose an authority assignment mechanism that divides the authority for deciding the outcome of different actions/events that occur within the game between client and servers on a per action/event basis. We show that such division of authority leads to a smoother game playing experience by implementing this mechanism in a massively multi-player online game called RPGQuest. In addition, we argue that cheat detection techniques can be easily implemented at the communication proxies if they are made aware of the game-play mechanics.", "references": ["Latency Compensation Methods in Client Server In-game Protocol Design and Optimization", "On the impact of delay on real-time multiplayer games", "Sensitivity of Quake3 Players to Network Latency", "The effect of latency and network limitations on mmorpgs: a field study of everquest2", "The effects of loss and latency on user performance in unreal tournament 2003", "Sync-MS: Synchronized Messaging Service for Real-Time Multi-Player Distributed Games", "A fair message exchange framework for distributed multi-player games", "Multiplayer Game Programming", "The fun of using tcp for an mmorpg", "Accuracy in dead-reckoning based distributed multi-player games", "Fairness in dead-reckoning based distributed multi-player games", "How to keep a dead man from shooting", "Massively Multiplayer Game Development 2", "Bandwidth requirement and state consistency in three multiplayer game architectures", "A Generic Proxy Systems for Networked Computer Games", "A Proxy Server Network Architecture for Real-Time Computer Games", "Zoned Federation of Game Servers: A Peer-to-Peer Approach to Scalable Multiplayer On-line Games", "A Framework for a Fidelity Based Agent Architecture for Distributed Interactive Simulation", "Hierarchical Structuring for Distributed Interactive Simulation", "Even Balance", "Trust and Reputation Model in Peer-to-Peer Networks"], "title": "Authority Assignment in Distributed Multi-Player Proxy-based Games"}
{"id": "C-62", "keywords": "innov;commodit;monitor;contract;incent", "abstract": "Today's Internet industry suffers from several well-known pathologies, but none is as destructive in the long term as its resistance to evolution. Rather than introducing new services, ISPs are presently moving towards greater commoditization. It is apparent that the network's primitive system of contracts does not align incentives properly. In this study, we identify the network's lack of accountability as a fundamental obstacle to correcting this problem: Employing an economic model, we argue that optimal routes and innovation are impossible unless new monitoring capability is introduced and incorporated with the contracting system. Furthermore, we derive the minimum requirements a monitoring system must meet to support first-best routing and innovation characteristics. Our work does not constitute a new protocol; rather, we provide practical and specific guidance for the design of monitoring systems, as well as a theoretical framework to explore the factors that influence innovation.", "references": ["Using Repeated Games to Design Incentive-Based Routing Systems", "On the Benefits and Feasibility of Incentive Based Routing Infrastructure", "Providing Packet Obituaries", "The Design Philosophy of the DARPA Internet Protocols", "Tussle in cyberspace: Defining tomorrow's internet", "Interconnection Agreements: Strategic Behaviour and Property Rights", "A BGP-based Mechanism for Lowest-Cost Routing", "Interconnection, Peering, and Settlements", "On the Interaction Between Overlay Routing and Traffic Engineering", "Pricing the Internet", "Towards an Evolvable Internet Architecture", "Economics of Network Pricing with Multiple ISPs"], "title": "Network Monitors and Contracting Systems: Competition and Innovation"}
{"id": "C-65", "keywords": "sensor network;data fusion;acoust sourc local;weapon classif;calib estim", "abstract": "The paper presents a wireless sensor network-based mobilecountersniper system. A sensor node consists of a helmetmountedmicrophone array, a COTS MICAz mote for internodecommunication and a custom sensorboard that implementsthe acoustic detection and Time of Arrival (ToA) estimationalgorithms on an FPGA. A 3-axis compass providesself orientation and Bluetooth is used for communicationwith the soldier's PDA running the data fusion and the userinterface. The heterogeneous sensor fusion algorithm canwork with data from a single sensor or it can fuse ToA orAngle of Arrival (AoA) observations of muzzle blasts andballistic shockwaves from multiple sensors. The system estimatesthe trajectory, the range, the caliber and the weapontype. The paper presents the system design and the resultsfrom an independent evaluation at the US Army AberdeenTest Center. The system performance is characterized by 1-degree trajectory precision and over 95% caliber estimationaccuracy for all shots, and close to 100% weapon estimationaccuracy for 4 out of 6 guns tested.", "references": ["BBN technologies website", "Acoustic sniper localization", "Fixed and wearable acoustic counter--sniper systems for law enforcement", "Description of muzzle blast by modified ideal scaling models", "The nesC language: a holistic approach to networked embedded systems", "System architecture directions for networked sensors", "Node-density independent localization", "Countersniper system for urban warfare", "Directed flood-routing framework for wireless sensor networks", "Shockwave and muzzle blast classification via joint time frequency and wavelet analysis", "TinyOS Hardware Platforms", "Crossbow MICAz", "PicoBlaze User Resources", "Optimal and wavelet-based shock wave detection and estimation", "On the scalability of routing-integrated time synchronization", "ShotSpotter website", "Sensor network-based countersniper system", "Measurements of small-caliber ballistic shock waves in air", "Technology evaluations and performance metrics for soldier-worn sensors for assist", "Flow pattern of a supersonic projectile"], "title": "Shooter Localization and Weapon Classification with Soldier-Wearable Networked Sensors"}
{"id": "C-66", "keywords": "web servic;qo;workow;schedul;heurist", "abstract": "Web services can be aggregated to create composite workflows that provide streamlined functionality for human users or other systems. Although industry standards and recent research have sought to define best practices and to improve end-to-end workflow composition, one area that has not fully been explored is the scheduling of a workflow's web service requests to actual service provisioning in a multi-tiered, multi-organisation environment. This issue is relevant to modern business scenarios where business processes within a workflow must complete within QoS-defined limits. Because these business processes are web service consumers, service requests must be mapped and scheduled across multiple web service providers, each with its own negotiated service level agreement. In this paper we provide heuristics for scheduling service requests from multiple business process workflows to web service providers such that a business value metric across all workflows is maximised. We show that a genetic search algorithm is appropriate to perform this scheduling, and through experimentation we show that our algorithm scales well up to a thousand workflows and produces better mappings than traditional approaches.", "references": ["DAML-S: Semantic Markup For Web Services,", "Job Shop Scheduling with Genetic Algorithms", "A Promising Genetic Algorithm Approach to Job-Shop Scheduling, Rescheduling, and Open-Shop Scheduling Problems", "Computers and Intractability: A Guide to the Theory of NP-Completeness", "Adaptation in Natural and Artificial Systems: An Introductory Analysis with Applications to Biology, Control, and Artificial Intelligence", "Genetic Algorithms in Search, Optimization and Machine Leaming", "Business Processes in a Web Services World", "Back-end Databases in Shared Dynamic Content Server Clusters", "Web Service Composition Current Solutions and Open Problems", "Dynamic Provisioning of Multi-Tier Internet Applications", "Quality Driven Web Services Composition"], "title": "Heuristics-Based Scheduling of Composite Web Service Workloads"}
{"id": "C-67", "keywords": "macintosh os x;xgrid;grid comput;cluster;highperform comput;rendezv;render", "abstract": "The Ringling School of Art and Design is a fully accredited four-year college of visual arts and design. With a student to computer ratio of better than 2-to-1, the Ringling School has achieved national recognition for its large-scale integration of technology into collegiate visual art and design education. We have found that Mac OS X is the best operating system to train future artists and designers. Moreover, we can now buy Macs to run high-end graphics, nonlinear video editing, animation, multimedia, web production, and digital video applications rather than expensive UNIX workstations. As visual artists cross from paint on canvas to creating in the digital realm, the demand for a high-performance computing environment grows. In our public computer laboratories, students use the computers most often during the workday; at night and on weekends the computers see only light use. In order to harness the lost processing time for tasks such as video rendering, we are testing Xgrid, a suite of Mac OS X applications recently developed by Apple for parallel and distributed high-performance computing. As with any new technology deployment, IT managers need to consider a number of factors as they assess, plan, and implement Xgrid. Therefore, we would like to share valuable information we learned from our implementation of an Xgrid environment with our colleagues. In our report, we will address issues such as assessing the needs for grid computing, potential applications, management tools, security, authentication, integration into existing infrastructure, application support, user training, and user support. Furthermore, we will discuss the issues that arose and the lessons learned during and after the implementation process.", "references": ["Apple Academic Research", "Search for Extraterrestrial Intelligence at home", "Alias", "Apple Computer, Xgrid", "Xgrid Guide", "Apple Mac OS X Features", "Xgrid Manual Page", "Xgrid Presentation", "Research Systems Unix Group", "Using the Radmind Command Line Tools to Maintain Multiple Mac OS X Machines", "POV-Ray", "Xgrid example: Parallel graphics rendering in POVray", "NEESgrid", "SAP", "Platform Computing", "Grid", "United Devices", "N1 Grid Engine 6", "Xgrig Users Mailing List"], "title": "A Holistic Approach to High-Performance Computing: Xgrid Experience"}
{"id": "C-68", "keywords": "mobil;vehicular network;automaton;latenc;replac polici;data carrier;markov model", "abstract": "On-demand delivery of audio and video clips in peer-to-peer vehicular ad-hoc networks is an emerging area of research. Our target environment uses data carriers, termed zebroids, where a mobile device carries a data item on behalf of a server to a client thereby minimizing its availability latency. In this study, we quantify the variation in availability latency with zebroids as a function of a rich set of parameters such as car density, storage per device, repository size, and replacement policies employed by zebroids. Using analysis and extensive simulations, we gain novel insights into the design of carrier-based systems. Significant improvements in latency can be obtained with zebroids at the cost of a minimal overhead. These improvements occur even in scenarios with lower accuracy in the predictions of the car routes. Two particularly surprising findings are: (1) a naive random replacement policy employed by the zebroids shows competitive performance, and (2) latency improvements obtained with a simplified instantiation of zebroids are found to be robust to changes in the popularity distribution of the data items.", "references": ["Corridor simulation", "Reversible markov chains and random walks on graphs", "Mobile Users: To Update or Not to Update", "MaxProp: Routing for Vehicle-Based Disruption-Tolerant Networking", "Replication Strategies in Unstructured Peer-to-Peer Networks", "Buffering and Caching in Large-Scale Video Servers", "PAVAN: A Policy Framework for Content Availabilty in Vehicular Ad-hoc Networks", "Comparison of Replication Strategies for Content Availability in C2P2 networks", "An Evaluation of Availability Latency in Carrier-based Vehicular Ad-hoc Networks", "C2p2: A peer-to-peer network for on-demand automobile information services", "Effective Replica Allocation in Ad Hoc Networks for Improving Data Accessibility", "A Replica Allocation Method Adapting to Topology Changes in Ad Hoc Networks", "Energy-efficient computing for wildlife tracking: design tradeoffs and early experiences with ZebraNet", "DakNet: Rethinking Connectivity in Developing Nations", "Cooperative Caching in Ad Hoc Networks", "Data mules: Modeling and analysis of a three-tier architecture for sparse sensor networks", "Single-Copy Routing in Intermittently Connected Mobile Networks", "Modern Operating Systems", "Epidemic routing for partially-connected ad hoc networks", "A message ferrying approach for data delivery in sparse mobile ad hoc networks", "User Mobility Modeling and Characterization of Mobility Pattern"], "title": "An Evaluation of Availability Latency in Carrier-based Vehicular ad-hoc Networks"}
{"id": "C-69", "keywords": "thin-client comput;remot displai;mobil;pervas web", "abstract": "Although web applications are gaining popularity on mobile wireless PDAs, web browsers on these systems can be quite slow and often lack adequate functionality to access many web sites. We have developed pTHINC, a PDA thin-client solution that leverages more powerful servers to run full-function web browsers and other application logic, then sends simple screen updates to the PDA for display. pTHINC uses server-side screen scaling to provide high-fidelity display and seamless mobility across a broad range of different clients and screen sizes, including both portrait and landscape viewing modes. pTHINC also leverages existing PDA control buttons to improve system usability and maximize available screen resolution for application display. We have implemented pTHINC on Windows Mobile and evaluated its performance on mobile wireless devices. Our results compared to local PDA web browsers and other thin-client approaches demonstrate that pTHINC provides superior web browsing performance and is the only PDA thin client that effectively supports crucial browser helper applications such as video playback.", "references": ["THINC: A Virtual Display Architecture for Thin-Client Computing", "Citrix Metaframe", "Microsoft Windows NT Server 4.0, Terminal Server Edition: Technical Reference", "Experience With Top Gun Wingman: A Proxy-Based Graphical Web Browser for the 3Com PalmPilot", "GoToMyPC", "Health Insurance Portability and Accountability Act", "i-Bench version 1.5", "On proxy agents, mobility, and web access", "Design and Implementation of a Soft Caching Proxy", "Improving Web Browsing on Wireless PDAs Using Thin-Client Computing", "TranSquid: Transcoding and caching proxy for heterogenous ecommerce environments", "NET VNC Viewer for PocketPC", "Measuring Thin-Client Performance Using Slow-Motion Benchmarking", "Designing Web Usability", "Opera Mini Browser", "Virtual Network Computing", "The X Window System", "Sun Secure Global Desktop", "Web Browsing Performance of Wireless Thin-Client Computing"], "title": "pTHINC: A Thin-Client Architecture for Mobile Wireless Web"}
{"id": "C-71", "keywords": "wireless sensor network;honeycomb structur;point-distribut index;sensor coverag;sensor group", "abstract": "We propose  a novel index for evaluation of point-distribution.  is the minimum distance between each pair of points normalized by the average distance between each pair of points. We find that a set of points that achieve a maximum value of  result in a honeycomb structure. We propose that  can serve as a good index to evaluate the distribution of the points, which can be employed in coverage-related problems in wireless sensor networks (WSNs). To validate this idea, we formulate a general sensorgrouping problem for WSNs and provide a general sensing model. We show that locally maximizing  at sensor nodes is a good approach to solve this problem with an algorithm called Maximizing- Node-Deduction (MIND). Simulation results verify that MIND outperforms a greedy algorithm that exploits sensor-redundancy we design. This demonstrates a good application of employing  in coverage-related problems for WSNs.", "references": ["A survey on wireless sensor networks", "Vononoi diagram - a survey of a fundamental geometric data structure", "GPS-less low-cost outdoor localization for very small devices", "Improving wireless sensor network lifetime through power aware organization", "Wireless sensor networks with energy efficient organization", "Coverage in wireless sensor networks", "A sensibility-based sleeping configuration protocol for dependable wireless sensor networks", "A quantitative measure of fairness and discrimination for resource allocation in shared computer systems", "MNP: Multihop network reprogramming service for sensor networks", "A study on the coverage of large-scale sensor networks", "Wireless sensor networks for habitat monitoring", "Explosure in wirless sensor networks: Theory and pratical solutions", "Power efficient organization of wireless sensor networks", "A node scheduling scheme for energy conservation in large wireless sensor networks", "Integrated coverage and connectivity configuration in wireless sensor networks", "Co-Grid: an efficient converage maintenance protocol for distributed sensor networks", "Differentiated surveillance for sensor networks", "PEAS: A robust energy conserving protocol for long-lived sensor networks", "A point-distribution index and its application in coverage-related problems"], "title": "A Point-Distribution Index and Its Application to Sensor-Grouping in Wireless Sensor Networks"}
{"id": "C-72", "keywords": "coordin spectrum sens;gossip protocol;fm aggreg;increment algorithm", "abstract": "Wireless radios of the future will likely be frequency-agile, that is, supporting opportunistic and adaptive use of the RF spectrum. Such radios must coordinate with each other to build an accurate and consistent map of spectral utilization in their surroundings. We focus on the problem of sharing RF spectrum data among a collection of wireless devices. The inherent requirements of such data and the time-granularity at which it must be collected makes this problem both interesting and technically challenging. We propose GUESS, a novel incremental gossiping approach to coordinated spectral sensing. It (1) reduces protocol overhead by limiting the amount of information exchanged between participating nodes, (2) is resilient to network alterations, due to node movement or node failures, and (3) allows exponentially-fast information convergence. We outline an initial solution incorporating these ideas and also show how our approach reduces network overhead by up to a factor of 2.4 and results in up to 2.7 times faster information convergence than alternative approaches.", "references": ["Unlicensed Operation in the TV Broadcast Bands and Additional Spectrum for Unlicensed Devices Below 900 MHz in the 3 GHz band", "In-Stat: Covering the Full Spectrum of Digital Communications Market Research, from Vendor to End-user", "Incremental Maintenance of Global Aggregates", "CORVUS: A Cognitive Radio Approach for Usage of Virtual Unlicensed Spectrum", "Implementation Issues in Spectrum Sensing for Cognitive Radios", "Spatially-Decaying Aggregation Over a Network: Model and Algorithms", "Probabilistic Counting Algorithms for Data Base Applications", "Random Walks in Peer-to-Peer Networks", "Previewing Intel's Cognitive Radio Chip", "Gossip-Based Computation of Aggregate Information", "Sensing-based Opportunistic Channel Access", "Search and Replication in Unstructured Peer-to-Peer Networks", "BRITE: an Approach to Universal Topology Generation", "Cooperative Sensing among Cognitive Radios", "Synopsis Diffusion for Robust Aggregation in Sensor Networks", "Fundamental Tradeoffs in Robust Spectrum Sensing for Opportunistic Frequency Reuse", "Distributed Coordination in Dynamic Spectrum Allocation Networks"], "title": "GUESS: Gossiping Updates for Efficient Spectrum Sensing"}
{"id": "C-74", "keywords": "messag orient middlewar;middlewar for mobil comput;epidem protocol;mobil ad-hoc network;context awar", "abstract": "The characteristics of mobile environments, with the possibility of frequent disconnections and fluctuating bandwidth, have forced a rethink of traditional middleware. In particular, the synchronous communication paradigms often employed in standard middleware do not appear to be particularly suited to ad hoc environments, in which not even the intermittent availability of a backbone network can be assumed. Instead, asynchronous communication seems to be a generally more suitable paradigm for such environments. Message oriented middleware for traditional systems has been developed and used to provide an asynchronous paradigm of communication for distributed systems, and, recently, also for some specific mobile computing systems. In this paper, we present our experience in designing, implementing and evaluating EMMA (Epidemic Messaging Middleware for Ad hoc networks), an adaptation of Java Message Service (JMS) for mobile ad hoc environments. We discuss in detail the design challenges and some possible solutions, showing a concrete example of the feasibility and suitability of the application of the asynchronous paradigm in this setting and outlining a research roadmap for the coming years.", "references": ["Cross-layering in Mobile ad Hoc Network Design", "Epidemic Algorithms for Replicated Database Maintenance", "Providing connectivity to the Saami nomadic community", "Supporting CORBA applications in a Mobile Environment", "Java Message Service Specification Version 1.1", "WebSphere MQ: Connecting your applications without complex programming", "Marrying Middleware and Mobile Computing", "WebSphere MQ EveryPlace Version 2.0", "Connecting remote communities", "Introducing Wireless JMS", "Middleware for Mobile Computing", "Microsoft Message Queuing (MSMQ) Version 2.0 Documentation", "Adaptive routing for intermittently connected mobile ad hoc networks", "Java Naming and Directory Interface (JNDI) Documentation Version 1.2", "Jini Specification Version 2.0", "Epidemic routing for Partially Connected Ad Hoc Networks", "The OMNeT++ discrete event simulation system", "JMS on Mobile Ad-Hoc Networks", "Pronto: Mobilegateway with publish-subscribe paradigm over wireless network"], "title": "Adapting Asynchronous Messaging Middleware to ad-hoc Networking"}
{"id": "C-75", "keywords": "intrus detect system;grid;system integr", "abstract": "This paper considers the composition of a DIDS (Distributed Intrusion Detection System) by integrating heterogeneous IDSs (Intrusion Detection Systems). A Grid middleware is used for this integration. In addition, an architecture for this integration is proposed and validated through simulation.", "references": ["Constructing A Grid Simulation with Differentiated Network Service Using GridSim", "Grid-based Intrusion Detection System", "The Physiology of the Grid: An Open Grid Service Architecture for Distributed System Integration", "The anatomy of the Grid: enabling scalable virtual organizations", "DIDMA: A Distributed Intrusion Detection System Using Mobile Agents", "Integrating Grid with Intrusion Detection", "A Performance-Based Grid Intrusion Detection System", "TCPdump Libpcap", "DIDS (Distributed Intrusion Detection System) - Motivation, Architecture and An Early Prototype", "A General Cooperative Intrusion Detection Architecture for MANETs", "GIDA: Toward Enabling Grid Intrusion Detection Systems", "Intrusion Detection message exchange requirements", "Distributed Intrusion Detection Based on Clustering", "Intrusion Detection Message exchange format data model and Extensible Markup Language (XML) Document Type Definition"], "title": "Composition of a DIDS by Integrating Heterogeneous IDSs on Grids"}
{"id": "C-76", "keywords": "event correl;fault manag;servic manag;servic level agreement", "abstract": "The paradigm shift from device-oriented to service-oriented management has also implications to the area of event correlation. Today's event correlation mainly addresses the correlation of events as reported from management tools. However, a correlation of user trouble reports concerning services should also be performed. This is necessary to improve the resolution time and to reduce the effort for keeping the service agreements. We refer to such a type of correlation as service-oriented event correlation. The necessity to use this kind of event correlation is motivated in the paper. To introduce service-oriented event correlation for an IT service provider, an appropriate modeling of the correlation workflow and of the information is necessary. Therefore, we examine the process management frameworks IT Infrastructure Library (ITIL) and enhanced Telecom Operations Map (eTOM) for their contribution to the workflow modeling in this area. The different kinds of dependencies that we find in our general scenario are then used to develop a workflow for the service-oriented event correlation. The MNM Service Model, which is a generic model for IT service management proposed by the Munich Network Management (MNM) Team, is used to derive an appropriate information modeling. An example scenario, the Web Hosting Service of the Leibniz Supercomputing Center (LRZ), is used to demonstrate the application of service-oriented event correlation.", "references": ["Yemanja - A Layered Event Correlation Engine for Multi-domain Server Farms", "Spectrum, Aprisma Corporation", "New Approach for Automated Generation of Service Dependency Models", "An Approach for Managing Service Dependencies with XML and the Resource Description Framework", "Towards generic Service Management Concepts - A Service Model Based Approach", "Integrated Event Management: Event Correlation using Dependency Graphs", "Discovering Dynamic Dependencies in Enterprise Environments for Problem Determination", "Integrated Management of Networked Systems - Concepts, Architectures and their Operational Application", "IT Infrastructure Library, Office of Government Commerce and IT Service Management Forum", "Alarm Correlation", "Real-time Telecommunication Network Management: Extending Event Correlation with Temporal Constraints", "A Coding Approach to Event Correlation", "Customer Service Management: A More Transparent View To Your Subscribed Services", "A Case-based Reasoning Approach for the Resolution of Faults in Communication Networks", "Service Level Management for Enterprise Networks", "Agilent Technologies", "Smarts Corporation", "Using Neural Networks for Alarm Correlation in Cellular Phone Networks", "High Speed and Robust Event Correlation"], "title": "Assured Service Quality by Improved Fault Management"}
{"id": "C-77", "keywords": "causal track;hass diagram;immedi predecessor;messag-pass;timestamp;vector clock", "abstract": "A distributed computation is usually modeled as a partially ordered set of relevant events (the relevant events are a subset of the primitive events produced by the computation). An important causality-related distributed computing problem, that we call the Immediate Predecessors Tracking (IPT) problem, consists in associating with each relevant event, on the fly and without using additional control messages, the set of relevant events that are its immediate predecessors in the partial order. So, IPT is the on-the-fly computation of the transitive reduction (i.e., Hasse diagram) of the causality relation defined by a distributed computation. This paper addresses the IPT problem: it presents a family of protocols that provides each relevant event with a timestamp that exactly identifies its immediate predecessors. The family is defined by a general condition that allows application messages to piggyback control information whose size can be smaller than $n$ (the number of processes). In that sense, this family defines message size-efficient IPT protocols. According to the way the general condition is implemented, different IPT protocols can be obtained. Two of them are exhibited.", "references": ["Tracking Immediate Predecessors in Distributed Computations", "Efficient -Causal BroadcastingDistributed Snapshots: Determining Global States of Distributed Systems", "Reachability Analysis of Distributed Executions", "Timestamps in Message-Passing Systems that Preserve Partial Ordering", "On-the-fly Analysis of Distributed Computations", "Shared Global States in Distributed Computations", "On-the-Fly Testing of Regular Patterns in Distributed Computations", "Principles of Distributed Systems", "Communication-Based Prevention of Useless Ckeckpoints in Distributed Computations", "Tracking Causality in Distributed Systems: a Suite of Efficient Protocols", "Consistency Issues in Distributed Checkpoints", "Efficient Distributed Detection of Conjunction of Local Predicates in Asynch Computations", "Clocks and the Ordering of Events in a Distributed System", "Efficient Detection of a Class of Stable Properties", "Virtual Time and Global States of Distributed Systems", "An Adaptive Causal Ordering Algorithm Suited to Mobile Computing Environment", "Logical Time: Capturing Causality in Distributed Systems", "An Efficient Implementation of Vector Clocks", "Consistent Global Checkpoints That Contain a Given Set of Local Checkpoints"], "title": "Tracking Immediate Predecessors in Distributed Computations"}
{"id": "C-78", "keywords": "event-base commun;sentient comput;componentbas system;middlewar architectur", "abstract": "In a future networked physical world, a myriad of smart sensors and actuators assess and control aspects of their environments and autonomously act in response to it. Examples range in telematics, traffic management, team robotics or home automation to name a few. To a large extent, such systems operate proactively and independently of direct human control driven by the perception of the environment and the ability to organize respective computations dynamically. The challenging characteristics of these applications include sentience and autonomy of components, issues of responsiveness and safety criticality, geographical dispersion, mobility and evolution. A crucial design decision is the choice of the appropriate abstractions and interaction mechanisms. Looking to the basic building blocks of such systems we may find components which comprise mechanical components, hardware and software and a network interface, thus these components have different characteristics compared to pure software components. They are able to spontaneously disseminate information in response to events observed in the physical environment or to events received from other component via the network interface. Larger autonomous components may be composed recursively from these building block.The paper describes an architectural framework and a middleware supporting a component-based system and an integrated view on events-based communication comprising the real world events and the events generated in the system. It starts by an outline of the component-based system construction. The generic event architecture GEAR is introduced which describes the event-based interaction between the components via a generic event layer. The generic event layer hides the different communication channels including the interactions through the environment. An appropriate middleware is presented which reflects these needs and allows to specify events which have quality attributes to express temporal constraints. This is complemented by the notion of event channels which are abstractions of the underlying network and allow to enforce quality attributes. They are established prior to interaction to reserve the needed computational and network resources for highly predictable event dissemination.", "references": ["Generic support for distributed applications", "Using a flexible real-time scheduling strategy in a distributed embedded application", "Linda in context", "Preliminary definition of cortex system architecture", "CORTEX project Annex 1, Description of Work", "Time bounded medium access control for ad hoc networks", "The many faces of publish subscribe", "Time triggered communication on CAN", "CAN Specification Version 2.0", "The design and performance of a real-time corba event service", "Location systems for ubiquitous computing", "The Clifford Paterson Lecture, 1999 Sentient Computing", "COSMIC: A Middleware for Event-Based Interaction on CAN", "Implementing the real-time publisher/subscriber model on the controller area network (CAN)", "Integrating subscription-based and connection-oriented communications into the embedded CORBA for the CAN Bus", "TTP - A Time-Triggered Protocol for Fault-Tolerant Real-Time Systems", "A Universal Smart Transducer Interface: TTP/A", "Real-time and Dependability Concepts", "Integration of a CAN-based connection-oriented communication model into Real-Time CORBA", "Local Interconnect Network: LIN Specification Package Revision 1.2", "Scheduling hard and soft real-time communication in the controller area network", "Steam: Event-based middleware for wireless ad hoc networks"], "title": "An Architectural Framework and a Middleware for Cooperating Smart Components"}
{"id": "C-79", "keywords": "resourc discoveri;mobil ad-hoc network", "abstract": "This paper describes a cross-layer approach to designing robust P2P system over mobile ad hoc networks. The design is based on simple functional primitives that allow routing at both P2P and network layers to be integrated to reduce overhead. With these primitives, the paper addresses various load balancing techniques. Preliminary simulation results are also presented.", "references": ["Peer-to-Peer: Harnessing the Power of Disruptive Technologies", "Konark -- A Service Discovery and Delivery Protocol for Ad-Hoc Networks", "Proem: A Peer-to-Peer Computing Platform for Mobile Ad-hoc Networks", "A Performance Analysis of 7DS a Peer-to-Peer Data Dissemination and Prefetching Tool for Mobile Users", "Scalable Service Discovery in Mobile Ad hoc Networks", "Supporting Cooperative Caching in Ad Hoc Networks", "A Lightweight Content Replication Scheme for Mobile Ad hoc Environments"], "title": "A Cross-Layer Approach to Resource Discovery and Distribution in Mobile ad-hoc Networks"}
{"id": "C-80", "keywords": "content address storag;relat databas system;databas cach;wide area network;bandwidth optim", "abstract": "With the growing use of dynamic web content generated from relational databases, traditional caching solutions for through put and latency improvements are ineffective. We describe a middleware layer called Ganesh that reduces the volume of data transmitted without semantic interpretation of queries or results. It achieves this reduction through the use of cryptographic hashing to detect similarities with previous results. These benefits do not require any compromise of the strict consistency semantics provided by the back-end database. Further, Ganesh does not require modifications to applications, web servers, or database servers, and works with closed-source applications and databases. Using two bench marks representative of dynamic web sites, measurements of our prototype show that it can increase end-to-end throughput by as much as two fold for non-data intensive applications and by as much as ten fold for dataintensive ones.", "references": ["An empirical evaluation of wide-area internet bottlenecks", "Cache tables: Paving the way for an adaptive database cache", "Dbcache: Database caching for web application servers", "Dbproxy: A dynamic data cache for web applications", "Compare-by-hash: A reasoned analysis", " Single instance storage in windows 2000", "Lessons from giant-scale services", "Syntactic clustering of the web", "Performance comparison of middleware architectures for generating dynamic web content", "C-JDBC: Flexible database clustering middleware", "Pastiche: Making backup cheap and easy", "Wide-area cooperative storage with CFS", "PAST: A large-scale, persistent peer-to-peer storage utility", "Application specific data replication for edge services", "Netem -- emulating real networks in the lab", "An analysis of compare-by-hash", "Jmob benchmarks", "Balancing performance and data freshness in web database servers", "Transparent mid-tier database caching in sql server", "Finding similar files in a large file system", "Simultaneous scalability and security for data-intensive web applications", "Handbook of Applied Cryptography", "Response time in man-computer conversational transactions", "Design, implementation, and evaluation of duplicate transfer detection in http", "A low-bandwidth network file system", "Method-based caching in multi-tiered server applications", "Ganymed: Scalable replication for transactional web applications", "Venti: A new approach to archival storage", "Fingerprinting by random polynomials", "Moving edge side includes to the real edge -- the clients", "Database Programming with JDBC and Java", "Value-based web caching", "Globedb: Autonomic data replication for web applications", "A protocol-independent technique for eliminating redundant network traffic", "Integrating portable and distributed storage", "Opportunistic use of content addressable storage for distributed file systems", "Evaluation of edge caching/offloading for dynamic content delivery"], "title": "Consistency-preserving Caching of Dynamic Database Content"}
{"id": "C-81", "keywords": "energi harvest;low power design;energi neutral oper", "abstract": "Harvesting energy from the environment is feasible in many applications to ameliorate the energy limitations in sensor networks. In this paper, we present an adaptive duty cycling algorithm that allows energy harvesting sensor nodes to autonomously adjust their duty cycle according to the energy availability in the environment. The algorithm has three objectives, namely (a) achieving energy neutral operation, i.e., energy consumption should not be more than the energy provided by the environment, (b) maximizing the system performance based on an application utility model subject to the above energy-neutrality constraint, and (c) adapting to the dynamics of the energy source at run-time. We present a model that enables harvesting sensor nodes to predict future energy opportunities based on historical data. We also derive an upper bound on the maximum achievable performance assuming perfect knowledge about the future behavior of the energy source. Our methods are evaluated using data gathered from a prototype solar energy harvesting platform and we show that our algorithm can utilize up to 58% more environmental energy compared to the case when harvesting-aware power management is not used.", "references": ["Toplogy Control of Multihop Wireless Networks Using Transmit Power Adjustment", "The simulation and evaluation of dynamic voltage scaling algorithms", "Dynamic Power Management: Design Techniques and CAD Tools", "Parasitic power harvesting in shoes", "Energy scavenging with shoe-mounted piezoelectrics", "Human-powered wearable computing", "Studying the feasibility of energy harvesting in a mobile sensor network", "The ecobot project", "Picoradio supports ad hoc ultra-low power wireless networking", "A compact, wireless, self-powered pushbutton controller", "The upper limit to solar energy conversion", "Darpa energy harvesting projects", "Ambient intelligence: industrial research on a visionary concept", "Design Considerations for Solar Energy Harvesting Wireless Embedded Systems", "Perpetual Environmentally Powered Sensor Networks", "DuraNode: Wireless Networked Sensor for Structural Health Monitoring", "An environmental energy harvesting framework for sensor networks", "Utilizing solar power in wireless sensor networks", "Power management in energy harvesting sensor networks"], "title": "Adaptive Duty Cycling for Energy Harvesting Systems"}
{"id": "C-83", "keywords": "pervas document edit and manag system;comput support collabor work;cscw;collabor document", "abstract": "Collaborative document processing has been addressed by many approaches so far, most of which focus on document versioning and collaborative editing. We address this issue from a different angle and describe the concept and architecture of a pervasive document editing and managing system. It exploits database techniques and real-time updating for sophisticated collaboration scenarios on multiple devices. Each user is always served with up-to-date documents and can organize his work based on document meta data. For this, we present our conceptual architecture for such a system and discuss it with an example.", "references": ["The Lowell Database Research Self Assessment", "Supporting Collaborative Layouting in Word Processing", "Concept and prototype of a collaborative business process environment for document processing", "Using Database Management Systems for Collaborative Text Editing", "Dynamic Collaborative Business Processes within Documents", "Using Text Editing Creation Time Meta Data for Document Management", "Embedded SOAP Server on the Operating System Level for Ad-hoc Automatic Real-Time Bidirectional Communication", "Revolution in Real-Time Communication and Collaboration: For Real This Time"], "title": "Concept and Architecture of a Pervasive Document Editing and Managing System"}
{"id": "C-84", "keywords": "cach;distribut system;peer-to-peer system;game-theoret model;nash equilibrium;price of anarchi+anarchi price", "abstract": "We analyze replication of resources by server nodes that act selfishly, using a game-theoretic approach. We refer to this as the selfish caching problem. In our model, nodes incur either cost for replicating resources or cost for access to a remote replica. We show the existence of pure strategy Nash equilibria and investigate the price of anarchy, which is the relative cost of the lack of coordination. The price of anarchy can be high due to undersupply problems, but with certain network topologies it has better bounds. With a payment scheme the game can always implement the social optimum in the best case by giving servers incentive to replicate.", "references": ["FARSITE: Federated, Available, and Reliable Storage for an Incompletely Trusted Environment", "Near-optimal Network Design with Selfish Agents", "SCAN: A Dynamic, Scalable, and Efficient Content Distribution Network", "Wide-area Cooperative Storage with CFS", "NetCache Architecture and Deploment", "Strategyproof cost-sharing Mechanisms for Set Cover and Facility Location Games", "Large-Scale Simulation of Replica Placement Algorithms for a Serverless Distributed File System", "The Complexity of Pure Nash Equilibria", "Summary Cache: A Scalable Wide-area Web Cache Sharing Protocol", "Computers and Intractability: A Guide to the Theory of NP-Completeness", "Market Sharing Games Applied to Content Distribution in Ad-Hoc Networks", "Cooperative Facility Location Games", "What Can Databases Do for Peer-to-Peer?", "Measurement, Modeling, and Analysis of a Peer-to-Peer File-Sharing Workload", "Squirrel: A Decentralized Peer-to-Peer Web Cache", "Primal-Dual Approximation Algorithms for Metric Facility Location and k-Median Problems", "On the Placement of Internet Instrumentation", "Constrained Mirror Placement on the Internet", "A Distributed, Self-stabilizing Protocol for Placement of Replicated Resources in Emerging Networks", "Worst-Case Equilibria", "OceanStore: An Architecture for Global-scale Persistent Storage", "On the Optimal Placement of Web Proxies in the Internet", "Improved Approximation Algorithms for Metric Facility Location Problems", "BRITE: Universal Topology Generation from a User's Perspective", "The Online Median Problem", "Discrete Location Theory", "A Course in Game Theory", "Group Strategyproof Mechanisms via Primal-Dual Algorithms", "On the Placement of Web Server Replicas", "A Dynamic Object Replication and Migration Protocol for an Internet Hosting Service", "Storage Management and Caching in PAST, A Large-scale, Persistent Peer-to-peer Storage Utility", "Taming Aggressive Replication in the Pangaea Wide-Area File System", "Coordinated En-route Web Caching", "Nash Equilibria in Competitive Societies, with Applications to Facility Location, Traffic Routing, and Auctions", "How to Model an Internetwork"], "title": "Selfish Caching in Distributed Systems: A Game-Theoretic Analysis"}
{"id": "H-35", "keywords": "inform retriev;learn to rank;boost", "abstract": "In this paper we address the issue of learning to rank for document retrieval. In the task, a model is automatically created with some training data and then is utilized for ranking of documents. The goodness of a model is usually evaluated with performance measures such as MAP (Mean Average Precision) and NDCG (Normalized Discounted Cumulative Gain). Ideally a learning algorithm would train a ranking model that could directly optimize the performance measures with respect to the training data. Existing methods, however, are only able to train ranking models by minimizing loss functions loosely related to the performance measures. For example, Ranking SVM and RankBoost train ranking models by minimizing classification errors on instance pairs. To deal with the problem, we propose a novel learning algorithm within the framework of boosting, which can minimize a loss function directly defined on the performance measures. Our algorithm, referred to as AdaRank, repeatedly constructs 'weak rankers' on the basis of reweighted training data and finally linearly combines the weak rankers for making ranking predictions. We prove that the training process of AdaRank is exactly that of enhancing the performance measure used. Experimental results on four benchmark datasets show that AdaRank significantly outperforms the baseline methods of BM25, Ranking SVM, and RankBoost.", "references": ["Modern Information Retrieval", "Learning to rank with nonsmooth cost functions", "Learning to rank using gradient descent", "Adapting ranking SVM to document retrieval", "Subset ranking using regression", "Overview of the TREC 2003 web track", "Boosting methods for regression", "An efficient boosting algorithm for combining preferences", "A decision-theoretic generalization of on-line learning and an application to boosting", "Additive logistic regression: A statistical view of boosting", "Learning rankings via convex hull separation", "The Elements of Statistical Learning", "Large Margin rank boundaries for ordinal regression", "Ohsumed: an interactive retrieval evaluation and new large test collection for research", "IR evaluation methods for retrieving highly relevant documents", "Optimizing search engines using clickthrough data", "A support vector method for multivariate performance measures", "Document language models, query models, and risk minimization for information retrieval", "Direct maximization of rank-based metrics for information retrieval", "Discriminative models for information retrieval", "The pagerank citation ranking: Bringing order to the web", "A language modeling approach to information retrieval", "A study of relevance propagation for web search", "The TREC-9 filtering track final report", "Boosting the margin: A new explanation for the effectiveness of voting methods", "Improved boosting algorithms using confidence-rated predictions", "Microsoft Research Asia at web track and terabyte track of TREC 2004", "Learning to rank", "Cost-sensitive learning of SVM for ranking", "Exploiting the hierarchical structure for link analysis", "SVM selective sampling for ranking with application to data retrieval"], "title": "AdaRank: A Boosting Algorithm for Information Retrieval"}
{"id": "H-37", "keywords": "support vector machin;spam filter;blog;splog", "abstract": "Spam is a key problem in electronic communication, including large-scale email systems and the growing number of blogs. Content-based filtering is one reliable method of combating this threat in its various forms, but some academic researchers and industrial practitioners disagree on how best to filter spam. The former have advocated the use of Support Vector Machines (SVMs) for content-based filtering, as this machine learning methodology gives state-of-the-art performance for text classification. However, similar performance gains have yet to be demonstrated for online spam filtering. Additionally, practitioners cite the high cost of SVMs as reason to prefer faster (if less statistically robust) Bayesian methods. In this paper, we offer a resolution to this controversy. First, we show that online SVMs indeed give state-of-the-art classification performance on online spam filtering on large benchmark data sets. Second, we show that nearly equivalent performance may be achieved by a Relaxed Online SVM (ROSVM) at greatly reduced computational cost. Our results are experimentally verified on email spam, blog spam, and splog detection tasks.", "references": ["Spam filtering using compression models", "Incremental and decremental support vector machine learning", "TREC 2006 spam track overview", "Batch and on-line spam filter comparison", "TREC 2005 spam track overview", "On-line supervised spam filter evaluation", "An introduction to support vector machines", "Alpha seeding for support vector machines", "Support vector machines for spam categorization", "Online discriminative spam filter training", "A plan for spam", "Better bayesian filtering", "Spam: It's not just for inboxes anymore", "Text categorization with suport vector machines: Learning with many relevant features", "Training linear svms in linear time", "Online learning with kernels", "SVMs for the blogosphere: Blog identification and splog detection", "Learning algorithms with optimal stability in neural networks", "On-line spam filter fusion", "Spam filtering with naive bayes - which naive bayes?", "Blocking blog spam with language model disagreement", "Sequenital minimal optimization: A fast algorithm for training support vector machines", "Learning with Kernels: Support Vector Machines", "On attacking statistical spam filters"], "title": "Relaxed Online SVMs for Spam Filtering"}
{"id": "H-38", "keywords": "random graph;pagerank;diffusionrank", "abstract": "While the PageRank algorithm has proven to be very effective for ranking Web pages, the rank scores of Web pages can be manipulated. To handle the manipulation problem and to cast a new insight on the Web structure, we propose a ranking algorithm called DiffusionRank. DiffusionRank is motivated by the heat diffusion phenomena, which can be connected to Web ranking because the activities flow on the Web can be imagined as heat flow, the link from a page to another can be treated as the pipe of an air-conditioner, and heat flow can embody the structure of the underlying Web graph. Theoretically we show that DiffusionRank can serve as a generalization of PageRank when the heat diffusion co-efficient  tends to infinity. In such a case 1== 0, DiffusionRank (PageRank) has low ability of anti-manipulation. When  = 0, DiffusionRank obtains the highest ability of anti-manipulation, but in such a case, the web structure is completely ignored. Consequently,  is an interesting factor that can control the balance between the ability of preserving the original Web and the ability of reducing the effect of manipulation. It is found empirically that, when  = 1, DiffusionRank has a Penicillin-like effect on the link manipulation. Moreover, DiffusionRank can be employed to find group-to-group relations on the Web, to divide the Web graph into several parts, and to find link communities. Experimental results show that the DiffusionRank algorithm achieves the above mentioned advantages as expected.", "references": ["Improving web search ranking by incorporating user behavior information", "Generalizing pagerank: damping functions for link-based ranking algorithms", "Laplacian eigenmaps for dimensionality reduction and data representation", "Random Graphs", "Learning to rank using gradient descent", "Ranking the web frontier", "Combating web spam with trustrank", "Exploiting the block structure of the web for computing pagerank", "Diffusion kernels on graphs and other discrete input spaces", "Diffusion kernels on statistical manifolds", "The many proofs and applications of perron's theorem", "Detecting spam web pages through content analysis", "The pagerank citation ranking: Bringing order to the web", "NHDC and PHDC: Non-propagating and propagating heat diffusion classifiers", "Predictive ranking: a novel page ranking approach by estimating the web structure", "Predictive random graph ranking on the web", "Ranking on data manifolds"], "title": "DiffusionRank: A Possible Penicillin for Web Spamming"}
{"id": "H-40", "keywords": "cross-languag inform retriev;queri log;queri translat;queri suggest;queri expans", "abstract": "Query suggestion aims to suggest relevant queries for a given query, which help users better specify their information needs. Previously, the suggested terms are mostly in the same language of the input query. In this paper, we extend it to cross-lingual query suggestion (CLQS): for a query in one language, we suggest similar or relevant queries in other languages. This is very important to scenarios of cross-language information retrieval (CLIR) and cross-lingual keyword bidding for search engine advertisement. Instead of relying on existing query translation technologies for CLQS, we present an effective means to map the input query of one language to queries of the other language in the query log. Important monolingual and cross-lingual information such as word translation relations and word co-occurrence statistics, etc. are used to estimate the cross-lingual query similarity with a discriminative model. Benchmarks show that the resulting CLQS system significantly out performs a baseline system based on dictionary-based query translation. Besides, the resulting CLQS is tested with French to English CLIR tasks on TREC collections. The results demonstrate higher effectiveness than the traditional query translation methods.", "references": ["Using Monolingual Click through Data to Build Cross-lingual Search Systems", "Phrasal Translation and Query Expansion Techniques for Cross-Language Information Retrieval", "Resolving Ambiguity for Cross-Language Retrieval", "The Mathematics of Statistical Machine Translation: Parameter Estimation", "LIBSVM: a Library for Support Vector Machines", "Novel Association Measures Using Web Search with Double Checking", "Translating Unknown Queries with Web Corpora for Cross-Language Information Retrieval", "Query Expansion by Mining User Logs", "Applying Machine Translation to Two-Stage Cross-Language Information Retrieval", "Improving query translation for CLIR using statistical Models", "Resolving Query Translation Ambiguity using a Decaying Co--occurrence Model and Syntactic Dependence Relations", "SVD Subspace Projections for Term Suggestion Ranking and Clustering", "Using Statistical Testing in the Evaluation of Retrieval Experiments", "Finding Similar Questions in Large Question and Answer Archives", "Optimizing Search Engines Using Clickthrough Data", "Cross-Lingual Relevance Models", "Anchor Text Mining for Translation Extraction of Query Terms", "Comparing Cross-Language Query Expansion Techniques by Degrading Translation Resources", "Iterative Translation Disambiguation for Cross-Language Information Retrieval", "Cross-Language Information Retrieval based on Parallel Text and Automatic Mining of Parallel Text from the Web", "A Systematic Comparison of Various Statistical Alignment Models", "Dictionary-Based Cross-Language Information Retrieval: Problems, Methods, and Research Findings", "OKAPI at TREC-3", "Relevance Weighting of Search Terms", "Tutorial on Support Vector Regression. Statistics and Computing", "Query Clustering Using User Logs", "Using the Web for Automated Translation Extraction in Cross-Language Information Retrieval"], "title": "Cross-Lingual Query Suggestion Using Query Logs of Different Languages"}
{"id": "H-41", "keywords": "rank;pagerank;hit;bm25f;mrr;map;ndcg", "abstract": "This paper describes a large-scale evaluation of the effectiveness of HITS in comparison with other link-based ranking algorithms, when used in combination with a state-of-the-art text retrieval algorithm exploiting anchor text. We quantified their effectiveness using three common performance measures: the mean reciprocal rank, the mean average precision, and the normalized discounted cumulative gain measurements. The evaluation is based on two large data sets: a breadth-first search crawl of 463 million web pages containing 17.6 billion hyperlinks and referencing 2.9 billion distinct URLs; and a set of 28,043 queries sampled from a query log, each query having on average 2,383 results, about 17 of which were labeled by judges. We found that HITS outperforms PageRank, but is about as effective as web-page in-degree. The same holds true when any of the link-based features are combined with the text retrieval algorithm. Finally, we studied the relationship between query specificity and the effectiveness of selected features, and found that link-based features perform better for general queries, whereas BM25F performs better for specific queries.", "references": ["Does authority mean quality? Predicting expert quality ratings of web documents", "Inside PageRank", "Finding authorities and hubs from link structures on the World Wide Web", "Link analysis ranking: algorithms, theory, and experiments", "The anatomy of a large-scale hypertextual Web search engine", "Learning to rank using gradient descent", "Learning to probabilistically identify authoritative documents", "Relevance weighting for query independent evidence", "Citation analysis as a tool in journal evaluation", "Web spam taxonomy", "Combating web spam with TrustRank", "Real life information retrieval: a study of user queries on the web", "Cumulated gain-based evaluation of IR techniques", "Extrapolation methods for accelerating PageRank computations", "Bibliographic coupling between scientific papers", "Authoritative sources in a hyperlinked environment", "Authoritative sources in a hyperlinked environment", "Deeper inside PageRank", "The stochastic approach for link-structure analysis (SALSA) and the TKC effect", "Stable algorithms for link analysis", "The PageRank citation ranking: Bringing order to the web", "A new paradigm for ranking pages on the World Wide Web", "Predicting fame and fortune: Pagerank or indegree?", "Microsoft Cambridge at TREC-13: Web and HARD tracks"], "title": "HITS on the Web: How does it Compare?"}
{"id": "H-42", "keywords": "ir evalu;trec;social network analysi;kleinberg' hit algorithm", "abstract": "We propose a novel method of analysing data gathered fromTREC or similar information retrieval evaluation experiments. We define two normalized versions of average precision, that we use to construct a weighted bipartite graph of TREC systems and topics. We analyze the meaning of well known - and somewhat generalized - indicators fromsocial network analysis on the Systems-Topics graph. We apply this method to an analysis of TREC 8 data; amongthe results, we find that authority measures systems performance, that hubness of topics reveals that some topics are better than others at distinguishing more or less effective systems, that with current measures a system that wants to be effective in TREC needs to be effective on easy topics, and that by using different effectiveness measures this is no longer the case.", "references": ["Improving the automatic retrieval of text documents", "Evaluating evaluation measure stability", "Mining the Web", "Statistical precision of information retrieval evaluation", "Authoritative sources in a hyperlinked environment", "An Introduction to Search Engines and Web Navigation", "The PageRank Citation Ranking: Bringing Order to the Web", "On GMAP - and other transformations", "Information retrieval system evaluation: effort, sensitivity, and reliability", "Ranking retrieval systems without relevance judgments", "TREC Common Evaluation Measures", "Text REtrieval Conference (TREC)", "The effect of topic set size on retrieval experiment error", "Overview of the TREC 2005 Robust Retrieval Track", "TREC - Experiment and Evaluation in Information Retrieval", "Social Network Analysis"], "title": "Hits hits TREC: exploring IR evaluation results with network analysis"}
{"id": "H-43", "keywords": "link structur;text content;factor analysi;matrix factor", "abstract": "The world wide web contains rich textual contents that areinterconnected via complex hyperlinks. This huge database violates the assumption held by most of conventional statistical methods that each web page is considered as an independent and identical sample. It is thus difficult to apply traditional mining or learning methods for solving web mining problems, e.g., web page classification, by exploiting both the content and the link structure. The research in this direction has recently received considerable attention but are still in an early stage. Though a few methods exploit both the link structure or the content information, some of them combine the only authority information with the content information, and the others first decompose the link structure into hub and authority features, then apply them as additional document features. Being practically attractive for its great simplicity, this paper aims to design an algorithm that exploits both the content and linkage information, by carrying out a joint factorization on both the linkage adjacency matrix and the document-term matrix, and derives a new representation for web pages in a low-dimensional factor space, without explicitly separating them as content, hub or authority factors. Further analysis can be performed based on the compact representation of web pages. In the experiments, the proposed method is compared with state-of-the-art methods and demonstrates an excellent accuracy in hypertext classification on the WebKB and Cora benchmarks.", "references": ["CMU world wide knowledge base (WebKB) project", "Web search via hub synthesis", "Enhanced hypertext categorization using hyperlinks", "LIBSVM: a library for support vector machines", "Learning to probabilistically identify authoritative documents", "The missing link - a probabilistic model of document content and hypertext connectivity", "Support-vector networks", "Indexing by latent semantic analysis", "Web document clustering using hyperlink structures", "Probabilistic latent semantic indexing", "Composite kernels for hypertext categorisation", "Authoritative sources in a hyperlinked environment", "SVMs for the Blogosphere: Blog Identification and Splog Detection", "Pagerank without hyperlinks: structural re-ranking using links induced by language models", "Automating the contruction of internet portals with machine learning", "A practical hypertext catergorization method using links and incrementally available class information", "PageRank citation ranking: bring order to the web", "General Intelligence objectively determined and measured", "Discriminative probabilistic models for relational data", "Document clustering based on non-negative matrix factorization", "A study of approaches to hypertext categorization", "Multi-label informed latent semantic indexing", "Linear prediction models with graph regularization for web-page categorization", "Learning from labeled and unlabeled data on a directed graph", "Semi-supervised learning on directed graphs"], "title": "Combining Content and Link for Classification using Matrix Factorization"}
{"id": "H-44", "keywords": "tempor text index;time-travel text search;web archiv", "abstract": "Text search over temporally versioned document collections such as web archives has received little attention as a research problem. As a consequence, there is no scalable and principled solution to search such a collection as of a specified time. In this work, we address this shortcoming and propose an efficient solution for time-travel text search by extending the inverted file index to make it ready for temporal search. We introduce approximate temporal coalescing as a tunable method to reduce the index size without significantly affecting the quality of results. In order to further improve the performance of time-travel queries, we introduce two principled techniques to trade off index size for its performance. These techniques can be formulated as optimization problems that can be solved to near-optimality. Finally, our approach is evaluated in a comprehensive series of experiments on two large-scale real-world datasets. Results unequivocally show that our methods make it possible to build an efficient "time machine" scalable to large versioned text collections.", "references": ["Pruned Query Evaluation Using Pre-Computed Impacts", "Pruning Strategies for Mixed-Mode Querying", "Versioning a Full-Text Information Retrieval System", "Modern Information Retrieval", "A Time Machine for Text search", "Coalescing in Temporal Databases", "Do Your Worst to Make the Best: Paradoxical Effects in PageRank Incremental Computations", "Indexing Shared Content in Information Retrieval Systems", "Optimization of Inverted Vector Searches", "Method and Apparatus for Generating and Searching Range-Based Index of Word Locations", "A Document-Centric Approach to Static Index Pruning in Text Retrieval Systems", "Static Index Pruning for Information Retrieval Systems", "Comparing Top k Lists", "Optimal Aggregation Algorithms for Middleware", "REHIST: Relative Error Histogram Construction Algorithms", "Efficient Indexing of Versioned Document Sequences", "Balancing Histogram Optimality and Practicality for Query Result Size Estimation", "Optimal Histograms with Quality Guarantees", "An Online Algorithm for Segmenting Time Series", "Optimization by Simulated Annealing", "Algorithm Design", "Introduction to Algorithms: A Creative Approach", "DyST: Dynamic and Scalable Temporal Text Indexing", "A Language Modeling Approach to Information Retrieval", "Okapi/Keenbow at TREC-8", "Comparison of Access Methods for Time-Evolving Data", "Full Text Search of Web Archive Collections", "Efficient Algorithms for Sequence Segmentation", "Top-k Query Evaluation with Probabilistic Guarantees", "Managing Gigabytes: Compressing and Indexing Documents and Images", "Efficient Search in Large Textual Collections with Redundancy", "Inverted Files for Text Search Engines"], "title": "A time machine for text search"}
{"id": "H-45", "keywords": "web search;queri perform predict;queri classif", "abstract": "Current prediction techniques, which are generally designed for content-based queries and are typically evaluated on relatively homogenous test collections of small sizes, face serious challenges in web search environments where collections are significantly more heterogeneous and different types of retrieval tasks exist. In this paper, we present three techniques to address these challenges. We focus on performance prediction for two types of queries in web search environments: content-based and Named-Page finding. Our evaluation is mainly performed on the GOV2 collection. In addition to evaluating our models for the two types of queries separately, we consider a more challenging and realistic situation that the two types of queries are mixed together without prior information on query types. To assist prediction under the mixed-query situation, a novel query classifier is adopted. Results show that our prediction of web query performance is substantially more accurate than the current state-of-the-art prediction techniques. Consequently, our paper provides a practical approach to performance prediction in real-world web settings.", "references": ["Ranking Robustness: A Novel Framework to Predict Query Performance", "What Makes a Query Difficult?", "The TREC 2005 Terabyte Track", "Inferring query performance using pre-retrieval predictors", "Robust, Web and Terabyte Retrieval with Hummingbird SearchServer at TREC 2004", "Predicting Query Performance", "On Ranking the Effectiveness of Searcher", "A Markov Random Filed Model for Term Dependencies", "Indri at TREC 2005: Terabyte Track", "Combining document representations for known-item search", "Information retrieval as statistical translation", "Indri search engine", "Taneja: On Generalized Information Measures and Their Applications", "A Framework for Selective Query Expansion", "A general language model for information retrieval", "Personal email contact with Vishwa Vinay and our own experiments", "Learning to Estimate Query Difficulty Including Applications to Missing Content Detection and Distributed Information retrieval"], "title": "Query Performance Prediction in Web Search Environments"}
{"id": "H-46", "keywords": "expertis search;expert find;intranet search;languag model", "abstract": "Expertise retrieval has been largely unexplored on data other than the W3C collection. At the same time, many intranets of universities and other knowledge-intensive organisations offer examples of relatively small but clean multilingual expertise data, covering broad ranges of expertise areas. We first present two main expertise retrieval tasks, along with a set of baseline approaches based on generative language modeling, aimed at finding expertise relations between topics and people. For our experimental evaluation, we introduce (and release) a new test set based on a crawl of a university site. Using this test set, we conduct two series of experiments. The first is aimed at determining the effectiveness of baseline expertise retrieval methods applied to the new test set. The second is aimed at assessing refined models that exploit characteristic features of the new test set, such as the organizational structure of the university, and the hierarchical structure of the topics in the test set. Expertise retrieval models are shown to be robust with respect to environments smaller than the W3C collection, and current techniques appear to be generalizable to other settings.", "references": ["Incorporating Context in the Language Modeling Framework for ad hoc Information Retrieval", "Finding similar experts", "Determining expert profiles (with an application to expert finding)", "Formal models for expert finding in enterprise corpora", "The role of artificial intelligence technologies in the implementation of people-finder knowledge management systems", "Expertise identification using email communications", "Integrating word relationships into language models", "Elements of Information Theory", "P@noptic expert: Searching for experts not just for documents", "Overview of the TREC-2005 Enterprise Track", "Working Knowledge: How Organizations Manage What They Know", "Accurate methods for the statistics of surprise and coincidence", "Tell me what you do and I'll tell you what you are: Learning occupation-related activities for biographies", "Relevance based language models", "Cross-lingual relevance models", "Voting for candidates: adapting data fusion techniques for an expert search task", "Foundations of Statistical Natural Language Processing", "Expertise browser: a quantitative approach to identifying expertise", "Hierarchical language models for expert finding in enterprise corpora", "Overview of the TREC 2006 Enterprise Track", "Language model information retrieval with document expansion"], "title": "Broad Expertise Retrieval in Sparse Data Environments"}
{"id": "H-47", "keywords": "contextu advertis;semant;match", "abstract": "Contextual advertising or Context Match (CM) refers to the placement of commercial textual advertisements within the content of a generic web page, while Sponsored Search (SS) advertising consists in placing ads on result pages from a web search engine, with ads driven by the originating query. In CM there is usually an intermediary commercial ad-network entity in charge of optimizing the ad selection with the twin goal of increasing revenue (shared between the publisher and the ad-network) and improving the user experience. With these goals in mind it is preferable to have ads relevant to the page content, rather than generic ads. The SS market developed quicker than the CM market, and most textual ads are still characterized by "bid phrases" representing those queries where the advertisers would like to have their ad displayed. Hence, the first technologies for CM have relied on previous solutions for SS, by simply extracting one or more phrases from the given page content, and displaying ads corresponding to searches on these phrases, in a purely syntactic approach. However, due to the vagaries of phrase extraction, and the lack of context, this approach leads to many irrelevant ads. To overcome this problem, we propose a system for contextual ad matching based on a combination of semantic and syntactic features.", "references": ["Modern Information Retrieval", "A training algorithm for optimal margin classifiers", "Efficient query evaluation using a two-level retrieval process", "Modeling the clickstream: Implications for web-based advertising efforts", "Sponsored search: A brief history", "Taxonomies by the numbers: building high-performance taxonomies", "Learning to advertise", "Impedance coupling in content-targeted advertising", "Relevance feedback in information retrieval", "Bandits for taxonomies: A model-based approach", "The Statistical Analysis of Discrete Data", "The term vectordatabase: fast access to indexing terms for web pages", "Understanding consumers attitude toward advertising", "Finding advertising keywords on web pages"], "title": "A Semantic Approach to Contextual Advertising"}
{"id": "H-48", "keywords": "inform retriev;evalu;queri expans", "abstract": "The effectiveness of information retrieval (IR) systems is influenced by the degree of term overlap between user queries and relevant documents. Query-document term mismatch, whether partial or total, is a fact that must be dealt with by IR systems. Query Expansion (QE) is one method for dealing with term mismatch. IR systems implementing query expansion are typically evaluated by executing each query twice, with and without query expansion, and then comparing the two result sets. While this measures an overall change in performance, it does not directly measure the effectiveness of IR systems in overcoming the inherent issue of term mismatch between the query and relevant documents, nor does it provide any insight into how such systems would behave in the presence of query-document term mismatch. In this paper, we propose a new approach for evaluating query expansion techniques. The proposed approach is attractive because it provides an estimate of system performance under varying degrees of query-document term mismatch, it makes use of readily available test collections, and it does not require any additional relevance judgments or any form of manual processing.", "references": ["Query difficulty, robustness and selective application of query expansion", "Information retrieval as statistical translation", "Query expansion using associated queries", "When Query Expansion Fails", "Questioning Query Expansion: An Examination of Behaviour and Parameters", "Document Expansion versus Query Expansion for Ad-hoc Retrieval", "Evaluating Evaluation Measure Stability", "Retrieval evaluation with incomplete information", "What Makes A Query Difficult?", "Informative Term Selection for Automatic Query Expansion", "Incremental Test Collections", "Minimal Test Collections for Retrieval Evaluation", "Efficient Construction of Large Test Collections", "Statistical Precision of Information Retrieval Evaluation", "A Language Modeling Framework for Selective Query Expansion", "Annual Review of Information Systems and Technology ", "CLARIT-TREC Experiments", "Semantic Term Matching in Axiomatic Approaches to Information Retrieval", "Dependence language model for information retrieval", "Relevance feedback revisited", "Towards Interactive Query Expansion", "Probabilistic latent semantic indexing", "The Association Thesaurus for Information Retrieval", "Query expansion reduction and its impact on retrieval effectiveness", "Comparing Cross-Language Query Expansion Techniques by Degrading Translation Resources", "Improving Automatic Query Expansion", "The limitations of term co--occurrence data for query expansion in document retrieval systems", "A language modeling approach to information retrieval", "Concept based query expansion", "On GMAP - and other transformations", "Relevance Weighting of Search Terms", "The SMART Retrieval System", "Automatic Information Organization and Retrieval", "The SMART Retrieval System: Experiments in Automatic Document Processing", "Automatic term class construction using relevance-a summary of work in automatic pseudoclassification", "On the Use of Spreading Activation Methods in Automatic Information Retrieval", "Word sense disambiguation and information retrieval", "Forming test collections with no system pooling", "Information Retrieval System Evaluation: Effort, Sensitivity, and Reliability", "The Retrieval Effects of Query Expansion on a Feedback Document Retrieval System", "A general language model for information retrieval", "Automatic Keyword Classification for Information Retrieval", "Scoring missing terms in information retrieval tasks", "Boolean Query Evaluation: A Comparison of Retrieval Performance", "On Expanding Query Vectors with Lexically Related Words", "Query Expansion Using Lexical-Semantic Relations"], "title": "A New Approach for Evaluating Query Expansion: Query-Document Term Mismatch"}
{"id": "H-49", "keywords": "autocorrel;regular;perform predict", "abstract": "Evaluation of information retrieval systems is one of the core tasks in information retrieval. Problems include the inability to exhaustively label all documents for a topic, generalizability from a small number of topics, and incorporating the variability of retrieval systems. Previous work addresses the evaluation of systems, the ranking of queries by difficulty, and the ranking of individual retrievals by performance. Approaches exist for the case of few and even no relevance judgments. Our focus is on zero-judgment performance prediction of individual retrievals. One common shortcoming of previous techniques is the assumption of uncorrelated document scores and judgments. If documents are embedded in a high-dimensional space (as they often are), we can apply techniques from spatial data analysis to detect correlations between document scores. We find that the low correlation between scores of topically close documents often implies a poor retrieval performance. When compared to a state of the art baseline, we demonstrate that the spatial analysis of retrieval scores provides significantly better prediction performance. These new predictors can also be incorporated with classic predictors to improve performance further. We also describe the first large-scale experiment to evaluate zero-judgment performance prediction for a massive number of retrieval systems over a variety collections in several languages.", "references": ["Query hardness estimation using jensen-shannon divergence among multiple scoring functions", "A statistical method for system evaluation using incomplete judgments", "What makes a query difficult?", "Minimal test collections for retrieval evaluation", "Spatial Autocorrelation", "Precision prediction based on ranked list coherence", "Using temporal profiles of queries for precision prediction", "Spatial Autocorrelation and Spatial Filtering", "Inferring Query Performance Using Pre-retrieval Predictors", "The use of hierarchic clustering in information retrieval", "Linkage and autocorrelation cause feature selection bias in relational learning", "Corpus structure, language models, and ad hoc information retrieval", "Relevance score normalization for metasearch", "A study of relevance propagation for web search", "Ranking retrieval systems without relevance judgments", "On ranking the effectiveness of searches", "Ranking robustness: a novel framework to predict query performance"], "title": "Performance Prediction Using Spatial Autocorrelation"}
{"id": "H-50", "keywords": "data fusion;metasearch engin;multipl criterium approach;outrank method;rank aggreg", "abstract": "Research in Information Retrieval usually shows performanceimprovement when many sources of evidence are combined to produce a ranking of documents (e.g., texts, pictures, sounds, etc.). In this paper, we focus on the rank aggregation problem, also called data fusion problem, where rankings of documents, searched into the same collection and provided by multiple methods, are combined in order to produce a new ranking. In this context, we propose a rank aggregation method within a multiple criteria framework using aggregation mechanisms based on decision rules identifying positive and negative reasons for judging whether a document should get a better rank than another. We show that the proposed method deals well with the Information Retrieval distinctive features. Experimental results are reported showing that the suggested method performs better than the well-known CombSUM and CombMNZ operators.", "references": ["Fusion of knowledge-intensive and statistical approaches for retrieving and annotating textual genomics documents", "Modern Information Retrieval", "Automatic combination of multiple ranked retrieval systems", "Combining evidence of multiple query representations for information retrieval", "Mmoire sur les lections au scrutin", "Searching distributed collections with inference networks", "Essai sur l'application de l'analyse a la probabilit des dcisions rendues a la pluralit des voix", "Ordinal ranking with intensity of preference", " Overview of the TREC-2002 Web Track", "Overview of the TREC-2004 Web Track", "Rank aggregation methods for the Web", "Combining fuzzy information from multiple systems", "Comparing and aggregating rankings with ties", "Comparing top k lists", "Combination of multiple searches", "A study of the overlap among document representations", "Collection selection and results merging with topically organized U.S. patents and TREC data", "Database merging strategy based on logistic regression", "Analyses of multiple evidence combination", "Probfuse: a probabilistic approach to data fusion", "Analyzing and Modeling Rank Data", "Metasearch consistency", "Analysis of the axiomatic foundations of collaborative filtering", "Web metasearch: rank vs. score based rank aggregation methods", "Liberalism against populism", "The outranking approach and the foundations of ELECTRE methods", "Ranking of suburban line extension projects on the Paris metro system by a multicriteria method", "Using sampled data and regression to merge search engine results", "An extension of the Condorcet criterion and Kemeny orders", "Inference networks for document retrieval", "Fusion via a linear combination of scores"], "title": "An Outranking Approach for Rank Aggregation in Information Retrieval"}
{"id": "H-52", "keywords": "speech retriev;speak term detect;out-of-vocabulari", "abstract": "We are interested in retrieving information from speech data like broadcast news, telephone conversations and roundtable meetings. Today, most systems use large vocabulary continuous speech recognition tools to produce word transcripts; the transcripts are indexed and query terms are retrieved from the index. However, query terms that are not part of the recognizer's vocabulary cannot be retrieved, and the recall of the search is affected. In addition to the output word transcript, advanced systems provide also phonetic transcripts, against which query terms can be matched phonetically. Such phonetic transcripts suffer from lower accuracy and cannot be an alternative to word transcripts.We present a vocabulary independent system that can handle arbitrary queries, exploiting the information provided by having both word transcripts and phonetic transcripts. A speech recognizer generates word confusion networks and phonetic lattices. The transcripts are indexed for query processing and ranking purpose.The value of the proposed method is demonstrated by the relative high performance ofour system, which received the highest overall ranking for US English speech data in the recent NIST Spoken Term Detection evaluation.", "references": ["NIST Spoken Term Detection 2006 Evaluation Website", "NIST Spoken Term Detection (STD) 2006 Evaluation Plan", "General indexation of weighted automata -- application to spoken utterance retrieval", "Mutual relevance feedback for multimodal query formulation in video retrieval", "Advances in phonetic word spotting", "Open-vocabulary speech indexing for voice and video mail retrieval", "Juru at TREC 10 -- Experiments with Index Pruning", "Indexing uncertainty for spoken document search", "Position specific posterior lattices for indexing speech", "Conditional and joint models for grapheme-to-phoneme conversion", "Phonetic searching applied to on-line distance learning modules", "The TREC spoken document retrieval track: A success story", "A general algorithm for word graph matrix decomposition", "The application of classical information retrieval techniques to spoken documents", "A system for unrestricted topic retrieval from radio news broadcasts", "An experimental study of an audio indexing system for the web", "Spoken document retrieval from call-center conversations", "Finding consensus in speech recognition: word error minimization and other applications of confusion networks", "The DET curve in assessment of detection task performance", "Subword-based approaches for spoken document retrieval", "Fast two-stage vocabulary-independent search in spontaneous speech", "Lattice-based search for spoken utterance retrieval", "Vocabulary-independent search in spontaneous speech", "AT&T at TREC-7", "Document expansion for speech retrieval", "The IBM 2004 conversational telephony system for rich transcription", "Dynamic match phone-lattice searches for very fast and accurate unrestricted vocabulary keyword spotting", "Effects of out of vocabulary words in spoken document retrieval"], "title": "Vocabulary Independent Spoken Term Detection"}
{"id": "H-53", "keywords": "stem;languag model;web search", "abstract": "Traditionally, stemming has been applied to Information Retrieval tasks by transforming words in documents to the their root form before indexing, and applying a similar transformation to query terms. Although it increases recall, this naive strategy does not work well for Web Search since it lowers precision and requires a significant amount of additional computation. In this paper, we propose a context sensitive stemming method that addresses these two issues. Two unique properties make our approach feasible for Web Search. First, based on statistical language modeling, we perform context sensitive analysis on the query side. We accurately predict which of its morphological variants is useful to expand a query term with before submitting the query to the search engine. This dramatically reduces the number of bad expansions, which in turn reduces the cost of additional computation and improves the precision at the same time. Second, our approach performs a context sensitive document matching for those expanded variants. This conservative strategy serves as a safeguard against spurious stemming, and it turns out to be very important for improving precision. Using word pluralization handling as an example of our stemming approach, our experiments on a major Web search engine show that stemming only 29% of the query traffic, we can improve relevance as measured by average Discounted Cumulative Gain (DCG5) by 6.1% on these queriesand 1.8% over all query traffic.", "references": ["Improving Web Search Ranking by Incorporating User Behavior Information", "Word Normalization and Decompounding in Mono- and Bilingual IR", "Using Terminological Feedback for Web Search Refinement: a Log-based Study", "Modern Information Retrieval", "An Empirical Study of Smoothing Techniques for Language Modeling", "A Framework for Selective Query Expansion", "Semantic Term Matching in Axiomatic Approaches to Information Retrieval", "Term Conflation for Information Retrieval", "How Effective is Suffixing?", "Stemming Algorithms -- A Case Study for Detailed Evaluation", "Cumulated Gain-Based Evaluation Evaluation of IR Techniques", "Generating Query Substitutions", "Viewing Stemming as Recall Enhancement", "Viewing Morphology as an Inference Process", "Automatic Retrieval and Clustering of Similar Words", "Development of a Stemming Algorithm", "An Evaluation of Some Conflation Algorithms for Information Retrieval", "An Algorithm for Suffix Stripping", "Query Segmentation for Web Search", "On Term Selection for Query Expansion", "Improving Retrieval Performance by Relevance Feedback", "Mining Dependency Relations for Query Expansion in Passage Retrieval", "Information Retrieval", "Fast and Effective Query Refinement", "Query Expansion using Local and Global Document Analysis", "Corpus--based Stemming using Cooccurrence of Word Variants"], "title": "Context Sensitive Stemming for Web Search"}
{"id": "H-54", "keywords": "document retriev;passag extract;biomed document", "abstract": "This paper presents a study of incorporating domain-specific knowledge (i.e., information about concepts and relationships between concepts in a certain domain) in an information retrieval (IR) system to improve its effectiveness in retrieving biomedical literature. The effects of different types of domain-specific knowledge in performance contribution are examined. Based on the TREC platform, we show that appropriate use of domain-specific knowledge in a proposed conceptual retrieval model yields about 23% improvement over the best reported result in passage retrieval in the Genomics Track of TREC 2006.", "references": ["Query expansion using the UMLS Metathesaurus", "Modern Information Retrieval", "Domain-specific synonym expansion and validation for biomedical information retrieval", "Creating an online dictionary of abbreviations from MEDLINE", "Word association norms, mutual information and lexicography", "askMEDLINE: a free-text, natural language query tool for MEDLINE PubMed", "Toward information extraction: identifying protein names from biological papers", "Assessing thesaurus-based query expansion using the UMLS Metathesaurus", "Binary codes capable of correcting deletions, insertions, and reversals", "The Role of Knowledge in Conceptual Retrieval: A Study in the Domain of Clinical Medicine", "The Unified Medical Language System", "An Effective Approach to Document Retrieval via Utilizing WordNet and Recognizing Phrases", "Detecting Gene Symbols and Names in Biological Texts: A First Step toward Pertinent Information Extraction", "Okapi Keenbow at TREC-8", "Evidence-Based Medicine: How to Practice and Teach EBM", "An interactive system for finding complementary literatures: a stimulus to scientific discovery", "Query expansion using lexical-semantic relations", "Concept-based biomedical text retrieval", "ADAM: Another Database of Abbreviations in MEDLINE"], "title": "Knowledge-intensive Conceptual Retrieval and Passage Extraction of Biomedical Literature"}
{"id": "H-60", "keywords": "probabilist inform retriev;invers document frequenc;idf;poisson distribut;inform theori;independ assumpt", "abstract": "This paper reports on theoretical investigations about the assumptions underlying the inverse document frequency (idf). We show that an intuitive idf-based probability function for the probability of a term being informative assumes disjoint document events. By assuming documents to be independent rather than disjoint, we arrive at a Poisson-based probability of being informative. The framework is useful for understanding and deciding the parameter estimation and combination in probabilistic retrieval models.", "references": ["An information-theoretic perspective of tf-idf measures", "Term frequency normalization via Pareto distributions", "Finding out about", "Probabilistic models for automatic indexing", "Taschenbuch der Mathematik", "Poisson mixtures", "Inverse document frequency: A measure of deviations from poisson", "Links between information construction and information gain: Entropy and bibliometric distribution", "N-poisson document modelling", "Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval", "An information-theoric measure of term specificity", "On modeling information retrieval with probabilistic inference"], "title": "A Frequency-based and a Poisson-based Definition of the Probability of Being Informative"}
{"id": "H-61", "keywords": "advertis;web;bayesian network;knn", "abstract": "The current boom of the Web is associated with the revenues originated from on-line advertising. While search-based advertising is dominant, the association of ads with a Web page (during user navigation) is becoming increasingly important. In this work, we study the problem of associating ads with a Web page, referred to as content-targeted advertising, from a computer science perspective. We assume that we have access to the text of the Web page, the keywords declared by an advertiser, and a text associated with the advertiser's business. Using no other information and operating in fully automatic fashion, we propose ten strategies for solving the problem and evaluate their effectiveness. Our methods indicate that a matching strategy that takes into account the semantics of the problem (referred to as AAK for "ads and keywords") can yield gains in average precision figures of 60% compared to a trivial vector-based strategy. Further, a more sophisticated impedance coupling strategy, which expands the text of the Web page to reduce vocabulary impedance with regard to an advertisement, can yield extra gains in average precision of 50%. These are first results. They suggest that great accuracy in content-targeted advertising can be attained with appropriate algorithms.", "references": ["Google content-targeted advertising", "Modern Information Retrieval", "Paid placement strategies for internet search engines", "Trec-5 ad hoc retrieval using k nearest-neighbors re-scoring", "Comparison of allocation rules for paid placement advertising in search engines", "Overview of TREC-7 very large collection track", "Ubiquitous advertising on the www: merging advertisement on the browser", "Unintrusive customization techniques for web advertising", "New metrics for new media: toward the development of web measurement standards", "Probabilistic Reasoning in Intelligent Systems: Networks of plausible inference", "A belief network model for IR", "CobWeb - a crawler for the brazilian web", "Evaluation of an inference network-based retrieval model", "Understanding consumers attitude toward advertising", "Ethical issues on content distribution to digital consumers via paid placement as opposed to website visibility in search engine results", "An investigation into search engines as a form of targeted advert delivery", "Expert network: Effective and efficient learning from human decisions in text categorization and retrieval"], "title": "Impedance Coupling in Content-targeted Advertising"}
{"id": "H-62", "keywords": "implicit feedback;person search;user model;interact retriev", "abstract": "Information retrieval systems (e.g., web search engines) are critical for overcoming information overload. A major deficiency of existing retrieval systems is that they generally lack user modeling and are not adaptive to individual users, resulting in inherently non-optimal retrieval performance. For example, a tourist and a programmer may use the same word "java" to search for different information, but the current search systems would return the same results. In this paper, we study how to infer a user's interest from the user's search context and use the inferred implicit user model for personalized search. We present a decision theoretic framework and develop techniques for implicit user modeling in information retrieval. We develop an intelligent client-side web search agent (UCAIR) that can perform eager implicit feedback, e.g., query expansion based on previous queries and immediate result reranking based on clickthrough information. Experiments on web search show that our search agent can improve search accuracy over the popular Google search engine.", "references": ["Hourly analysis of a very large topically categorized web query log", "Overview of the TREC 2004 terabyte track", "Implicit interest indicators", "Overview of the TREC 2003 web track", "Relevance feedback and personalization: A language modeling perspective", "Google Personalized", "Results and challenges in web search evaluation", "Dynamic web log session identification with statistical language models", "Scaling personalized web search", "Optimizing search engines using clickthrough data", "Implicit feedback for inferring user preference: A bibliography", "Implicit feedback for inferring user preference: A bibliography", "Patterns of search: Analyzing and modeling web query refinement", "Relevance-based language models", "Improving automatic query expansion", "My Yahoo!", "As google goes, so goes the nation", "The probability ranking principle in ir", "Relevance feedback in information retrieval", "Improving retrieval performance by retrieval feedback", "Introduction to Modern Information Retrieval", "Context-sensitive information retrieval using implicit feedback", "Exploiting query history for document ranking in interactive information retrieval", "Modern information retrieval: A brief overview", "Adaptive web search based on user profile constructed without any effort from users", "Personalization and privacy", "A simulated study of implicit feedback models", "Query expansion using local and global document analysis", "Model-based feedback in KL divergence retrieval model"], "title": "Implicit User Modeling for Personalized Search"}
{"id": "H-63", "keywords": "wireless data broadcast;index;locat base servic;data stage;locat depend data", "abstract": "Data dissemination through wireless channels for broadcasting information to consumers is becoming quite common. Many dissemination schemes have been proposed but most of them push data to wireless channels for general consumption. Push based broadcast [1] is essentially asymmetric, i.e., the volume of data being higher from the server to the users than from the users back to the server. Push based scheme requires some indexing which indicates when the data will be broadcast and its position in the broadcast. Access latency and tuning time are the two main parameters which may be used to evaluate an indexing scheme. Two of the important indexing schemes proposed earlier were tree based and the exponential indexing schemes. None of these schemes were able to address the requirements of location dependent data (LDD) which is highly desirable feature of data dissemination. In this paper, we discuss the broadcast of LDD in our project DAta in Your Space (DAYS), and propose a scheme for indexing LDD. We argue that this scheme, when applied to LDD, significantly improves performance in terms of tuning time over the above mentioned schemes. We prove our argument with the help of simulation results.", "references": ["Broadcast disk: Data management for asymmetric communications environments", "Optimizing index allocation for sequential data broadcasting in wireless mobile computing", "Performance evaluation of a wireless hierarchical data dissemination system", "Power conservative multi-attribute queries on data broadcast", "Power efficient filtering of data on air", "Data on air -- Organization and access", "Wake on wireless: An event driven energy saving strategy for battery operated devices", "Energy-efficient indexing for information dissemination in wireless systems", "Energy efficient filtering of non uniform broadcast", "Energy management on handheld devices", "Information Mapping and Indexing in DAYS", "InfoSpace: Hybrid and Adaptive Public Data Dissemination System for Ubiquitous Computing", "Discovering and using Web Services in M-Commerce", "Indexing Location Dependent Data in broadcast environment", "Data Staging on Untrusted Surrogates", "Location dependent query processing", "Exponential Index: A Parameterized Distributed Indexing Scheme for Data on Air"], "title": "Location based Indexing Scheme for DAYS"}
{"id": "H-64", "keywords": "machin learn;inform architectur;interfac design", "abstract": "This paper describes ongoing research into the application of machine learning techniques for improving access to governmental information in complex digital libraries. Under the auspices of the GovStat Project, our goal is to identify a small number of semantically valid concepts that adequately spans the intellectual domain of a collection. The goal of this discovery is twofold. First we desire a practical aid for information architects. Second, automatically derived document-concept relationships are a necessary precondition for real-world deployment of many dynamic interfaces. The current study compares concept learning strategies based on three document representations: keywords, titles, and full-text. In statistical and user-based studies, human-created keywords provide significant improvements in concept learning over both title-only and full-text representations.", "references": ["An Introduction to Categorical Data Analysis", "Dynamic queries for information exploration: an implementation and evaluation", "Modern Information Retrieval", "Combining labeled and unlabeled data with co-training", "Hierarchical classification of web content", "Implications of the recursive representation problem for automatic concept identification in on-line governmental information", "How many clusters? which clustering method? answers via model-based cluster analysis", "Data clustering: a review", "A probabilistic analysis of the Rocchio algorithm with TFIDF for text categorization", "Text categorization with support vector machines: learning with many relevant features", "Principal Component Analysis", "Finding Groups in Data: an Introduction to Cluster Analysis", "Toward a general relation browser: a GUI for information architects", "Bow: A toolkit for statistical language modeling, text retrieval, classification and clustering", "Machine Learning", "Clustering algorithms", "Estimating the number of clusters in a dataset via the gap statistic", "The Nature of Statistical Learning Theory"], "title": "Machine Learning for Information Architecture in a Large Governmental Website"}
{"id": "H-69", "keywords": "web object;imag search;rank", "abstract": "Vertical search is a promising direction as it leverages domain-specific knowledge and can provide more precise information for users. In this paper, we study the Web object-ranking problem, one of the key issues in building a vertical search engine. More specifically, we focus on this problem in cases when objects lack relationships between different Web communities, and take high-quality photo search as the test bed for this investigation. We proposed two score fusion methods that can automatically integrate as many Web communities (Web forums) with rating information as possible. The proposed fusion methods leverage the hidden links discovered by a duplicate photo detection algorithm, and aims at minimizing score differences of duplicate photos in different forums. Both intermediate results and user studies show the proposed fusion methods are practical and efficient solutions to Web object ranking in cases we have described. Though the experiments were conducted on high-quality photo ranking, the proposed algorithms are also applicable to other ranking problems, such as movie ranking and music ranking.", "references": ["Modern Information Retrieval", "Large-scale duplicate detection for web image search", "The anatomy of a large-scale hypertextual web search engine", "Learning to rank using gradient descent", "Rank aggregation methods for the web", "Comparing top k lists", "An efficient boosting algorithm for combining preferences", "Formula for calculating the top rated 250 titles in imdb", "Optimizing search engines using clickthrough data", "Authoritative sources in a hyperlinked environment", "Discriminative models for information retrieval", "Object-level web information retrieval", "Object-level ranking: Bringing order to web objects", "The pagerank citation ranking: Bringing order to the web", "Evaluation of image appeal in consumer photography", "Hitwise search engine ratings", "Color image quality on the internet", "Classification of digital photos taken by photographers or home users", "Link fusion: a unified link analysis framework for multi-type interrelated data objects"], "title": "Ranking Web Objects from Multiple Communities"}
{"id": "H-73", "keywords": "distribut inform retriev;resourc select", "abstract": "This paper presents a unified utility framework for resource selection of distributed text information retrieval. This new framework shows an efficient and effective way to infer the probabilities of relevance of all the documents across the text databases. With the estimated relevance information, resource selection can be made by explicitly optimizing the goals of different applications. Specifically, when used for database recommendation, the selection is optimized for the goal of high-recall (include as many relevant documents as possible in the selected databases); when used for distributed document retrieval, the selection targets the high-precision goal (high precision in the final merged list of documents). This new model provides a more solid framework for distributed information retrieval. Empirical studies show that it is at least as effective as other state-of-the-art algorithms.", "references": ["Croft, editor, Advances in Information Retrieval", "TREC and TIPSTER experiments with INQUERY", "Database selection using actual physical and acquired logical collection resources in a massive domain-specific operational environment", "Methods for distributed information retrieval", "Merging results from isolated search engines", "A comparison of techniques for selecting text collections", "A Decision-Theoretic approach to database selection in networked IR", "STARTS: Stanford proposal for internet meta-searching", "QProber: A System for Automatic Classification of Hidden-Web Databases", " Distributed search over the hidden web: Hierarchical database sampling and selection", "Content-based information retrieval in peer-to-peer networks", "Building efficient and effective metasearch engines", "Evaluating different method of estimating retrieval quality for resource selection", "The MIND architecture for heterogeneous multimedia federated digital libraries", "The impact of database selection on distributed searching", "Comparing the performance of database selection algorithms", "Search for the invisible web", "Using sampled data and regression to merge search engine results", "Relevant document distribution estimation method for resource selection", "A Semi-Supervised learning method to merge search engine results"], "title": "Unified Utility Maximization Framework for Resource Selection"}
{"id": "H-77", "keywords": "inform extract;metada extract;machin learn;search", "abstract": "We propose a machine learning approach to title extraction from general documents. By general documents, we mean documents that can belong to any one of a number of specific genres, including presentations, book chapters, technical papers, brochures, reports, and letters. Previously, methods have been proposed mainly for title extraction from research papers. It has not been clear whether it could be possible to conduct automatic title extraction from general documents. As a case study, we consider extraction from Office including Word and PowerPoint. In our approach, we annotate titles in sample documents (for Word and PowerPoint respectively) and take them as training data, train machine learning models, and perform title extraction using the trained models. Our method is unique in that we mainly utilize formatting information such as font size as features in the models. It turns out that the use of formatting information can lead to quite accurate extraction from general documents. Precision and recall for title extraction from Word is 0.810 and 0.837 respectively, and precision and recall for title extraction from PowerPoint is 0.875 and 0.895 respectively in an experiment on intranet data. Other important new findings in this work include that we can train models in one domain and apply them to another domain, and more surprisingly we can even train models in one language and apply them to another language. Moreover, we can significantly improve search ranking results in do document retrieval by using the extracted titles.", "references": ["A maximum entropy approach to natural language processing", "Discriminative training methods for hidden markov models: theory and experiments with perceptron algorithms", "Support-vector networks", "A maximum entropy approach to information extraction from semi-structured and free text", "Columbia newsblaster: multilingual news summarization on the Web", "Factorial hidden markov models", "Data and metadata for finding and reminding", "eBizSearch: a niche search engine for e-Business", "Knowledge-based metadata extraction from PostScript files", "Automatic document metadata extraction using support vector machines", "Information retrieval on the Web", "Conditional random fields: probabilistic models for segmenting and labeling sequence data", "The perceptron algorithm with uneven margins", "Automatic Metadata generation & evaluation", "Effective enterprise information retrieval across new content formats", "A dynamic feature generation system for automated metadata extraction in preservation of digital materials", "Maximum entropy markov models for information extraction and segmentation", "Digital document metadata in organizations: roles analytical approaches and future research directions", "Table extraction using conditional random fields", "Unsupervised statistical models for prepositional phrase attachment", "Simple BM25 extension to multiple weighted fields", "Metadata based Web mining for relevance", "MetaExtract: An NLP system to automatically assign metadata", "Internet search engines' response to metadata Dublin Core implementation", "Recognising and using named entities: focused named entity recognition using machine learning"], "title": "Automatic Extraction of Titles from General Documents using Machine Learning"}
{"id": "H-79", "keywords": "static rank;search engin;pagerank;ranknet;relev", "abstract": "Since the publication of Brin and Page's paper on PageRank, many in the Web community have depended on PageRank for the static (query-independent) ordering of Web pages. We show that we can significantly outperform PageRank using features that are independent of the link structure of the Web. We gain a further boost in accuracy by using data on the frequency at which users visit Web pages. We use RankNet, a ranking machine learning algorithm, to combine these and other static features based on anchor text and domain characteristics. The resulting model achieves a static ranking pairwise accuracy of 67.3% (vs. 56.7% for PageRank or 50% for random).", "references": ["Does authority mean quality? Predicting expert quality ratings of Web documents", "Automatic combination of multiple ranked retrieval systems", "PageRank as a function of the damping factor", "A machine learning architecture for optimizing web search engines", "The anatomy of a large-scale hypertextual web search engine", "Efficient PageRank approximation via graph aggregation", "Learning to rank using gradient descent", "Static index pruning for information retrieval systems", "Impact of search engines on page popularity", "Page Quality: In search of an unbiased web ranking", "Relevance weighting for query independent evidence", "Adversarial Classification", "Log-linear models for label-ranking", "Evaluating implicit measures to improve the search experiences", "Efficient computation of PageRank", "Topic-sensitive PageRank", "Very large scale retrieval and Web search", "Support vector learning for ordinal regression", "Statistical profiles of highly-rated Web sites", "Optimizing search engines using clickthrough data", "Accurately Interpreting Clickthrough Data as Implicit Feedback", "Authoritative sources in a hyperlinked environment", "Deeper inside PageRank", "The effect of the back button in a random walk: application for PageRank", "A uniform approach to accelerated PageRank computation", "Static approximation of dynamically generated Web pages", "The PageRank citation ranking: Bringing order to the web", "User-centric Web crawling", "The intelligent surfer: probabilistic combination of link and content information in PageRank", "Predicting fame and fortune: PageRank or indegree?", "Query-independent evidence in home page finding"], "title": "Beyond PageRank: Machine Learning for Static Ranking"}
{"id": "H-81", "keywords": "visual inform retriev;content-base imag retriev;content-base video retriev;similar measur;distanc measur;similar percept;mpeg-7", "abstract": "In visual information retrieval the careful choice of suitable proximity measures is a crucial success factor. The evaluation presented in this paper aims at showing that the distance measures suggested by the MPEG-7 group for the visual descriptors can be beaten by general-purpose measures. Eight visual MPEG-7 descriptors were selected and 38 distance measures implemented. Three media collections were created and assessed, performance indicators developed and more than 22500 tests performed. Additionally, a quantisation model was developed to be able to use predicate-based distance measures on continuous data as well. The evaluation shows that the distance measures recommended in the MPEG-7-standard are among the best but that other measures perform even better.", "references": ["An extension of the coefficient of divergence for use with multiple characters", "A profile similarity coefficient invariant over variable reflection", "Visual information retrieval", "A framework for visual information retrieval", "Visual similarity measurement with the Feature Contrast Model", "How good are the visual MPEG-7 features?", "Multivariate analysis and multidimensional geometry", "Mixed data classificatory programs", "Color and texture descriptors", "The problem is epistemology, not statistics: Replace significance tests by confidence intervals and quantify accuracy of risky numerical predictions", "On the coefficients of racial likeness", "Similarity is a geometer", "Similarity measures", "Similarity structures and similarity measures", "The TREC-2002 video track report", "Content-based image retrieval at the end of the early years", "Features of similarity"], "title": "Distance Measures for MPEG-7-based Retrieval"}
{"id": "H-82", "keywords": "hide web crawl;deep web crawler;keyword queri;adapt algorithm;queri select", "abstract": "An ever-increasing amount of information on the Web today is available only through search interfaces: the users have to type in a set of keywords in a search form in order to access the pages from certain Web sites. These pages are often referred to as the hidden Web or the deep Web. Since there are no static links to the hidden Web pages, search engines cannot discover and index such pages and thus do not return them in the results. However, according to recent studies, the content provided by many hidden Web sites is often of very high quality and can be extremely valuable to many users. In this paper, we study how we can build an effective hidden Web crawler that can autonomously discover and download pages from the hidden Web. Since the only "entry point" to a hidden Web site is a query interface, the main challenge that a hidden Web crawler has to face is how to automatically generate meaningful queries to issue to the site. We provide a theoretical framework to investigate the query generation problem for the hidden Web and we propose effective policies for generating queries automatically. Our policies proceed iteratively, issuing a different query in every iteration. We experimentally evaluate the effectiveness of these policies on 4 real hidden Web sites and our results are very promising. For instance, in one experiment, one of our policies downloaded more than 90% of a hidden Web site (that contains 14 million documents) after issuing fewer than 100 queries.", "references": ["Querying text databases for efficient information extraction", "Modeling query-based access to text databases", "Old Search Engine the Library Tries to Fit Into a Google World", "Siphoning hidden-web data through keyword-based interfaces", "The deep web: Surfacing hidden value", "A technique for measuring the relative size and overlap of public web search engines", "Syntactic clustering of the web", "Automatic discovery of language models for text databases", "Query-based sampling of text databases", "Structured databases on the web: Observations and implications", "Finding replicated web collections", "Learning to query the web", "Automated discovery of search interfaces on the web", "Introduction to Algorithms", "Database techniques for the world-wide web: A survey", "Statistical schema matching across web query interfaces", "Distributed search over the hidden web: Hierarchical database sampling and selection", "Probe count and classify: Categorizing hidden web databases", "The Open Archives Initiative: Building a low-barrier interoperability framework", "Searching the World Wide Web", "Dpro: A probabilistic approach for hidden web database selection using dynamic probing", "DP9-An OAI Gateway Service for Web Crawlers", "Fractal Geometry of Nature", "What's new on the web? the evolution of the web from a search engine perspective", "Downloading hidden web content", "Does search engine's power threaten web's independence?", "Crawling the hidden web", "Human Behavior and the Principle of Least-Effort"], "title": "Downloading Textual Hidden Web Content Through Keyword Queries"}
{"id": "H-83", "keywords": "algorithm;experiment", "abstract": "Localized search engines are small-scale systems that index a particular community on the web. They offer several benefits over their large-scale counterparts in that they are relatively inexpensive to build, and can provide more precise and complete search capability over their relevant domains. One disadvantage such systems have over large-scale search engines is the lack of global PageRank values. Such information is needed to assess the value of pages in the localized search domain within the context of the web as a whole. In this paper, we present well-motivated algorithms to estimate the global PageRank values of a local domain. The algorithms are all highly scalable in that, given a local domain of size n, they use O(n) resources that include computation time, bandwidth, and storage. We test our methods across a variety of localized domains, including site-specific domains and topic-specific domains. We demonstrate that by crawling as few as n or 2n additional pages, our methods can give excellent global PageRank estimates.", "references": [" Crawling a country: better strategies than breadth-first for web page ordering", "Do your worst to make the best: paradoxical effects in pagerank incremental computations", "The anatomy of a large-scale hypertextual web search engine", "Focused crawling: a new approach to topic-specific web resource discovery", "Local methods for estimating pagerank values", "Efficient crawling through url ordering", "The second eigenvalue of the Google matrix", "Learning retrieval functions from implicit feedback", "Exploiting the block structure of the web for computing pagerank", "Extrapolation methods for accelerating pagerank computation", "Deeper inside pagerank", "Updating the stationary vector of an irreducible markov chain with an eye on Google's pagerank", "How much information 2003?", "A uniform approach to accelerated pagerank computation", "Stochastic complementation, uncoupling markov chains, and the theory of nearly reducible systems", "Query chains: learning to rank from implicit feedback", "Crawling the hidden web", "Focused crawling for both topical relevance and quality of medical information", "Computing pagerank in a distributed internet search system"], "title": "Estimating the Global PageRank of Web Communities"}
{"id": "H-84", "keywords": "event;depend;thread;cluster", "abstract": "With the overwhelming volume of online news available today, there is an increasing need for automatic techniques to analyze and present news to the user in a meaningful and efficient manner. Previous research focused only on organizing news stories by their topics into a flat hierarchy. We believe viewing a news topic as a flat collection of stories is too restrictive and inefficient for a user to understand the topic quickly. In this work, we attempt to capture the rich structure of events and their dependencies in a news topic through our event models. We call the process of recognizing events and their dependencies  event threading. We believe our perspective of modeling the structure of a topic is more effective in capturing its semantics than a flat list of on-topic stories. We formally define the novel problem, suggest evaluation metrics and present a few techniques for solving the problem. Besides the standard word based features, our approaches take into account novel features such as temporal locality of stories for event recognition and time-ordering for capturing dependencies. Our experiments on a manually labeled data sets show that our models effectively identify the events and capture dependencies among them.", "references": ["Topic detection and tracking pilot study: Final report", "Flexible intrinsic evaluation of hierarchical clustering for tdt", "Topic Detection and Tracking: Event based Information Organization", "Temporal summaries of new topics", "Catching the drift: Probabilistic content models, with applications to generation and summarization", "Discovering and comparing topic hierarchies", "Threading electronic mail: a preliminary study", "Investigations on event evolution in tdt", "Hierarchical text classification and evaluation", "Learning approaches for detecting and tracking news events"], "title": "Event Threading within News Topics"}
{"id": "H-85", "keywords": "interpret implicit relev feedback;user behavior model;predict relev prefer", "abstract": "Evaluating user preferences of web search results is crucial for search engine development, deployment, and maintenance. We present a real-world study of modeling the behavior of web search users to predict web search result preferences. Accurate modeling and interpretation of user behavior has important applications to ranking, click spam detection, web search personalization, and other tasks. Our key insight to improving robustness of interpreting implicit feedback is to model query-dependent deviations from the expected "noisy" user behavior. We show that our model of clickthrough interpretation improves prediction accuracy over state-of-the-art clickthrough methods. We generalize our approach to model user behavior beyond clickthrough, which results in higher preference prediction accuracy than models based on clickthrough information alone. We report results of a large-scale experimental evaluation that show substantial improvements over published implicit feedback interpretation methods.", "references": ["Improving Web Search Ranking by Incorporating User Behavior", "HARD Track Overview in TREC 2003: High Accuracy Retrieval from Documents", "The Anatomy of a Large-scale Hypertextual Web Search Engine", "Learning to Rank using Gradient Descent", "The WinMine Toolkit", "Inferring user interest", "Evaluating implicit measures to improve the search experience", "Learning users' interests by unobtrusively observing their normal behavior", "Optimizing Search Engines Using Clickthrough Data", "Accurately Interpreting Clickthrough Data as Implicit Feedback", "Making Large-Scale SVM Learning Practical", "Implicit feedback for inferring user preference: A bibliography", "GroupLens: Applying collaborative filtering to usenet news", "Information filtering based on user behavior analysis and best match text retrieval", "Implicit feedback for recommender systems", "Modeling information content using observable behavior", "The Use of Proximal Information Scent to Forage for Distal Content on the World Wide Web", "Query Chains: Learning to Rank from Implicit Feedback", "Evaluating the Robustness of Learning from Implicit Feedback", "Introduction to modern information retrieval"], "title": "Learning User Interaction Models for Predicting Web Search Result Preferences"}
{"id": "H-87", "keywords": "adapt filter;topic track;cross-benchmark evalu;logist regress;rocchio", "abstract": "This paper reports a cross-benchmark evaluation of regularized logistic regression (LR) and incremental Rocchio for adaptive filtering. Using four corpora from the Topic Detection and Tracking (TDT) forum and the Text Retrieval Conferences (TREC) we evaluated these methods with non-stationary topics at various granularity levels, and measured performance with different utility settings. We found that LR performs strongly and robustly in optimizing T11SU (a TREC utility function) while Rocchio is better for optimizing Ctrk (the TDT tracking cost), a high-recall oriented objective function. Using systematic cross-corpus parameter optimization with both methods, we obtained the best results ever reported on TDT5, TREC10 and TREC11. Relevance feedback on a small portion (0.05~0.2%) of the TDT5 test documents yielded significant performance improvements, measuring up to a 54% reduction in Ctrk and a 20.9% increase in T11SU (with b=0.1), compared to the results of the top-performing system in TDT2004 without relevance feedback information.", "references": ["Incremental relevance feedback for information filtering", "Learning while filtering documents", "Topic detection and tracking overview", "Overview of the TDT 2004 Evaluation and Results", "Elements of Statistical Learning", "The TREC-9 filtering track final report", "The TREC-10 filtering track final report", "The TREC 2002 filtering track report", "Microsoft Cambridge at TREC-9", "Boosting and Rocchio applied to text filtering", "Margin-based local regression for adaptive filtering", "Maximum likelihood estimation for filtering thresholds", "Using Bayesian priors to combine classifiers for adaptive filtering", "Robustness of regularized linear classification methods in text categorization", "Text Categorization Based on Regularized Linear Classification Methods"], "title": "Robustness of Adaptive Filtering Methods In a Cross-benchmark Evaluation"}
{"id": "H-88", "keywords": "xml;rank;inform retriev", "abstract": "The direct application of standard ranking techniques to retrieve individual elements from a collection of XML documents often produces a result set in which the top ranks are dominated by a large number of elements taken from a small number of highly relevant documents. This paper presents and evaluates an algorithm that re-ranks this result set, with the aim of minimizing redundant content while preserving the benefits of element retrieval, including the benefit of identifying topic-focused components contained within relevant documents. The test collection developed by the INitiative for the Evaluation of XML Retrieval (INEX) forms the basis for the evaluation.", "references": ["Holistic twig joins: Optimal XML pattern matching", "Searching XML documents via XML fragments", "MultiText experiments for INEX 2004", "Tolerance to irrelevance: A user-effort oriented evaluation of retrieval systems without predefined retrieval unit", "A comprehensive XQuery to SQL translation using dynamic interval encoding", "XIRQL: A query language for information retrieval in XML documents", "Initiative for the Evaluation of XML Retrieval", "Initiative for the Evaluation of XML Retrieval", "Cumulated gain-based evaluation of IR techniques", "Length normalization in XML retrieval", "The overlap problem in content-oriented XML retrieval evaluation", "Reliability tests for the XCG and inex-2002 metrics", "TRIX 2004~---~Struggling with the overlap", "Configurable indexing and ranking for XML information retrieval", "Retrieving the most relevant XML components", "Component ranking and automatic query refinement for XML retrieval", "Hierarchical language models for XML component retrieval", "Hybrid XML retrieval re-visited", "Providing consistent and exhaustive relevance assessments for XML retrieval evaluation", "Simple BM25 extension to multiple weighted fields", "Okapi at TREC-7: Automatic ad hoc, filtering, VLC and interactive track", "NEXI, now and next", "An algebra for structured queries in bayesian networks"], "title": "Controlling Overlap in Content-Oriented XML Retrieval"}
{"id": "H-90", "keywords": "queri histori;queri expans;interact retriev;context", "abstract": "A major limitation of most existing retrieval models and systems is that the retrieval decision is made based solely on the query and document collection; information about the actual user and search context is largely ignored. In this paper, we study how to exploit implicit feedback information, including previous queries and clickthrough information, to improve retrieval accuracy in an interactive information retrieval setting. We propose several context-sensitive retrieval algorithms based on statistical language models to combine the preceding queries and clicked document summaries with the current query for better ranking of documents. We use the TREC AP data to create a test collection with search context information, and quantitatively evaluate our models using this test set. Experiment results show that using implicit feedback, especially the clicked document summaries, can improve retrieval performance substantially.", "references": ["Haystack: Per-user information environments", "Challenges in information retrieval and language modeling", "Searchpad: Explicit capture of search context to support web search", "Relevance feedback and personalization: A language modeling perspective", "Probabilistic query expansion using query logs", "Implicit queries (IQ) for contextualized search (demo description)", "Placing search in context: The concept revisited", "Query session based term suggestion for interactive web search", "Dynamic web log session identification with statistical language models", "Scaling personalized web search", "Optimizing search engines using clickthrough data", "Display time as implicit feedback: Understanding task effects", "Implicit feedback for inferring user preference", "Relevance feedback information retrieval", "Exploiting query history for document ranking in interactive information retrieval", "A session-based search engine", "Adaptive web search based on user profile constructed without any effort from users", "A simulated study of implicit feedback models", "Model-based feedback in the KL-divergence retrieval model", "A study of smoothing methods for language models applied to ad hoc information retrieval"], "title": "Context-Sensitive Information Retrieval Using Implicit Feedback"}
{"id": "H-92", "keywords": "web search;implicit relev feedback;web search rank", "abstract": "We show that incorporating user behavior data can significantly improve ordering of top results in real web search setting. We examine alternatives for incorporating feedback into the ranking process and explore the contributions of user feedback compared to other common web search features. We report results of a large scale evaluation over 3,000 queries and 12 million user interactions with a popular web search engine. We show that incorporating implicit feedback can augment other features, improving the accuracy of a competitive web search ranking algorithms by as much as 31% relative to the original performance.", "references": ["Learning User Interaction Models for Predicting Web Search Result Preferences", "HARD Track Overview in TREC 2003", "Modern Information Retrieval", "The Anatomy of a Large-scale Hypertextual Web Search Engine", "Learning to Rank using Gradient Descent", "The WinMine Toolkit", "Inferring user interest", "Evaluating implicit measures to improve the search experience", "Learning users' interests by unobtrusively observing their normal behavior", "IR evaluation methods for retrieving highly relevant documents", "Optimizing Search Engines Using Clickthrough Data", "Accurately Interpreting Clickthrough Data as Implicit Feedback", "Making Large-Scale SVM Learning Practical", "Implicit feedback for inferring user preference: A bibliography", "GroupLens: Applying collaborative filtering to usenet news", "Information filtering based on user behavior analysis and best match text retrieval", "Implicit feedback for recommender systems", "Modeling information content using observable behavior", "The SST method: a tool for analyzing web information search processes", "The Use of Proximal Information Scent to Forage for Distal Content on the World Wide Web", "Query Chains: Learning to Rank from Implicit Feedback", "Evaluating the Robustness of Learning from Implicit Feedback", "Simple BM25 extension to multiple weighted fields", "Introduction to modern information retrieval", "Overview of TREC", "Optimizing web search using web click-through data", "Microsoft Cambridge at TREC 13: Web and Hard Tracks"], "title": "Improving Web Search Ranking by Incorporating User Behavior Information"}
{"id": "H-95", "keywords": "geograph ir;text mine;queri process", "abstract": "This paper proposes simple techniques for handling place references in search engine queries, an important aspect of geographical information retrieval. We address not only the detection, but also the disambiguation of place references, by matching them explicitly with concepts at an ontology. Moreover, when a query does not reference any locations, we propose to use information from documents matching the query, exploiting geographic scopes previously assigned to these documents. Evaluation experiments, using topics from CLEF campaigns and logs from real search engine queries, show the effectiveness of the proposed approaches.", "references": ["Web-a-Where: Geotagging Web content", "A Geographic Knowledge Base for Semantic Web Applications", "Overview of MUC-7/MET-2", "GeoCLEF: the CLEF 2005 cross-language geographic information retrieval track", "Categorizing Web queries according to geographical locality", "Proper words in proper places: The thesaurus of geographic names", "Spatial information retrieval and geographical ontologies: An overview of the SPIRIT project", "Analyzing search engine queries for the use of geographic terms", "editors", "KDD CUP-2005 report: Facing a great challenge", "Experiments with geographic knowledge for information extraction", "Spelling correction for search engine queries", "A graph-ranking algorithm for geo-referencing documents", "The role of gazetteers in geographic knowledge discovery on the web", "Introduction to the CoNLL-2003 shared task: Language-Independent Named Entity Recognition", "Classifying search engine queries using the Web as background knowledge", "Detecting dominant locations from search queries"], "title": "Handling Locations in Search Engine Queries"}
{"id": "H-96", "keywords": "implicit relev feedback;relev feedback", "abstract": "Implicit relevance feedback (IRF) is the process by which a search system unobtrusively gathers evidence on searcher interests from their interaction with the system. IRF is a new method of gathering information on user interest and, if IRF is to be used in operational IR systems, it is important to establish when it performs well and when it performs poorly. In this paper we investigate how the use and effectiveness of IRF is affected by three factors: search task complexity, the search experience of the user and the stage in the search. Our findings suggest that all three of these factors contribute to the utility of IRF.", "references": ["Searchers' assessments of task complexity for web searching", "Experimental components for the evaluation of interactive information retrieval systems", "Strategic help for user interfaces for information retrieval", "Research methods in librarianship: Techniques and interpretation", "The ostensive model of developing information needs", "Relevance feedback and other query modification techniques", "Implicit feedback for inferring user preference", "A case for interaction: A study of interactive information retrieval behavior and effectiveness", "Statistics using ranks: A unified approach", "Information filtering based on user behavior analysis and best match text retrieval", "Improving retrieval performance by relevance feedback", "Nonparametric statistics for the behavioural sciences", "Implicit feedback for interactive information retrieval", "An implicit feedback approach for interactive information retrieval", "A simulated study of implicit feedback models", "The impact of fluid documents on reading and browsing: An observational study"], "title": "A Study of Factors Affecting the Utility of Implicit Relevance Feedback"}
{"id": "H-97", "keywords": "text classif;speech act;featur select;e-mail;n-gram;svm", "abstract": "E-mail users face an ever-growing challenge in managing their inboxes due to the growing centrality of email in the workplace for task assignment, action requests, and other roles beyond information dissemination. Whereas Information Retrieval and Machine Learning techniques are gaining initial acceptance in spam filtering and automated folder assignment, this paper reports on a new task: automated action-item detection, in order to flag emails that require responses, and to highlight the specific passage(s) indicating the request(s) for action. Unlike standard topic-driven text classification, action-item detection requires inferring the senders intent, and as such responds less well to pure bag-of-words classification. However, using enriched feature sets, such as n-grams (up to n=4) with chi-squared feature selection, and contextual cues for action-item location improve performance by up to 10% over unigrams, using in both cases state of the art classifiers such as SVMs with automated model selection via embedded cross-validation.", "references": ["Topic detection and tracking pilot study: Final report", "Automated learning of decision rules for text categorization", "Assessing agreement on classification tasks: The kappa statistic", "High precision extraction of grammatical relations", "Learning to classify email into speech acts", "Task-focused summarization of email", "Extracting social networks and contact information from email and the web", "A Probabilistic Theory of Pattern Recognition", "Inductive learning algorithms and representations for text categorization", "Large margin classification using the perceptron algorithm", "Making large-scale svm learning practical", "A patent search and classification system", "An evaluation of phrasal and clustered representations on a text categorization task", "A pairwise ensemble approach for accurate genre classification", "A comparison study of kernels for multi-label text classification using category association", "A comparison of event models for naive bayes text classification", "Machine learning in automated text categorization", "Information Retrieval", "An evaluation of statistical approaches to text categorization", "Learning approaches to topic detection and tracking", "A re-examination of text categorization methods", "Topic-conditioned novelty detection"], "title": "Feature Representation for Effective Action-Item Detection"}
{"id": "H-98", "keywords": "text classif;cost-sensit learn;activ learn;classifi combin", "abstract": "Text classifiers that give probability estimates are more readily applicable in a variety of scenarios. For example, rather than choosing one set decision threshold, they can be used in a Bayesian risk model to issue a run-time decision which minimizes a user-specified cost function dynamically chosen at prediction time. However, the quality of the probability estimates is crucial. We review a variety of standard approaches to converting scores (and poor probability estimates) from text classifiers to high quality estimates and introduce new models motivated by the intuition that the empirical score distribution for the "extremely irrelevant", "hard to discriminate", and "obviously relevant" items are often significantly different. Finally, we analyze the experimental performance of these models over the outputs of two text classifiers. The analysis demonstrates that one of these models is theoretically attractive (introducing few new parameters while increasing flexibility), computationally efficient, and empirically preferable.", "references": ["Assessing the calibration of naive bayes' posterior estimates", "Using asymmetric distributions to improve classifier probabilities: A comparison of new and standard parametric methods", "A continuous speech recognition system embedding mlp into hmm", "Verification of forecasts expressed in terms of probability", "The comparison and evaluation of forecasters", "Comparing probability forecasters: Basic binary concepts and multivariate extensions", "Beyond independence: Conditions for the optimality of the simple bayesian classifier", "Pattern Classification", "Hierarchical classification of web content", "Inductive learning algorithms and representations for text categorization", "Large margin classification using the perceptron algorithm", "Rational decisions", "Text categorization with support vector machines: Learning with many relevant features", "The Laplace Distribution and Generalizations: A Revisit with Applications to Communications, Economics, Engineering, and Finance", "A sequential algorithm for training text classifiers: Corrigendum and additional data", "A sequential algorithm for training text classifiers", "Training algorithms for linear text classifiers", "On the reconciliation of probability assessments", "Modeling score distributions for combining the outputs of search engines", "A comparison of event models for naive bayes text classification", "Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods", "Active learning for class probability estimation and ranking", "Scoring rules and the evaluation of probability assessors", "A re-examination of text categorization methods", "Obtaining calibrated probability estimates from decision trees and naive bayesian classifiers", "Reducing multiclass to binary by coupling probability estimates"], "title": "Using Asymmetric Distributions to Improve Text Classifier Probability Estimates"}
{"id": "I-37", "keywords": "multiag learn;distribut machin learn;framework and architectur;unsupervis cluster", "abstract": "This paper proposes a framework for agent-based distributed machine learning and data mining based on (i) the exchange of meta-level descriptions of individual learning processes among agents and (ii) online reasoning about learning success and learning progress by learning agents. We present an abstract architecture that enables agents to exchange models of their local learning processes and introduces a number of different methods for integrating these processes. This allows us to apply existing agent interaction mechanisms to distributed machine learning tasks, thus leveraging the powerful coordination methods available in agent-based computing, and enables agents to engage in meta-reasoning about their own learning decisions. We apply this architecture to a real-world distributed clustering application to illustrate how the conceptual framework can be used in practical systems in which different learners may be using different datasets, hypotheses and learning algorithms. We report on experimental results obtained using this system, review related work on the subject, and discuss potential future extensions to the framework.", "references": ["Papyrus: A System for Data Mining over Local and Wide Area Clusters and Super-Clusters", "An Empirical Comparison of Voting Classification Algorithms: Bagging, Boosting, and Variants", "Survey of Clustering Data Mining Techniques", "Agents that Learn from Distributed Dynamic Data sources", "Creating ensembles of classifiers", "Model Averaging for Prediction with Discrete Bayesian Networks", "A Cluster Separation Measure", "A Heterogeneous Multiagent Learning System", "A Consensus Framework for Integrating Distributed Clusterings Under Limited Knowledge Sharing", "Coactive Learning for Distributed Data Mining", " Agent-based distributed data mining: The KDEC scheme", "Recycling Data for Multiagent Learning", "Cooperative multiagent learning: The state of the art", "Distributed Data Mining: Algorithms, Systems, and Applications", "Scaling up: Distributed machine learning with cooperation", "Extending learning to multiple agents: Issues and a model for multiagent machine learning (ma-ml)", "The contract-net protocol: High-level communication and control in a distributed problem solver", "Jam: Java Agents for Meta-Learning over Distributed Databases", "Market-Inspired Approach to Collaborative Learning", "Recommender systems: a market-based design", "A Multiagent Perspective of Parallel and Distributed Machine Learning", "What is 'multi' in multiagent learning?"], "title": "A Framework for Agent-Based Distributed Machine Learning and Data Mining"}
{"id": "I-38", "keywords": "combinatori auction;coordin;task and resourc alloc", "abstract": "Distributed allocation and multiagent coordination problems can be solved through combinatorial auctions. However, most of the existing winner determination algorithms for combinatorial auctions are centralized. The PAUSE auction is one of a few efforts to release the auctioneer from having to do all the work (it might even be possible to get rid of the auctioneer). It is an increasing price combinatorial auction that naturally distributes the problem of winner determination amongst the bidders in such a way that they have an incentive to perform the calculation. It can be used when we wish to distribute the computational load among the bidders or when the bidders do not wish to reveal their true valuations unless necessary. PAUSE establishes the rules the bidders must obey. However, it does not tell us how the bidders should calculate their bids. We have developed a couple of bidding algorithms for the bidders in a PAUSE auction. Our algorithms always return the set of bids that maximizes the bidder's utility. Since the problem is NP-Hard, run time remains exponential on the number of items, but it is remarkably better than an exhaustive search. In this paper we present our bidding algorithms, discuss their virtues and drawbacks, and compare the solutions obtained by them to the revenue-maximizing solution found by a centralized winner determination algorithm.", "references": ["Decentralized computation procurement and computational robustness in a smart market", "Combinatorial Auctions", "Taming the computational complexity of combinatorial auctions: Optimal and approximate approaches", "A combinatorial auction with multiple winners for universal service", "PAUSE: A computationally tractable combinatorial auction", "Towards a universal test suite for combinatorial auction algorithms", "Algorithms for distributed winner determination in combinatorial auctions", "Auctions with endogenously determined allowable combinations", "Distributed implementations of vickrey-clarke-groves auctions", "Computationally manageable combinational auctions", "An algorithm for winner determination in combinatorial auctions", "CABOB: a fast optimal algorithm for winner determination in combinatorial auctions"], "title": "Bidding Algorithms for a Distributed Combinatorial Auction"}
{"id": "I-42", "keywords": "distribut constraint satisfact and optim;multi-agent coordin", "abstract": "Distributed Constraint Optimization (DCOP) is a general framework that can model complex problems in multiagent systems. Several current algorithms that solve general DCOP instances, including ADOPT and DPOP, arrange agents into a traditional pseudotree structure. We introduce an extension to the DPOP algorithm that handles an extended set of pseudotree arrangements. Our algorithm correctly solves DCOP instances for pseudotrees that include edges between nodes in separate branches. The algorithm also solves instances with traditional pseudotree arrangements using the same procedure as DPOP. We compare our algorithm with DPOP using several metrics including the induced width of the pseudotrees, the maximum dimensionality of messages and computation, and the maximum sequential path cost through the algorithm. We prove that for some problem instances it is not possible to generate a traditional pseudotree using edge-traversal heuristics that will outperform a cross-edged pseudotree. We use multiple heuristics to generate pseudotrees and choose the best pseudotree in linear space-time complexity. For some problem instances we observe significant improvements in message and computation sizes compared to DPOP.", "references": ["Exploiting problem structure for distributed constraint optimization", "A dynamic distributed constraint satisfaction approach to resource allocation", "An asynchronous complete method for distributed constraint optimization", "Frodo: A framework for open/distributed constraint optimization", "A-dpop: Approximations in distributed optimization", "Dpop: A scalable method for multiagent constraint optimization", "M-dpop: Faithful distributed implementation of efficient social choice problems", "Solving meeting scheduling problems using distributed pseudotree-optimization procedure", "Distributed constraint satisfaction for formalizing distributed problem solving", "The distributed constraint satisfaction problem: Formalization and algorithms"], "title": "A Complete Distributed Constraint Optimization Method For Non-Traditional Pseudotree Arrangements"}
{"id": "I-43", "keywords": "control;multi-agent system;robot;target dynam;dynam base control", "abstract": "In this paper we introduce Dynamics Based Control (DBC), an approach to planning and control of an agent in stochastic environments. Unlike existing approaches, which seek to optimize expected rewards (e.g., in Partially Observable Markov Decision Problems (POMDPs)), DBC optimizes system behavior towards specified system dynamics. We show that a recently developed planning and control approach, Extended Markov Tracking (EMT) is an instantiation of DBC. EMT employs greedy action selection to provide an efficient control algorithm in Markovian environments. We exploit this efficiency in a set of experiments that applied multi-target EMT to a class of area-sweeping problems (searching for moving targets). We show that such problems can be naturally defined and efficiently solved using the DBC framework, and its EMT instantiation.", "references": ["Behavior-Based Robotics", "A gentle tutorial of the EM algorithm and its application to parameter estimation for Gaussian mixture and Hidden Markov Models", "Elements of information theory", "A survey of research in distributed, continual planning", "Actor-Critic algorithms", "A rendezvous-evasion game on discrete locations with joint randomization", "On the complexity of solving Markov decision problems", "On the undecidability of probabilistic planning and related stochastic optimization problems", "A view of the EM algorithm that justifies incremental, sparse, and other variants", "Security in multiagent systems by policy randomization", "Point-based value iteration: An anytime algorithm for pomdps", "Wiley Series in Probability and Mathematical Statistics: Applied Probability and Statistics Section", "Extended Markov Tracking with an application to control", "Multiagent coordination by Extended Markov Tracking", "On the response of EMT-based control to interacting targets and models", "Optimal Control and Estimation", "Conflicts in teamwork: Hybrids to the"], "title": "Dynamics Based Control with an Application to Area-Sweeping Problems"}
{"id": "I-45", "keywords": "commit machin;agent interact;agent orient program languag;belief desir intent;bdi", "abstract": "Although agent interaction plays a vital role in MAS, and message-centric approaches to agent interaction have their drawbacks, present agent-oriented programming languages do not provide support for implementing agent interaction that is flexible and robust. Instead, messages are provided as a primitive building block. In this paper we consider one approach for modelling agent interactions: the commitment machines framework. This framework supports modelling interactions at a higher level (using social commitments), resulting in more flexible interactions. We investigate how commitment-based interactions can be implemented in conventional agent-oriented programming languages. The contributions of this paper are: a mapping from a commitment machine to a collection of BDI-style plans; extensions to the semantics of BDI programming languages; and an examination of two issues that arise when distributing commitment machines (turn management and race conditions) and solutions to these problems.", "references": ["Multiagent Programming: Languages", "Hermes: Designing goal-oriented agent interactions", "Hermes: Implementing goal-oriented agent interactions", "Hermes versus prometheus: A comparative evaluation of two agent interaction design approaches", "Teamwork", "Communication for goal directed agents", "Towards a testbed for multi-party dialogues", "Using a performative subsumption lattice to support commitment-based conversations", "STAPLE: An agent programming language based on the joint intention theory", "Representing and executing protocols as joint actions", "Towards flexible teamwork in persistent teams: Extended report", "An AgentSpeak meta-interpreter and its applications", "Designing commitment-based agent interactions", "Implementing flexible and robust agent interactions using distributed commitment machines", "Enhancing commitment machines", "Declarative & procedural goals in intelligent agent systems", "Towards design tools for protocol development", "Flexible protocol specification and execution: Applying event calculus planning using commitments", "Reasoning about commitments in the event calculus: An approach for specifying and executing protocols"], "title": "Implementing Commitment-Based Interactions"}
{"id": "I-46", "keywords": "open comput system;tempor and strateg logic;model methodolog;model check", "abstract": "We propose a new class of representations that can be used for modeling (and model checking) temporal, strategic and epistemic properties of agents and their teams. Our representations borrow the main ideas from interpreted systems of Halpern, Fagin et al.; however, they are also modular and compact in the way concurrent programs are. We also mention preliminary results on model checking alternating-time temporal logic for this natural class of models.", "references": ["Reactive modules", "Alternating-time Temporal Logic", "Alternating-time Temporal Logic", "MOCHA user manual", "Automatic verification of finite-state concurrent systems using temporal logic specifications", "sometimes and not never revisited: On branching versus linear time temporal logic", "Reasoning about Knowledge", "Model checking for combined logics", "Comparing semantics of logics for multiagent systems", "Reasoning about knowledge: a survey", "Modelling knowledge and action in distributed systems", "Do agents make model checking explode (computationally)?", "Model checking abilities of agents: A closer look", "Modular interpreted systems: A preliminary report", "An automata-theoretic approach to branching-time model checking", "Expressiveness and complexity of ATL", "Symbolic Model Checking: An Approach to the State Explosion Problem", "Applying SAT methods in unbounded symbolic model checking", "Verifying epistemic properties of multiagent systems via bounded model checking", "The complexity of symbolic model checking temporal-epistemic logics", "Alternating-time logic with imperfect recall", "On the complexity of practical ATL model checking"], "title": "Modular Interpreted Systems"}
{"id": "I-47", "keywords": "social interact;softwar connector;oper semant", "abstract": "The social stance advocated by institutional frameworks and most multiagent system methodologies has resulted in a wide spectrum of organizational and communicative abstractions which have found currency in several programming frameworks and software platforms. Still, these tools and frameworks are designed to support a limited range of interaction capabilities that constrain developers to a fixed set of particular, pre-defined abstractions. The main hypothesis motivating this paper is that the variety of multiagent interaction mechanisms -- both, organizational and communicative, share a common semantic core. In the realm of software architectures, the paper proposes a connector-based model of multiagent interactions which attempts to identify the essential structure underlying multiagent interactions. Furthermore, the paper also provides this model with a formal execution semantics which describes the dynamics of social interactions. The proposed model is intended as the abstract machine of an organizational programming language which allows programmers to accommodate an open set of interaction mechanisms.", "references": ["A Formal Basis for Architectural Connection", "Engineering open environments with electronic institutions", "Role-based semantics for agent communication: embedding of the 'mental attitudes' and 'social commitments' semantics", "A survey of programming languages and platforms for multiagent systems", "Jason and the golden fleece of agent-oriented programming", "Specifying and analysing agent-based social institutions using answer set programming", "Omni: Introducing social structure, norms and ontologies into agent organizations", "ISLANDER: an electronic institutions editor", "AMELI: An agent-based middleware for electronic institutions", "From agents to organizations: An organizational view of multiagent systems", "Foundation for Intelligent Physical Agents", "Norm-oriented programming of electronic institutions", "The MADKIT agent platform architecture", "The JADE project home page", "Agent Technology: Computing as Interaction -- A Roadmap for Agent-Based Computing", "A formal framework for inter-agent dialogues", "Towards a taxonomy of software connectors", "Agent oriented software engineering with ingenias", "Types and Programming Languages", "Voting in multiagent systems", "A structural approach to operational semantics", "Speech Acts", "A computational theory of normative positions", "Agent-based abstractions for software development", "Agentcities / opennet testbed", "Developing multiagent systems: The Gaia methodology"], "title": "Operational Semantics of Multiagent Interactions"}
{"id": "I-48", "keywords": "norm system;goal;logic;game;complex", "abstract": "We develop a model of normative systems in which agents are assumed to have multiple goals of increasing priority, and investigate the computational complexity and game theoretic properties of this model. In the underlying model of normative systems, we use Kripke structures to represent the possible transitions of a multiagent system. A normative system is then simply a subset of the Kripke structure, which contains the arcs that are forbidden by the normative system. We specify an agent's goals as a hierarchy of formulae of Computation Tree Logic (CTL), a widely used logic for representing the properties of Kripke structures: the intuition is that goals further up the hierarchy are preferred by the agent over those that appear further down the hierarchy. Using this scheme, we define a model of ordinal utility, which in turn allows us to interpret our Kripke-based normative systems as games, in which agents must determine whether to comply with the normative system or not. We then characterise the computational complexity of a number of decision problems associated with these Kripke-based normative system games; for example, we show that the complexity of checking whether there exists a normative system which has the property of being a Nash implementation is NP-complete.", "references": ["On the logic of normative systems", "Alternating-time temporal logic", "Game Theory and the Social Contract Volume 1: Playing Fair", "Game Theory and the Social Contract Volume 2: Just Playing", "Complexity of mechanism design", "Complexity results about nash equilibria", "The complexity of computing a Nash equilibrium", "Temporal and modal logic", "Sometimes and not never revisited: on branching time versus linear time temporal logic", "Choosing social laws for multiagent systems: Minimality and simplicity", "A Course in Game Theory", "Computational Complexity", "On the synthesis of useful social laws for artificial agent societies", "On social laws for artificial agent societies: Off-line design", "Social laws in alternating time: Effectiveness, feasibility, and synthesis", "On obligations and normative ability"], "title": "Normative System Games"}
{"id": "I-49", "keywords": "autom negoti;negoti strategi;multi-criterion decis make", "abstract": "In this paper, we present a new protocol to address multilateral multi-issue negotiation in a cooperative context. We consider complex dependencies between multiple issues by modelling the preferences of the agents with a multi-criteria decision aid tool, also enabling us to extract relevant information on a proposal assessment. This information is used in the protocol to help in accelerating the search for a consensus between the cooperative agents. In addition, the negotiation procedure is defined in a crisis management context where the common objective of our agents is also considered in the preferences of a mediator agent.", "references": ["JADE", "Using similarity criteria to make issue trade-offs in automated negotiations", "Optimal negotiation of multiple issues in incomplete information settings", "A comparative study of game theoretic and evolutionary models of bargaining for software agents", "On efficient procedures for multi-issue negotiation", "The application of fuzzy integrals in multicriteria decision making", "Fuzzy Measures and Integrals", "Cooperation-based multilateral multi-issue negotiation for crisis management", "A negotiation protocol for agents with nonlinear utility functions", "Negotiating complex contracts", "Determination of the criteria to be improved first in order to improve as much as possible the overall evaluation", "MYRIAD: a tool suite for MCDA", "Towards genetically optimised multiagent multi-issue negotiations", "Bilateral multi-issue contract negotiation for task redistribution using a mediation service", "Non cooperative games", "Game Theory", "Modeling complex multi-issue negotiations using utility graphs", "Perfect equilibrium in a bargaining model", "Adaptive, confidence-based multiagent negotiation strategy", "Automated contract negotiation using a mediation service"], "title": "A Multilateral Multi-issue Negotiation Protocol"}
{"id": "I-50", "keywords": "multi-agent system;tempor logic;plausibl;belief", "abstract": "Logics of knowledge and belief are often too static and inflexible to be used on real-world problems. In particular, they usually offer no concept for expressing that some course of events is more likely to happen than another. We address this problem and extend CTLK (computation tree logic with knowledge) with a notion of plausibility, which allows for practical and counterfactual reasoning. The new logic CTLKP (CTLK with plausibility) includes also a particular notion of belief. A plausibility update operator is added to this logic in order to change plausibility assumptions dynamically. Furthermore, we examine some important properties of these concepts. In particular, we show that, for a natural class of models, belief is a KD45 modality. We also show that model checking CTLKP is PTIME-complete and can be done in time linear with respect to the size of models and formulae.", "references": ["Alternating-time Temporal Logic", "Agents, beliefs and plausible behavior in a temporal setting", "Temporal and modal logic", "sometimes and not never revisited: On branching versus linear time temporal logic", "Reasoning about Knowledge", "Reasoning about knowledge and probability", "A knowledge-based framework for belief change, Part I: Foundations", "A knowledge-based framework for belief change, Part II: Revision and update", "Reasoning about knowledge: a survey", "Modelling knowledge and action in distributed systems", "A general framework for reasoning about rational agents", "Agents that know how to play", "Intentions and strategies in game-like scenarios", "Constructive knowledge: What agents can achieve under incomplete information", "Probabilistic dynamic epistemic logic", "Knowledge, certainty, belief, and conditionalisation (abbreviated version)", "Model checking CTL+ and FCTL is hard", "Belief as defeasible knowledge", "Verifying epistemic properties of multiagent systems via bounded model checking", "A computationally grounded logic of knowledge, belief and certainty", "Social laws in alternating time: Effectiveness, feasibility and synthesis"], "title": "Agents, Beliefs, and Plausible Behavior in a Temporal Setting"}
{"id": "I-51", "keywords": "multi-agent learn;argument;case-base reason", "abstract": "In this paper we will present an argumentation framework for learning agents (AMAL) designed for two purposes: (1) for joint deliberation, and (2) for learning from communication. The AMAL framework is completely based on learning from examples: the argument preference relation, the argument generation policy, and the counterargument generation policy are case-based techniques. For join deliberation, learning agents share their experience by forming a committee to decide upon some joint decision. We experimentally show that the argumentation among committees of agents improves both the individual and joint performance. For learning from communication, an agent engages into arguing with other agents in order to contrast its individual hypotheses and receive counterexamples; the argumentation process improves their learning scope and individual performance.", "references": ["Case-based reasoning: Foundational issues, methodological variations, and system approaches", "Lazy induction of descriptions for relational case-based learning", "Dynamic argument systems: A formal model of argumentation processes based on situation calculus", "Formalizing Defeasible Argumentation using Labelled Deductive Systems", "Automatically selecting strategies for multi-case-base reasoning", "Knowledge and experience reuse through communications among competent (peer) agents", "Collaborative case-based reasoning: Applications in personalized route planning", "Justification-based multiagent learning", "The explanatory power of symbolic similarity in case-based reasoning", "On the comparison of theories: Preferring the most specific explanation", "Retrieval and reasoning in distributed case bases", "Reaching agreements through argumentation: a logical model and implementation", "Agents that reason and negotiate by arguing", "Explanation component of software systems"], "title": "Learning and Joint Deliberation through Argumentation in Multi-Agent Systems"}
{"id": "I-52", "keywords": "argument;negoti", "abstract": "This paper proposes a unified and general framework for argumentation-based negotiation, in which the role of argumentation is formally analyzed. The framework makes it possible to study the outcomes of an argumentation-based negotiation. It shows what an agreement is, how it is related to the theories of the agents, when it is possible, and how this can be attained by the negotiating agents in this case. It defines also the notion of concession, and shows in which situation an agent will make one, as well as how it influences the evolution of the dialogue.", "references": ["Towards a formal framework for the search of a consensus between autonomous agents", "Arguments, dialogue, and negotiation", "Reaching agreement through argumentation: A possibilistic approach", "Explaining qualitative decision under uncertainty by argumentation", "On the acceptability of arguments and its fundamental role in nonmonotonic reasoning, logic programming and n-person games", "Automated negotiation: Prospects, methods and challenges", "Adaptive agent negotiation via argumentation", "Reaching agreements through argumentation: a logical model and implementation", "Negotiation through argumentation---a preliminary report", "Argumentation-based negotiation", "Rules of Encounter: Designing Conventions for Automated Negotiation Among Computers", "Persuasive argumentation in negotiation", "Negotiation and defeasible reasons for choice"], "title": "A Unified and General Framework for Argumentation-based Negotiation"}
{"id": "I-53", "keywords": "game-theori;coalit format;approxim;shaplei valu", "abstract": "The Shapley value is one of the key solution concepts for coalition games. Its main advantage is that it provides a unique and fair solution, but its main problem is that, for many coalition games, the Shapley value cannot be determined in polynomial time. In particular, the problem of finding this value for the voting game is known to be #P-complete in the general case. However, in this paper, we show that there are some specific voting games for which the problem is computationally tractable. For other general voting games, we overcome the problem of computational complexity by presenting a new randomized method for determining the approximate Shapley value. The time complexity of this method is linear in the number of players. We also show, through empirical studies, that the percentage error for the proposed method is always less than 20% and, in most cases, less than 5%.", "references": ["Acceptable points in general cooperative n-person games", "Complexity and approximation: Combinatorial optimization problems and their approximability properties", "Generating functions for computing power indices efficiently", "Data Analysis Techniques in High Energy Physics Experiments", "Computing Shapley values, manipulating value division schemes, and checking core membership in multi-issue domains", "On the complexity of cooperative solution concepts", "An analysis of the shapley value and its uncertainty for the voting game", "Advanced Level Statistics", "Marginal contribution nets: A compact representation scheme for coalitional games", "Multi-attribute coalition games", "Theories of Coalition Formation", "Values for large games iv: Evaluating the electoral college exactly", "Microeconomic Theory", "A Course in Game Theory", "Computational Complexity", "N-person Game Theory: Concepts and Applications", "Introduction to the shapley value", "Coalitions among computationally bounded agents", "A value for n person games", "A kernel-oriented model for coalition-formation in general environments: Implemetation and results", "Methods for task allocation via agent coalition formation", "An introduction to error analysis: The study of uncertainties in physical measurements"], "title": "A Randomized Method for the Shapley Value for the Voting Game"}
{"id": "I-54", "keywords": "game-theori;negoti;approxim;onlin comput", "abstract": "This paper analyzes bilateral multi-issue negotiation between self-interested autonomous agents. The agents have time constraints in the form of both deadlines and discount factors. There are m > 1 issues for negotiation where each issue is viewed as a pie of size one. The issues are "indivisible" (i.e., individual issues cannot be split between the parties; each issue must be allocated in its entirety to either agent). Here different agents value different issues differently. Thus, the problem is for the agents to decide how to allocate the issues between themselves so as to maximize their individual utilities. For such negotiations, we first obtain the equilibrium strategies for the case where the issues for negotiation are known a priori to the parties. Then, we analyse their time complexity and show that finding the equilibrium offers is an NP-hard problem, even in a complete information setting. In order to overcome this computational complexity, we then present negotiation strategies that are approximately optimal but computationally efficient, and show that they form an equilibrium. We also analyze the relative error (i.e., the difference between the true optimum and the approximate). The time complexity of the approximate equilibrium strategies is O(nm/2) where n is the negotiation deadline and  the relative error. Finally, we extend the analysis to online negotiation where different issues become available at different time points and the agents are uncertain about their valuations for these issues. Specifically, we show that an approximate equilibrium exists for online negotiation and show that the expected difference between the optimum and the approximate is O(m). These approximate strategies also have polynomial time complexity.", "references": ["Complexity and approximation: Combinatorial optimization problems and their approximability properties", "Issue-by-issue negotiations: the role of information and time preference", "Online Computation and Competitive Analysis", "Fair division: from cake cutting to dispute resolution", "Bargaining frictions, bargaining procedures and implied costs in multiple-issue bargaining", "Multi-issue negotiation with deadlines", "The importance of the agenda in bargaining", "A multiphase dual algorithm for the zero-one integer programming problem", "Adaptive limited-supply online auctions", "Fast approximation algorithms for the knapsack and sum of subset problems", "Multi-issue bargaining with endogenous agenda", "Decisions with Multiple Objectives: Preferences and Value Trade-offs", "Strategic negotiation in multiagent environments", "Truth revelation in approximately efficient combinatorial auctions", "A classification scheme for negotiation in electronic commerce", "Stochastic online knapsack problems", "Knapsack problems: Algorithms and computer implementations", "A Course in Game Theory", "The Art and Science of Negotiation", "Rules of Encounter", "Perfect equilibrium in a bargaining model", "Levelled commitment contracts and strategic breach", "Bargaining with deadlines", "An essay on bargaining", "Methods for task allocation via agent coalition formation", "Computing approximate Bayes Nash equilibria in tree games of incomplete information", "Bargaining Theory"], "title": "Approximate and Online Multi-Issue Negotiation"}
{"id": "I-55", "keywords": "autom negoti;integr negoti;multi-criterion decis make", "abstract": "It is well established by conflict theorists and others that successful negotiation should incorporate "creating value" as well as "claiming value." Joint improvements that bring benefits to all parties can be realised by (i) identifying attributes that are not of direct conflict between the parties, (ii) tradeoffs on attributes that are valued differently by different parties, and (iii) searching for values within attributes that could bring more gains to one party while not incurring too much loss on the other party. In this paper we propose an approach for maximising joint gains in automated negotiations by formulating the negotiation problem as a multi-criteria decision making problem and taking advantage of several optimisation techniques introduced by operations researchers and conflict theorists. We use a mediator to protect the negotiating parties from unnecessary disclosure of information to their opponent, while also allowing an objective calculation of maximum joint gains. We separate out attributes that take a finite set of values (simple attributes) from those with continuous values, and we show that for simple attributes, the mediator can determine the Pareto-optimal values. In addition we show that if none of the simple attributes strongly dominates the other simple attributes, then truth telling is an equilibrium strategy for negotiators during the optimisation of simple attributes. We also describe an approach for improving joint gains on non-simple attributes, by moving the parties in a series of steps, towards the Pareto-optimal frontier.", "references": ["A demonstration of methods for studying negotiations between physicians and health care managers", "Multicriteria Optimization", "Generating pareto solutions in a two-party setting: Constraint proposal methods", "Searching for joint gains in multi-party negotiations", "Automated Service Negotiation Between Autonomous Computational Agents", "Minimizing negotiation process losses with computerized negotiation support systems", "An agent architecture for multi-attribute negotiation using incomplete preference information", "Decisions with Multiple Objectives: Preferences and Value Trade-Offs", "Rational agents, contract curves, and non-efficient compromises", "Protocols for negotiating complex contracts", "Multiagent negotiation under time constraints", "Efficient multi-attribute negotiation with incomplete information", "The manager as negotiator: The negotiator's dilemma: Creating and claiming value", "A classification scheme for negotiation in electronic commerce", "Agents that buy and sell", "Two-person cooperative games", "The Art and Science of Negotiation", "Negotiation Analysis: The Science and Art of Collaborative Decision Making", "Agents in electronic commerce: Component technologies for automated negotiation and coalition formation", "Negotiation analysis: A characterization and review", "Knowledge matters: The effect of tactical descriptions on negotiation behavior and outcome", "Integrative negotiation among agents situated in organizations"], "title": "Searching for Joint Gains in Automated Negotiations Based on Multi-criteria Decision Making Theory"}
{"id": "I-56", "keywords": "distribut constraint satisfact problem;belief-desireintent model;agent negoti", "abstract": "This paper presents a novel, unified distributed constraint satisfaction framework based on automated negotiation. The Distributed Constraint Satisfaction Problem (DCSP) is one that entails several agents to search for an agreement, which is a consistent combination of actions that satisfies their mutual constraints in a shared environment. By anchoring the DCSP search on automated negotiation, we show that several well-known DCSP algorithms are actually mechanisms that can reach agreements through a common Belief-Desire-Intention (BDI) protocol, but using different strategies. A major motivation for this BDI framework is that it not only provides a conceptually clearer understanding of existing DCSP algorithms from an agent model perspective, but also opens up the opportunities to extend and develop new strategies for DCSP. To this end, a new strategy called Unsolicited Mutual Advice (UMA) is proposed. Performance evaluation shows that the UMA strategy can outperform some existing mechanisms in terms of computational cycles.", "references": ["Dynamic distributed resource allocation: A distributed constraint satisfaction approach", "Simulating large railway networks using distributed constraint satisfaction", "Distributed Constraint Satisfaction : Foundations of Cooperation in Multiagent Systems", "The distributed constraint satisfaction problem : Formalization and algorithms", "Using cooperative mediation to solve distributed constraint satisfaction problems", "Foundation of Constraint Satisfaction", "Soft Real-Time, Cooperative Negotiation for Distributed Resource Allocation", "Secure distributed constraint satisfaction: Reaching agreement without revealing private information", "Rules of Encounter", "Distributed constraint satisfaction algorithm for complex local problems", "Intentions, Plans and Practical Reason", "Multiagent System : A Modern Approach to Distributed Artificial Intelligence", "Minimizing conflicts: A heuristic repair method for constraint satisfaction and scheduling problems"], "title": "Unifying Distributed Constraint Algorithms in a BDI Negotiation Framework"}
{"id": "I-57", "keywords": "multi-dimension trust;rumour propog;dirichlet distribut", "abstract": "In this paper we develop a novel probabilistic model of computational trust that explicitly deals with correlated multi-dimensional contracts. Our starting point is to consider an agent attempting to estimate the utility of a contract, and we show that this leads to a model of computational trust whereby an agent must determine a vector of estimates that represent the probability that any dimension of the contract will be successfully fulfilled, and a covariance matrix that describes the uncertainty and correlations in these probabilities. We present a formalism based on the Dirichlet distribution that allows an agent to calculate these probabilities and correlations from their direct experience of contract outcomes, and we show that this leads to superior estimates compared to an alternative approach using multiple independent beta distributions. We then show how agents may use the sufficient statistics of this Dirichlet distribution to communicate and fuse reputation within a decentralised reputation system. Finally, we present a novel solution to the problem of rumour propagation within such systems. This solution uses the notion of private and shared information, and provides estimates consistent with a centralised reputation system, whilst maintaining the anonymity of the agents, and avoiding bias and overconfidence.", "references": ["Estimation with Applications to Tracking and Navigation", "The foundations of expected expected utility", "Statistical Distributions", "Task delegation using experience-based multi-dimensional trust", "Modelling multi-dimensional trust", "The beta reputation system", "Agent-based trust model involving multiple qualities", "Trust in multiagent systems", "Robust, low-bandwidth, multi-vehicle mapping", "REGRET: A reputation model for gregarious societies", "TRAVOS: Trust and reputation in the context of inaccurate information sources", "Network Management in Decentralised Sensing Systems"], "title": "Rumours and Reputation: Evaluating Multi-Dimensional Trust within a Decentralised Reputation System"}
{"id": "I-58", "keywords": "secur of agent system+agent system secur;game theori;bayesian and stackelberg game", "abstract": "In adversarial multiagent domains, security, commonly defined as the ability to deal with intentional threats from other agents, is a critical issue. This paper focuses on domains where these threats come from unknown adversaries. These domains can be modeled as Bayesian games; much work has been done on finding equilibria for such games. However, it is often the case in multiagent security domains that one agent can commit to a mixed strategy which its adversaries observe before choosing their own strategies. In this case, the agent can maximize reward by finding an optimal strategy, without requiring equilibrium. Previous work has shown this problem of optimal strategy selection to be NP-hard. Therefore, we present a heuristic called ASAP, with three key advantages to address the problem. First, ASAP searches for the highest-reward strategy, rather than a Bayes-Nash equilibrium, allowing it to find feasible strategies that exploit the natural first-mover advantage of the game. Second, it provides strategies which are simple to understand, represent, and implement. Third, it operates directly on the compact, Bayesian game representation, without requiring conversion to normal form. We provide an efficient Mixed Integer Linear Program (MILP) implementation for ASAP, along with experimental results illustrating significant speedups and higher rewards over other approaches.", "references": ["Multiple UAV cooperative search under collision avoidance and limited range communication constraints", "Introduction to Linear Optimization", "Bayesian games for threat prediction and situation analysis", "Theoretical analysis of multiagent patrolling problem", "Choosing the best strategy to commit to", "Game Theory", "Virtual patrol: A new power conservation design for surveillance using sensor networks", "A generalized Nash solution for two-person bargaining games with incomplete information", "Generating and solving imperfect information games", "Representations and solutions for game-theoretic problems", "Equilibrium points of bimatrix games", "Playing large games using simple strategies", "multiagent patrolling: an empirical analysis on alternative architectures", "Security in multiagent systems by policy randomization", "Stackelberg scheduling strategies", "Patrolling in a stochastic environment", "Mixed-integer programming methods for finding nash equilibria", "Computing approximate Bayes-Nash equilibria with tree-games of incomplete information"], "title": "An Efficient Heuristic Approach for Security Against Multiple Adversaries"}
{"id": "I-59", "keywords": "multi-agent system;recommend system;inform filter;privaci;trust", "abstract": "Recommender Systems are used in various domains to generate personalized information based on personal user data. The ability to preserve the privacy of all participants is an essential requirement of the underlying Information Filtering architectures, because the deployed Recommender Systems have to be accepted by privacy-aware users as well as information and service providers. Existing approaches neglect to address privacy in this multilateral way. We have developed an approach for privacy-preserving Recommender Systems based on Multiagent System technology which enables applications to generate recommendations via various filtering techniques while preserving the privacy of all participants. We describe the main modules of our solution as well as an application we have implemented based on this approach.", "references": ["Hippocratic databases", "Privacy-preserving data mining", "Privacy-preserving demographic filtering", "Privacy-preserving indexing of documents on the network", "Hybrid recommender systems: Survey and experiments", "Collaborative filtering with privacy", "Private information retrieval", "An architecture for agent-based privacy-preserving information filtering", "Item-based top-N recommendation algorithms", "Political artifacts and personal privacy: The yenta multiagent distributed matchmaking system", "Foundation for Intelligent Physical Agents", "Agent-based telematic services and telecom applications", "Flexible OS support and applications for trusted computing", "Applying security standards to multi agent systems", "How to play any mental game", "Privacy preserving clustering", "The platform for enterprise privacy practices: Privacy-enabled management of customer data", "The impact of social networks on multiagent recommender systems", "PocketLens: Toward a personal recommender system", "SVD-based collaborative filtering with privacy", "Advanced Security Infrastructure for Multiagent-Applications in the Telematic Area", "The prisoners' problem and the subliminal channel", "Impacts of user privacy preferences on personalized systems: a comparative study", "Environments for multiagent systems: State-of-the-art and research challenges", "Berlintainment: An agent-based context-aware entertainment planning system", "Intelligent agents: Theory and practice", "Protocols for secure computation"], "title": "An Agent-Based Approach for Privacy-Preserving Recommender Systems"}
{"id": "I-60", "keywords": "agent-base deploi applic;artifici social system", "abstract": "As more and more cars are equipped with GPS and Wi Fi transmitters, it becomes easier to design systems that will allow cars to interact autonomously with each other, e.g., regarding traffic on the roads. Indeed, car manufacturers are already equipping their cars with such devices. Though, currently these systems are a proprietary, we envision a natural evolution where agent applications will be developed for vehicular systems, e.g., to improve car routing in dense urban areas. Nonetheless, this new technology and agent applications may lead to the emergence of self-interested car owners, who will care more about their own welfare than the social welfare of their peers. These car owners will try to manipulate their agents such that they transmit false data to their peers. Using a simulation environment, which models a real transportation network in a large city, we demonstrate the benefits achieved by self-interested agents if no counter-measures are implemented.", "references": ["Peer-to-peer cooperative driving", "A novel architecture for supporting vehicular communication", "Spawn: A swarming protocol for vehicular ad-hoc wireless networks", "Autonomous gossiping: A self-organizing epidemic algorithm for selective information dissemination in mobile ad-hoc networks", "Early stopping in byzantine agreement", "Threat assessment algorithm", "The byzantine general problem", "Policies for sharing distributed probabilistic beliefs", "Gossip with malicious parties", "Tolerating malicious gossip", "A Course In Game Theory", "Routing in gossip networks", "A study of gossiping in transportation networks", "Optimal routing in gossip networks", "A method for sharing traffic jam information using inter-vehicle communication", "Truthful multicast routing in selfish wireless networks", "A social mechanism of reputation management in electronic communities"], "title": "On the Benefits of Cheating by Self-Interested Agents in Vehicular Networks"}
{"id": "I-61", "keywords": "air traffic control;multiag system;reinforc learn;optim", "abstract": "Air traffic flow management is one of the fundamental challenges facing the Federal Aviation Administration (FAA) today. The FAA estimates that in 2005 alone, there were over 322,000 hours of delays at a cost to the industry in excess of three billion dollars. Finding reliable and adaptive solutions to the flow management problem is of paramount importance if the Next Generation Air Transportation Systems are to achieve the stated goal of accommodating three times the current traffic volume. This problem is particularly complex as it requires the integration and/or coordination of many factors including: new data (e.g., changing weather info), potentially conflicting priorities (e.g., different airlines), limited resources (e.g., air traffic controllers) and very heavy traffic volume (e.g., over 40,000 flights over the US airspace). In this paper we use FACET -- an air traffic flow simulator developed at NASA and used extensively by the FAA and industry -- to test a multi-agent algorithm for traffic flow management. An agent is associated with a fix (a specific location in 2D space) and its action consists of setting the separation required among the airplanes going though that fix. Agents use reinforcement learning to set this separation and their actions speed up or slow down traffic to manage congestion. Our FACET based results show that agents receiving personalized rewards reduce congestion by up to 45% over agents receiving a global reward and by up to 67% over a current industry approach (Monte Carlo estimation).", "references": ["Efficient evaluation functions for multi-rover systems", "Multi agent reward analysis for learning in noisy domains", "Handling communiction restrictions and team formation in congestion games", "FACET: Future ATM concepts evaluation tool", "A geometric optimization approach to aircraft conflict resolution", "Free flight separation assurance using distributed algorithms", "Central east pacific flight routing", "A cooperative multi-agent approach to free flight", "Optimal strategies for free flight air traffic conflict resolution", "FACET: Future ATM concepts evaluation tool", "Autonomous agents for air-traffic deconfliction", "Benefits of direct-to in national airspace system", "Aggregate flow model for air-traffic management", "Reinforcement Learning: An Introduction", "Conflict resolution for air traffic management", "Collectives and the Design of Complex Systems", "Optimal payoff functions for members of collectives"], "title": "Distributed Agent-Based Air Traffic Flow Management"}
{"id": "I-62", "keywords": "heurist search;q-decomposit;margin revenu", "abstract": "This paper contributes to solve effectively stochastic resource allocation problems known to be NP-Complete. To address this complex resource management problem, a Q-decomposition approach is proposed when the resources which are already shared among the agents, but the actions made by an agent may influence the reward obtained by at least another agent. The Q-decomposition allows to coordinate these reward separated agents and thus permits to reduce the set of states and actions to consider. On the other hand, when the resources are available to all agents, no Q-decomposition is possible and we use heuristic search. In particular, the bounded Real-time Dynamic Programming (bounded RTDP) is used. Bounded RTDP concentrates the planning on significant states only and prunes the action space. The pruning is accomplished by proposing tight upper and lower bounds on the value function.", "references": ["Learning to act using real-time dynamic programming", "An iterative algorithm for solving constrained decentralized markov decision processes", "Faster heuristic search algorithms for planning with uncertainty and full feedback", "Labeled LRTDP approach: Improving the convergence of real-time dynamic programming", "A heuristic search algorithm that finds solutions with loops", "Bounded real-time dynamic programming: RTDP with monotone upper bounds and performance guarantees", "Microeconomics", "On-line Q-learning using connectionist systems", "Q-decomposition for reinforcement learning agents", "How to dynamically merge markov decision processes", "Focused real-time dynamic programming for MDPs: Squeezing more out of a heuristic", "Modeling and solving a resource allocation problem with soft constraint techniques"], "title": "A Q-decomposition and Bounded RTDP Approach to Resource Allocation"}
{"id": "I-63", "keywords": "task and resourc alloc in agent system;multiag plan", "abstract": "Optimal resource scheduling in multiagent systems is a computationally challenging task, particularly when the values of resources are not additive. We consider the combinatorial problem of scheduling the usage of multiple resources among agents that operate in stochastic environments, modeled as Markov decision processes (MDPs). In recent years, efficient resource-allocation algorithms have been developed for agents with resource values induced by MDPs. However, this prior work has focused on static resource-allocation problems where resources are distributed once and then utilized in infinite-horizon MDPs. We extend those existing models to the problem of combinatorial resource scheduling, where agents persist only for finite periods between their (predefined) arrival and departure times, requiring resources only for those time periods. We provide a computationally efficient procedure for computing globally optimal resource assignments to agents over time. We illustrate and empirically analyze the method in the context of a stochastic job-scheduling domain.", "references": ["Adaptive control of constrained Markov chains: Criteria and policies", "Dynamic Programming", "Solving concisely expressed combinatorial auction problems", "Bidding languages for combinatorial auctions", "Integrated Resource Allocation and Planning in Stochastic Multiagent Environments", "Optimal resource allocation and policy formulation in loosely-coupled Markov decision processes", "Computationally efficient combinatorial auctions for resource allocation in weakly-coupled MDPs", "Resource allocation among agents with preferences induced by factored MDPs", "Mechanism design and deliberative agents", "Bidding and allocation in combinatorial auctions", "An MDP-based approach to Online Mechanism Design", "Approximately efficient online mechanism design", "Markov Decision Processes", "Computationally manageable combinational auctions", "An algorithm for optimal winner determination in combinatorial auctions"], "title": "Combinatorial Resource Scheduling for Multiagent MDPs"}
{"id": "I-64", "keywords": "organiz-self design;task and resourc alloc", "abstract": "Organizations are an important basis for coordination in multiagent systems. However, there is no best way to organize and all ways of organizing are not equally effective. Attempting to optimize an organizational structure depends strongly on environmental features including problem characteristics, available resources, and agent capabilities. If the environment is dynamic, the environmental conditions or the problem task structure may change over time. This precludes the use of static, design-time generated, organizational structures in such systems. On the other hand, for many real environments, the problems are not totally unique either: certain characteristics and conditions change slowly, if at all, and these can have an important effect in creating stable organizational structures. Organizational-Self Design (OSD) has been proposed as an approach for constructing suitable organizational structures at runtime. We extend the existing OSD approach to include worth-oriented domains, model other resources in addition to only processor resources and build in robustness into the organization. We then evaluate our approach against the contract-net approach and show that our OSD agents perform better, are more efficient, and more flexible to changes in the environment.", "references": ["Dynamic reorganization of decision-making groups", "Computational organization theory", "The analysis of coordination in an information system application - emergency medical services", "The use of meta-level control for coordination in a distributed problem solving network", "Environment centered analysis and design of coordination mechanisms", "Coordinating mutually exclusive resources using GPGP", "An experimental evaluation of domain-independent fault handling services in open multiagent systems", "Towards Dynamic Reorganization of Agent Societies", "Using self-diagnosis to adapt organizational structures", "Organization self-design of distributed production systems", "Evolution of the gpgp/tTowards adaptive fault tolerance for distributed multiagent systemsms domain-independent coordination framework", "Agent cloning: an approach to agent mobility and resource allocation", "An organizational self-design model for organizational change", "Coordination decision support assistants", "Design-to-criteria scheduling: Real-time agent control"], "title": "Organizational Self-Design in Semi-dynamic Environments"}
{"id": "I-65", "keywords": "dynam inuenc diagram;decis-make;agent model", "abstract": "We develop a new graphical representation for interactive partially observable Markov decision processes (I-POMDPs) that is significantly more transparent and semantically clear than the previous representation. These graphical models called interactive dynamic influence diagrams (I-DIDs) seek to explicitly model the structure that is often present in real-world problems by decomposing the situation into chance and decision variables, and the dependencies between the variables. I-DIDs generalize DIDs, which may be viewed as graphical representations of POMDPs, to multiagent settings in the same way that I-POMDPs generalize POMDPs. I-DIDs may be used to compute the policy of an agent online as the agent acts and observes in a setting that is populated by other interacting agents. Using several examples, we show how I-DIDs may be applied and demonstrate their usefulness.", "references": ["Interactive epistemology i: Knowledge", "The challenge of poker", "Hierarchies of beliefs and common knowledge", "Behavioral Game Theory: Experiments in Strategic Interaction", "Cooperation and punishment in public goods experiments", "The Theory of Learning in Games", "Game Theory", "A language for modeling agent's decision-making processes in games", "A framework for sequential planning in multiagent settings", "Rational coordination in multi-agent environments", "Games with incomplete information played by bayesian players", "The Principles and Applications of Decision Analysis", "Planning and actinInteractive dynamic influence diagramsg in partially observable stochastic domains", "Multi-agent influence diagrams for representing and solving games", "Exact solutions to interactive pomdps using behavioral equivalence", "Artificial Intelligence: A Modern Approach", "Evaluating influence diagrams", "Learning models of other agents using influence diagrams"], "title": "Graphical Models for Online Solutions to Interactive POMDPs"}
{"id": "I-66", "keywords": "multi-agent system;partial observ markov decis process;pomdp;distribut pomdp;global optim solut", "abstract": "Distributed Partially Observable Markov Decision Problems (Distributed POMDPs) are a popular approach for modeling multi-agent systems acting in uncertain domains. Given the significant complexity of solving distributed POMDPs, particularly as we scale up the numbers of agents, one popular approach has focused on approximate solutions. Though this approach is efficient, the algorithms within this approach do not provide any guarantees on solution quality. A second less popular approach focuses on global optimality, but typical results are available only for two agents, and also at considerable computational cost. This paper overcomes the limitations of both these approaches by providing SPIDER, a novel combination of three key features for policy generation in distributed POMDPs: (i) it exploits agent interaction structure given a network of agents (i.e. allowing easier scale-up to larger number of agents); (ii) it uses a combination of heuristics to speedup policy search; and (iii) it allows quality guaranteed approximations, allowing a systematic tradeoff of solution quality for time. Experimental results show orders of magnitude improvement in performance when compared with previous global optimal algorithms.", "references": ["Solving transition independent decentralized Markov decision processes", "Bounded policy iteration for decentralized POMDPs", "The complexity of decentralized control of MDPs", "Approximate solutions for partially observable stochastic games with common payoffs", "Dynamic programming for partially observable stochastic games", "Distributed sensor nets: A multiagent perspective", "Taking dcop to the real world: Efficient complete solutions for distributed event scheduling", "An asynchronous complete method for distributed constraint optimization", "Taming decentralized POMDPs: Towards efficient policy computation for multiagent settings", "Networked distributed POMDPs: A synthesis of distributed constraint optimization and POMDPs", "Learning to cooperate via policy search", "A scalable method for multiagent constraint optimization", "A heuristic search algorithm for solving decentralized POMDPs"], "title": "Letting loose a SPIDER on a network of POMDPs: Generating quality guaranteed policies"}
{"id": "I-68", "keywords": "multi-agent system;decentr markov decis process;tempor constraint;local optim solut", "abstract": "Decentralized Markov Decision Processes (DEC-MDPs) are a popular model of agent-coordination problems in domains with uncertainty and time constraints but very difficult to solve. In this paper, we improve a state-of-the-art heuristic solution method for DEC-MDPs, called OC-DEC-MDP, that has recently been shown to scale up to larger DEC-MDPs. Our heuristic solution method, called Value Function Propagation (VFP), combines two orthogonal improvements of OC-DEC-MDP. First, it speeds up OC-DEC-MDP by an order of magnitude by maintaining and manipulating a value function for each state (as a function of time) rather than a separate value for each pair of sate and time interval. Furthermore, it achieves better solution qualities than OC-DEC-MDP because, as our analytical results show, it does not overestimate the expected total reward like OC-DEC- MDP. We test both improvements independently in a crisis-management domain as well as for other types of domains. Our experimental results demonstrate a significant speedup of VFP over OC-DEC-MDP as well as higher solution qualities in a variety of situations.", "references": ["Decentralized MDPs with Event-Driven Interactions", "Transition-Independent Decentralized Markov Decision Processes", "The complexity of decentralized control of Markov decision processes", "A polynomial algorithm for decentralized Markov decision processes with temporal constraints", "An iterative algorithm for solving constrained decentralized Markov decision processes", "Sequential optimality and coordination in multiagent systems", "Exact solutions to time-dependent MDPs", "Optimizing information exchange in cooperative multi-agent systems", "Lazy approximation for solving continuous finite-horizon MDPs", "Risk-sensitive planning with one-switch utility functions: Value iteration", "Coordinated plan management using multiagent MDPs", "Taming decentralized POMDPs: Towards efficient policy computation for multiagent settings", "Networked distributed POMDPs: A synergy of distributed constraint optimization and POMDPs"], "title": "On Opportunistic Techniques for Solving Decentralized Markov Decision Processes with Temporal Constraints"}
{"id": "I-70", "keywords": "ontolog;cooper;emerg behavior", "abstract": "Ontologies building from text is still a time-consuming task which justifies the growth of Ontology Learning. Our system named Dynamo is designed along this domain but following an original approach based on an adaptive multi-agent architecture. In this paper we present a distributed hierarchical clustering algorithm, core of our approach. It is evaluated and compared to a more conventional centralized algorithm. We also present how it has been improved using a multi-criteria approach. With those results in mind, we discuss the limits of our system and add as perspectives the modifications required to reach a complete ontology building solution.", "references": ["Construction of a regional ontology from text and its use within a documentary system", "Text analysis for ontology and terminology engineering", "Collaborative ontology building with wiki", "Agent-Oriented Methodologies", "Background and foreground knowledge in dynamic ontology construction", "A corpus-based conceptual clustering method for verb frames and ontology acquisition", "Ontology Engineering: a Survey and a Return on Experience", "Living Design for Open Computational Systems", "A Theory of emergent computation based on cooperative self-organization for adaptive artificial systems", "Dynamic ontologies on the web", "Terminology extraction from text to build an ontology in surgical intensive care", "Reconciling Ontological Differences by Assistant Agents", "Ontology learning for the Semantic Web", "Mining Ontologies from Text", "Foundations of Statistical Natural Language Processing", "Dynamic decentralized any-time hierarchical clustering"], "title": "A Multi-Agent System for Building Dynamic Ontologies"}
{"id": "I-71", "keywords": "semant align;distribut logic;channel refin", "abstract": "Ontology matching is currently a key technology to achieve the semantic alignment of ontological entities used by knowledge-based applications, and therefore to enable their interoperability in distributed environments such as multi-agent systems. Most ontology matching mechanisms, however, assume matching prior integration and rely on semantics that has been coded a priori in concept hierarchies or external sources. In this paper, we present a formal model for a semantic alignment procedure that incrementally aligns differing conceptualisations of two or more agents relative to their respective perception of the environment or domain they are acting in. It hence makes the situation in which the alignment occurs explicit in the model. We resort to Channel Theory to carry out the formalisation.", "references": ["Information Flow: The Logic of Distributed Systems", "Local models semantics, or contextual reasoning = locality + compatibility", "Semantic matching", "IF-Map: An ontology-mapping method based on information-flow theory", "Ontology mapping: The sate of the art", "Semantic integration in the Information Flow Framework", "CyC: A large-scale investment in knowledge infrastructure", "PowerMap: Mapping the real Semantic Web on the fly", "Dynamic Ontology Refinement", "Progressive ontology alignment for meaning coordination: An information-theoretic foundation", "A survey of schema-based matching approaches", "The Origins of Ontologies and Communication Conventions in Multi-Agent Systems", "ANEMONE: An Effective Minimal Ontology Negotiation Environment"], "title": "A Formal Model for Situated Semantic Alignment"}
{"id": "I-72", "keywords": "negoti;induct learn;ontolog;semant similar", "abstract": "In online, dynamic environments, the services requested by consumers may not be readily served by the providers. This requires the service consumers and providers to negotiate their service needs and offers. Multiagent negotiation approaches typically assume that the parties agree on service content and focus on finding a consensus on service price. In contrast, this work develops an approach through which the parties can negotiate the content of a service. This calls for a negotiation approach in which the parties can understand the semantics of their requests and offers and learn each other's preferences incrementally over time. Accordingly, we propose an architecture in which both consumers and producers use a shared ontology to negotiate a service. Through repetitive interactions, the provider learns consumers' needs accurately and can make better targeted offers. To enable fast and accurate learning of preferences, we develop an extension to Version Space and compare it with existing learning techniques. We further develop a metric for measuring semantic similarity between services and compare the performance of our approach using different similarity metrics.", "references": ["On possibilistic case-based reasoning for selecting partners for multi-attribute agent negotiation", "A comment on issue-by-issue negotiations", "Managing e-market negotiation in context with a multiagent system", "Using similarity criteria to make issue trade-offs in automated negotiations", "Optimal agents for multi-issue negotiation", "A note on the utility of incremental learning", "Splitting and merging version spaces to learn disjunctive concepts", "An information-theoretic definition of similarity", "Agents that buy and sell", "Machine Learning", "OWL: Web ontology language guide", "Foundations of Soft Case-Based Reasoning", "Induction of decision trees", "Dialogues for negotiation: Agent varieties and dialogue sequences", "Value-oriented electronic commerce", "Ontologies for supporting negotiation in e-commerce", "Features of similarity", "Incremental induction of decision trees", "Verb semantics and lexical selection"], "title": "Learning Consumer Preferences Using Semantic Similarity"}
{"id": "I-73", "keywords": "reput;trust;ontolog;art testb", "abstract": "In open MAS it is often a problem to achieve agents' interoperability. The heterogeneity of its components turns the establishment of interaction or cooperation among them into a non trivial task, since agents may use different internal models and the decision about trust other agents is a crucial condition to the formation of agents' cooperation. In this paper we propose the use of an ontology to deal with this issue. We experiment this idea by enhancing the ART reputation model with semantic data obtained from this ontology. This data is used during interaction among heterogeneous agents when exchanging reputation values and may be used for agents that use different reputation models.", "references": ["Abstracting Interaction Patterns: A Programming Paradigm for Open Distributed Systems", "Towards a Functional Ontology of Reputation", "Using a Functional Ontology of Reputation to Interoperate Different Agent Reputation Models", "Principles of trust in MAS: cognitive anatomy, social importance and quantification", "Reputation in Artificial Societies: Social Beliefs for Social Order", "A specification of the agent reputation and trust (art) testbed: experimentation and competition for trust in agent societies", "Notions of Reputation in Multi-Agents Systems: A Review", "Decentralized monitoring of agent communication with a reputation model", "ReGret: Reputation in gregarious societies", "Review on Computational Trust and Reputation Models", "An experience on reputation models interoperability based on a functional ontology", "Enabling technologies for inter-operability", "An Evidential Model of Distributed Reputation Management", "Trust Management Through Reputation Mechanisms"], "title": "Exchanging Reputation Values among Heterogeneous Agent Reputation Models: An Experience on ART Testbed"}
{"id": "I-74", "keywords": "argument;dialogu;relev", "abstract": "Work on argumentation-based dialogue has defined frameworks within which dialogues can be carried out, established protocols that govern dialogues, and studied different properties of dialogues. This work has established the space in which agents are permitted to interact through dialogues. Recently, there has been increasing interest in the mechanisms agents might use to choose how to act  the rhetorical maneuvering that they use to navigate through the space defined by the rules of the dialogue. Key in such considerations is the idea of relevance, since a usual requirement is that agents stay focussed on the subject of the dialogue and only make relevant remarks. Here we study several notions of relevance, showing how they can be related to both the rules for carrying out dialogues and to rhetorical maneuvering.", "references": ["On the acceptability of arguments in preference-based argumentation framework", "Arguments, dialogue, and negotiation", "Strategic and tactic reasoning for communicating agents", "A logic-based theory of deductive arguments", "Handling controversial arguments in bipolar argumentation frameworks", "Trends in agent communication language", "Agent theory for team formation by dialogue", "On the acceptability of arguments and its fundamental role in nonmonotonic reasoning, logic programming and n-person games", "Dialectic proof procedures for assumption-based, admissable argumentation", "To commit or not to commit", "More on non-cooperation in Dialogue Logic", "Non-cooperation in Dialogue Logic", "Mathematical models of dialogue", "Reaching agreements through argumentation: a logical model and implementation", "Loose lips sink ships: A heuristic for argumentation", "Negotiation through argumentation - a preliminary report", "An analysis of formal inter-agent dialogues", "On the outcomes of formal inter-agent dialogues", "On dialogue systems with speech acts, arguments, and counterarguments", "Relating protocols for dynamic dispute with logics for defeasible argumentation", "Modelling reasoning with precedents in a formal dialogue game", "Towards a theory of negotiation strategy", "Dialogue frames in agent communications", "Adaptive strategies for practical argument-based negotiation", "Ultima ratio: should Hamlet kill Claudius", "Argumentation: Planning other agents" plans", "Commitment in Dialogue: Basic Concepts of Interpersonal Reasoning"], "title": "On the relevance of utterances in formal inter-agent dialogues"}
{"id": "I-75", "keywords": "negoti and argument;agent commun languag and protocol", "abstract": "We investigate the properties of a multiagent system where each (distributed) agent locally perceives its environment. Upon perception of an unexpected event, each agent locally computes its favoured hypothesis and tries to propagate it to other agents, by exchanging hypotheses and supporting arguments (observations). However, we further assume that communication opportunities are severely constrained and change dynamically. In this paper, we mostly investigate the convergence of such systems towards global consistency. We first show that (for a wide class of protocols that we shall define), the communication constraints induced by the topology will not prevent the convergence of the system, at the condition that the system dynamics guarantees that no agent will ever be isolated forever, and that agents have unlimited time for computation and arguments exchange. As this assumption cannot be made in most situations though, we then set up an experimental framework aiming at comparing the relative efficiency and effectiveness of different interaction protocols for hypotheses exchange. We study a critical situation involving a number of agents aiming at escaping from a burning building. The results reported here provide some insights regarding the design of optimal protocol for hypotheses refinement in this context.", "references": ["When agents communicate hypotheses in critical situations", "Support-based distributed search: a new approach for multiagent constraint processing", "Argumentation as distributed constraint satisfaction: Applications and results", "Jennings. Is it worth arguing?", "Arguments and counterexamples in case-based joint deliberation", "Explanation and prediction: An architecture for default and abductive reasoning", "Argumention-based negotiation", "A protocol for multi-agent diagnosis with spatially distributed knowledge", "Reaching diagnostic agreement in multiagent diagnosis", "Preliminary study - using robocuprescue simulations for disasters prevention"], "title": "Hypotheses Refinement under Topological Communication Constraints"}
{"id": "I-76", "keywords": "negoti;extend abduct;relax;logic program", "abstract": "This paper studies a logical framework for automated negotiation between two agents. We suppose an agent who has a knowledge base represented by a logic program. Then, we introduce methods of constructing counter-proposals in response to proposals made by an agent. To this end, we combine the techniques of extended abduction in artificial intelligence and relaxation in cooperative query answering for databases. These techniques are respectively used for producing conditional proposals and neighborhood proposals in the process of negotiation. We provide a negotiation protocol based on the exchange of these proposals and develop procedures for computing new proposals.", "references": ["Arguments, dialogue, and negotiation", "A new framework for knowledge revision of abductive agents through their interaction", "Repeated negotiation of logic programs", "Cooperative query answering via type abstraction hierarchy", "Negotiating logic programs", "Relaxation as a platform for cooperative answering", "Classical negation in logic programs and disjunctive databases", "Abductive framework for nonmonotonic theory change", "The role of abduction in logic programming", "Adaptive agent negotiation via argumentation", "Logical foundation of negotiation: outcome, concession and adaptation", "A theory and methodology of inductive learning", "Agents that reason and negotiate by arguing", "An abductive logic programming architecture for negotiating agents", "An abductive framework for computing knowledge base updates"], "title": "Negotiation by Abduction and Relaxation"}
{"id": "I-77", "keywords": "multiag system;negoti", "abstract": "Successful negotiators prepare by determining their position along five dimensions: Legitimacy, Options, Goals, Independence, and Commitment, (LOGIC). We introduce a negotiation model based on these dimensions and on two primitive concepts: intimacy (degree of closeness) and balance (degree of fairness). The intimacy is a pair of matrices that evaluate both an agent's contribution to the relationship and its opponent's contribution each from an information view and from a utilitarian view across the five LOGIC dimensions. The balance is the difference between these matrices. A relationship strategy maintains a target intimacy for each relationship that an agent would like the relationship to move towards in future. The negotiation strategy maintains a set of Options that are in-line with the current intimacy level, and then tactics wrap the Options in argumentation with the aim of attaining a successful deal and manipulating the successive negotiation balances towards the target intimacy.", "references": [" Inequity in social exchange", "Environment engineering for multiagent systems", "Reversal of preference in allocation decisions: judging an alternative versus choosing among alternatives", " Co-Opetition: A Revolution Mindset That Combines Competition and Cooperation", "Bayesian Inference and Maximum Entropy Methods in Science and Engineering", "Bargaining with information", "Getting to Yes: Negotiating agreements without giving in Penguin Books", "IF-Map: An ontology-mapping method based on information-flow theory", "Essentials of Negotiation", "An approach for measuring semantic similarity between words using multiple information sources", "Information Theory, Inference and Learning Algorithms", "Common sense and maximum entropy", "An information-based model for trust", "Trust and honour in information-based agency", "Information-based agency", "The negotiated allocations of benefits and burdens: The impact of outcome valence, contribution, and relationship", "Friends, lovers, colleagues, strangers: The effects of relationships on the process and outcome of negotiations"], "title": "The LOGIC Negotiation Model"}
{"id": "J-33", "keywords": "auction;multiattribut auction", "abstract": "We investigate the space of two-sided multiattribute auctions, focusing on the relationship between constraints on the offers traders can express through bids, and the resulting computational problem of determining an optimal set of trades. We develop a formal semantic framework for characterizing expressible offers, and show conditions under which the allocation problem can be separated into first identifying optimal pairwise trades and subsequently optimizing combinations of those trades. We analyze the bilateral matching problem while taking into consideration relevant results from multiattribute utility theory. Network flow models we develop for computing global allocations facilitate classification of the problem space by computational complexity, and provide guidance for developing solution algorithms. Experimental trials help distinguish tractable problem classes for proposed solution techniques.", "references": ["Network Flows", "Graphical models for preference and utility", "The Future of e-Markets: Multi-Dimensional Market Mechanisms", "Configurable offers and winner determination in multi-attribute auctions", "Multi-attribute auctions for electronic procurement", "Eliciting bid taker non-price preferences in (combinatorial) auctions", "The design of multidimensional auctions", "Design competition through multidimensional auctions", "Topological methods in cardinal utility theory", "Electronic call market trading", "Multiattribute utility representation for willingness-to-pay functions", "Exchange market for complex goods: Theory and experiments", "A multiplier adjustment method for the generalized assignment problem", "Exchanges for complex commodities: Search for optimal matches", "GAI networks for decision making under certainty", "Computational aspects of clearing continuous call double auctions with assignment constraints and indivisible demand", "Decisions with Multiple Objectives: Preferences and Value Tradeoffs", "Bidding and allocation in combinatorial auctions", "Models for iterative multiattribute procurement auctions", "Side constraints and non-price attributes in markets", "Market-based allocation with indivisible bids", "Procurement auctions for differentiated goods", "Preference elicitation in proxied multiattribute auctions", "A parametrization of the auction design space"], "title": "Bid Expressiveness and Clearing Algorithms in Multiattribute Double Auctions"}
{"id": "J-34", "keywords": "comput financ;market microstructur", "abstract": "We study the stability properties of the dynamics of the standard continuous limit-order mechanism that is used in modern equity markets. We ask whether such mechanisms are susceptible to "buttery effects" --- the iniction of large changes on common measures of market activity by only small perturbations of the order sequence. We show that the answer depends strongly on whether the market consists of "absolute" traders (who determine their prices independent of the current order book state) or "relative" traders (who determine their prices relative to the current bid and ask). We prove that while the absolute trader model enjoys provably strong stability properties, the relative trader model is vulnerable to great instability. Our theoretical results are supported by large-scale experiments using limit order data from INET, a large electronic exchange for NASDAQ stocks.", "references": ["Optimal control of execution costs", "Market microstructure: a survey of microfoundations, empirical results and policy implications", "Statistical properties of stock order books: empirical results and models", "The informational content of an open limit order book", "Optimal trade execution of equities in a limit order market", "The price dynamics of common trading strategies", "Empirical market microstructure: Economic and statistical perspectives on the dynamics of trade in securities markets", "Optimal Trading Strategies", "Competitive algorithms for VWAP and limit order trading", "Reinforcement learning for optimized trade execution"], "title": "(In)Stability properties of limit order dynamics"}
{"id": "J-35", "keywords": "game theori;p2p network;scrip system", "abstract": "A model of providing service in a P2P network is analyzed. It is shown that by adding a scrip system, a mechanism that admits a reasonable Nash equilibrium that reduces free riding can be obtained. The effect of varying the total amount of money (scrip) in the system on efficiency (i.e., social welfare) is analyzed, and it is shown that by maintaining the appropriate ratio between the total amount of money and the number of agents, efficiency is maximized. The work has implications for many online systems, not only P2P networks but also a wide variety of online forums for which scrip systems are popular, but formal analyses have been lacking.", "references": ["Free riding on Gnutella", "Exchange-based incentive mechanisms for peer-to-peer file sharing", "Sybilproof reputation mechanisms", "Cornell Information Technologies", "The sybil attack", "Cooperation in the prisoner's dilemma with anonymous random matching", "eMule Project", "eMule Project", "Robust incentive techniques for peer-to-peer networks", "Pricing wifi at starbucks: issues in online mechanism design", "The social cost of cheap pseudonyms", "Propagation of trust and distrust", "A reputation system for peer-to-peer networks", "Link spam detection based on mass estimation", "Fileteller: Paying and getting paid for file storage", "Where do we stand on maximum entropy?", "The Eigentrust algorithm for reputation management in P2P networks", "Social norms and community enforcement", "LogiSense Corporation", "Mixing of random walks and other diffusions on a graph", "Open Source Technology Group", "OSMB LLC", "Markov Decision Processes", "Sharman Networks Ltd", "Karma: A secure economic framework for peer-to-peer resource sharing", "Building trust in decentralized peer-to-peer electronic communities", "Making eigenvector-based reputation systems robust to collusion"], "title": "Efficiency and Nash Equilibria in a Scrip System for P2P Networks"}
{"id": "J-36", "keywords": "game theori;inform acquisit;socrat game;nash equilibrium;correl equilibrium;algorithm", "abstract": "In traditional game theory, players are typically endowed with exogenously given knowledge of the structure of the game--either full omniscient knowledge or partial but fixed information. In real life, however, people are often unaware of the utility of taking a particular action until they perform research into its consequences. In this paper, we model this phenomenon. We imagine a player engaged in a question and- answer session, asking questions both about his or her own preferences and about the state of reality; thus we call this setting "Socratic" game theory. In a Socratic game, players begin with an a priori probability distribution over many possible worlds, with a different utility function for each world. Players can make queries, at some cost, to learn partial information about which of the possible worlds is the actual world, before choosing an action. We consider two query models: (1) an unobservable-query model, in which players learn only the response to their own queries, and (2) an observable-query model, in which players also learn which queries their opponents made.The results in this paper consider cases in which the underlying worlds of a two-player Socratic game are either constant-sum games or strategically zero-sum games, a class that generalizes constant-sum games to include all games in which the sum of payoffs depends linearly on the interaction between the players. When the underlying worlds are constant sum, we give polynomial-time algorithms to find Nash equilibria in both the observable- and unobservable-query models. When the worlds are strategically zero sum, we give efficient algorithms to find Nash equilibria in unobservablequery Socratic games and correlated equilibria in observablequery Socratic games.", "references": ["Faster approximation algorithms for the minimum latency problem", "Subjectivity and correlation in randomized strategies", "Correlated equilibrium as an expression of Bayesian rationality", "Information acquisition and efficient mechanism design", "Information in mechanism design", "The complexity of decentralized control of Markov Decision Processes", "The minimum latency problem", "Optimal plans for aggregation", "Query strategies for priced information", "3-NASH is PPAD-complete", "Settling the complexity of 2-player Nash-equilibrium", "Auctions and information acquisition: Sealed-bid or dynamic formats?", "Complexity results about Nash equilibria", "Location of bank accounts to optimize float: An analytic study of exact and approximate algorithms", "Gathering information before signing a contract", "The complexity of computing a Nash equilbrium", "Three-player games are hard", "Approximation algorithms for the test cover problem", "On making the right choice: The deliberation-without-attention effect", "Approximate solutions for partially observable stochastic games with common payoffs", "The complexity of pure Nash equilibria", "Multi-stage Information Acquisition in Auction Design", "Optimality and domination in repeated games with bounded players", "Efficient algorithms for learning to play repeated games against computationally bounded adversaries", "Drew Fudenberg and Jean Tirole", "An improved approximation ratio for the minimum latency problem", "Reducibility among equilibrium problems", "The ellipsoid method and its consequences in combinatorial optimization", " Sorting and selection with structured costs", "Dynamic programming for partially observable stochastic games", "Games with incomplete information played by Bayesian players", "Existence of correlated equilibria", "Time-dependent utility and action under uncertainty", "A New Introduction to Modal Logic", "When choice is demotivating: Can one desire too much of a good thing?", "Bounded rationality and strategic complexity in repeated games", "Selection with monotone comparison costs", "A polynomial algorithm in linear programming", "The complexity of two-person zero-sum games in extensive form", "Efficient computation of equilibria for extensive two-person games", "Mechanism Design for Computationally Limited Agents", "Bargaining with limited computation: Deliberation equilibrium", "Costly valuation computation in auctions", "Strategic deliberation and truthful revelation: An impossibility result", "Equilibrium points of bimatrix games", "Playing large games using simple strategies", "An efficient exact algorithm for singly connected graphical games", "Information acquisition and the excess refund puzzle", "Computation of equilibria in finite games", "On minimizing a set of tests", "Strategically zero-sum games: The class of games whose completely mixed equilibria cannot be improved upon", "Equilibrium points in n-person games", "Finitely repeated games with finite automata", "On the complexity of the parity argument and other inefficient proofs of existence", "Algorithms, games, and the internet", "Computing correlated equilibria in multi-player games", "Computing equilibria in multiplayer games", "On bounded rationality and computational complexity", "Auction design with costly preference elicitation", "Information acquisition in auctions", "The LP formulation of finite zero-sum games with incomplete information", "Strategic implications of uncertainty over one's own private value in auctions", "Mid-auction information acquisition", "Modeling Bounded Rationality", "The Paradox of Choice: Why More is Less", "Models of Bounded Rationality", "Choice in context: Tradeoff contrast and extremeness aversion", "Dynamic models of deliberation and the theory of games", "Reinforcement Learning: An Introduction", "Theory of Games and Economic Behavior", "Computing equilibria for two-person games", "Approximate reasoning using anytime algorithms"], "title": "Playing games in many possible worlds"}
{"id": "J-37", "keywords": "game theori;sequenti game of imperfect inform;autom abstract;equilibrium find;comput poker", "abstract": "Finding an equilibrium of an extensive form game of imperfect information is a fundamental problem in computational game theory, but current techniques do not scale to large games. To address this, we introduce the ordered game isomorphism and the related ordered game isomorphic abstraction transformation. For a multi-player sequential game of imperfect information with observable actions and an ordered signal space, we prove that any Nash equilibrium in an abstracted smaller game, obtained by one or more applications of the transformation, can be easily converted into a Nash equilibrium in the original game. We present an algorithm, GameShrink, for abstracting the game using our isomorphism exhaustively. Its complexity is (n2), where n is the number of nodes in a structure we call the signal tree. It is no larger than the game tree, and on nontrivial games it is drastically smaller, so GameShrink has time and space complexity sublinear in the size of the game tree. Using GameShrink, we find an equilibrium to a poker game with 3.1 billion nodes--over four orders of magnitude more than in the largest poker game solved previously. We discuss several electronic commerce applications for GameShrink. To address even larger games, we introduce approximation methods that do not preserve equilibrium, but nevertheless yield (ex post) provably close-to-optimal strategies.", "references": ["Zum Hilbertschen Aufbau der reellen Zahlen", "Computer analysis of sprouts", "Some two-person games involving bluffing", "Approximating game-theoretic optimal strategies for full-scale poker", "The challenge of poker", "Combinatorics", "Weak isomorphism of extensive games", "Settling the complexity of 2-player Nash equilibrium", "Linear Programming", "Game transformations and game equivalence", "On the strategic equivalence of extensive form games", "Flows in Networks", "Finding equilibria in large sequential games of imperfect information", "Optimal Rhode Island Hold'em poker", "A competitive Texas Hold'em poker player via automated abstraction and real-time equilibrium computation", "A Texas Hold'em poker player based on automated abstraction and real-time equilibrium computation", "Partition search", "GIB: Steps toward an expert-level bridge-playing program", "A global Newton method to compute Nash equilibria", "Automatically generating abstractions for planning", "On the strategic stability of equilibria", "The complexity of two-person zero-sum games in extensive form", "Finding mixed strategies with small supports in extensive form games", "Efficient computation of equilibria for extensive two-person games", "Representations and solutions for game-theoretic problems", "Sequential equilibria", "Extensive games", "A simplified two-person poker", "Extensive games and the problem of information", "Equilibrium points of bimatrix games", "Playing large games using simple strategies", "On state-space abstraction for anytime evaluation of Bayesian networks", " Microeconomic Theory", "Computation of equilibria in finite games", "Computing sequential equilibria for two-player games", "Equilibrium points in n-person games", "A simple three-person poker game", "Rationality in extensive form games", "State-space approximations for extensive form games", "Simple search methods for finding a Nash equilibrium", "Reduction of a game with complete memory to a matrix game", "Sequences of take-it-or-leave-it offers: Near-optimal auctions without full valuation revelation", "Mixed-integer programming methods for finding Nash equilibria", "Exponentially many steps for finding a Nash equilibrium in a bimatrix game", "Spieltheoretische behandlung eines oligopolmodells mit nachfragetrgheit", "Evolutionary stability in extensive two-person games -- correction and further development", "Abstraction methods for game theoretic poker", "Computer bridge: A big win for AI planning", "Efficiency of a good but not linear set union algorithm", "Equivalence of games in extensive form", "Theory of games and economic behavior", "Efficient computation of behavior strategies", "Computing equilibria for two-person games", "Computing equilibria of two-person games from the extensive form", "Primal-Dual Interior-Point Methods"], "title": "Finding Equilibria in Large Sequential Games of Imperfect Information"}
{"id": "J-38", "keywords": "coalit game theori;multi-attribut model;compact represent", "abstract": "We study coalitional games where the value of cooperation among the agents are solely determined by the attributes the agents possess, with no assumption as to how these attributes jointly determine this value. This framework allows us to model diverse economic interactions by picking the right attributes. We study the computational complexity of two coalitional solution concepts for these games -- the Shapley value and the core. We show how the positive results obtained in this paper imply comparable results for other games studied in the literature.", "references": ["Computers and Intractability: A Guide to the Theory of NP-Completeness", "Marginal contribution nets: A compact representation scheme for coalitional games", "Totally balanced games and games of flow", "Microeconomic Theory", "A Course in Game Theory", "A value for n-person games", "Task allocation via coalition formation among autonomous agents", "A kernel-oriented model for autonomous-agent coalition-formation in general environments: Implentation and results", "Theory of Games and Economic Behvaior", "Coalitional games in open anonymous environments"], "title": "Multi-Attribute Coalitional Games"}
{"id": "J-39", "keywords": "onlin auction;option;proxi bid;sequenti auction problem;ebai", "abstract": "Bidders on eBay have no dominant bidding strategy when faced with multiple auctions each offering an item of interest. As seen through an analysis of 1,956 auctions on eBay for a Dell E193FP LCD monitor, some bidders win auctions at prices higher than those of other available auctions, while others never win an auction despite placing bids in losing efforts that are greater than the closing prices of other available auctions. These misqueues in strategic behavior hamper the efficiency of the system, and in so doing limit the revenue potential for sellers. This paper proposes a novel options-based extension to eBay's proxy-bidding system that resolves this strategic issue for buyers in commoditized markets. An empirical analysis of eBay provides a basis for computer simulations that investigate the market effects of the options-based scheme, and demonstrates that the options-based scheme provides greater efficiency than eBay, while also increasing seller revenue.", "references": ["Developing a bidding agent for multiple heterogeneous auctions", "User heterogeneity and its impact on electronic auctionmarket design: An empirical exploration", "Optimal bidding in on-line auctions", "Sequential auctions for the allocation of resources with complementarities", "Decision procedures for multiple auctions", "Mutually destructive bidding: The FCC auction design problem", "Consumer heterogeneity and competitive price-matching guarantees", "Investment under Uncertainty", "Managing risks in multiple online auctions: An options approach", "Shopbots and pricebots", "Inference with an incomplete model of English auctions", "Online auctions with re-usable goods", "Preemption and delay in eBay auctions", "A robust open ascending-price multi-unit auction protocol against false-name bids", "Price-matching policies: An empirical case", "Estimating bidders' valuation distributions in online auctions", "Competitive analysis of incentive compatible on-line auctions", "Price matching in a model of equilibrium price dispersion", "Business-to-business electronic commerce", "Last-minute bidding and the rules for ending second-price auctions: Evidence from eBay and Amazon auctions on the Internet", "Internet auctions with many traders", "Mechanism design for online real-time scheduling", "Innovative approaches to competitive mineral leasing", "Leveled commitment contracts and strategic breach", "Issues in automated negotiation and electronic commerce: Extending the Contract Net framework", "Mining for bidding strategies on eBay", "Late and multiple bidding in competing second price Internet auctions", "Is last minute bidding bad?", "An equilibrium model of a dynamic auction marketplace"], "title": "The Sequential Auction Problem on eBay: An Empirical Analysis and a Solution"}
{"id": "J-40", "keywords": "game theori;evolutionari stabl strategi;network", "abstract": "We study a natural extension of classical evolutionary game theory to a setting in which pairwise interactions are restricted to the edges of an undirected graph or network. We generalize the definition of an evolutionary stable strategy (ESS), and show a pair of complementary results that exhibit the power of randomization in our setting: subject to degree or edge density conditions, the classical ESS of any game are preserved when the graph is chosen randomly and the mutation set is chosen adversarially, or when the graph is chosen adversarially and the mutation set is chosen randomly. We examine natural strengthenings of our generalized ESS definition, and show that similarly strong resultsnare not possible for them.", "references": ["Winning Ways for Your Mathematical Plays", "On the evolution of imitative behavior", "The statistical mechanics of strategic interaction", "The statistical mechanics of best-response strategy revision", "Random Graphs", "Communication and coordination in social networks", "Learning, local interaction, and coordination", "Altruists, egoists, and hooligans in a local interaction model", "Probability and Random Processes", "A survey of models of network formation: Stability and efficiency", "Correlated equilibria in graphical games", "Economic properties of social networks", "Graphical models for game theory", "Evolutionary dynamics on graphs", "Contagion", "Why imitate and if so, how?", "Evolution and the Theory of Games", "How to cheat against a simple mixed strategy ESS", "Evolutionary Game Theory"], "title": "Networks Preserving Evolutionary Equilibria and the Power of Randomization"}
{"id": "J-41", "keywords": "sponsor search;search engin;slot alloc;auction theori;rank by bid;rank by revenu", "abstract": "Billions of dollars are spent each year on sponsored search, a form of advertising where merchants pay for placement alongside web search results. Slots for ad listings are allocated via an auction-style mechanism where the higher a merchant bids, the more likely his ad is to appear above other ads on the page. In this paper we analyze the incentive, efficiency, and revenue properties of two slot auction designs: "rank by bid" (RBB) and "rank by revenue" (RBR), which correspond to stylized versions of the mechanisms currently used by Yahoo! and Google, respectively. We also consider first- and second-price payment rules together with each of these allocation rules, as both have been used historically. We consider both the "short-run" incomplete information setting and the "long-run" complete information setting. With incomplete information, neither RBB nor RBR are truthful with either first or second pricing. We find that the informational requirements of RBB are much weaker than those of RBR, but that RBR is efficient whereas RBB is not. We also show that no revenue ranking of RBB and RBR is possible given an arbitrary distribution over bidder values and relevance. With complete information, we find that no equilibrium exists with first pricing using either RBB or RBR. We show that there typically exists a multitude of equilibria with second pricing, and we bound the divergence of (economic) value in such equilibria from the value obtained assuming all merchants bid truthfully.", "references": ["Revenue maximization when bidders have budgets", "Personal Communication", "Multi-unit auctions with budget-constrained bidders", "Antisocial agents and Vickrey auctions", "Strategic bidder behavior in sponsored search auctions", "Internet advertising and the generalized second price auction: Selling billions of dollars worth of keywords", "Implementing sponsored search in web search engines: Computational evaluation of alternative mechanisms", "Characterization of satisfactory mechanisms for the revelation of preferences for public goods", "Groves schemes on restricted domains", "A formal analysis of search auctions including predictions on click fraud and bidding tactics", "Auction Theory", "Designing online auctions with past performance information", "Stochastic and contingent payment auctions", "Adwords and generalized on-line matching", "Putting Auction Theory to Work", "Monotone comparative statics", "On the existence of pure and mixed strategy Nash equilibria in discontinuous games", "Position auctions", " Counterspeculation, auctions and competitive sealed tenders"], "title": "An Analysis of Alternative Slot Auction Designs for Sponsored Search"}
{"id": "J-42", "keywords": "e-commerc;recommend system;viral market", "abstract": "We present an analysis of a person-to-person recommendation network, consisting of 4 million people who made 16 million recommendations on half a million products. We observe the propagation of recommendations and the cascade sizes, which we explain by a simple stochastic model. We analyze how user behavior varies within user communities defined by a recommendation network. Product purchases follow a long tail where a significant share of purchases belongs to rarely sold items. We establish how the recommendation network grows over time and how effective it is from the viewpoint of the sender and receiver of the recommendations. While on average recommendations are not very effective at inducing purchases and do not spread very far, we present a model that successfully identifies communities, product, and pricing categories for which viral marketing seems to be very effective.", "references": ["The Long Tail: Why the Future of Business Is Selling Less of More", "Infectious Diseases of Humans: Dynamics and Control", "Profiting from obscurity: What the long tail means for the economics of e-commerce", "The Mathematical Theory of Infectious Diseases and its Applications", "A new product growth for model consumer durables", "Managing customerinitiated contacts with manufacturers: The impact on share of category requirements and word-of-mouth behavior", "Hotmale", "Social ties and word-of-mouth referral behavior", "Consumer surplus in the digital economy: Estimating the value of increased product variety at online booksellers", "As consumer attitudes shift, so must marketing strategies", "Complex contagion and the weakness of long ties", "The effect of word-of-mouth on sales: Online book reviews", "Optimal structure identification with greedy search", "Finding community structure in very large networks", "A multi-stage model of word-of-mouth through electronic referrals", "On the evolution of random graphs", "Structure, cooperation, and the flow of market information", "Talk of the network: A complex systems look at the underlying process of word-of-mouth", "It may be a long time before the long tail is wagging the web", "Threshold models of collective behavior", "The strength of weak ties", "Information diffusion through blogspace", "Network-based marketing: Identifying likely adopters via consumer networks. Statist", "Nonequilibrium phase transition in the coevolution of networks and opinions", "What exactly is viral marketing & quest", "Maximizing the spread of infuence in a social network", "Reverse small world experiment", "The dynamics of viral marketing", "Patterns of influence in a recommendation network", "Amazon.com recommendations: item-to-item collaborative filtering", "Applying quantitative marketing techniques to the internet", "Trust among strangers in internet transactions: Empirical analysis of ebays reputation system", "Mining knowledge-sharing sites for viral marketing", "Diffusion of Innovations", "Diffusion in organizations and social movements: From hybrid corn to poison pills", "Knowledge-sharing and influence in online social networks via viral marketing", "An experimental study of the small world problem", "A simple model of global cascades on random networks", "Social structure and opinion formation", "Modeling interdependent consumer preferences"], "title": "The Dynamics of Viral Marketing"}
{"id": "J-44", "keywords": "recommend system;collabor filter;neighborhood;user-base and item-base algorithm;scout;promot;connector", "abstract": "Recommender systems aggregate individual user ratings into predictions of products or services that might interest visitors. The quality of this aggregation process crucially affects the user experience and hence the effectiveness of recommenders in e-commerce. We present a characterization of nearest-neighbor collaborative filtering that allows us to disaggregate global recommender performance measures into contributions made by each individual rating. In particular, we formulate three roles---scouts, promoters, and connectors---that capture how users receive recommendations, how items get recommended, and how ratings of these two types are themselves connected, respectively. These roles find direct uses in improving recommendations for users, in better targeting of items and, most importantly, in helping monitor the health of the system as a whole. For instance, they can be used to track the evolution of neighborhoods, to identify rating subspaces that do not contribute (or contribute negatively) to system performance, to enumerate users who are in danger of leaving, and to assess the susceptibility of the system to attacks such as shilling. We argue that the three rating roles presented here provide broad primitives to manage a recommender system and its community.", "references": ["Is seeing believing?: How recommender system interfaces affect user's opinions", "An algorithmic framework for performing collaborative filtering", "Evaluating collaborative filtering recommender systems", "Personal communication", "Shilling recommender systems for fun and profit", "Discovering frequent episodes and learning hidden markov models: A formal connection", "A collaborative filtering algorithm and evaluation metric that accurately model the user experience", "Collaborative recommendation: A robustness analysis", "Scouts, promoters, and connectors: The roles of ratings in nearest-neighbor collaborative filtering", "Getting to know you: Learning new user preferences in recommender systems", "Influence in ratings-based recommender systems: An algorithm-independent approach", "GroupLens: An open architecture for collaborative filtering of netnews", "Item-based collaborative filtering recommendation algorithms", "Methods and metrics for cold-start recommendation"], "title": "Scouts, Promoters, and Connectors: The Roles of Ratings in Nearest Neighbor Collaborative Filtering"}
{"id": "J-45", "keywords": "empir mechan design;game theori", "abstract": "Our proposed methods employ learning and search techniques to estimate outcome features of interest as a function of mechanism parameter settings. We illustrate our approach with a design task from a supply-chain trading competition. Designers adopted several rule changes in order to deter particular procurement behavior, but the measures proved insufficient. Our empirical mechanism analysis models the relation between a key design parameter and outcomes, confirming the observed behavior and indicating that no reasonable parameter settings would have been likely to achieve the desired effect. More generally, we show that under certain conditions, the estimator of optimal mechanism parameter setting based on empirical data is consistent.", "references": ["The supply chain trading agent competition", "A stochastic programming approach to scheduling in TAC SCM", "Generalized confidence intervals for the largest value of some functions of parameters under normality", "Evolution of market mechanism through a continuous space of auction-types", "Active learning with statistical models. Journal of Artificial Intelligence Research", "An algorithm for automatically designing deterministic mechanisms without payments", "Evolutionary games in economics", "Statistical Theory: A Medley of Core Topics", "An analysis of the 2004 supply chain management trading agent competition", "Microeconomic Theory", "Optimal auction design", "Simulation optimization", "TacTex-03: A supply chain management agent", "Automated agents versus virtual humans: an evolutionary game theoretic comparison of two double-auction market designs", "Co-evolution of auction mechanisms and trading strategies: towards a novel approach to microeconomic design", "Using genetic programming to optimise pricing rules for a double-auction market", "Learning payoff functions in infinite games", "Analyzing complex strategic interactions in multi-agent systems", "Strategic interactions in a supply chain game"], "title": "Empirical Mechanism Design: Methods, with Application to a Supply-Chain Scenario"}
{"id": "J-47", "keywords": "combinatori auction;prefer elicit;ascend auction;commun complex", "abstract": "We embark on a systematic analysis of the power and limitations of iterative combinatorial auctions. Most existing iterative combinatorial auctions are based on repeatedly suggesting prices for bundles of items, and querying the bidders for their "demand" under these prices. We prove a large number of results showing the boundaries of what can be achieved by auctions of this kind. We first focus on auctions that use a polynomial number of demand queries, and then we analyze the power of different kinds of ascending-price auctions.", "references": ["Ascending auctions with package bidding", "Single transferable vote resists strategic voting", "The computational difficulty of manipulating an election", "Preference elicitation and query learning", "Preference elicitation in combinatorial auctions: Extended abstract", "How many candidates are needed to make elections hard to manipulate?", "Complexity of manipulating elections with few candidates", "Vote elicitation: Complexity and strategy-proofness", "On ascending auctions for heterogeneous objects", "Manipulation of voting schemes", "Effectiveness of query types and policies for preference elicitation in combinatorial auctions", "Communication Complexity", "Applying learning algorithms to preference elicitation", "The communication requirements of efficient allocations and supporting prices", "iBundle: An efficient ascending price bundle auction", "An implementation of the contract net protocol based on marginal cost calculations", "Preference elicitation in combinatorial auctions", "Towards a characterization of polynomial preference elicitation with value queries in combinatorial auctions", "Strategy-proofness and Arrow's conditions: existence and correspondence theorems for voting procedures and social welfare functions", "The communication requirements of social choice rules and supporting budget sets", "AkBA: A progressive, anonymous-price combinatorial auction", "Some complexity questions related to distributed computing", "On polynomial-time preference elicitation with value queries"], "title": "On the Computational Power of Iterative Auctions"}
{"id": "J-49", "keywords": "inform market;opinion pool;expert opinion;predict accuraci;forecast", "abstract": "In this paper, we examine the relative forecast accuracy of information markets versus expert aggregation. We leverage a unique data source of almost 2000 people's subjective probability judgments on 2003 US National Football League games and compare with the "market probabilities" given by two different information markets on exactly the same events. We combine assessments of multiple experts via linear and logarithmic aggregation functions to form pooled predictions. Prices in information markets are used to derive market predictions. Our results show that, at the same time point ahead of the game, information markets provide as accurate predictions as pooled expert assessments. In screening pooled expert predictions, we find that arithmetic average is a robust and efficient pooling function; weighting expert assessments according to their past performance does not improve accuracy of pooled predictions; and logarithmic aggregation functions offer bolder predictions than linear aggregation functions. The results provide insights into the predictive performance of information markets, and the relative merits of selecting among various opinion pooling methods.", "references": ["Aggregating subjective forecasts: Some empirical rCombining probability distributions from experts in risk analysisesults", "Forecaster diversity and the benefits of combining forecasts", "How to use expert advice", "Predicting the future", "Experts in Uncertainty: Opinion and Subjective Probability in Science", "Group Techniques for Program Planners: A Guide to Nominal Group and Delphi Processes", "Efficient capital market: A review of theory and empirical work", "Information aggregation in an experimental market", "Forecasting elections: A market alternative to polls", "Anatomy of an experimental political stock market", "Wishes, expectations, and actions: A survey on price formation in election stock markets", "Group consensus probability distributions: a critical survey", "A conflict between two axioms for combining subjective distributions", "Pooling operators with the marginalization property", "Characterization of externally Bayesian pooling operators", "Combining probability distributions: A critique and an annotated bibliography", "An introduction to the theory of rational expectations under asymmetric information", "The use of knowledge in society", "Recovering probability distribution from options prices", "The Delphi Method: Techniques and Applications", "Decision analysis expert use", "Combining expert judgments: A bayesian approach", "An axiomatic approach to expert resolution", "Computer-Intensive Methods for Testing Hypotheses: An Introduction", "The real power of artificial markets", "Extracting collective probabilistic forecasts from web games", "Rational expectations and the aggregation of diverse information in laboratory security markets", "Prediction markets: Does money matter?", "Internet-based virtual stock markets for business forecasting", "Bayesian aggregation", "The consensus of subjective probability distributions", "Prediction markets"], "title": "Information Markets vs. Opinion Pools: An Empirical Comparison"}
{"id": "J-50", "keywords": "commun complex;vote", "abstract": "We determine the communication complexity of the common voting rules. The rules (sorted by their communication complexity from low to high) are plurality, plurality with runoff, single transferable vote (STV), Condorcet, approval, Bucklin, cup, maximin, Borda, Copeland, and ranked pairs. For each rule, we first give a deterministic communication protocol and an upper bound on the number of bits communicated in it; then, we give a lower bound on (even the nondeterministic) communication requirements of the voting rule. The bounds match for all voting rules except STV and maximin.", "references": ["Ascending auctions with package bidding", "Single transferable vote resists strategic voting", "The computational difficulty of manipulating an election", "Preference elicitation and query learning", "Preference elicitation in combinatorial auctions: Extended abstract", "How many candidates are needed to make elections hard to manipulate?", "Complexity of manipulating elections with few candidates", "Vote elicitation: Complexity and strategy-proofness", "On ascending auctions for heterogeneous objects", "Manipulation of voting schemes", "Effectiveness of query types and policies for preference elicitation in combinatorial auctions", "Communication Complexity", "Applying learning algorithms to preference elicitation", "The communication requirements of efficient allocations and supporting prices", "iBundle: An efficient ascending price bundle auction", "An implementation of the contract net protocol based on marginal cost calculations", "Preference elicitation in combinatorial auctions", "Towards a characterization of polynomial preference elicitation with value queries in combinatorial auctions", "Strategy-proofness and Arrow's conditions: existence and correspondence theorems for voting procedures and social welfare functions", "The communication requirements of social choice rules and supporting budget sets", "AkBA: A progressive, anonymous-price combinatorial auction", "Some complexity questions related to distributed computing", "On polynomial-time preference elicitation with value queries"], "title": "Communication Complexity of Common Voting Rules"}
{"id": "J-51", "keywords": "game theori;domin;iter domin", "abstract": "We study various computational aspects of solving games using dominance and iterated dominance. We first study both strict and weak dominance (not iterated), and show that checking whether a given strategy is dominated by some mixed strategy can be done in polynomial time using a single linear program solve. We then move on to iterated dominance. We show that determining whether there is some path that eliminates a given strategy is NP-complete with iterated weak dominance. This allows us to also show that determining whether there is a path that leads to a unique solution is NP-complete. Both of these results hold both with and without dominance by mixed strategies. (A weaker version of the second result (only without dominance by mixed strategies) was already known [7].) Iterated strict dominance, on the other hand, is path-independent (both with and without dominance by mixed strategies) and can therefore be done in polynomial time.We then study what happens when the dominating strategy is allowed to place positive probability on only a few pure strategies. First, we show that finding the dominating strategy with minimum support size is NP-complete (both for strict and weak dominance). Then, we show that iterated strict dominance becomes path-dependent when there is a limit on the support size of the dominating strategies, and that deciding whether a given strategy can be eliminated by iterated strict dominance under this restriction is NP-complete (even when the limit on the support size is 3).Finally, we study Bayesian games. We show that, unlike in normal form games, deciding whether a given pure strategy is dominated by another pure strategy in a Bayesian game is NP-complete (both with strict and weak dominance); however, deciding whether a strategy is dominated by some mixed strategy can still be done in polynomial time with a single linear program solve (both with strict and weak dominance). Finally, we show that iterated dominance using pure strategies can require an exponential number of iterations in a Bayesian game (both with strict and weak dominance).", "references": ["Uniform proofs of order independence for various strategy elimination procedures", "Computing Nash equilibria of action-graph games", "A continuation method for Nash equilibria in structured games", "Complexity results about Nash equilibria", "Game Theory", "On the order of eliminating dominated strategies", "The complexity of eliminating dominated strategies", "Dynamic programming for partially observable stochastic games", "Graphical models for game theory", "A polynomial algorithm in linear programming", "A note on strategy elimination in bimatrix games", "Local-effect games", "Playing large games using simple strategies", "A polynomial-time Nash equilibrium algorithm for repeated games", "Order independence for iterated weak dominance", "Corrigendum, order independence for iterated weak dominance", "Microeconomic Theory", "Game Theory: Analysis of Conflict", "A Course in Game Theory", "Algorithms, games and the Internet", "Simple search methods for finding a Nash equilibrium"], "title": "Complexity of (Iterated) Dominance"}
{"id": "J-52", "keywords": "contract;hidden-action;incent;mechan design;moralhazard;multi-hop;princip-agent model;rout", "abstract": "In multi-hop networks, the actions taken by individual intermediate nodes are typically hidden from the communicating endpoints; all the endpoints can observe is whether or not the end-to-end transmission was successful. Therefore, in the absence of incentives to the contrary, rational (i.e., selfish) intermediate nodes may choose to forward packets at a low priority or simply not forward packets at all. Using a principal-agent model, we show how the hidden-action problem can be overcome through appropriate design of contracts, in both the direct (the endpoints contract with each individual router) and recursive (each router contracts with the next downstream router) cases. We further demonstrate that per-hop monitoring does not necessarily improve the utility of the principal or the social welfare in the system. In addition, we generalize existing mechanisms that deal with hidden-information to handle scenarios involving both hidden-information and hidden-action.", "references": ["Resilient Overlay Networks", "Frugal path mechanisms", "Loose Source Routing as a Mechanism for Traffic Policies", "Providing Packet Obituaries", "Observation-based cooperation enforcement in ad hoc networks", "An Architecture for Differentiated Service", "Performance Analysis of the CONFIDANT Protocol: Cooperation of Nodes - Fairness in Dynamic Ad Hoc Networks", "Coping with False Accusations in Misbehavior Reputation Systems For Mobile Ad Hoc Networks", "The effect of rumor spreading in reputation systems for mobile ad-hoc networks", "Stimulating Cooperation in Self-Organizing Mobile Ad-Hoc Networks", "Hidden Action and Incentives", "A BGP-based Mechanism for Lowest-Cost Routing", "Mechanism Design for Policy Routing", "Distributed Algorithmic Mechanism Design: Recent Results and Future Directions", "Learning and implementation on the internet", "Moral Hazard in Teams", "Ariadne: A Secure On-Demand Routing Protocol for Ad Hoc Networks", "SEAD: Secure Efficient Distance Vector Routing for Mobile Ad Hoc Networks", "A Micro-Payment Scheme Encouraging Collaboration in Multi-Hop Cellular Networks", "Routing as a service", "Mitigating Routing Misbehavior in Mobile Ad Hoc Networks", "Microeconomic Theory", "Algorithmic Mechanism Design", "A Secure Routing Protocol for Ad Hoc Networks", "Overcoming rational manipulation in mechanism implementation", "Moral Hazard in Sequential Teams", "Network sensitivity to hot-potato disruptions", "Dynamics of hot-potato routing in IP networks", "NIRA: A New Internet Routing Architecture", "Sprite: A Simple, Cheat-Proof, Credit-Based System for Mobile Ad-Hoc Networks", "Feedback-based Routing"], "title": "Hidden-Action in Multi-Hop Routing"}
{"id": "J-53", "keywords": "price-anticip mechan;resourc alloc;nash equilibrium;fair;price of anarchi+anarchi price", "abstract": "In this paper we formulate the fixed budget resource allocation game to understand the performance of a distributed market-based resource allocation system. Multiple users decide how to distribute their budget bids) among multiple machines according to their individual preferences to maximize their individual utility. We look at both the efficiency and the fairness of the allocation at the equilibrium, where fairness is evaluated through the measures of utility uniformity and envy-freeness. We show analytically and through simulations that despite being highly decentralized, such a system converges quickly to an equilibrium and unlike the social optimum that achieves high efficiency but poor fairness, the proposed allocation scheme achieves a nice balance of high degrees of efficiency and fairness at the equilibrium.", "references": ["Resource Allocation in Federated Distributed Computing Infrastructures", "Computational Resource Exchanges for Distributed Resource Allocation", "Market-based Proportional Resource Sharing for Clusters", "A Price-anticipating Resource Allocation Mechanism for Distributed Shared Clusters", "Microeconomic Algorithms for Load Balancing in Distributed Computer Systems", "Globus: A Metacomputing Infrastructure Toolkit", "Fibonacci Heaps and Their Uses in Improved Network Optimization Algorithms", "Data Structures for Weighted Matching and Nearest Common Ancestors with Linking", "Strategic Buyers in a Sum Bid Game for Flat Networks", "Efficiency Loss in a Network Resource Allocation Game", "Charging and Rate Control for Elastic Traffic", "Rate Control in Communication Networks: Shadow Prices, Proportional Fairness and Stability", "The Hungarian Method for the Assignment Problem", "Tycoon: an Implemention of a Distributed Market-Based Resource Allocation System", "Congestion Games with Player-Specific Payoff Functions", "Potential Games", "Algorithms, Games, and the Internet", "Combinatorial Optimization", "Scheduling", "The Popcorn Market: Online Markets for Computational Resources", "A Class of Games Possessing Pure-Strategy Nash Equilibria", "Optimal Allocation of a Divisible Good to Strategic Buyers", "A Microeconomic Scheduler for Parallel Computers", "Equity, Envy, and Efficiency", "Spawn: A Distributed Computational Economy", "Auction Protocols for Decentralized Scheduling", "Classifying Scheduling Policies with respect to Unfairness in an M/GI/1", "On the Efficiency and Fairness of a Fixed Budget Resource Allocation Game"], "title": "A Price-Anticipating Resource Allocation Mechanism for Distributed Shared Clusters"}
{"id": "J-55", "keywords": "auction;mechan design;competit analysi", "abstract": "We investigate the class of single-round, sealed-bid auctions for a set of identical items to bidders who each desire one unit. We adopt the worst-case competitive framework defined by [9, 5] that compares the profit of an auction to that of an optimal single-price sale of least two items. In this paper, we first derive an optimal auction for three items, answering an open question from [8]. Second, we show that the form of this auction is independent of the competitive framework used. Third, we propose a schema for converting a given limited-supply auction into an unlimited supply auction. Applying this technique to our optimal auction for three items, we achieve an auction with a competitive ratio of 3.25, which improves upon the previously best-known competitive ratio of 3.39 from [7]. Finally, we generalize a result from [8] and extend our understanding of the nature of the optimal competitive auction by showing that the optimal competitive auction occasionally offers prices that are higher than all bid values.", "references": ["Truthful mechanisms for one-parameter agents", "Market research and market design", "Online Computation and Competitive Analysis", "The Simple Economics of Optimal Auctions", "Competitive generalized auctions", "Competitive auctions and digital goods", "Competitiveness via consensus", "A lower bound on the competitive ratio of truthful auctions", "Competitive auctions and digital goods", "Truth Revelation in Approximately Efficient Combinatorial Auctions", "Strategyproof Sharing of Submodular Costs: Budget Balance Versus Efficiency", "Optimal Auction Design", "Algorithmic Mechanism Design", "Optimal pricing mechanisms with unknown demand", "Counterspeculation, Auctions, and Competitive Sealed Tenders"], "title": "From Optimal Limited To Unlimited Supply Auctions"}
{"id": "J-56", "keywords": "combinatori auction;bid withdraw;robust;constraint program;weight super solut", "abstract": "Bids submitted in auctions are usually treated as enforceable commitments in most bidding and auction theory literature. In reality bidders often withdraw winning bids before the transaction when it is in their best interests to do so. Given a bid withdrawal in a combinatorial auction, finding an alternative repair solution of adequate revenue without causing undue disturbance to the remaining winning bids in the original solution may be difficult or even impossible. We have called this the "Bid-taker's Exposure Problem". When faced with such unreliable bidders, it is preferable for the bid-taker to preempt such uncertainty by having a solution that is robust to bid withdrawal and provides a guarantee that possible withdrawals may be repaired easily with a bounded loss in revenue.In this paper, we propose an approach to addressing the Bid-taker's Exposure Problem. Firstly, we use the Weighted Super Solutions framework [13], from the field of constraint programming, to solve the problem of finding a robust solution. A weighted super solution guarantees that any subset of bids likely to be withdrawn can be repaired to form a new solution of at least a given revenue by making limited changes. Secondly, we introduce an auction model that uses a form of leveled commitment contract [26, 27], which we have called mutual bid bonds, to improve solution reparability by facilitating backtracking on winning bids by the bid-taker. We then examine the trade-off between robustness and revenue in different economically motivated auction scenarios for different constraints on the revenue of repair solutions. We also demonstrate experimentally that fewer winning bids partake in robust solutions, thereby reducing any associated overhead in dealing with extra bidders. Robust solutions can also provide a means of selectively discriminating against distrusted bidders in a measured manner.", "references": ["Leveled commitment contracts with myopic and strategic agents", "Fahiem Bacchus and George Katsirelos", "Constraint Processing", "Combinatorial auctions: A survey", "Reverse auctions: Bad idea", "Supermodels and Robustness", "Withdrawable bids as winner's curse insurance", "Robust solutions for constraint satisfaction and optimization", "Super solutions in constraint programming", "Combinatorial and quantity-discount procurement auctions benefit Mars Incorporated and its suppliers", "Super solutions for combinatorial auctions", "Weighted super solutions for constraint programs", "Business insurance", "On the sensitivity of incremental algorithms for combinatorial auctions", "Towards a universal test suite for combinatorial auction algorithms", "Associated general contractors of America white paper on reverse auctions for procurement of construction", "A basic guide to surety bonds", "Combination bidding in multi-unit auctions", "Mechanism design with execution uncertainty", "Global constraints and filtering algorithms", "Combinatorial auction design", "Computationally manageable combinatorial auctions", "Contradicting conventional wisdom in constraint satisfaction", "Algorithm for optimal winner determination in combinatorial auctions", "Leveled Commitment Contracts and Strategic Breach", "Leveled commitment contracting: A backtracking instrument for multiagent systems", "Algorithms for optimizing leveled commitment contracts", "Surplus equivalence of leveled commitment contracts", "Combinatorial auctions for supply chain formation", "On reformulation of constraint satisfaction problems", "Access spectrum bid withdrawal"], "title": "Robust Solutions for Combinatorial Auctions"}
{"id": "J-57", "keywords": "coalit game theori;represent;treewidth", "abstract": "We present a new approach to representing coalitional games based on rules that describe the marginal contributions of the agents. This representation scheme captures characteristics of the interactions among the agents in a natural and concise manner. We also develop efficient algorithms for two of the most important solution concepts, the Shapley value and the core, under this representation. The Shapley value can be computed in time linear in the size of the input. The emptiness of the core can be determined in time exponential only in the treewidth of a graphical interpretation of our representation.", "references": ["Treewidth: Algorithmic techniques and results", "Complexity of determining nonemptiness of the core", "Computing Shapley values, manipulating value division schemes, and checking core membership in multi-issue domains", "On the complexity of cooperative solution concepts", "Taming the computational complexity of combinatorial auctions: Optimal and approximate approaches", "Graphical models for game theory", "On the value of private information", "Algoirthms for combinatorial coalition formation and payoff division in an electronic marketplace", "Microeconomic Theory", "Bidding and allocation in combinatorial auctions", "A Course in Game Theory", "The Complexity of Boolean Functions"], "title": "Marginal Contribution Nets: A Compact Representation Scheme for Coalitional Games"}
{"id": "J-58", "keywords": "selfish agent;mechan design;price;demand game", "abstract": "The family of Vickrey-Clarke-Groves (VCG) mechanisms is arguably the most celebrated achievement in truthful mechanism design. However, VCG mechanisms have their limitations. They only apply to optimization problems with a utilitarian (or affine) objective function, and their output should optimize the objective function. For many optimization problems, finding the optimal output is computationally intractable. If we apply VCG mechanisms to polynomial-time algorithms that approximate the optimal solution, the resulting mechanisms may no longer be truthful.In light of these limitations, it is useful to study whether we can design a truthful non-VCG payment scheme that is computationally tractable for a given allocation rule O. In this paper, we focus our attention on emphbinary demand games in which the agents' only available actions are to take part in the a game or not to. For these problems, we prove that a truthful mechanism M=(O, P) exists with a proper payment method P iff the allocation rule O satisfies a certain monotonicity property. We provide a general framework to design such P. We further propose several general composition-based techniques to compute P efficiently for various types of output. In particular, we show how P can be computed through "or/and" combinations, round-based combinations, and some more complex combinations of the outputs from subgames.", "references": ["An approximate truthful mechanism for combinatorial auctions with single parameter agents", "Truthful mechanisms for one-parameter agents", "Deterministic truthful approximation schemes for scheduling related machines", "A greedy heuristic for the set covering problem", "Multipart pricing of public goods", "On Dominant Strategy Mechanisms", "Strategyproof cost-sharing mechanisms for set cover and facility location games", "Approximation and collusion in multicast cost sharing", "A BGP-based mechanism for lowest-cost routing", "Characterization of satisfactory mechanisms for the revelation of preferences for public goods", "Incentives in teams", "Truth revelation in approximately efficient combinatorial auctions", "Truthful approximation mechanisms for restricted combinatorial auctions: extended abstract", "Algorithmic mechanism design", "Improved approximation algorithms for the vertex cover problem in graphs and hypergraphs", "A local ratio theorem for approximating the weighted vertex cover problem", "Improved steiner tree approximation in graphs", "An 11/6-approximation algorithm for the network Steiner problem", "Efficient bounds for the stable set, vertex cover, and set packing problems", "An approximate solution for the steiner problem in graphs", "Counterspeculation, auctions and competitive sealed tenders", "Truthful low-cost unicast in selfish wireless networks", "Design multicast protocols for non-cooperative networks", "Truthful multicast in selfish wireless networks"], "title": "Towards Truthful Mechanisms for Binary Demand Games: A General Framework"}
{"id": "J-59", "keywords": "queue problem;shaplei valu;cost share;job schedul", "abstract": "A set of jobs need to be served by a single server which can serve only one job at a time. Jobs have processing times and incur waiting costs (linear in their waiting time). The jobs share their costs through compensation using monetary transfers. We characterize the Shapley value rule for this model using fairness axioms. Our axioms include a bound on the cost share of jobs in a group, efficiency, and some independence properties on the the cost share of a job.", "references": ["A Note on Maniquet's Characterization of the Shapley Value in Queueing Problems", "No-envy in Queuing Problems", "Sequencing Games: A Survey", "Sequencing Games", "Sequencing and Cooperation", "Strategyproof Cost-sharing Mechanisms for Set Cover and Facility Location Games", "Incentive Mechanisms for Priority Queueing Problems", "Sharing the Cost of Multicast Transmissions", "The Split Core for Sequencing Games", "Contributions to Theory of Games IV, chapter A Bargaining Model for Cooperative n-person Games", "Applications of Approximate Algorithms to Cooperative Games", "Equitable Cost Allocations via Primal-Dual Type Algorithms", "Sequencing Games without a Completely Specified Initial Order", "Sequencing Games without Initial Order", "A Characterization of the Shapley Value in Queueing Problems", "Cost sharing in a job scheduling problem", "Essays on First Best Implementable Incentive Problems", "Mechanism design in queueing problems", "Achieving the first best in sequencing problems", "Handbook of Social Choice and Welfare, chapter Axiomatic Cost and Surplus Sharing", "On Scheduling Fees to Prevent Merging, Splitting and Transferring of Jobs", "Split-proof Probabilistic Scheduling", "Characterization of Additive Cost Sharing Methods", "Group Strategyproof Mechanisms via Primal-Dual Algorithms", "Contributions to the Theory of Games II, chapter A Value for n-person Games", "Various Optimizers for Single-Stage Production", "On incentive compatibility and budget balancedness in public decision making"], "title": "Cost Sharing in a Job Scheduling Problem Using the Shapley Value"}
{"id": "J-60", "keywords": "distribut algorithm mechan design;peer-to-peer", "abstract": "Algorithmic Mechanism Design focuses on Dominant Strategy Implementations. The main positive results are the celebrated Vickrey-Clarke-Groves (VCG) mechanisms and computationally efficient mechanisms for severely restricted players ('single-parameter domains'). As it turns out, many natural social goals cannot be implemented using the dominant strategy concept [35, 32, 22, 20]. This suggests that the standard requirements must be relaxed in order to construct general-purpose mechanisms.We observe that in many common distributed environments computational entities can take advantage of the network structure to collect and distribute information. We thus suggest a notion of partially informed environments. Even if the information is recorded with some probability, this enables us to implement a wider range of social goals, using the concept of iterative elimination of weakly dominated strategies. As a result, cooperation is achieved independent of agents' belief. As a case study, we apply our methods to derive Peer-to-Peer network mechanism for file sharing.", "references": ["Truthful mechanisms for one-parameter agents", "An approximate truthful mechanism for combinatorial auctions with single parameter agent", "Single-parameter domains and implementation in undominated strategies", "Incentive compatible multi-unit combinatorial auctions", "Incentive compatibility in multi-unit auctions", "Auctions with severely bounded communication", "Auctions with severely bounded communication", "Approximation techniques for utilitarian mechanism design", "A game-theoretic framework for incentives in p2p systems", "Multipart pricing of public goods", "Sharing the cost of multicast transmissions", "Distributed algorithmic mechanism design: Recent results and future directions", "Robust incentive techniques for peer-to-peer networks", "Competitive auctions", "Incentives for sharing in peer-to-peer networks", "Bundling equilibrium in combinatorial auctions", "Characterization of ex post equilibrium in the vcg combinatorial auctions", "A crash course in implementation theory", "Approximately-strategyproof and tractable multi-unit auctions", "Towards a characterization of truthful combinatorial auctions", "Online ascending auctions for gradually expiring goods", "Truth revelation in approximately efficient combinatorial auctions", "Microeconomic Theory", "Nash equilibrium and welfare optimality", "Implementation theory", "Randomized truthful auctions of digital goods are randomizations over truthful auctions", "Implementation, contract and renegotiation in environments with complete information", "Subgame perfect implementation", "Strategyproof sharing of submodular costs: Budget balance versus efficiency", "Algorithms for selfish agents", "Computationally feasable vcg mechanisms", "Algorithmic mechanism design", "A Course in Game Theory", "Algorithms, games, and the internet", "The characterization of implementable choice rules", "Dominant strategy implementation with quasi-linear preferences", "On dominant strategy mechanisms", "Rationality and emotions in ultimatum bargaining"], "title": "On Decentralized Incentive Compatible Mechanisms for Partially Informed Environments"}
{"id": "I-61", "keywords": "combinatori exchang;threshold payment;vcg;prefer elicit", "abstract": "We present the first design for a fully expressive iterative combinatorial exchange (ICE). The exchange incorporates a tree-based bidding language that is concise and expressive for CEs. Bidders specify lower and upper bounds on their value for different trades. These bounds allow price discovery and useful preference elicitation in early rounds, and allow termination with an efficient trade despite partial information on bidder valuations. All computation in the exchange is carefully optimized to exploit the structure of the bid-trees and to avoid enumerating trades. A proxied interpretation of a revealed-preference activity rule ensures progress across rounds. A VCG-based payment scheme that has been shown to mitigate opportunities for bargaining and strategic behavior is used to determine final payments. The exchange is fully implemented and in a validation phase.", "references": ["The clock-proxy auction: A practical combinatorial auction design", "Mechanisms for a spatially distributed market", "Auctions for the safe, efficient, and equitable allocation of airspace system resources", "Introduction to Linear Optimization", "The package assignment model", "Solving concisely expressed combinatorial auction problems", "Bidding languages for combinatorial auctions", "Preference elicitation in combinatorial auctions", "editors", "On ascending Vickrey auctions for heterogeneous objects", "Combinatorial auctions: A survey", "Testing linear pricing algorithms for use in ascending combinatorial auctions", "SHARP: an architecture for secure resource peering", "Effectiveness of query types and policies for preference elicitation in combinatorial auctions", "Auction Theory", "Calculation and analysis of Nash equilibria of Vickrey-based payment rules for combinatorial exchanges", "A new and improved design for multi-object iterative auctions", "A proposal for a rapid transition to market allocation of spectrum", "Applying learning algorithms to preference elicitation", "A dominant strategy double auction", "Putting auction theory to work: The simultaneous ascending auction", "Efficient mechanisms for bilateral trading", "Bidding and allocation in combinatorial auctions", "Achieving budget-balance with Vickrey-based payment schemes in exchanges", "Iterative combinatorial auctions: Theory and practice", "A combinatorial mechanism for airport time slot allocation", "Computationally manageable combinatorial auctions", "Preference elicitation in combinatorial auctions", "Constructing and clearing combinatorial exchanges using preference elicitation", "A kBA: A progressive, anonymous-price combinatorial auction"], "title": "ICE: An Iterative Combinatorial Exchange"}
{"id": "J-62", "keywords": "domin strategi;mechan design;strategyproof;truth;weak monoton", "abstract": "Weak monotonicity is a simple necessary condition for a social choice function to be implementable by a truthful mechanism. Roberts [10] showed that it is sufficient for all social choice functions whose domain is unrestricted. Lavi, Mu'alem and Nisan [6] proved the sufficiency of weak monotonicity for functions over order-based domains and Gui, Muller and Vohra [5] proved sufficiency for order-based domains with range constraints and for domains defined by other special types of linear inequality constraints. Here we show the more general result, conjectured by Lavi, Mu'alem and Nisan [6], that weak monotonicity is sufficient for functions defined on any convex domain.", "references": ["Truthful mechanisms for one-parameter agents", "Incentive compatible multi unit combinatorial auctions", "Incentive compatibility in multi-unit auctions", "Competitive Auctions", "Dominant strategy mechanisms with multidimensional types", "Towards a characterization of truthful combinatorial auctions", "Truth revelation in approximately efficient combinatorial auctions", "Microeconomic Theory", "Algorithms for selfish agents", "The characterization of implementable choice rules", "A necessary and sufficient condition for rationalizability in a quasi-linear context", "Dominant strategy implementation with quasi-linear preferences"], "title": "Weak Monotonicity Suffices for Truthfulness on Convex Domains"}
{"id": "J-63", "keywords": "mechan design;effici market;negoti-rang mechan", "abstract": "This paper introduces a new class of mechanisms based on negotiation between market participants. This model allows us to circumvent Myerson and Satterthwaite's impossibility result and present a bilateral market mechanism that is efficient, individually rational, incentive compatible, and budget balanced in the single-unit heterogeneous setting. The underlying scheme makes this combination of desirable qualities possible by reporting a price range for each buyer-seller pair that defines a zone of possible agreements, while the final price is left open for negotiation.", "references": ["Incentive Compatible Multi-Unit Combinatorial Auctions", "Multipart Pricing of Public Goods", "Sharing the Cost of Multicast Transmissions", "Competitive Generalized Auctions", "Optimal Solutions for Multi-Unit Combinatorial Auctions: Branch and Bound Heuristics", "Linear Programming helps solving Large Multi-unit Combinatorial Auctions", "Incentives in teams", "Towards a Characterization of Truthful Combinatorial Auctions", "Truth revelation in rapid, approximately efficient combinatorial auctions", "Efficient Mechanisms for Bilateral Trading", "Algorithmic Mechanism Design", "Achieving Budget-Balance with Vickrey-Based Payment Schemes in Exchanges", "Counterspeculation, Auctions and Competitive Sealed Tenders"], "title": "Negotiation-Range Mechanisms: Exploring the Limits of Truthful Efficient Markets"}
{"id": "J-65", "keywords": "privaci;electron commerc;immedi gratif;hyperbol discount;self-control problem", "abstract": "Dichotomies between privacy attitudes and behavior have been noted in the literature but not yet fully explained. We apply lessons from the research on behavioral economics to understand the individual decision making process with respect to privacy in electronic commerce. We show that it is unrealistic to expectindividual rationality in this context. Models of self-control problems and immediate gratification offer more realistic descriptions of the decision process and are more consistent with currently available data. In particular, we show why individuals who may genuinely want to protect their privacy might not do so because of psychological distortions well documented in the behavioral literature; we show that these distortions may affect not only 'naive' individuals but also 'sophisticated' ones; and we prove that this may occur also when individuals perceive the risks from not protecting their privacy as significant.", "references": ["On the economics of anonymity", "Losses, gains, and hyperbolic discounting: An experimental approach to information security attitudes and behavior", "Conditioning prices on purchase history", "The market for 'lemons:' quality uncertainty and the market mechanism", "A theory of rational addiction", "Understanding the privacy space", "Optimal design of privacy policies", "Untraceable electronic mail, return addresses, and digital pseudonyms", "Blind signatures for untraceable payments", "Personalization versus privacy: An empirical examination of the online consumer's dilemma", "Privacy online: Fair information practices in the electronic marketplace", "Professional attitudes and actual behavior", "Towards measuring anonymity", "eMarketer: The great online privacy debate", "Identity theft heads the ftc's top 10 consumer fraud complaints of 2001", "Privacy, consumers, and costs - How the lack of privacy costs consumers and why business studies of privacy costs are biased and incomplete", "Online information privacy: Measuring the cost-benefit trade-off", "First major post-9.11 privacy survey finds consumers demanding companies do more to protect privacy; public wants company privacy policies to be independently verified", "Smoking today and stopping tomorrow: A limited foresight perspective", "Seventy percent of US consumers worry about online privacy, but few take protective action", "Causes of underinsurance against natural disasters", "Essays on hyperbolic discounting", "Attitudes versus actions", "Projection bias in predicting future utility", "Privacy, economics, and price discrimination on the Internet", "Choice and procrastination", "An economic theory of privacy", "The economics of privacy", "Nowhere to turn: Victims speak out on identity theft", "The economics of immediate gratification", "Towards an information theoretic metric for anonymity", "Paying for privacy: Consumers and infrastructures", "Models of bounded rationality", "What does it mean to know a cumulative risk? Adolescents' perceptions of short-term and long-term consequences of smoking", "E-privacy in 2nd generation e-commerce: Privacy preferences versus actual behavior", "An introduction to privacy in economics and politics", "The paradoxical value of privacy", "Private demands and demands for privacy: Dynamic pricing and the market for customer information", "Why we can't be bothered to read privacy policies: Models of privacy economics as a lemons market", "The right to privacy", "Optimistic biases about personal risks", "Why Johnny can't encrypt: A usability evaluation of PGP 5.0"], "title": "Privacy in Electronic Commerce and the Economics of Immediate Gratification"}
{"id": "J-66", "keywords": "express negoti;donat to chariti;market clear;mechan design", "abstract": "When donating money to a (say, charitable) cause, it is possible touse the contemplated donation as negotiating material to induce other parties interested in the charity to donate more. Such negotiation is usually done in terms of matching offers, where one party promises to pay a certain amount if others pay a certain amount. However, in their current form, matching offers allow for only limited negotiation. For one, it is not immediately clear how multiple parties can make matching offers at the same time without creating circular dependencies. Also, it is not immediately clear how to make adonation conditional on other donations to multiple charities, when the donator has different levels of appreciation for the different charities. In both these cases, the limited expressiveness of matching offers causes economic loss: it may happen that an arrangement that would have made all parties (donators as well as charities) better off cannot be expressed in terms of matching offers and will therefore notoccur.In this paper, we introduce a bidding language for expressing very general types of matching offers over multiple charities. We formulate the corresponding clearing problem (deciding how much each bidder pays, and how much each charity receives), and show that it is NP-complete to approximate to any ratio even in very restricted settings. We givea mixed-integer program formulation of the clearing problem, and show that for concave bids, the program reduces to a linear program. We then show that the clearing problem for a subclass of concave bids is at least as hard as the decision variant of linear programming. Subsequently, we show that the clearing problem is much easier when bids are quasilinear---for surplus, the problem decomposes across charities, and for payment maximization, a greedy approach isoptimal if the bids are concave (although this latter problem is weakly NP-complete when the bids are not concave). For the quasilinear setting, we study the mechanism design question. We show that anex-post efficient mechanism is impossible even with only one charity and a very restricted class of bids. We also show that there may bebenefits to linking the charities from a mechanism design stand point.", "references": ["The property rights doctrine and demand revelation under incomplete information", "Ascending auctions with package bidding", "Incentive compatible multi-unit combinatorial auctions", "Multipart pricing of public goods", "Complexity of mechanism design", "Incentives and incomplete information", "Some simplified NP-complete graph problems", "Red cross statement on official donation locations", "Optimal solutions for multi-unit combinatorial auctions:Branch and bound heuristics", "Incentives in teams", "A polynomial algorithm in linear programming", "Towards a characterization of truthful combinatorial auctions", "Truth revelation in rapid,approximately efficient combinatorial auctions", "Microeconomic Theory", "Efficient mechanisms for bilateral trading", "Integer and Combinatorial Optimization", "Bidding and allocation in combinatorial auctions", "Computationally feasible VCG mechanisms", "iBundle: An efficient ascending price bundle auction", "Computationally manageable combinatorial auctions", "Algorithm for optimal winner determination in combinatorial auctions", "CABOB: A fast optimal algorithm for combinatorial auctions", "Global AIDS Funds Is Given Attention, but Not Money", "Counterspeculation, auctions, and competitive sealed tenders", "AkBA: A progressive,anonymous-price combinatorial auction", "The characterization of strategy/false-name proof combinatorial auction protocols:Price-oriented, rationing-free protocol"], "title": "Expressive Negotiation over Donations to Charities"}
{"id": "J-67", "keywords": "mechan design;game theori;onlin algorithm;schedul", "abstract": "For the problem of online real-time scheduling of jobs on a single processor, previous work presents matching upper and lower bounds on the competitive ratio that can be achieved by a deterministic algorithm. However, these results only apply to the non-strategic setting in which the jobs are released directly to the algorithm. Motivated by emerging areas such as grid computing, we instead consider this problem in an economic setting, in which each job is released to a separate, self-interested agent. The agent can then delay releasing the job to the algorithm, inflate its length, and declare an arbitrary value and deadline for the job, while the center determines not only the schedule, but the payment of each agent. For the resulting mechanism design problem (in which we also slightly strengthen an assumption from the non-strategic setting), we present a mechanism that addresses each incentive issue, while only increasing the competitive ratio by one. We then show a matching lower bound for deterministic mechanisms that never pay the agents.", "references": ["Approximation and collusion in multicast cost sharing", "Reducing truth-telling online mechanisms to online optimization", "Incentive-compatible online auctions for digital goods", "On the competitiveness of on-line real-time task scheduling", "Online algorithms for market clearing", "Online computation and competitive analysis", "Economic models for resource management and scheduling in grid computing", "The popcorn project: Distributed computation over the internet in java", "Distributed algorithmic mechanism design: Recent results and future directions", "Online algorithms: The state of the art", "Pricing wifi at starbucks- issues in online mechanism design", "Bounds for certain multiprocessor anomalies", "Speed is as powerful as clairvoyance", "On-line scheduling with tight deadlines", "D-over: An optimal on-line scheduling algorithm for overloaded real-time systems", "Competitive analysis of online auctions", "Microeconomic theory", "Algorithmic mechanism design", "Algorithms, games, and the internet"], "title": "Mechanism Design for Online Real-Time Scheduling"}
{"id": "J-69", "keywords": "incent;peer-to-peer;free-ride;reput;collus;cheap pseudonym;whitewash;prison dilemma", "abstract": "Lack of cooperation (free riding) is one of the key problems that confronts today's P2P systems. What makes this problem particularly difficult is the unique set of challenges that P2P systems pose: large populations, high turnover, a symmetry of interest, collusion, zero-cost identities, and traitors. To tackle these challenges we model the P2P system using the Generalized Prisoner's Dilemma (GPD),and propose the Reciprocative decision function as the basis of a family of incentives techniques. These techniques are fullydistributed and include: discriminating server selection, maxflow-based subjective reputation, and adaptive stranger policies. Through simulation, we show that these techniques can drive a system of strategic users to nearly optimal levels of cooperation.", "references": ["Free Riding on Gnutella", "The Evolution of Cooperation", "A Game-Theoretic Framework for Incentives in P2P Systems", "Security for Structured Peer-to-Peer Overlay Networks", "Incentives build robustness in bittorrent", "Modeling Incentives for Collaboration in Mobile Ad Hoc Networks", "The Sybil Attack", "A BGP-based Mechanism for Lowest-Cost Routing", "Sharing the Cost of Multicast Transmissions", "Distributed Algorithmic Mechanism Design: Recent Results and Future Directions", "The Social Cost of Cheap Pseudonyms", "The Theory of Learning in Games", "Incentives For Sharing in Peer-to-Peer Networks", "Balances of Power on EBay: Peers or Unquals?", "Are Contributions to P2P Technical Forums Private or Public Goods? -- An Empirical Investigation", "The Tragedy of the Commons", "Evolutionary Games and Population Dynamics", "The EigenTrust Algorithm for Reputation Management in P2P Networks", "Peer-to-Peer: Harnessing the Power of Disruptive Technologies", "Prisoner's Dilemma", "Cooperative Peer Groups in Nice", "Attack-Resistant Trust Metrics for Public Key Certification", "Preserving Peer Replicas by Rate-Limited Sampled Voting", "Mitigating Routing Misbehavior in Mobile Ad Hoc Networks", "A Game Theoretical Approach to Evaluate Cooperation Enforcement Mechanisms in Mobile Ad Hoc Networks", "Evolution of Indirect Reciprocity by Image Scoring", "The Logic of Collective Action: Public Goods and the Theory of Groups", "Priority Forwarding in Ad Hoc Networks with Self-Ineterested Parties", "To Share or Not to Share: An Analysis of Incentives to Contribute in Collaborative File Sharing Environments", "Authentication Metric Analysis and Design", "A Measurement Study of Peer-to-Peer File Sharing Systems", "Evolution and the Theory of Games", "Modeling Cooperation in Mobile Ad Hoc Networks: a Formal Description of Selfishness", "KARMA: A Secure Economic Framework for P2P Resource Sharing", "To Play or to Control: A Game-based Control-Theoretic Approach to Peer-to-Peer Incentive Engineering", "Strategyproof mechanisms for ad hoc network formation"], "title": "Robust Incentive Techniques for Peer-to-Peer Networks"}
{"id": "J-70", "keywords": "autom mechan design;combinatori auction;revenu maxim", "abstract": "Often, an outcome must be chosen on the basis of the preferences reported by a group of agents. The key difficulty is that the agents may report their preferences insincerely to make the chosen outcome more favorable to themselves. Mechanism design is the art of designing the rules of the game so that the agents are motivated to report their preferences truthfully, and a desirable outcome is chosen. In a recently proposed approach---called automated mechanism design ---a mechanism is computed for the preference aggregation setting at hand. This has several advantages, but the downside is that the mechanism design optimization problem needs to be solved anew each time. Unlike the earlier work on automated mechanism design that studied a benevolent designer, in this paper we study automated mechanism design problems where the designer is self-interested. In this case, the center cares only about which outcome is chosen and what payments are made to it. The reason that the agents' preferences are relevant is that the center is constrained to making each agent at least as well off as the agent would have been had it not participated in the mechanism. In this setting, we show that designing optimal deterministic mechanisms is NP-complete in two important special cases: when the center is interested only in the payments made to it, and when payments are not possible and the center is interested only in the outcome chosen. We then show how allowing for randomization in the mechanism makes problems in this setting computationally easy. Finally, we show that the payment-maximizing AMD problem is closely related to an interesting variant of the optimal (revenue-maximizing) combinatorial auction design problem, where the bidders have "best-only" preferences. We show that here, too, designing an optimal deterministic auction is NP-complete, but designing an optimal randomized auction is easy.", "references": ["Optimal multi-object auctions", "The property rights doctrine and demand revelation under incomplete information", "Bundling and optimal auctions of multiple products", "Multipart pricing of public goods", "Complexity of mechanism design", "Automated mechanism design: Complexity results stemming from the single-agent setting", "Computational criticisms of the revelation principle", "Incentives and incomplete information", "Sharing the cost of muliticast transmissions", "Manipulation of voting schemes", "Incentives in teams", "Vickrey prices and shortest paths: What is an edge worth?", "A polynomial algorithm in linear programming", "The minimum satisfiability problem", "Truth revelation in rapid, approximately efficient combinatorial auctions", "Microeconomic Theory", "Optimal multi-unit auctions", "Optimal auction design", "Computationally feasible VCG mechanisms", "Algorithmic mechanism design", "Designing networks for selfish users is hard", "Issues in computational Vickrey auctions", "Strategy-proofness and Arrow's conditions: existence and correspondence theorems for voting procedures and social welfare functions", "Counterspeculation, auctions, and competitive sealed tenders", "Research problems in combinatorial auctions"], "title": "Self-interested Automated Mechanism Design and Implications for Optimal Combinatorial Auctions"}
{"id": "J-71", "keywords": "dynam pari-mutuel market;continu doubl auction;autom market maker;compound secur market;combinatori bet;risk alloc;inform aggreg;trade;hedg;specul;bet;wager;gambl", "abstract": "I develop a new mechanism for risk allocation and information speculation called a dynamic pari-mutuel market (DPM). ADPM acts as hybrid between a pari-mutuel market and a continuous double auction (CDA), inheriting some of the advantages of both. Like a pari-mutuel market, a DPM offers infinite buy-in liquidity and zero risk for the market institution; like a CDA, a DPM cancontinuously react to new information, dynamically incorporate information into prices, and allow traders to lock in gains or limit losses by selling prior to event resolution. The trader interface can be designed to mimic the familiar double auction format with bid-ask queues, though with an addition variable called the payoff per share. The DPM price function can be viewed as an automated market maker always offering to sell at some price, and moving the price appropriately according to demand. Since the mechanism is pari-mutuel (i.e., redistributive), it is guaranteed to pay out exactly the amount of money taken in. Iexplore a number of variations on the basic DPM, analyzing the properties of each, and solving in closed form for their respective price functions.", "references": ["Probability and utility estimates for racetrack bettors", "Inducing liquidity in thin financial markets through combined-value trading mechanisms", "Forecasting uncertain events with small groups", "Information incorporation in online in-game sports betting markets", "Information aggregation in an experimental market", "Anatomy of an experimental political stock market", "Wishes, expectations, and actions: A survey on price formation in election stock markets", "Betting boolean-style: A framework for trading in securities based on logical formulas", "Informed traders and price variations in the betting market for professional basketball games", "Decision markets", "Combinatorial information market design", "Could gambling save science? Encouraging an honest consensus", "Recovering probability distributions from options prices", "Separating probability elicitation from utilities", "The real power of artificial markets", "Extracting collective probabilistic forecasts from web games", "Parimutuel betting markets as information aggregation devices: Experimental results", "Markets as information gathering tools", "Rational expectations and the aggregation of diverse information in laboratory security markets", "Policy analysis market: An electronic commerce application of a combinatorial information market", "Orange juice and weather", "Gambling and rationality", "How accurate do markets predict the outcome of an event? The Euro 2000 soccer championships experiment", "Horse racing: Testing the efficient markets model", "Anomalies: Parimutuel betting markets: Racetracks and lotteries", "Utility analysis and group behavior: An empirical study", "Good probability assessors"], "title": "A Dynamic Pari-Mutuel Market for Hedging, Wagering, and Information Aggregation"}
{"id": "J-72", "keywords": "combinatori auction;prefer elicit;learn", "abstract": "We consider the parallels between the preference elicitation problem in combinatorial auctions and the problem of learning an unknown function from learning theory. We show that learning algorithms can be used as a basis for preference elicitation algorithms. The resulting elicitation algorithms perform a polynomial number of queries. We also give conditions under which the resulting algorithms have polynomial communication. Our conversion procedure allows us to generate combinatorial auction protocols from learning algorithms for polynomials, monotone DNF, and linear-threshold functions. In particular, we obtain an algorithm that elicits XOR bids with polynomial communication.", "references": ["Integer programming for combinatorial auction winner determination", "Learning regular sets from queries and counterexamples", "Queries and concept learning", "The Package Assignment Model", "Preference elicitation and query learning", "Partial-revelation VCG mechanism for combinatorial auctions", "Taming the computational complexity of combinatorial auctions: Optimal and approximate approaches", "Using value queries in combinatorial auctions", "An Introduction to Computational Learning Theory", "Learning quickly when irrelevant attributes abound: A new linear-threshold algorithm", "Bidding and allocation in combinatorial auctions", "The communication requirements of efficient allocations and supporting Lindahl prices", "Price-based information certificates for minimal-revelation combinatorial auctions", "Auction design with costly preference elicitation", "Iterative combinatorial auctions: Theory and practice", "CABOB: A fast optimal algorithm for combinatorial auctions", "Learning sparse multivariate polynomials over a field with queries and counterexamples", "A theory of the learnable", "On polynomial-time preference elicitation with value-queries"], "title": "Applying Learning Algorithms to Preference Elicitation"}
{"id": "J-73", "keywords": "onlin trade;competit analysi;vwap", "abstract": "We introduce new online models for two important aspectsof modern financial markets: Volume Weighted Average Pricetrading and limit order books. We provide an extensivestudy of competitive algorithms in these models and relatethem to earlier online algorithms for stock trading.", "references": ["Competitive non-preemptive call control", "Online Computation and Competitive Analysis", "Optimal search and one-way trading online algorithms", "The Penn-Lehman automated trading project"], "title": "Competitive Algorithms for VWAP and Limit Order Trading"}
{"id": "J-74", "keywords": "game theori;seal-bid auction;cheat", "abstract": "Motivated by the rise of online auctions and their relative lack of security, this paper analyzes two forms of cheating in sealed-bid auctions. The first type of cheating we consider occurs when the seller examines the bids of a second-price auction before the auction clears and then submits a shill bid in order to increase the payment of the winning bidder. In the second type, a bidder cheats in a first-price auction by examining the competing bids before submitting his own bid. In both cases, we derive equilibrium strategies when bidders are aware of the possibility of cheating. These results provide insights into sealed-bid auctions even in the absence of cheating, including some counterintuitive results on the effects of overbidding in a first-price auction.", "references": ["The Design and Implementation of a Secure Auction Service", "Game Theory", "Collusive bidder behavior at single-object second-price and english auctions", "Electronic auctions with private bids", "Equilibrium bid functions for auctions with an uncertain number of bidders", "Auction theory: A guide to the literature", "Bidding clubs in first-price auctions", "Bidding rings", "Privacy preserving auctions and mechanism design", "Optimal auctions", "Two models of bid-taker cheating in vickrey auctions", "Counterspeculations, auctions, and competitive sealed tenders"], "title": "On Cheating in Sealed-Bid Auctions"}