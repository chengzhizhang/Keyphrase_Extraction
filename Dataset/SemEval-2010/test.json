{"id": "J-28", "keywords": ["approxim algorithm", "multi-unit auction", "strategyproof"], "title": "Approximately-Strategyproof and Tractable Multi-Unit Auctions", "abstract": "We present an approximately-efficient and approximately-strategyproof auction mechanism for a single-good multiunit allocation problem. The bidding language allows marginal-decreasing piecewise-constant curves and quantity-based side constraints. We develop a fully polynomial-time approximation scheme for the multiunit allocation problem, which computes a (1+\u03b5) approximation in worst-case time T=O(n3/\u03b5), given n bids each with a constant number of pieces. We integrate this approximation scheme within a Vickrey\u2013Clarke\u2013Groves (VCG) mechanism and compute payments for an asymptotic cost of O(T log n). The maximal possible gain from manipulation to a bidder in the combined scheme is bounded by \u03b5V/(1+\u03b5), where V is the total surplus in the efficient outcome.", "references": ["Ascending auctions with package bidding", "Linear programming and Vickrey auctions", "The package assignment model", "uelson. Bargaining under incomplete information", "Multipart pricing of public goods", "Combinatorial auctions: A survey", "Bid evaluation in procurement auctions with piece-wise linear supply curves", "Distributed Algorithmic Mechanism Design: Recent Results and Future Directions", "Computers and Intractability: A Guide to the Theory of NP-Completeness", "Computational complexity of approximation algorithms for combinatorial problems", "Incentives in teams", "Computational aspects of clearing continuous call double auctions with assignment constraints and indivisible demand", "Auction Theory", "Efficient mechanism design", "Truth revelation in approximately efficient combinatorial auctions", "Optimal auction design", "Efficient mechanisms for bilateral trading", "Computationally feasible VCG mechanisms", "Achieving budget-balance with Vickrey-based payment schemes in exchanges", "Computationally manageable combinatorial auctions", "Almost dominant strategy implementation", "Counterspeculation, auctions, and competitive sealed tenders"], "full_text": "1. INTRODUCTION In this paper we present a fully polynomial-time approximation scheme for the single-good multi-unit auction problem. Our scheme is both approximately efficient and approximately strategyproof. The auction settings considered in our paper are motivated by recent trends in electronic commerce; for instance, corporations are increasingly using auctions for their strategic sourcing. We consider both a reverse auction variation and a forward auction variation, and propose a compact and expressive bidding language that allows marginal-decreasing piecewise constant curves. In the reverse auction, we consider a single buyer with a demand for M units of a good and n suppliers, each with a marginal-decreasing piecewise-constant cost function. In addition, each supplier can also express an upper bound, or capacity constraint on the number of units she can supply. The reverse variation models, for example, a procurement auction to obtain raw materials or other services (e.g. circuit boards, power suppliers, toner cartridges), with flexible-sized lots. In the forward auction, we consider a single seller with M units of a good and n buyers, each with a marginal-decreasing piecewise-constant valuation function. A buyer can also express a lower bound, or minimum lot size, on the number of units she demands. The forward variation models, for example, an auction to sell excess inventory in flexible-sized lots. We consider the computational complexity of implementing the Vickrey-Clarke-Groves mechanism for the multiunit auction problem. The Vickrey-Clarke-Groves (VCG) mechanism has a number of interesting economic properties in this setting, including strategyproofness, such that truthful bidding is a dominant strategy for buyers in the forward auction and sellers in the reverse auction, and allocative efficiency, such that the outcome maximizes the total surplus in the system. However, as we discuss in Section 2, the application of the VCG-based approach is limited in the reverse direction to instances in which the total payments to the sellers are less than the value of the outcome to the buyer. Otherwise, either the auction must run at a loss in these instances, or the buyer cannot be expected to voluntarily choose to participate. This is an example of the budget-deficit problem that often occurs in efficient mechanism design . The computational problem is interesting, because even with marginal-decreasing bid curves, the underlying allocation problem turns out to (weakly) intractable. For instance, the classic 0/1 knapsack is a special case of this problem.1 We model the However, the problem can be solved easily by a greedy scheme if we remove all capacity constraints from the seller and all 166 allocation problem as a novel and interesting generalization of the classic knapsack problem, and develop a fully polynomialtime approximation scheme, computing a (1 + )-approximation in worst-case time T = O(n3 /\u03b5), where each bid has a fixed number of piecewise constant pieces. Given this scheme, a straightforward computation of the VCG payments to all n agents requires time O(nT). We compute approximate VCG payments in worst-case time O(\u03b1T log(\u03b1n/\u03b5)), where \u03b1 is a constant that quantifies a reasonable no-monopoly assumption. Specifically, in the reverse auction, suppose that C(I) is the minimal cost for procuring M units with all sellers I, and C(I \\ i) is the minimal cost without seller i. Then, the constant \u03b1 is defined as an upper bound for the ratio C(I \\i)/C(I), over all sellers i. This upper-bound tends to 1 as the number of sellers increases. The approximate VCG mechanism is ( \u03b5 1+\u03b5 )-strategyproof for an approximation to within (1 + ) of the optimal allocation. This means that a bidder can gain at most ( \u03b5 1+\u03b5 )V from a nontruthful bid, where V is the total surplus from the efficient allocation. As such, this is an example of a computationally-tractable \u03b5-dominance result.2 In practice, we can have good confidence that bidders without good information about the bidding strategies of other participants will have little to gain from attempts at manipulation. Section 2 formally defines the forward and reverse auctions, and defines the VCG mechanisms. We also prove our claims about \u03b5-strategyproofness. Section 3 provides the generalized knapsack formulation for the multi-unit allocation problems and introduces the fully polynomial time approximation scheme. Section 4 defines the approximation scheme for the payments in the VCG mechanism. Section 5 concludes. 1.1 Related Work There has been considerable interest in recent years in characterizing polynomial-time or approximable special cases of the general combinatorial allocation problem, in which there are multiple different items. The combinatorial allocation problem (CAP) is both NP-complete and inapproximable (e.g. ). Although some polynomial-time cases have been identified for the CAP , introducing an expressive exclusive-or bidding language quickly breaks these special cases. We identify a non-trivial but approximable allocation problem with an expressive exclusiveor bidding language-the bid taker in our setting is allowed to accept at most one point on the bid curve. The idea of using approximations within mechanisms, while retaining either full-strategyproofness or \u03b5-dominance has received some previous attention. For instance, Lehmann et al. propose a greedy and strategyproof approximation to a single-minded combinatorial auction problem. Nisan & Ronen discussed approximate VCG-based mechanisms, but either appealed to particular maximal-in-range approximations to retain full strategyproofness, or to resource-bounded agents with information or computational limitations on the ability to compute strategies. Feigenminimum-lot size constraints from the buyers. However, this may not be an example of what Feigenbaum & Shenker refer to as a tolerably-manipulable mechanism because we have not tried to bound the effect of such a manipulation on the efficiency of the outcome. VCG mechanism do have a natural self-correcting property, though, because a useful manipulation to an agent is a reported value that improves the total value of the allocation based on the reports of other agents and the agent\"s own value. baum & Shenker have defined the concept of strategically faithful approximations, and proposed the study of approximations as an important direction for algorithmic mechanism design. Schummer and Parkes et al have previously considered \u03b5-dominance, in the context of economic impossibility results, for example in combinatorial exchanges. Eso et al. have studied a similar procurement problem, but for a different volume discount model. This earlier work formulates the problem as a general mixed integer linear program, and gives some empirical results on simulated data. Kalagnanam et al. address double auctions, where multiple buyers and sellers trade a divisible good. The focus of this paper is also different: it investigates the equilibrium prices using the demand and supply curves, whereas our focus is on efficient mechanism design. Ausubel has proposed an ascending-price multi-unit auction for buyers with marginal-decreasing values , with an interpretation as a primal-dual algorithm . 2. APPROXIMATELY-STRATEGYPROOF VCG AUCTIONS In this section, we first describe the marginal-decreasing piecewise bidding language that is used in our forward and reverse auctions. Continuing, we introduce the VCG mechanism for the problem and the \u03b5-dominance results for approximations to VCG outcomes. We also discuss the economic properties of VCG mechanisms in these forward and reverse auction multi-unit settings. 2.1 Marginal-Decreasing Piecewise Bids We provide a piecewise-constant and marginal-decreasing bidding language. This bidding language is expressive for a natural class of valuation and cost functions: fixed unit prices over intervals of quantities. See Figure 1 for an example. In addition, we slightly relax the marginal-decreasing requirement to allow: a bidder in the forward auction to state a minimal purchase amount, such that she has zero value for quantities smaller than that amount; a seller in the reverse auction to state a capacity constraint, such that she has an effectively infinite cost to supply quantities in excess of a particular amount. Reverse Auction Bid 5 10 20 25 10 Quantity Price 5 10 20 25 10 Quantity Price Forward Auction Bid Figure 1: Marginal-decreasing, piecewise constant bids. In the forward auction bid, the bidder offers $10 per unit for quantity in the range [5, 10), $8 per unit in the range [10, 20), and $7 in the range . Her valuation is zero for quantities outside the range . In the reverse auction bid, the cost of the seller is \u221e outside the range . In detail, in a forward auction, a bid from buyer i can be written as a list of (quantity-range, unit-price) tuples, ((u1 i , p1 i ), (u2 i , p2 i ), . . . , (umi\u22121 i , pmi\u22121 i )), with an upper bound umi i on the quantity. The interpretation is that the bidder\"s valuation in the 167 (semi-open) quantity range [uj i , uj+1 i ) is pj i for each unit. Additionally, it is assumed that the valuation is 0 for quantities less than u1 i as well as for quantities more than um i . This is implemented by adding two dummy bid tuples, with zero prices in the range [0, u1 i ) and (umi i , \u221e). We interpret the bid list as defining a price function, pbid,i(q) = qpj i , if uj i \u2264 q < uj+1 i , where j = 1, 2, . . . , mi \u22121. In order to resolve the boundary condition, we assume that the bid price for the upper bound quantity umi i is pbid,i(umi i ) = umi i pmi\u22121 i . A seller\"s bid is similarly defined in the reverse auction. The interpretation is that the bidder\"s cost in the (semi-open) quantity range [uj i , uj+1 i ) is pj i for each unit. Additionally, it is assumed that the cost is \u221e for quantities less than u1 i as well as for quantities more than um i . Equivalently, the unit prices in the ranges [0, u1 i ) and (um i , \u221e) are infinity. We interpret the bid list as defining a price function, pask,i(q) = qpj i , if uj i \u2264 q < uj+1 i . 2.2 VCG-Based Multi-Unit Auctions We construct the tractable and approximately-strategyproof multiunit auctions around a VCG mechanism. We assume that all agents have quasilinear utility functions; that is, ui(q, p) = vi(q)\u2212 p, for a buyer i with valuation vi(q) for q units at price p, and ui(q, p) = p \u2212 ci(q) for a seller i with cost ci(q) at price p. This is a standard assumption in the auction literature, equivalent to assuming risk-neutral agents . We will use the term payoff interchangeably for utility. In the forward auction, there is a seller with M units to sell. We assume that this seller has no intrinsic value for the items. Given a set of bids from I agents, let V (I) denote the maximal revenue to the seller, given that at most one point on the bid curve can be selected from each agent and no more than M units of the item can be sold. Let x\u2217 = (x\u2217 1, . . . , x\u2217 N ) denote the solution to this winner- determination problem, where x\u2217 i is the number of units sold to agent i. Similarly, let V (I \\ i) denote the maximal revenue to the seller without bids from agent i. The VCG mechanism is defined as follows: 1. Receive piecewise-constant bid curves and capacity constraints from all the buyers. 2. Implement the outcome x\u2217 that solves the winner-determination problem with all buyers. 3. Collect payment pvcg,i = pbid,i(x\u2217 i ) \u2212 [V (I) \u2212 V (I \\ i)] from each buyer, and pass the payments to the seller. In this forward auction, the VCG mechanism is strategyproof for buyers, which means that truthful bidding is a dominant strategy, i.e. utility maximizing whatever the bids of other buyers. In addition, the VCG mechanism is allocatively-efficient, and the payments from each buyer are always positive.3 Moreover, each buyer pays less than its value, and receives payoff V (I)\u2212V (I \\ i) in equilibrium; this is precisely the marginal-value that buyer i contributes to the economic efficiency of the system. In the reverse auction, there is a buyer with M units to buy, and n suppliers. We assume that the buyer has value V > 0 to purchase all M units, but zero value otherwise. To simplify the mechanism design problem we assume that the buyer will truthfully announce this value to the mechanism.4 The winner3 In fact, the VCG mechanism maximizes the expected payoff to the seller across all efficient mechanisms, even allowing for Bayesian-Nash implementations . Without this assumption, the Myerson-Satterthwaite impossibility result would already imply that we should not expect an efficient trading mechanism in this setting. determination problem in the reverse auction is to determine the allocation, x\u2217 , that minimizes the cost to the buyer, or forfeits trade if the minimal cost is greater than value, V . Let C(I) denote the minimal cost given bids from all sellers, and let C(I \\i) denote the minimal cost without bids from seller i. We can assume, without loss of generality, that there is an efficient trade and V \u2265 C(I). Otherwise, then the efficient outcome is no trade, and the outcome of the VCG mechanism is no trade and no payments. The VCG mechanism implements the outcome x\u2217 that minimizes cost based on bids from all sellers, and then provides payment pvcg,i = pask,i(x\u2217 i )+[V \u2212C(I)\u2212max(0, V \u2212C(I\\i))] to each seller. The total payment is collected from the buyer. Again, in equilibrium each seller\"s payoff is exactly the marginal-value that the seller contributes to the economic efficiency of the system; in the simple case that V \u2265 C(I \\ i) for all sellers i, this is precisely C(I \\ i) \u2212 C(I). Although the VCG mechanism remains strategyproof for sellers in the reverse direction, its applicability is limited to cases in which the total payments to the sellers are less than the buyer\"s value. Otherwise, there will be instances in which the buyer will not choose to voluntarily participate in the mechanism, based on its own value and its beliefs about the costs of sellers. This leads to a loss in efficiency when the buyer chooses not to participate, because efficient trades are missed. This problem with the size of the payments, does not occur in simple single-item reverse auctions, or even in multi-unit reverse auctions with a buyer that has a constant marginal-valuation for each additional item that she procures.5 Intuitively, the problem occurs in the reverse multi-unit setting because the buyer demands a fixed number of items, and has zero value without them. This leads to the possibility of the trade being contingent on the presence of particular, so-called pivotal sellers. Define a seller i as pivotal, if C(I) \u2264 V but C(I\\i) > V . In words, there would be no efficient trade without the seller. Any time there is a pivotal seller, the VCG payments to that seller allow her to extract all of the surplus, and the payments are too large to sustain with the buyer\"s value unless this is the only winning seller. Concretely, we have this participation problem in the reverse auction when the total payoff to the sellers, in equilibrium, exceeds the total payoff from the efficient allocation: V \u2212 C(I) \u2265 [V \u2212 C(I) \u2212 max(0, V \u2212 C(I \\ i))] As stated above, first notice that we require V > C(I \\ i) for all sellers i. In other words, there must be no pivotal sellers. Given this, it is then necessary and sufficient that: V \u2212 C(I) \u2265 (C(I \\ i) \u2212 C(I)) (1) To make the reverse auction symmetric with the forward direction, we would need a buyer with a constant marginal-value to buy the first M units, and zero value for additional units. The payments to the sellers would never exceed the buyer\"s value in this case. Conversely, to make the forward auction symmetric with the reverse auction, we would need a seller with a constant (and high) marginal-cost to sell anything less than the first M units, and then a low (or zero) marginal cost. The total payments received by the seller can be less than the seller\"s cost for the outcome in this case. 168 In words, the surplus of the efficient allocation must be greater than the total marginal-surplus provided by each seller.6 Consider an example with 3 agents {1, 2, 3}, and V = 150 and C(123) = 50. Condition (1) holds when C(12) = C(23) = 70 and C(13) = 100, but not when C(12) = C(23) = 80 and C(13) = 100. In the first case, the agent payoffs \u03c0 = (\u03c00, \u03c01, \u03c02, \u03c03), where 0 is the seller, is (10, 20, 50, 20). In the second case, the payoffs are \u03c0 = (\u221210, 30, 50, 30). One thing we do know, because the VCG mechanism will maximize the payoff to the buyer across all efficient mechanisms , is that whenever Eq. 1 is not satisfied there can be no efficient auction mechanism.7 2.3 \u03b5-Strategyproofness We now consider the same VCG mechanism, but with an approximation scheme for the underlying allocation problem. We derive an \u03b5-strategyproofness result, that bounds the maximal gain in payoff that an agent can expect to achieve through a unilateral deviation from following a simple truth-revealing strategy. We describe the result for the forward auction direction, but it is quite a general observation. As before, let V (I) denote the value of the optimal solution to the allocation problem with truthful bids from all agents, and V (I \\i) denote the value of the optimal solution computed without bids from agent i. Let \u02c6V (I) and \u02c6V (I \\ i) denote the value of the allocation computed with an approximation scheme, and assume that the approximation satisfies: (1 + ) \u02c6V (I) \u2265 V (I) for some > 0. We provide such an approximation scheme for our setting later in the paper. Let \u02c6x denote the allocation implemented by the approximation scheme. The payoff to agent i, for announcing valuation \u02c6vi, is: vi(\u02c6xi) + j=i \u02c6vj (\u02c6xj) \u2212 \u02c6V (I \\ i) The final term is independent of the agent\"s announced value, and can be ignored in an incentive-analysis. However, agent i can try to improve its payoff through the effect of its announced value on the allocation \u02c6x implemented by the mechanism. In particular, agent i wants the mechanism to select \u02c6x to maximize the sum of its true value, vi(\u02c6xi), and the reported value of the other agents, \u00c8j=i \u02c6vj (\u02c6xj). If the mechanism\"s allocation algorithm is optimal, then all the agent needs to do is truthfully state its value and the mechanism will do the rest. However, faced with an approximate allocation algorithm, the agent can try to improve its payoff by announcing a value that corrects for the approximation, and causes the approximation algorithm to implement the allocation that exactly maximizes the total reported value of the other agents together with its own actual value . This condition is implied by the agents are substitutes requirement , that has received some attention in the combinatorial auction literature because it characterizes the case in which VCG payments can be supported in a competitive equilibrium. Useful characterizations of conditions that satisfy agents are substitutes, in terms of the underlying valuations of agents have proved quite elusive. Moreover, although there is a small literature on maximallyefficient mechanisms subject to requirements of voluntaryparticipation and budget-balance (i.e. with the mechanism neither introducing or removing money), analytic results are only known for simple problems (e.g. ). We can now analyze the best possible gain from manipulation to an agent in our setting. We first assume that the other agents are truthful, and then relax this. In both cases, the maximal benefit to agent i occurs when the initial approximation is worst-case. With truthful reports from other agents, this occurs when the value of choice \u02c6x is V (I)/(1 + \u03b5). Then, an agent could hope to receive an improved payoff of: V (I) \u2212 V (I) 1 + \u03b5 1 + \u03b5 V (I) This is possible if the agent is able to select a reported type to correct the approximation algorithm, and make the algorithm implement the allocation with value V (I). Thus, if other agents are truthful, and with a (1 + \u03b5)-approximation scheme to the allocation problem, then no agent can improve its payoff by more than a factor \u03b5/(1 + \u03b5) of the value of the optimal solution. The analysis is very similar when the other agents are not truthful. In this case, an individual agent can improve its payoff by no more than a factor /(1 + ) of the value of the optimal solution given the values reported by the other agents. Let V in the following theorem define the total value of the efficient allocation, given the reported values of agents j = i, and the true value of agent i. THEOREM 1. A VCG-based mechanism with a (1 + \u03b5)allocation algorithm is (1+ \u2212V ) strategyproof for agent i, and agent i can gain at most this payoff through some non-truthful strategy. Notice that we did not need to bound the error on the allocation problems without each agent, because the -strategyproofness result follows from the accuracy of the first-term in the VCG payment and is independent of the accuracy of the second-term. However, the accuracy of the solution to the problem without each agent is important to implement a good approximation to the revenue properties of the VCG mechanism. 3. THEGENERALIZED KNAPSACK PROBLEM In this section, we design a fully polynomial approximation scheme for the generalized knapsack, which models the winnerdetermination problem for the VCG-based multi-unit auctions. We describe our results for the reverse auction variation, but the formulation is completely symmetric for the forward-auction. In describing our approximation scheme, we begin with a simple property (the Anchor property) of an optimal knapsack solution. We use this property to develop an O(n2 ) time 2-approximation for the generalized knapsack. In turn, we use this basic approximation to develop our fully polynomial-time approximation scheme (FPTAS). One of the major appeals of our piecewise bidding language is its compact representation of the bidder\"s valuation functions. We strive to preserve this, and present an approximation scheme that will depend only on the number of bidders, and not the maximum quantity, M, which can be very large in realistic procurement settings. The FPTAS implements an (1 + \u03b5) approximation to the optimal solution x\u2217 , in worst-case time T = O(n3 /\u03b5), where n is the number of bidders, and where we assume that the piecewise bid for each bidder has O(1) pieces. The dependence on the number of pieces is also polynomial: if each bid has a maximum 169 of c pieces, then the running time can be derived by substituting nc for each occurrence of n. 3.1 Preliminaries Before we begin, let us recall the classic 0/1 knapsack problem: we are given a set of n items, where the item i has value vi and size si, and a knapsack of capacity M; all sizes are integers. The goal is to determine a subset of items of maximum value with total size at most M. Since we want to focus on a reverse auction, the equivalent knapsack problem will be to choose a set of items with minimum value (i.e. cost) whose size exceeds M. The generalized knapsack problem of interest to us can be defined as follows: Generalized Knapsack: Instance: A target M, and a set of n lists, where the ith list has the form Bi = (u1 i , p1 i ), . . . , (umi\u22121 i , pmi\u22121 i ), (umi i (i), \u221e) , where uj i are increasing with j and pj i are decreasing with j, and uj i , pj i , M are positive integers. Problem: Determine a set of integers xj i such that 1. (One per list) At most one xj i is non-zero for any i, 2. (Membership) xj i = 0 implies xj i \u2208 [uj i , uj+1 i ), 3. (Target) \u00c8i \u00c8j xj i \u2265 M, and 4. (Objective) \u00c8i \u00c8j pj i xj i is minimized. This generalized knapsack formulation is a clear generalization of the classic 0/1 knapsack. In the latter, each list consists of a single point (si, vi).8 The connection between the generalized knapsack and our auction problem is transparent. Each list encodes a bid, representing multiple mutually exclusive quantity intervals, and one can choose any quantity in an interval, but at most one interval can be selected. Choosing interval [uj i , uj+1 i ) has cost pj i per unit. The goal is to procure at least M units of the good at minimum possible cost. The problem has some flavor of the continuous knapsack problem. However, there are two major differences that make our problem significantly more difficult: (1) intervals have boundaries, and so to choose interval [uj i , uj+1 i ) requires that at least uj i and at most uj+1 i units must be taken; (2) unlike the classic knapsack, we cannot sort the items (bids) by value/size, since different intervals in one list have different unit costs. 3.2 A 2-Approximation Scheme We begin with a definition. Given an instance of the generalized knapsack, we call each tuple tj i = (uj i , pj i ) an anchor. Recall that these tuples represent the breakpoints in the piecewise constant curve bids. We say that the size of an anchor tj i is uj i , In fact, because of the one per list constraint, the generalized problem is closer in spirit to the multiple choice knapsack problem , where the underling set of items is partitioned into disjoint subsets U1, U2, . . . , Uk, and one can choose at most one item from each subset. PTAS do exist for this problem , and indeed, one can convert our problem into a huge instance of the multiple choice knapsack problem, by creating one group for each list; put a (quantity, price) point tuple (x, p) for each possible quantity for a bidder into his group (subset). However, this conversion explodes the problem size, making it infeasible for all but the most trivial instances. the minimum number of units available at this anchor\"s price pj i . The cost of the anchor tj i is defined to be the minimum total price associated with this tuple, namely, cost(tj i ) = pj i uj i if j < mi, and cost(tmi i ) = pmi\u22121 i umi i . In a feasible solution {x1, x2, . . . , xn} of the generalized knapsack, we say that an element xi = 0 is an anchor if xi = uj i , for some anchor uj i . Otherwise, we say that xi is midrange. We observe that an optimal knapsack solution can always be constructed so that at most one solution element is midrange. If there are two midrange elements x and x , for bids from two different agents, with x \u2264 x , then we can increment x and decrement x, until one of them becomes an anchor. See Figure 2 for an example. LEMMA 1. [Anchor Property] There exists an optimal solution of the generalized knapsack problem with at most one midrange element. All other elements are anchors. 35 Price Quantity (i) Optimal solution with 2 midrange bids (ii) Optimal soltution with Figure 2: (i) An optimal solution with more than one bid not anchored (2,3); (ii) an optimal solution with only one bid (3) not anchored. We use the anchor property to first obtain a polynomial-time 2-approximation scheme. We do this by solving several instances of a restricted generalized-knapsack problem, which we call iKnapsack, where one element is forced to be midrange for a particular interval. Specifically, suppose element x for agent l is forced to lie in its jth range, [uj , uj+1 ), while all other elements, x1, . . . , xl\u22121, xl+1, xn, are required to be anchors, or zero. This corresponds to the restricted problem iKnapsack( , j), in which the goal is to obtain at least M \u2212 uj units with minimum cost. Element x is assumed to have already contributed uj units. The value of a solution to iKnapsack( , j) represents the minimal additional cost to purchase the rest of the units. We create n \u2212 1 groups of potential anchors, where ith group contains all the anchors of the list i in the generalized knapsack. The group for agent l contains a single element that represents the interval [0, uj+1 \u2212uj ), and the associated unit-price pj . This interval represents the excess number of units that can be taken from agent l in iKnapsack( , j), in addition to uj , which has already been committed. In any other group, we can choose at most one anchor. The following pseudo-code describes our algorithm for this restriction of the generalized knapsack problem. U is the union of all the tuples in n groups, including a tuple t for agent l. The size of this special tuple is defined as uj+1 \u2212 uj , and the cost is defined as pj l (uj+1 \u2212uj ). R is the number of units that remain to be acquired. S is the set of tuples accepted in the current tentative 170 solution. Best is the best solution found so far. Variable Skip is only used in the proof of correctness. Algorithm Greedy( , j) 1. Sort all tuples of U in the ascending order of unit price; in case of ties, sort in ascending order of unit quantities. 2. Set mark(i) = 0, for all lists i = 1, 2, . . . , n. Initialize R = M \u2212 uj , S = Best = Skip = \u2205. 3. Scan the tuples in U in the sorted order. Suppose the next tuple is tk i , i.e. the kth anchor from agent i. If mark(i) = 1, ignore this tuple; otherwise do the following steps: \u2022 if size(tk i ) > R and i = return min {cost(S) + Rpj , cost(Best)}; \u2022 if size(tk i ) > R and cost(tk i ) \u2264 cost(S) return min {cost(S) + cost(tk i ), cost(Best)}; \u2022 if size(tk i ) > R and cost(tk i ) > cost(S) Add tk i to Skip; Set Best to S \u222a {tk i } if cost improves; \u2022 if size(tk i ) \u2264 R then add tk i to S; mark(i) = 1; subtract size(tk i ) from R. The approximation algorithm is very similar to the approximation algorithm for knapsack. Since we wish to minimize the total cost, we consider the tuples in order of increasing per unit cost. If the size of tuple tk i is smaller than R, then we add it to S, update R, and delete from U all the tuples that belong to the same group as tk i . If size(tk i ) is greater than R, then S along with tk i forms a feasible solution. However, this solution can be far from optimal if the size of tk i is much larger than R. If total cost of S and tk is smaller than the current best solution, we update Best. One exception to this rule is the tuple t . Since this tuple can be taken fractionally, we update Best if the sum of S\"s cost and fractional cost of t is an improvement. The algorithm terminates in either of the first two cases, or when all tuples are scanned. In particular, it terminates whenever we find a tk i such that size(tk i ) is greater than R but cost(tk i ) is less than cost(S), or when we reach the tuple representing agent l and it gives a feasible solution. LEMMA 2. Suppose A\u2217 is an optimal solution of the generalized knapsack, and suppose that element (l, j) is midrange in the optimal solution. Then, the cost V (l, j), returned by Greedy( , j), satisfies: V ( , j) + cost(tj ) \u2264 2cost(A\u2217 PROOF. Let V ( , j) be the value returned by Greedy( , j) and let V \u2217 ( , j) be an optimal solution for iKnapsack( , j). Consider the set Skip at the termination of Greedy( , j). There are two cases to consider: either some tuple t \u2208 Skip is also in V \u2217 ( , j), or no tuple in Skip is in V \u2217 ( , j). In the first case, let St be the tentative solution S at the time t was added to Skip. Because t \u2208 Skip then size(t) > R, and St together with t forms a feasible solution, and we have: V ( , j) \u2264 cost(Best) \u2264 cost(St) + cost(t). Again, because t \u2208 Skip then cost(t) > cost(St), and we have V ( , j) < 2cost(t). On the other hand, since t is included in V \u2217 ( , j), we have V \u2217 ( , j) \u2265 cost(t). These two inequalities imply the desired bound: V \u2217 ( , j) \u2264 V ( , j) < 2V \u2217 ( , j). In the second case, imagine a modified instance of iKnapsack( , j), which excludes all the tuples of the set Skip. Since none of these tuples were included in V \u2217 ( , j), the optimal solution for the modified problem should be the same as the one for the original. Suppose our approximation algorithm returns the value V ( , j) for this modified instance. Let t be the last tuple considered by the approximation algorithm before termination on the modified instance, and let St be the corresponding tentative solution set in that step. Since we consider tuples in order of increasing per unit price, and none of the tuples are going to be placed in the set Skip, we must have cost(St ) < V \u2217 ( , j) because St is the optimal way to obtain size(St ). We also have cost(t ) \u2264 cost(St ), and the following inequalities: V ( , j) \u2264 V ( , j) \u2264 cost(St ) + cost(t ) < 2V \u2217 ( , j) The inequality V ( , j) \u2264 V ( , j) follows from the fact that a tuple in the Skip list can only affect the Best but not the tentative solutions. Therefore, dropping the tuples in the set Skip can only make the solution worse. The above argument has shown that the value returned by Greedy( , j) is within a factor 2 of the optimal solution for iKnapsack( , j). We now show that the value V ( , j) plus cost(tj ) is a 2-approximation of the original generalized knapsack problem. Let A\u2217 be an optimal solution of the generalized knapsack, and suppose that element xj is midrange. Let x\u2212 to be set of the remaining elements, either zero or anchors, in this solution. Furthermore, define x = xj \u2212 uj . Thus, cost(A\u2217 ) = cost(xl) + cost(tj l ) + cost(x\u2212l) It is easy to see that (x\u2212 , x ) is an optimal solution for iKnapsack( , j). Since V ( , j) is a 2-approximation for this optimal solution, we have the following inequalities: V ( , j) + cost(tj ) \u2264 cost(tj ) + 2(cost(x ) + cost(x\u2212 )) \u2264 2(cost(x ) + cost(tj ) + cost(x\u2212 )) \u2264 2cost(A\u2217 This completes the proof of Lemma 2. It is easy to see that, after an initial sorting of the tuples in U, the algorithm Greedy( , j) takes O(n) time. We have our first polynomial approximation algorithm. THEOREM 2. A 2-approximation of the generalized knapsack problem can be found in time O(n2 ), where n is number of item lists (each of constant length). PROOF. We run the algorithm Greedy( , j) once for each tuple (l, j) as a candidate for midrange. There are O(n) tuples, and it suffices to sort them once, the total cost of the algorithm is O(n2 ). By Lemma 1, there is an optimal solution with at most one midrange element, so our algorithm will find a 2-approximation, as claimed. The dependence on the number of pieces is also polynomial: if each bid has a maximum of c pieces, then the running time is O((nc)2 ). 171 3.3 An Approximation Scheme We now use the 2-approximation algorithm presented in the preceding section to develop a fully polynomial approximation (FPTAS) for the generalized knapsack problem. The high level idea is fairly standard, but the details require technical care. We use a dynamic programming algorithm to solve iKnapsack( , j) for each possible midrange element, with the 2-approximation algorithm providing an upper bound on the value of the solution and enabling the use of scaling on the cost dimension of the dynamic programming (DP) table. Consider, for example, the case that the midrange element is x , which falls in the range [uj , uj+1 ). In our FPTAS, rather than using a greedy approximation algorithm to solve iKnapsack( , j), we construct a dynamic programming table to compute the minimum cost at which at least M \u2212 uj+1 units can be obtained using the remaining n \u2212 1 lists in the generalized knapsack. Suppose G[i, r] denotes the maximum number of units that can be obtained at cost at most r using only the first i lists in the generalized knapsack. Then, the following recurrence relation describes how to construct the dynamic programming table: G[0, r] = 0 G[i, r] = max \u00b4 G[i \u2212 1, r] max j\u2208\u03b2(i,r) {G[i \u2212 1, r \u2212 cost(tj i )] + uj i } where \u03b2(i, r) = {j : 1 \u2264 j \u2264 mi, cost(tj i ) \u2264 r}, is the set of anchors for agent i. As convention, agent i will index the row, and cost r will index the column. This dynamic programming algorithm is only pseudo-polynomial, since the number of column in the dynamic programming table depends upon the total cost. However, we can convert it into a FPTAS by scaling the cost dimension. Let A denote the 2-approximation to the generalized knapsack problem, with total cost, cost(A). Let \u03b5 denote the desired approximation factor. We compute the scaled cost of a tuple tj i , denoted scost(tj i ), as scost(tj i ) = n cost(tj i ) \u03b5cost(A) (2) This scaling improves the running time of the algorithm because the number of columns in the modified table is at most , and independent of the total cost. However, the computed solution might not be an optimal solution for the original problem. We show that the error introduced is within a factor of \u03b5 of the optimal solution. As a prelude to our approximation guarantee, we first show that if two different solutions to the iKnapsack problem have equal scaled cost, then their original (unscaled) costs cannot differ by more than \u03b5cost(A). LEMMA 3. Let x and y be two distinct feasible solutions of iKnapsack( , j), excluding their midrange elements. If x and y have equal scaled costs, then their unscaled costs cannot differ by more than \u03b5cost(A). PROOF. Let Ix and Iy, respectively, denote the indicator functions associated with the anchor vectors x and y-there is 1 in position Ix[i, k] if the xk i > 0. Since x and y has equal scaled cost, i= k scost(tk i )Ix[i, k] = i= k scost(tk i )Iy[i, k] (3) However, by (2), the scaled costs satisfy the following inequalities: (scost(tk i ) \u2212 1)\u03b5cost(A) \u2264 cost(tk i ) \u2264 scost(tk i )\u03b5cost(A) (4) Substituting the upper-bound on scaled cost from (4) for cost(x), the lower-bound on scaled cost from (4) for cost(y), and using equality (3) to simplify, we have: cost(x) \u2212 cost(y) \u2264 \u03b5cost(A) i= k Iy[i, k] \u2264 \u03b5cost(A), The last inequality uses the fact that at most n components of an indicator vector are non-zero; that is, any feasible solution contains at most n tuples. Finally, given the dynamic programming table for iKnapsack( , j), we consider all the entries in the last row of this table, G[n\u22121, r]. These entries correspond to optimal solutions with all agents except l, for different levels of cost. In particular, we consider the entries that provide at least M \u2212 uj+1 units. Together with a contribution from agent l, we choose the entry in this set that minimizes the total cost, defined as follows: cost(G[n \u2212 1, r]) + max {uj , M \u2212 G[n \u2212 1, r]}pj where cost is the original, unscaled cost associated with entry G[n\u22121, r]. It is worth noting, that unlike the 2-approximation scheme for iKnapsack( , j), the value computed with this FPTAS includes the cost to acquire uj l units from l. The following lemma shows that we achieve a (1+\u03b5)-approximation. LEMMA 4. Suppose A\u2217 is an optimal solution of the generalized knapsack problem, and suppose that element (l, j) is midrange in the optimal solution. Then, the solution A(l, j) from running the scaled dynamic-programming algorithm on iKnapsack( , j) satisfies cost(A(l, j)) \u2264 (1 + 2\u03b5)cost(A\u2217 PROOF. Let x\u2212 denote the vector of the elements in solution A\u2217 without element l. Then, by definition, cost(A\u2217 ) = cost(x\u2212 ) + pj xj . Let r = scost(x\u2212 ) be the scaled cost associated with the vector x\u2212 . Now consider the dynamic programming table constructed for iKnapsack( , j), and consider its entry G[n \u2212 1, r]. Let A denote the 2-approximation to the generalized knapsack problem, and A(l, j) denote the solution from the dynamic-programming algorithm. Suppose y\u2212 is the solution associated with this entry in our dynamic program; the components of the vector y\u2212 are the quantities from different lists. Since both x\u2212 and y\u2212 have equal scaled costs, by Lemma 3, their unscaled costs are within \u03b5cost(A) of each other; that is, cost(y\u2212 ) \u2212 cost(x\u2212 ) \u2264 \u03b5cost(A). Now, define yj = max{uj , M \u2212 \u00c8i= \u00c8j yj i }; this is the contribution needed from to make (y\u2212 , yj ) a feasible solution. Among all the equal cost solutions, our dynamic programming tables chooses the one with maximum units. Therefore, i= j yj i \u2265 i= j xj 172 Therefore, it must be the case that yj \u2264 xj . Because (yj , y\u2212 ) is also a feasible solution, if our algorithm returns a solution with cost cost(A(l, j)), then we must have cost(A(l, j)) \u2264 cost(y\u2212 ) + pj yj \u2264 cost(x\u2212 ) + \u03b5cost(A) + pj xj \u2264 (1 + 2\u03b5)cost(A\u2217 ), where we use the fact that cost(A) \u2264 2cost(A\u2217 ). Putting this together, our approximation scheme for the generalized knapsack problem will iterate the scheme described above for each choice of the midrange element (l, j), and choose the best solution from among these O(n) solutions. For a given midrange, the most expensive step in the algorithm is the construction of dynamic programming table, which can be done in O(n2 /\u03b5) time assuming constant intervals per list. Thus, we have the following result. THEOREM 3. We can compute an (1 + \u03b5) approximation to the solution of a generalized knapsack problem in worst-case time O(n3 /\u03b5). The dependence on the number of pieces is also polynomial: if each bid has a maximum of c pieces, then the running time can be derived by substituting cn for each occurrence of n. 4. COMPUTING VCG PAYMENTS We now consider the related problem of computing the VCG payments for all the agents. A naive approach requires solving the allocation problem n times, removing each agent in turn. In this section, we show that our approximation scheme for the generalized knapsack can be extended to determine all n payments in total time O(\u03b1T log(\u03b1n/\u03b5)), where 1 \u2264 C(I\\i)/C(I) \u2264 \u03b1, for a constant upper bound, \u03b1, and T is the complexity of solving the allocation problem once. This \u03b1-bound can be justified as a no monopoly condition, because it bounds the marginal value that a single buyer brings to the auction. Similarly, in the reverse variation we can compute the VCG payments to each seller in time O(\u03b1T log(\u03b1n/\u03b5)), where \u03b1 bounds the ratio C(I\\ i)/C(I) for all i. Our overall strategy will be to build two dynamic programming tables, forward and backward, for each midrange element (l, j) once. The forward table is built by considering the agents in the order of their indices, where as the backward table is built by considering them in the reverse order. The optimal solution corresponding to C(I \\ i) can be broken into two parts: one corresponding to first (i \u2212 1) agents and the other corresponding to last (n \u2212 i) agents. As the (i \u2212 1)th row of the forward table corresponds to the sellers with first (i\u22121) indices, an approximation to the first part will be contained in (i \u2212 1)th row of the forward table. Similarly, (n\u2212 i)th row of the backward table will contain an approximation for the second part. We first present a simple but an inefficient way of computing the approximate value of C(I \\ i), which illustrates the main idea of our algorithm. Then we present an improved scheme, which uses the fact that the elements in the rows are sorted, to compute the approximate value more efficiently. In the following, we concentrate on computing an allocation with xj being midrange, and some agent i = l removed. This will be a component in computing an approximation to C(I \\ i), the value of the solution to the generalized knapsack without bids from agent i. We begin with the simple scheme. 4.1 A Simple Approximation Scheme We implement the scaled dynamic programming algorithm for iKnapsack( , j) with two alternate orderings over the other sellers, k = l, one with sellers ordered 1, 2, . . . , n, and one with sellers ordered n, n \u2212 1, . . . , 1. We call the first table the forward table, and denote it F , and the second table the backward table, and denote it Bl. The subscript reminds us that the agent is midrange.9 In building these tables, we use the same scaling factor as before; namely, the cost of a tuple tj i is scaled as follows: scost(tj i ) = ncost(tj i ) \u03b5cost(A) where cost(A) is the upper bound on C(I), given by our 2approximation scheme. In this case, because C(I \\ i) can be \u03b1 times C(I), the scaled value of C(I \\ i) can be at most n\u03b1/\u03b5. Therefore, the cost dimension of our dynamic program\"s table will be n\u03b1/\u03b5. FlTable F (i\u22121)l 2 3 i\u22121 1 m\u22121 m n\u22121 2 31 m\u22121 m B (n\u2212i) n\u22121 n\u22122 n\u2212i lh Table Bl Figure 3: Computing VCG payments. m = n\u03b1 Now, suppose we want to compute a (1 + )-approximation to the generalized knapsack problem restricted to element (l, j) midrange, and further restricted to remove bids from some seller i = l. Call this problem iKnapsack\u2212i ( , j). Recall that the ith row of our DP table stores the best solution possible using only the first i agents excluding agent l, all of them either cleared at zero, or on anchors. These first i agents are a different subset of agents in the forward and the backward tables. By carefully combining one row of Fl with one row of Bl we can compute an approximation to iKnapsack\u2212i ( , j). We consider the row of Fl that corresponds to solutions constructed from agents {1, 2, . . . , i \u2212 1}, skipping agent l. We consider the row of Bl that corresponds to solutions constructed from agents {i+1, i+2, . . . , n}, again skipping agent l. The rows are labeled Fl(i \u2212 1) and Bl(n \u2212 i) respectively.10 The scaled costs for acquiring these units are the column indices for these entries. To solve iKnapsack\u2212i ( , j) we choose one entry from row F (i\u22121) and one from row B (n\u2212i) such that their total quantity exceeds M \u2212 uj+1 and their combined cost is minimum over all such combinations. Formally, let g \u2208 Fl(i \u2212 1), and h \u2208 Bl(n \u2212 1) denote entries in each row, with size(g), size(h), denoting the number of units and cost(g) and cost(h) denoting the unscaled cost associated with the entry. We compute the following, subject We could label the tables with both and j, to indicate the jth tuple is forced to be midrange, but omit j to avoid clutter. 10 To be precise, the index of the rows are (i \u2212 2) and (n \u2212 i) for Fl and Bl when l < i, and (i \u2212 1) and (n \u2212 i \u2212 1), respectively, when l > i. 173 to the condition that g and h satisfy size(g) + size(h) > M \u2212 uj+1 min g\u2208F (i\u22121),h\u2208B (n\u2212i) \u00d2cost(g) + cost(h) + pj \u00b7 max{uj , M \u2212 size(g) \u2212 size(h)} \u00d3 (5) LEMMA 5. Suppose A\u2212i is an optimal solution of the generalized knapsack problem without bids from agent i, and suppose that element (l, j) is the midrange element in the optimal solution. Then, the expression in Eq. 5, for the restricted problem iKnapsack\u2212i ( , j), computes a (1 + \u03b5)-approximation to A\u2212i PROOF. From earlier, we define cost(A\u2212i ) = C(I \\ i). We can split the optimal solution, A\u2212i , into three disjoint parts: xl corresponds to the midrange seller, xi corresponds to first i \u2212 1 sellers (skipping agent l if l < i), and x\u2212i corresponds to last n \u2212 i sellers (skipping agent l if l > i). We have: cost(A\u2212i ) = cost(xi) + cost(x\u2212i) + pj xj Let ri = scost(xi) and r\u2212i = scost(x\u2212i). Let yi and y\u2212i be the solution vectors corresponding to scaled cost ri and r\u2212i in F (i \u2212 1) and B (n \u2212 i), respectively. From Lemma 3 we conclude that, cost(yi) + cost(y\u2212i) \u2212 cost(xi) \u2212 cost(x\u2212i) \u2264 \u03b5cost(A) where cost(A) is the upper-bound on C(I) computed with the 2-approximation. Among all equal scaled cost solutions, our dynamic program chooses the one with maximum units. Therefore we also have, (size(yi) \u2265 size(xi)) and (size(y\u2212i) \u2265 size(x\u2212i)) where we use shorthand size(x) to denote total number of units in all tuples in x. Now, define yj l = max(uj l , M \u2212size(yi)\u2212size(y\u2212i)). From the preceding inequalities, we have yj l \u2264 xj l . Since (yj l , yi, y\u2212i) is also a feasible solution to the generalized knapsack problem without agent i, the value returned by Eq. 5 is at most cost(yi) + cost(y\u2212i) + pj l yj l \u2264 C(I \\ i) + \u03b5cost(A) \u2264 C(I \\ i) + 2cost(A\u2217 )\u03b5 \u2264 C(I \\ i) + 2C(I \\ i)\u03b5 This completes the proof. A naive implementation of this scheme will be inefficient because it might check (n\u03b1/\u03b5)2 pairs of elements, for any particular choice of (l, j) and choice of dropped agent i. In the next section, we present an efficient way to compute Eq. 5, and eventually to compute the VCG payments. 4.2 Improved Approximation Scheme Our improved approximation scheme for the winner-determination problem without agent i uses the fact that elements in F (i \u2212 1) and B (n \u2212 i) are sorted; specifically, both, unscaled cost and quantity (i.e. size), increases from left to right. As before, let g and h denote generic entries in F (i \u2212 1) and B (n \u2212 i) respectively. To compute Eq. 5, we consider all the tuple pairs, and first divide the tuples that satisfy condition size(g) + size(h) > M \u2212 uj+1 l into two disjoint sets. For each set we compute the best solution, and then take the best between the two sets. [case I: size(g) + size(h) \u2265 M \u2212 uj l ] The problem reduces to min g\u2208F (i\u22121), h\u2208B (n\u2212i) \u00d2cost(g) + cost(h) + pj l uj \u00d3 (6) We define a pair (g, h) to be feasible if size(g) + size(h) \u2265 M \u2212 uj l . Now to compute Eq. 6, we do a forward and backward walk on F (i \u2212 1) and B (n \u2212 i) respectively. We start from the smallest index of F (i \u2212 1) and move right, and from the highest index of B (n \u2212 i) and move left. Let (g, h) be the current pair. If (g, h) is feasible, we decrement B\"s pointer (that is, move backward) otherwise we increment F\"s pointer. The feasible pairs found during the walk are used to compute Eq. 6. The complexity of this step is linear in size of F (i \u2212 1), which is O(n\u03b1/\u03b5). [case II: M \u2212 uj+1 l \u2264 size(g) + size(h) \u2264 M \u2212 uj l ] The problem reduces to min g\u2208F (i\u22121), h\u2208B (n\u2212i) \u00d2cost(g) + cost(h) + pj l (M \u2212 size(g) \u2212 size(h)) To compute the above equation, we transform the above problem to another problem using modified cost, which is defined as: mcost(g) = cost(g) \u2212 pj l \u00b7 size(g) mcost(h) = cost(h) \u2212 pj l \u00b7 size(h) The new problem is to compute min g\u2208F (i\u22121), h\u2208B (n\u2212i) \u00d2mcost(g) + mcost(h) + pj l M \u00d3 (7) The modified cost simplifies the problem, but unfortunately the elements in F (i \u2212 1) and B (n \u2212 i) are no longer sorted with respect to mcost. However, the elements are still sorted in quantity and we use this property to compute Eq. 7. Call a pair (g, h) feasible if M \u2212 uj+1 l \u2264 size(g) + size(h) \u2264 M \u2212 uj l . Define the feasible set of g as the elements h \u2208 B (n \u2212 i) that are feasible given g. As the elements are sorted by quantity, the feasible set of g is a contiguous subset of B (n \u2212 i) and shifts left as g increases. 2 3 4 5 10 20 30 40 50 60 Begin End B (n\u2212i) 3 1 6 F (i\u22121)l Figure 4: The feasible set of g = 3, defined on B (n \u2212 i), is {2, 3, 4} when M \u2212 uj+1 l = 50 and M \u2212 uj l = 60. Begin and End represent the start and end pointers to the feasible set. Therefore, we can compute Eq. 7 by doing a forward and backward walk on F (i \u2212 1) and B (n \u2212 i) respectively. We walk on B (n \u2212 i), starting from the highest index, using two pointers, Begin and End, to indicate the start and end of the current feasible set. We maintain the feasible set as a min heap, where the key is modified cost. To update the feasible set, when we increment F\"s pointer(move forward), we walk left on B, first using End to remove elements from feasible set which are no longer 174 feasible and then using Begin to add new feasible elements. For a given g, the only element which we need to consider in g\"s feasible set is the one with minimum modified cost which can be computed in constant time with the min heap. So, the main complexity of the computation lies in heap updates. Since, any element is added or deleted at most once, there are O(n\u03b1 ) heap updates and the time complexity of this step is O(n\u03b1 log n\u03b1 ). 4.3 Collecting the Pieces The algorithm works as follows. First, using the 2 approximation algorithm, we compute an upper bound on C(I). We use this bound to scale down the tuple costs. Using the scaled costs, we build the forward and backward tables corresponding to each tuple (l, j). The forward tables are used to compute C(I). To compute C(I \\ i), we iterate over all the possible midrange tuples and use the corresponding forward and backward tables to compute the locally optimal solution using the above scheme. Among all the locally optimal solutions we choose one with the minimum total cost. The most expensive step in the algorithm is computation of C(I \\ i). The time complexity of this step is O(n2 log n\u03b1 as we have to iterate over all O(n) choices of tj l , for all l = i, and each time use the above scheme to compute Eq. 5. In the worst case, we might need to compute C(I \\ i) for all n sellers, in which case the final complexity of the algorithm will be O(n3 log n\u03b1 ). THEOREM 4. We can compute an /(1+ )-strategyproof approximation to the VCG mechanism in the forward and reverse multi-unit auctions in worst-case time O(n3 log n\u03b1 ). It is interesting to recall that T = O(n3 ) is the time complexity of the FPTAS to the generalized knapsack problem with all agents. Our combined scheme computes an approximation to the complete VCG mechanism, including payments to O(n) agents, in time complexity O(T log(n/\u03b5)), taking the no-monopoly parameter, \u03b1, as a constant. Thus, our algorithm performs much better than the naive scheme, which computes the VCG payment for each agent by solving a new instance of generalized knapsack problem. The speed up comes from the way we solve iKnapsack\u2212i ( , j). Time complexity of computing iKnapsack\u2212i ( , j) by creating a new dynamic programming table will be O(n2 but by using the forward and backward tables, the complexity is reduced to O(n log n ). We can further improve the time complexity of our algorithm by computing Eq. 5 more efficiently. Currently, the algorithm uses heap, which has logarithmic update time. In worst case, we can have two heap update operations for each element, which makes the time complexity super linear. If we can compute Eq. 5 in linear time then the complexity of computing the VCG payment will be same as the complexity of solving a single generalized knapsack problem. 5. CONCLUSIONS We presented a fully polynomial-time approximation scheme for the single-good multi-unit auction problem, using marginal decreasing piecewise constant bidding language. Our scheme is both approximately efficient and approximately strategyproof within any specified factor \u03b5 > 0. As such it is an example of computationally tractable \u03b5-dominance result, as well as an example of a non-trivial but approximable allocation problem. It is particularly interesting that we are able to compute the payments to n agents in a VCG-based mechanism in worst-case time O(T log n), where T is the time complexity to compute the solution to a single allocation problem.", "body1": "In this paper we present a fully polynomial-time approximation scheme for the single-good multi-unit auction problem. In the reverse auction, we consider a single buyer with a demand for M units of a good and n suppliers, each with a marginal-decreasing piecewise-constant cost function. In the forward auction, we consider a single seller with M units of a good and n buyers, each with a marginal-decreasing piecewise-constant valuation function. We consider the computational complexity of implementing the Vickrey-Clarke-Groves mechanism for the multiunit auction problem. Given this scheme, a straightforward computation of the VCG payments to all n agents requires time O(nT). This means that a bidder can gain at most ( \u03b5 1+\u03b5 )V from a nontruthful bid, where V is the total surplus from the efficient allocation. Section 2 formally defines the forward and reverse auctions, and defines the VCG mechanisms. 1.1 Related Work There has been considerable interest in recent years in characterizing polynomial-time or approximable special cases of the general combinatorial allocation problem, in which there are multiple different items. The idea of using approximations within mechanisms, while retaining either full-strategyproofness or \u03b5-dominance has received some previous attention. However, this may not be an example of what Feigenbaum & Shenker refer to as a tolerably-manipulable mechanism because we have not tried to bound the effect of such a manipulation on the efficiency of the outcome. baum & Shenker have defined the concept of strategically faithful approximations, and proposed the study of approximations as an important direction for algorithmic mechanism design. Eso et al. Kalagnanam et al. In this section, we first describe the marginal-decreasing piecewise bidding language that is used in our forward and reverse auctions. 2.1 Marginal-Decreasing Piecewise Bids We provide a piecewise-constant and marginal-decreasing bidding language. Reverse Auction Bid 5 10 20 25 10 Quantity Price 5 10 20 25 10 Quantity Price Forward Auction Bid Figure 1: Marginal-decreasing, piecewise constant bids. In detail, in a forward auction, a bid from buyer i can be written as a list of (quantity-range, unit-price) tuples, ((u1 i , p1 i ), (u2 i , p2 i ), . Additionally, it is assumed that the valuation is 0 for quantities less than u1 i as well as for quantities more than um i . A seller\"s bid is similarly defined in the reverse auction. 2.2 VCG-Based Multi-Unit Auctions We construct the tractable and approximately-strategyproof multiunit auctions around a VCG mechanism. We assume that this seller has no intrinsic value for the items. Given a set of bids from I agents, let V (I) denote the maximal revenue to the seller, given that at most one point on the bid curve can be selected from each agent and no more than M units of the item can be sold. 3. In this forward auction, the VCG mechanism is strategyproof for buyers, which means that truthful bidding is a dominant strategy, i.e. In addition, the VCG mechanism is allocatively-efficient, and the payments from each buyer are always positive.3 Moreover, each buyer pays less than its value, and receives payoff V (I)\u2212V (I \\ i) in equilibrium; this is precisely the marginal-value that buyer i contributes to the economic efficiency of the system. In the reverse auction, there is a buyer with M units to buy, and n suppliers. determination problem in the reverse auction is to determine the allocation, x\u2217 , that minimizes the cost to the buyer, or forfeits trade if the minimal cost is greater than value, V . Let C(I) denote the minimal cost given bids from all sellers, and let C(I \\i) denote the minimal cost without bids from seller i. The VCG mechanism implements the outcome x\u2217 that minimizes cost based on bids from all sellers, and then provides payment pvcg,i = pask,i(x\u2217 i )+[V \u2212C(I)\u2212max(0, V \u2212C(I\\i))] to each seller. Although the VCG mechanism remains strategyproof for sellers in the reverse direction, its applicability is limited to cases in which the total payments to the sellers are less than the buyer\"s value. Given this, it is then necessary and sufficient that: V \u2212 C(I) \u2265 (C(I \\ i) \u2212 C(I)) (1) To make the reverse auction symmetric with the forward direction, we would need a buyer with a constant marginal-value to buy the first M units, and zero value for additional units. 168 In words, the surplus of the efficient allocation must be greater than the total marginal-surplus provided by each seller.6 Consider an example with 3 agents {1, 2, 3}, and V = 150 and C(123) = 50. One thing we do know, because the VCG mechanism will maximize the payoff to the buyer across all efficient mechanisms , is that whenever Eq. As before, let V (I) denote the value of the optimal solution to the allocation problem with truthful bids from all agents, and V (I \\i) denote the value of the optimal solution computed without bids from agent i. The payoff to agent i, for announcing valuation \u02c6vi, is: vi(\u02c6xi) + j=i \u02c6vj (\u02c6xj) \u2212 \u02c6V (I \\ i) The final term is independent of the agent\"s announced value, and can be ignored in an incentive-analysis. Moreover, although there is a small literature on maximallyefficient mechanisms subject to requirements of voluntaryparticipation and budget-balance (i.e. We can now analyze the best possible gain from manipulation to an agent in our setting. Let V in the following theorem define the total value of the efficient allocation, given the reported values of agents j = i, and the true value of agent i. THEOREM 1. Notice that we did not need to bound the error on the allocation problems without each agent, because the -strategyproofness result follows from the accuracy of the first-term in the VCG payment and is independent of the accuracy of the second-term. However, the accuracy of the solution to the problem without each agent is important to implement a good approximation to the revenue properties of the VCG mechanism. PROBLEM In this section, we design a fully polynomial approximation scheme for the generalized knapsack, which models the winnerdetermination problem for the VCG-based multi-unit auctions. In describing our approximation scheme, we begin with a simple property (the Anchor property) of an optimal knapsack solution. We strive to preserve this, and present an approximation scheme that will depend only on the number of bidders, and not the maximum quantity, M, which can be very large in realistic procurement settings. The FPTAS implements an (1 + \u03b5) approximation to the optimal solution x\u2217 , in worst-case time T = O(n3 /\u03b5), where n is the number of bidders, and where we assume that the piecewise bid for each bidder has O(1) pieces. 3.1 Preliminaries Before we begin, let us recall the classic 0/1 knapsack problem: we are given a set of n items, where the item i has value vi and size si, and a knapsack of capacity M; all sizes are integers. Problem: Determine a set of integers xj i such that 1. This generalized knapsack formulation is a clear generalization of the classic 0/1 knapsack. The goal is to procure at least M units of the good at minimum possible cost. Recall that these tuples represent the breakpoints in the piecewise constant curve bids. The cost of the anchor tj i is defined to be the minimum total price associated with this tuple, namely, cost(tj i ) = pj i uj i if j < mi, and cost(tmi i ) = pmi\u22121 i umi i . In a feasible solution {x1, x2, . LEMMA 1. We use the anchor property to first obtain a polynomial-time 2-approximation scheme. Specifically, suppose element x for agent l is forced to lie in its jth range, [uj , uj+1 ), while all other elements, x1, . The group for agent l contains a single element that represents the interval [0, uj+1 \u2212uj ), and the associated unit-price pj . The following pseudo-code describes our algorithm for this restriction of the generalized knapsack problem. 2. 3. The approximation algorithm is very similar to the approximation algorithm for knapsack. LEMMA 2. Again, because t \u2208 Skip then cost(t) > cost(St), and we have V ( , j) < 2cost(t). In the second case, imagine a modified instance of iKnapsack( , j), which excludes all the tuples of the set Skip. The above argument has shown that the value returned by Greedy( , j) is within a factor 2 of the optimal solution for iKnapsack( , j). We now show that the value V ( , j) plus cost(tj ) is a 2-approximation of the original generalized knapsack problem. Let A\u2217 be an optimal solution of the generalized knapsack, and suppose that element xj is midrange. Furthermore, define x = xj \u2212 uj . Since V ( , j) is a 2-approximation for this optimal solution, we have the following inequalities: V ( , j) + cost(tj ) \u2264 cost(tj ) + 2(cost(x ) + cost(x\u2212 )) \u2264 2(cost(x ) + cost(tj ) + cost(x\u2212 )) \u2264 2cost(A\u2217 This completes the proof of Lemma 2. It is easy to see that, after an initial sorting of the tuples in U, the algorithm Greedy( , j) takes O(n) time. THEOREM 2. PROOF. 171 3.3 An Approximation Scheme We now use the 2-approximation algorithm presented in the preceding section to develop a fully polynomial approximation (FPTAS) for the generalized knapsack problem. Suppose G[i, r] denotes the maximum number of units that can be obtained at cost at most r using only the first i lists in the generalized knapsack. Let A denote the 2-approximation to the generalized knapsack problem, with total cost, cost(A). LEMMA 3. These entries correspond to optimal solutions with all agents except l, for different levels of cost. LEMMA 4. Now, define yj = max{uj , M \u2212 \u00c8i= \u00c8j yj i }; this is the contribution needed from to make (y\u2212 , yj ) a feasible solution. Among all the equal cost solutions, our dynamic programming tables chooses the one with maximum units. For a given midrange, the most expensive step in the algorithm is the construction of dynamic programming table, which can be done in O(n2 /\u03b5) time assuming constant intervals per list. THEOREM 3. The dependence on the number of pieces is also polynomial: if each bid has a maximum of c pieces, then the running time can be derived by substituting cn for each occurrence of n. We now consider the related problem of computing the VCG payments for all the agents. Our overall strategy will be to build two dynamic programming tables, forward and backward, for each midrange element (l, j) once. In the following, we concentrate on computing an allocation with xj being midrange, and some agent i = l removed. 4.1 A Simple Approximation Scheme We implement the scaled dynamic programming algorithm for iKnapsack( , j) with two alternate orderings over the other sellers, k = l, one with sellers ordered 1, 2, . FlTable F (i\u22121)l 2 3 i\u22121 1 m\u22121 m n\u22121 2 31 m\u22121 m B (n\u2212i) n\u22121 n\u22122 n\u2212i lh Table Bl Figure 3: Computing VCG payments. Recall that the ith row of our DP table stores the best solution possible using only the first i agents excluding agent l, all of them either cleared at zero, or on anchors. 173 to the condition that g and h satisfy size(g) + size(h) > M \u2212 uj+1 min g\u2208F (i\u22121),h\u2208B (n\u2212i) \u00d2cost(g) + cost(h) + pj \u00b7 max{uj , M \u2212 size(g) \u2212 size(h)} \u00d3 (5) LEMMA 5. Now, define yj l = max(uj l , M \u2212size(yi)\u2212size(y\u2212i)). A naive implementation of this scheme will be inefficient because it might check (n\u03b1/\u03b5)2 pairs of elements, for any particular choice of (l, j) and choice of dropped agent i. 4.2 Improved Approximation Scheme Our improved approximation scheme for the winner-determination problem without agent i uses the fact that elements in F (i \u2212 1) and B (n \u2212 i) are sorted; specifically, both, unscaled cost and quantity (i.e. [case I: size(g) + size(h) \u2265 M \u2212 uj l ] The problem reduces to min g\u2208F (i\u22121), h\u2208B (n\u2212i) \u00d2cost(g) + cost(h) + pj l uj \u00d3 (6) We define a pair (g, h) to be feasible if size(g) + size(h) \u2265 M \u2212 uj l . [case II: M \u2212 uj+1 l \u2264 size(g) + size(h) \u2264 M \u2212 uj l ] The problem reduces to min g\u2208F (i\u22121), h\u2208B (n\u2212i) \u00d2cost(g) + cost(h) + pj l (M \u2212 size(g) \u2212 size(h)) To compute the above equation, we transform the above problem to another problem using modified cost, which is defined as: mcost(g) = cost(g) \u2212 pj l \u00b7 size(g) mcost(h) = cost(h) \u2212 pj l \u00b7 size(h) The new problem is to compute min g\u2208F (i\u22121), h\u2208B (n\u2212i) \u00d2mcost(g) + mcost(h) + pj l M \u00d3 (7) The modified cost simplifies the problem, but unfortunately the elements in F (i \u2212 1) and B (n \u2212 i) are no longer sorted with respect to mcost. 2 3 4 5 10 20 30 40 50 60 Begin End B (n\u2212i) 3 1 6 F (i\u22121)l Figure 4: The feasible set of g = 3, defined on B (n \u2212 i), is {2, 3, 4} when M \u2212 uj+1 l = 50 and M \u2212 uj l = 60. Therefore, we can compute Eq. 4.3 Collecting the Pieces The algorithm works as follows. The most expensive step in the algorithm is computation of C(I \\ i). It is interesting to recall that T = O(n3 ) is the time complexity of the FPTAS to the generalized knapsack problem with all agents. If we can compute Eq.", "body2": "We consider both a reverse auction variation and a forward auction variation, and propose a compact and expressive bidding language that allows marginal-decreasing piecewise constant curves. circuit boards, power suppliers, toner cartridges), with flexible-sized lots. The forward variation models, for example, an auction to sell excess inventory in flexible-sized lots. For instance, the classic 0/1 knapsack is a special case of this problem.1 We model the However, the problem can be solved easily by a greedy scheme if we remove all capacity constraints from the seller and all 166 allocation problem as a novel and interesting generalization of the classic knapsack problem, and develop a fully polynomialtime approximation scheme, computing a (1 + )-approximation in worst-case time T = O(n3 /\u03b5), where each bid has a fixed number of piecewise constant pieces. The approximate VCG mechanism is ( \u03b5 1+\u03b5 )-strategyproof for an approximation to within (1 + ) of the optimal allocation. As such, this is an example of a computationally-tractable \u03b5-dominance result.2 In practice, we can have good confidence that bidders without good information about the bidding strategies of other participants will have little to gain from attempts at manipulation. Section 5 concludes. We identify a non-trivial but approximable allocation problem with an expressive exclusiveor bidding language-the bid taker in our setting is allowed to accept at most one point on the bid curve. Feigenminimum-lot size constraints from the buyers. VCG mechanism do have a natural self-correcting property, though, because a useful manipulation to an agent is a reported value that improves the total value of the allocation based on the reports of other agents and the agent\"s own value. Schummer and Parkes et al have previously considered \u03b5-dominance, in the context of economic impossibility results, for example in combinatorial exchanges. This earlier work formulates the problem as a general mixed integer linear program, and gives some empirical results on simulated data. Ausubel has proposed an ascending-price multi-unit auction for buyers with marginal-decreasing values , with an interpretation as a primal-dual algorithm . We also discuss the economic properties of VCG mechanisms in these forward and reverse auction multi-unit settings. In addition, we slightly relax the marginal-decreasing requirement to allow: a bidder in the forward auction to state a minimal purchase amount, such that she has zero value for quantities smaller than that amount; a seller in the reverse auction to state a capacity constraint, such that she has an effectively infinite cost to supply quantities in excess of a particular amount. In the reverse auction bid, the cost of the seller is \u221e outside the range . The interpretation is that the bidder\"s valuation in the 167 (semi-open) quantity range [uj i , uj+1 i ) is pj i for each unit. In order to resolve the boundary condition, we assume that the bid price for the upper bound quantity umi i is pbid,i(umi i ) = umi i pmi\u22121 i . We interpret the bid list as defining a price function, pask,i(q) = qpj i , if uj i \u2264 q < uj+1 i . In the forward auction, there is a seller with M units to sell. We assume that this seller has no intrinsic value for the items. Implement the outcome x\u2217 that solves the winner-determination problem with all buyers. Collect payment pvcg,i = pbid,i(x\u2217 i ) \u2212 [V (I) \u2212 V (I \\ i)] from each buyer, and pass the payments to the seller. utility maximizing whatever the bids of other buyers. In addition, the VCG mechanism is allocatively-efficient, and the payments from each buyer are always positive.3 Moreover, each buyer pays less than its value, and receives payoff V (I)\u2212V (I \\ i) in equilibrium; this is precisely the marginal-value that buyer i contributes to the economic efficiency of the system. Without this assumption, the Myerson-Satterthwaite impossibility result would already imply that we should not expect an efficient trading mechanism in this setting. determination problem in the reverse auction is to determine the allocation, x\u2217 , that minimizes the cost to the buyer, or forfeits trade if the minimal cost is greater than value, V . Otherwise, then the efficient outcome is no trade, and the outcome of the VCG mechanism is no trade and no payments. Again, in equilibrium each seller\"s payoff is exactly the marginal-value that the seller contributes to the economic efficiency of the system; in the simple case that V \u2265 C(I \\ i) for all sellers i, this is precisely C(I \\ i) \u2212 C(I). In other words, there must be no pivotal sellers. The total payments received by the seller can be less than the seller\"s cost for the outcome in this case. In the second case, the payoffs are \u03c0 = (\u221210, 30, 50, 30). We describe the result for the forward auction direction, but it is quite a general observation. Let \u02c6x denote the allocation implemented by the approximation scheme. Useful characterizations of conditions that satisfy agents are substitutes, in terms of the underlying valuations of agents have proved quite elusive. ). In this case, an individual agent can improve its payoff by no more than a factor /(1 + ) of the value of the optimal solution given the values reported by the other agents. Let V in the following theorem define the total value of the efficient allocation, given the reported values of agents j = i, and the true value of agent i. A VCG-based mechanism with a (1 + \u03b5)allocation algorithm is (1+ \u2212V ) strategyproof for agent i, and agent i can gain at most this payoff through some non-truthful strategy. Notice that we did not need to bound the error on the allocation problems without each agent, because the -strategyproofness result follows from the accuracy of the first-term in the VCG payment and is independent of the accuracy of the second-term. However, the accuracy of the solution to the problem without each agent is important to implement a good approximation to the revenue properties of the VCG mechanism. We describe our results for the reverse auction variation, but the formulation is completely symmetric for the forward-auction. One of the major appeals of our piecewise bidding language is its compact representation of the bidder\"s valuation functions. We strive to preserve this, and present an approximation scheme that will depend only on the number of bidders, and not the maximum quantity, M, which can be very large in realistic procurement settings. The dependence on the number of pieces is also polynomial: if each bid has a maximum 169 of c pieces, then the running time can be derived by substituting nc for each occurrence of n. , (umi\u22121 i , pmi\u22121 i ), (umi i (i), \u221e) , where uj i are increasing with j and pj i are decreasing with j, and uj i , pj i , M are positive integers. (Objective) \u00c8i \u00c8j pj i xj i is minimized. Choosing interval [uj i , uj+1 i ) has cost pj i per unit. Given an instance of the generalized knapsack, we call each tuple tj i = (uj i , pj i ) an anchor. the minimum number of units available at this anchor\"s price pj i . The cost of the anchor tj i is defined to be the minimum total price associated with this tuple, namely, cost(tj i ) = pj i uj i if j < mi, and cost(tmi i ) = pmi\u22121 i umi i . See Figure 2 for an example. 35 Price Quantity (i) Optimal solution with 2 midrange bids (ii) Optimal soltution with Figure 2: (i) An optimal solution with more than one bid not anchored (2,3); (ii) an optimal solution with only one bid (3) not anchored. We do this by solving several instances of a restricted generalized-knapsack problem, which we call iKnapsack, where one element is forced to be midrange for a particular interval. We create n \u2212 1 groups of potential anchors, where ith group contains all the anchors of the list i in the generalized knapsack. In any other group, we can choose at most one anchor. Sort all tuples of U in the ascending order of unit price; in case of ties, sort in ascending order of unit quantities. , n. Initialize R = M \u2212 uj , S = Best = Skip = \u2205. If mark(i) = 1, ignore this tuple; otherwise do the following steps: \u2022 if size(tk i ) > R and i = return min {cost(S) + Rpj , cost(Best)}; \u2022 if size(tk i ) > R and cost(tk i ) \u2264 cost(S) return min {cost(S) + cost(tk i ), cost(Best)}; \u2022 if size(tk i ) > R and cost(tk i ) > cost(S) Add tk i to Skip; Set Best to S \u222a {tk i } if cost improves; \u2022 if size(tk i ) \u2264 R then add tk i to S; mark(i) = 1; subtract size(tk i ) from R. In particular, it terminates whenever we find a tk i such that size(tk i ) is greater than R but cost(tk i ) is less than cost(S), or when we reach the tuple representing agent l and it gives a feasible solution. Because t \u2208 Skip then size(t) > R, and St together with t forms a feasible solution, and we have: V ( , j) \u2264 cost(Best) \u2264 cost(St) + cost(t). These two inequalities imply the desired bound: V \u2217 ( , j) \u2264 V ( , j) < 2V \u2217 ( , j). Therefore, dropping the tuples in the set Skip can only make the solution worse. The above argument has shown that the value returned by Greedy( , j) is within a factor 2 of the optimal solution for iKnapsack( , j). We now show that the value V ( , j) plus cost(tj ) is a 2-approximation of the original generalized knapsack problem. Let x\u2212 to be set of the remaining elements, either zero or anchors, in this solution. Thus, cost(A\u2217 ) = cost(xl) + cost(tj l ) + cost(x\u2212l) It is easy to see that (x\u2212 , x ) is an optimal solution for iKnapsack( , j). Since V ( , j) is a 2-approximation for this optimal solution, we have the following inequalities: V ( , j) + cost(tj ) \u2264 cost(tj ) + 2(cost(x ) + cost(x\u2212 )) \u2264 2(cost(x ) + cost(tj ) + cost(x\u2212 )) \u2264 2cost(A\u2217 This completes the proof of Lemma 2. We have our first polynomial approximation algorithm. A 2-approximation of the generalized knapsack problem can be found in time O(n2 ), where n is number of item lists (each of constant length). The dependence on the number of pieces is also polynomial: if each bid has a maximum of c pieces, then the running time is O((nc)2 ). In our FPTAS, rather than using a greedy approximation algorithm to solve iKnapsack( , j), we construct a dynamic programming table to compute the minimum cost at which at least M \u2212 uj+1 units can be obtained using the remaining n \u2212 1 lists in the generalized knapsack. However, we can convert it into a FPTAS by scaling the cost dimension. As a prelude to our approximation guarantee, we first show that if two different solutions to the iKnapsack problem have equal scaled cost, then their original (unscaled) costs cannot differ by more than \u03b5cost(A). If x and y have equal scaled costs, then their unscaled costs cannot differ by more than \u03b5cost(A). Finally, given the dynamic programming table for iKnapsack( , j), we consider all the entries in the last row of this table, G[n\u22121, r]. It is worth noting, that unlike the 2-approximation scheme for iKnapsack( , j), the value computed with this FPTAS includes the cost to acquire uj l units from l. The following lemma shows that we achieve a (1+\u03b5)-approximation. Since both x\u2212 and y\u2212 have equal scaled costs, by Lemma 3, their unscaled costs are within \u03b5cost(A) of each other; that is, cost(y\u2212 ) \u2212 cost(x\u2212 ) \u2264 \u03b5cost(A). Now, define yj = max{uj , M \u2212 \u00c8i= \u00c8j yj i }; this is the contribution needed from to make (y\u2212 , yj ) a feasible solution. Putting this together, our approximation scheme for the generalized knapsack problem will iterate the scheme described above for each choice of the midrange element (l, j), and choose the best solution from among these O(n) solutions. Thus, we have the following result. We can compute an (1 + \u03b5) approximation to the solution of a generalized knapsack problem in worst-case time O(n3 /\u03b5). The dependence on the number of pieces is also polynomial: if each bid has a maximum of c pieces, then the running time can be derived by substituting cn for each occurrence of n. Similarly, in the reverse variation we can compute the VCG payments to each seller in time O(\u03b1T log(\u03b1n/\u03b5)), where \u03b1 bounds the ratio C(I\\ i)/C(I) for all i. Then we present an improved scheme, which uses the fact that the elements in the rows are sorted, to compute the approximate value more efficiently. We begin with the simple scheme. Therefore, the cost dimension of our dynamic program\"s table will be n\u03b1/\u03b5. m = n\u03b1 Now, suppose we want to compute a (1 + )-approximation to the generalized knapsack problem restricted to element (l, j) midrange, and further restricted to remove bids from some seller i = l. Call this problem iKnapsack\u2212i ( , j). 10 To be precise, the index of the rows are (i \u2212 2) and (n \u2212 i) for Fl and Bl when l < i, and (i \u2212 1) and (n \u2212 i \u2212 1), respectively, when l > i. Therefore we also have, (size(yi) \u2265 size(xi)) and (size(y\u2212i) \u2265 size(x\u2212i)) where we use shorthand size(x) to denote total number of units in all tuples in x. 5 is at most cost(yi) + cost(y\u2212i) + pj l yj l \u2264 C(I \\ i) + \u03b5cost(A) \u2264 C(I \\ i) + 2cost(A\u2217 )\u03b5 \u2264 C(I \\ i) + 2C(I \\ i)\u03b5 This completes the proof. 5, and eventually to compute the VCG payments. For each set we compute the best solution, and then take the best between the two sets. The complexity of this step is linear in size of F (i \u2212 1), which is O(n\u03b1/\u03b5). Define the feasible set of g as the elements h \u2208 B (n \u2212 i) that are feasible given g. As the elements are sorted by quantity, the feasible set of g is a contiguous subset of B (n \u2212 i) and shifts left as g increases. Begin and End represent the start and end pointers to the feasible set. Since, any element is added or deleted at most once, there are O(n\u03b1 ) heap updates and the time complexity of this step is O(n\u03b1 log n\u03b1 ). Among all the locally optimal solutions we choose one with the minimum total cost. We can compute an /(1+ )-strategyproof approximation to the VCG mechanism in the forward and reverse multi-unit auctions in worst-case time O(n3 log n\u03b1 ). In worst case, we can have two heap update operations for each element, which makes the time complexity super linear. 5 in linear time then the complexity of computing the VCG payment will be same as the complexity of solving a single generalized knapsack problem.", "introduction": "In this paper we present a fully polynomial-time approximation scheme for the single-good multi-unit auction problem. Our scheme is both approximately efficient and approximately strategyproof. The auction settings considered in our paper are motivated by recent trends in electronic commerce; for instance, corporations are increasingly using auctions for their strategic sourcing. We consider both a reverse auction variation and a forward auction variation, and propose a compact and expressive bidding language that allows marginal-decreasing piecewise constant curves. In the reverse auction, we consider a single buyer with a demand for M units of a good and n suppliers, each with a marginal-decreasing piecewise-constant cost function. In addition, each supplier can also express an upper bound, or capacity constraint on the number of units she can supply. The reverse variation models, for example, a procurement auction to obtain raw materials or other services (e.g. circuit boards, power suppliers, toner cartridges), with flexible-sized lots. In the forward auction, we consider a single seller with M units of a good and n buyers, each with a marginal-decreasing piecewise-constant valuation function. A buyer can also express a lower bound, or minimum lot size, on the number of units she demands. The forward variation models, for example, an auction to sell excess inventory in flexible-sized lots. We consider the computational complexity of implementing the Vickrey-Clarke-Groves mechanism for the multiunit auction problem. The Vickrey-Clarke-Groves (VCG) mechanism has a number of interesting economic properties in this setting, including strategyproofness, such that truthful bidding is a dominant strategy for buyers in the forward auction and sellers in the reverse auction, and allocative efficiency, such that the outcome maximizes the total surplus in the system. However, as we discuss in Section 2, the application of the VCG-based approach is limited in the reverse direction to instances in which the total payments to the sellers are less than the value of the outcome to the buyer. Otherwise, either the auction must run at a loss in these instances, or the buyer cannot be expected to voluntarily choose to participate. This is an example of the budget-deficit problem that often occurs in efficient mechanism design . The computational problem is interesting, because even with marginal-decreasing bid curves, the underlying allocation problem turns out to (weakly) intractable. For instance, the classic 0/1 knapsack is a special case of this problem.1 We model the However, the problem can be solved easily by a greedy scheme if we remove all capacity constraints from the seller and all 166 allocation problem as a novel and interesting generalization of the classic knapsack problem, and develop a fully polynomialtime approximation scheme, computing a (1 + )-approximation in worst-case time T = O(n3 /\u03b5), where each bid has a fixed number of piecewise constant pieces. Given this scheme, a straightforward computation of the VCG payments to all n agents requires time O(nT). We compute approximate VCG payments in worst-case time O(\u03b1T log(\u03b1n/\u03b5)), where \u03b1 is a constant that quantifies a reasonable no-monopoly assumption. Specifically, in the reverse auction, suppose that C(I) is the minimal cost for procuring M units with all sellers I, and C(I \\ i) is the minimal cost without seller i. Then, the constant \u03b1 is defined as an upper bound for the ratio C(I \\i)/C(I), over all sellers i. This upper-bound tends to 1 as the number of sellers increases. The approximate VCG mechanism is ( \u03b5 1+\u03b5 )-strategyproof for an approximation to within (1 + ) of the optimal allocation. This means that a bidder can gain at most ( \u03b5 1+\u03b5 )V from a nontruthful bid, where V is the total surplus from the efficient allocation. As such, this is an example of a computationally-tractable \u03b5-dominance result.2 In practice, we can have good confidence that bidders without good information about the bidding strategies of other participants will have little to gain from attempts at manipulation. Section 2 formally defines the forward and reverse auctions, and defines the VCG mechanisms. We also prove our claims about \u03b5-strategyproofness. Section 3 provides the generalized knapsack formulation for the multi-unit allocation problems and introduces the fully polynomial time approximation scheme. Section 4 defines the approximation scheme for the payments in the VCG mechanism. 1.1 Related Work There has been considerable interest in recent years in characterizing polynomial-time or approximable special cases of the general combinatorial allocation problem, in which there are multiple different items. The combinatorial allocation problem (CAP) is both NP-complete and inapproximable (e.g. Although some polynomial-time cases have been identified for the CAP , introducing an expressive exclusive-or bidding language quickly breaks these special cases. We identify a non-trivial but approximable allocation problem with an expressive exclusiveor bidding language-the bid taker in our setting is allowed to accept at most one point on the bid curve. The idea of using approximations within mechanisms, while retaining either full-strategyproofness or \u03b5-dominance has received some previous attention. propose a greedy and strategyproof approximation to a single-minded combinatorial auction problem. Nisan & Ronen discussed approximate VCG-based mechanisms, but either appealed to particular maximal-in-range approximations to retain full strategyproofness, or to resource-bounded agents with information or computational limitations on the ability to compute strategies. Feigenminimum-lot size constraints from the buyers. However, this may not be an example of what Feigenbaum & Shenker refer to as a tolerably-manipulable mechanism because we have not tried to bound the effect of such a manipulation on the efficiency of the outcome. VCG mechanism do have a natural self-correcting property, though, because a useful manipulation to an agent is a reported value that improves the total value of the allocation based on the reports of other agents and the agent\"s own value. baum & Shenker have defined the concept of strategically faithful approximations, and proposed the study of approximations as an important direction for algorithmic mechanism design. Schummer and Parkes et al have previously considered \u03b5-dominance, in the context of economic impossibility results, for example in combinatorial exchanges. have studied a similar procurement problem, but for a different volume discount model. This earlier work formulates the problem as a general mixed integer linear program, and gives some empirical results on simulated data. address double auctions, where multiple buyers and sellers trade a divisible good. The focus of this paper is also different: it investigates the equilibrium prices using the demand and supply curves, whereas our focus is on efficient mechanism design. Ausubel has proposed an ascending-price multi-unit auction for buyers with marginal-decreasing values , with an interpretation as a primal-dual algorithm .", "conclusion": "We presented a fully polynomial-time approximation scheme for the single-good multi-unit auction problem, using marginal decreasing piecewise constant bidding language.. Our scheme is both approximately efficient and approximately strategyproof within any specified factor \u03b5 > 0.. As such it is an example of computationally tractable \u03b5-dominance result, as well as an example of a non-trivial but approximable allocation problem.. It is particularly interesting that we are able to compute the payments to n agents in a VCG-based mechanism in worst-case time O(T log n), where T is the time complexity to compute the solution to a single allocation problem."}
{"id": "J-22", "keywords": ["predict market", "express bet", "order match", "comput complex"], "title": "Betting on Permutations", "abstract": "We consider a permutation betting scenario, where people wager on the final ordering of n candidates: for example, the outcome of a horse race. We examine the auctioneer problem of risklessly matching up wagers or, equivalently, finding arbitrage opportunities among the proposed wagers. Requiring bidders to explicitly list the orderings that they'd like to bet on is both unnatural and intractable, because the number of orderings is n! and the number of subsets of orderings is 2n!. We propose two expressive betting languages that seem natural for bidders, and examine the computational complexity of the auctioneer problem in each case. Subset betting allows traders to bet either that a candidate will end up ranked among some subset of positions in the final ordering, for example, \"horse A will finish in positions 4, 9, or 13-21\", or that a position will be taken by some subset of candidates, for example \"horse A, B, or D will finish in position 2\". For subset betting, we show that the auctioneer problem can be solved in polynomial time if orders are divisible. Pair betting allows traders to bet on whether one candidate will end up ranked higher than another candidate, for example \"horse A will beat horse B\". We prove that the auctioneer problem becomes NP-hard for pair betting. We identify a sufficient condition for the existence of a pair betting match that can be verified in polynomial time. We also show that a natural greedy algorithm gives a poor approximation for indivisible orders.", "references": ["The role of securities in the optimal allocation of risk-bearing", "Results from a dozen years of election futures markets research", "Introduction to Algorithms", "Combinatorial Auctions", "Wishes, expectations, and actions: A survey on price formation in election stock markets", "Betting boolean-style: A framework for trading in securities based on logical formulas", "The ellipsoid method and its consequences in combinatorial optimization", "Geometric Algorithms and Combinatorial Optimization", " Combinatorial information market design", "Reducibility among combinatorial problems", "Approximation algorithms for cycle packing problems", "The hungarian method for the assignment problem", "Algorithms for the assignment and transportation problems", "Bidding and allocation in combinatorial auctions", "The real power of artificial markets", "Efficiency of experimental security markets with insider information: An application of rational expectations models", "Rational expectations and the aggregation of diverse information in laboratory security markets", "Algorithm for optimal winner determination in combinatorial auctions", "Packing directed cycles efficiently"], "full_text": "1. INTRODUCTION Buying or selling a financial security in effect is a wager on the security\"s value. For example, buying a stock is a bet that the stock\"s value is greater than its current price. Each trader evaluates his expected profit to decide the quantity to buy or sell according to his own information and subjective probability assessment. The collective interaction of all bets leads to an equilibrium that reflects an aggregation of all the traders\" information and beliefs. In practice, this aggregate market assessment of the security\"s value is often more accurate than other forecasts relying on experts, polls, or statistical inference . Consider buying a security at price fifty-two cents, that pays $ US Presidential election. The transaction is a commitment to accept a fifty-two cent loss if a Democrat does not win in return for a forty-eight cent profit if a Democrat does win. In this case of an event-contingent security, the price-the market\"s value of the security-corresponds directly to the estimated probability of the event. Almost all existing financial and betting exchanges pair up bilateral trading partners. For example, one trader willing to accept an x dollar loss if a Democrat does not win in return for a y dollar profit if a Democrat wins is matched up with a second trader willing to accept the opposite. However in many scenarios, even if no bilateral agreements exist among traders, multilateral agreements may be possible. For example, if one trader bets that the Democratic candidate will receive more votes than the Republican candidate, a second trader bets that the Republican candidate will receive more votes than the Libertarian candidate, and a third trader bets that the Libertarian candidate will receive more votes than the Democratic candidate, then, depending on the odds they each offer, there may be a three-way agreeable match even though no two-way matches exist. We propose an exchange where traders have considerable flexibility to naturally and succinctly express their wagers, 326 and examine the computational complexity of the auctioneer\"s resulting matching problem of identifying bilateral and multilateral agreements. In particular, we focus on a setting where traders bet on the outcome of a competition among n candidates. For example, suppose that there are n candidates in an election (or n horses in a race, etc.) and thus n! possible orderings of candidates after the final vote tally. Traders may like to bet on arbitrary properties of the final ordering, for example candidate D will win, candidate D will finish in either first place or last place, candidate D will defeat candidate R, candidates D and R will both defeat candidate L, etc. The goal of the exchange is to search among all the offers to find two or more that together form an agreeable match. As we shall see, the matching problem can be set up as a linear or integer program, depending on whether orders are divisible or indivisible, respectively. Attempting to reduce the problem to a bilateral matching problem by explicitly creating n! securities, one for each possible final ordering, is both cumbersome for the traders and computationally infeasible even for modest sized n. Moreover, traders\" attention would be spread among n! independent choices, making the likelihood of two traders converging at the same time and place seem remote. There is a tradeoff between the expressiveness of the bidding language and the computational complexity of the matching problem. We want to offer traders the most expressive bidding language possible while maintaining computational feasibility. We explore two bidding languages that seem natural from a trader perspective. Subset betting, described in Section 3.2, allows traders to bet on which positions in the ranking a candidate will fall, for example candidate D will finish in position 1, 3-5, or 10. Symetrically, traders can also bet on which candidates will fall in a particular position. In Section 4, we derive a polynomial-time algorithm for matching (divisible) subset bets. The key to the result is showing that the exponentially big linear program has a corresponding separation problem that reduces to maximum weighted bipartite matching and consequently we can solve it in time polynomial in the number of orders. Pair betting, described in Section 3.3, allows traders to bet on the final ranking of any two candidates, for example candidate D will defeat candidate R. In Section 5, we show that optimal matching of (divisible or indivisible) pair bets is NP-hard, via a reduction from the unweighted minimum feedback arc set problem. We also provide a polynomiallyverifiable sufficient condition for the existence of a pairbetting match and show that a greedy algorithm offers poor approximation for indivisible pair bets. 2. BACKGROUND AND RELATED WORK We consider permutation betting, or betting on the outcome of a competition among n candidates. The final outcome or state s \u2208 S is an ordinal ranking of the n candidates. For example, the candidates could be horses in a race and the outcome the list of horses in increasing order of their finishing times. The state space S contains all n! mutually exclusive and exhaustive permutations of candidates. In a typical horse race, people bet on properties of the outcome like horse A will win, horse A will show, or finish in either first or second place, or horses A and B will finish in first and second place, respectively. In practice at the racetrack, each of these different types of bets are processed in separate pools or groups. In other words, all the win bets are processed together, and all the show bets are processed together, but the two types of bets do not mix. This separation can hurt liquidity and information aggregation. For example, even though horse A is heavily favored to win, that may not directly boost the horse\"s odds to show. Instead, we describe a central exchange where all bets on the outcome are processed together, thus aggregating liquidity and ensuring that informational inference happens automatically. Ideally, we\"d like to allow traders to bet on any property of the final ordering they like, stated in exactly the language they prefer. In practice, allowing too flexible a language creates a computational burden for the auctioneer attempting to match willing traders. We explore the tradeoff between the expressiveness of the bidding language and the computational complexity of the matching problem. We consider a framework where people propose to buy securities that pay $1 if and only if some property of the final ordering is true. Traders state the price they are willing to pay per share and the number of shares they would like to purchase. (Sell orders may not be explicitly needed, since buying the negation of an event is equivalent to selling the event.) A divisible order permits the trader to receive fewer shares than requested, as long as the price constraint is met; an indivisible order is an all-or-nothing order. The description of bets in terms of prices and shares is without loss of generality: we can also allow bets to be described in terms of odds, payoff vectors, or any of the diverse array of approaches practiced in financial and gambling circles. In principle, we can do everything we want by explicitly offering n! securities, one for every state s \u2208 S (or in fact any set of n! linearly independent securities). This is the so-called complete Arrow-Debreu securities market for our setting. In practice, traders do not want to deal with low-level specification of complete orderings: people think more naturally in terms of high-level properties of orderings. Moreover, operating n! securities is infeasible in practice from a computational point of view as n grows. A very simple bidding language might allow traders to bet only on who wins the competition, as is done in the win pool at racetracks. The corresponding matching problem is polynomial, however the language is not very expressive. A trader who believes that A will defeat B, but that neither will win outright cannot usefully impart his information to the market. The price space of the market reveals the collective estimates of win probabilities but nothing else. Our goal is to find languages that are as expressive and intuitive as possible and reveal as much information as possible, while maintaining computational feasibility. Our work is in direct analogy to work by Fortnow et. al. . Whereas we explore permutation combinatorics, Fortnow et. al. explore Boolean combinatorics. The authors consider a state space of the 2n possible outcomes of n binary variables. Traders express bets in Boolean logic. The authors show that divisible matching is co-NP-complete and indivisible matching is \u03a3p 2-complete. Hanson describes a market scoring rule mechanism which can allow betting on combinatorial number of outcomes. The market starts with a joint probability distribution across all outcomes. It works like a sequential version of a scoring rule. Any trader can change the probability distribution as long as he agrees to pay the most recent trader 327 according to the scoring rule. The market maker pays the last trader. Hence, he bears risk and may incur loss. Market scoring rule mechanisms have a nice property that the worst-case loss of the market maker is bounded. However, the computational aspects on how to operate the mechanism have not been fully explored. Our mechanisms have an auctioneer who does not bear any risk and only matches orders. Research on bidding languages and winner determination in combinatorial auctions considers similar computational challenges in finding an allocation of items to bidders that maximizes the auctioneer\"s revenue. Combinatorial auctions allow bidders to place distinct values on bundles of goods rather than just on individual goods. Uncertainty and risk are typically not considered and the central auctioneer problem is to maximize social welfare. Our mechanisms allow traders to construct bets for an event with n! outcomes. Uncertainty and risk are considered and the auctioneer problem is to explore arbitrage opportunities and risklessly match up wagers. 3. PERMUTATION BETTING In this section, we define the matching and optimal matching problems that an auctioneer needs to solve in a general permutation betting market. We then illustrate the problem definitions in the context of the subset-betting and pairbetting markets. 3.1 Securities, Orders and Matching Problems Consider an event with n competing candidates where the outcome (state) is a ranking of the n candidates. The bidding language of a market offering securities in the future outcomes determines the type and number of securities available and directly affects what information can be aggregated about the outcome. A fully expressive bidding language can capture any possible information that traders may have about the final ranking; a less expressive language limits the type of information that can be aggregated though it may enable a more efficient solution to the matching problem. For any bidding language and number of securities in a permutation betting market, we can succinctly represent the problem of the auctioneer to risklessly match offers as follows. Consider an index set of bets or orders O which traders submit to the auctioneer. Each order i \u2208 O is a triple (bi, qi, \u03c6i), where bi denotes how much the trader is willing to pay for a unit share of security \u03c6i and qi is the number of shares of the security he wants to purchase at price bi. Naturally, bi \u2208 (0, 1) since a unit of the security pays off at most $1 when the event is realized. Since order i is defined for a single security \u03c6i, we will omit the security variable whenever it is clear from the context. The auctioneer can accept or reject each order, or in a divisible world accept a fraction of the order. Let xi be the fraction of order i \u2208 O accepted. In the indivisible version of the market xi = 0 or 1 while in the divisible version xi \u2208 . Further let Ii(s) be the indicator variable for whether order i is winning in state s, that is Ii(s) = 1 if the order is paid back $1 in state s and Ii(s) = 0 otherwise. There are two possible problems that the auctioneer may want to solve. The simpler one is to find a subset of orders that can be matched risk-free, namely a subset of orders which accepted together give a nonnegative profit to the auctioneer in every possible outcome. We call this problem the existence of a match or sometimes simply, the matching problem. The more complex problem is for the auctioneer to find the optimal match with respect to some criterion such as profit, trading volume, etc. Definition 1 (Existence of match, indivisible orders). Given a set of orders O, does there exist a set of xi \u2208 {0, 1}, i \u2208 O, with at least one xi = 1 such that (bi \u2212 Ii(s))qixi \u2265 0, \u2200s \u2208 S? (1) Similarly we can define the existence of a match with divisible orders. Definition 2 (Existence of match, divisible orders). Given a set of orders O, does there exist a set of xi \u2208 , i \u2208 O, with at least one xi > 0 such that (bi \u2212 Ii(s))qixi \u2265 0, \u2200s \u2208 S? (2) The existence of a match is a decision problem. It only returns whether trade can occur at no risk to the auctioneer. In addition to the risk-free requirement, the auctioneer can optimize some criterion in determining the orders to accept. Some reasonable objectives include maximizing the total trading volume in the market or the worst-case profit of the auctioneer. The following optimal matching problems are defined for an auctioneer who maximizes his worst-case profit. Definition 3 (Optimal match, indivisible orders). Given a set of orders O, choose xi \u2208 {0, 1} such that the following mixed integer programming problem achieves its optimality max xi,c c (3) s.t. i bi \u2212 Ii(s) qixi \u2265 c, \u2200s \u2208 S xi \u2208 {0, 1}, \u2200i \u2208 O. Definition 4 (Optimal match, divisible orders). Given a set of orders O, choose xi \u2208 such that the following linear programming problem achieves its optimality max xi,c c (4) s.t. i bi \u2212 Ii(s) qixi \u2265 c, \u2200s \u2208 S 0 \u2264 xi \u2264 1, \u2200i \u2208 O. The variable c is the worst-case profit for the auctioneer. Note that, strictly speaking, the optimal matching problems do not require to solve the optimization problems (3) and (4), because only the optimal set of orders are needed. The optimal worst-case profit may remain unknown. 3.2 Subset Betting A subset betting market allows two different types of bets. Traders can bet on a subset of positions a candidate may end up at, or they can bet on a subset of candidates that will occupy a particular position. A security \u03b1|\u03a6 where \u03a6 is a subset of positions pays off $1 if candidate \u03b1 stands at a position that is an element of \u03a6 and it pays $0 otherwise. For example, security \u03b1|{2, 4} pays $1 when candidate \u03b1 328 is ranked second or fourth. Similarly, a security \u03a8|j where \u03a8 is a subset of candidates pays off $1 if any of the candidates in the set \u03a8 ranks at position j. For instance, security {\u03b1, \u03b3}|2 pays off $1 when either candidate \u03b1 or candidate \u03b3 is ranked second. The auctioneer in a subset betting market faces a nontrivial matching problem, that is to determine which orders to accept among all submitted orders i \u2208 O. Note that although there are only n candidates and n possible positions, the number of available securities to bet on is exponential since a trader may bet on any of the 2n subsets of candidates or positions. With this, it is not immediately clear whether one can even find a trading partner or a match for trade to occur, or that the auctioneer can solve the matching problem in polynomial time. In the next section, we will show that somewhat surprisingly there is an elegant polynomial solution to both the matching and optimal matching problems, based on classic combinatorial problems. When an order is accepted, the corresponding trader pays the submitted order price bi to the auctioneer and the auctioneer pays the winning orders $1 per share after the outcome is revealed. The auctioneer has to carefully choose which orders and what fractions of them to accept so as to be guaranteed a nonnegative profit in any future state. The following example illustrates the matching problem for indivisible orders in the subset-betting market. Example 1. Suppose n = 3. Objects \u03b1, \u03b2, and \u03b3 compete for positions 1, 2, and 3 in a competition. The auctioneer receives the following 4 orders: (1) buy 1 share \u03b1|{1} at price $0.6; (2) buy 1 share \u03b2|{1, 2} at price $0.7; (3) buy 1 share \u03b3|{1, 3} at price $0.8; and (4) buy 1 share \u03b2|{3} at price $0.7. There are 6 possible states of ordering: \u03b1\u03b2\u03b3, \u03b1\u03b3\u03b2, \u03b2\u03b1\u03b3, \u03b2\u03b3\u03b1, \u03b3\u03b1\u03b2,and \u03b3\u03b2\u03b1. The corresponding statedependent profit of the auctioneer for each order can be calculated as the following vectors, c1 = (\u22120.4, \u22120.4, 0.6, 0.6, 0.6, 0.6) c2 = (\u22120.3, 0.7, \u22120.3, \u22120.3, 0.7, \u22120.3) c3 = (\u22120.2, 0.8, \u22120.2, 0.8, \u22120.2, \u22120.2) c4 = ( 0.7, \u22120.3, 0.7, 0.7, \u22120.3, 0.7). 6 columns correspond to the 6 future states. For indivisible orders, the auctioneer can either accept orders (2) and (4) and obtain profit vector c = (0.4, 0.4, 0.4, 0.4, 0.4, 0.4), or accept orders (2), (3), and (4) and has profit across state c = (0.2, 1.2, 0.2, 1.2, 0.2, 0.2). 3.3 Pair Betting A pair betting market allows traders to bet on whether one candidate will rank higher than another candidate, in an outcome which is a permutation of n candidates. A security \u03b1 > \u03b2 pays off $ 1 if candidate \u03b1 is ranked higher than candidate \u03b2 and $ 0 otherwise. There are a total of N(N \u22121) different securities offered in the market, each corresponding to an ordered pair of candidates. Traders place orders of the form buy qi shares of \u03b1 > \u03b2 at price per share no greater than bi. bi in general should be between 0 and 1. Again the order can be either indivisible or divisible and the auctioneer needs to decide what fraction xi of each order to accept so as not to incur any loss, with .99 .99 .5 .5 .5 .99 .99 .99 .99 Figure 1: Every cycle has negative worst-case profit of \u22120.02 (for the cycles of length 4) or less (for the cycles of length 6), however accepting all edges in full gives a positive worst-case profit of 0.44. xi \u2208 {0, 1} for indivisible and xi \u2208 for divisible orders. The same definitions for existence of a match and optimal match from Section 3.1 apply. The orders in the pair-betting market have a natural interpretation as a graph, where the candidates are nodes in the graph and each order which ranks a pair of candidates \u03b1 > \u03b2 is represented by a directed edge e = (\u03b1, \u03b2) with price be and weight qe. With this interpretation, it is tempting to assume that a necessary condition for a match is to have a cycle in the graph with a nonnegative worst-case profit. Assuming qe = 1 for all e, this is a cycle C with a total of |C| edges such that the worst-case profit for the auctioneer is e\u2208C be \u2212 (|C| \u2212 1) \u2265 0, since in the worst-case state the auctioneer needs to pay $,1 to every order in the cycle except one. However, the example in Figure 1 shows that this is not the case: we may have a set of orders in which every single cycle has a negative worst-case profit, and yet there is a positive worstcase match overall. The edge labels in the figure are the prices be; both the optimal divisible and indivisible solution in this case accept all orders in full, xe = 1. 4. COMPLEXITY OF SUBSET BETTING The matching problems of the auctioneer in any permutation market, including the subset betting market have n! constraints. Brute-force methods would take exponential time to solve. However, given the special form of the securities in the subset betting market, we can show that the matching problems for divisible orders can be solved in polynomial time. Theorem 1. The existence of a match and the optimal match problems with divisible orders in a subset betting market can both be solved in polynomial time. 329 Proof. Consider the linear programming problem (4) for finding an optimal match. This linear program has |O| + 1 variables, one variable xi for each order i and the profit variable c. It also has exponentially many constraints. However, we can solve the program in time polynomial in the number of orders |O| by using the ellipsoid algorithm, as long as we can efficiently solve its corresponding separation problem in polynomial time . The separation problem for a linear program takes as input a vector of variable values and returns if the vector is feasible, or otherwise it returns a violated constraint. For given values of the variables, a violated constraint in Eq. (4) asks whether there is a state or permutation s in which the profit is less than c, and can be rewritten as Ii(s)qixi < biqixi \u2212 c \u2200s \u2208 S. (5) Thus it suffices to show how to find efficiently a state s satisfying the above inequality (5) or verify that the opposite inequality holds for all states s. We will show that the separation problem can be reduced to the maximum weighted bipartite matching1 problem . The left hand side in Eq. (5) is the total money that the auctioneer needs to pay back to the winning traders in state s. The first term on the right hand side is the total money collected by the auctioneer and it is fixed for a given solution vector of xi\"s and c. A weighted bipartite graph can be constructed between the set of candidates and the set of positions. For every order of the form \u03b1|\u03a6 there are edges from candidate node \u03b1 to every position node in \u03a6. For orders of the form \u03a8|j there are edges from each candidate in \u03a8 to position j. For each order i we put weight qixi on each of these edges. All multi-edges with the same end points are then replaced with a single edge that carries the total weight of the multi-edge. Every state s then corresponds to a perfect matching in the bipartite graph. In addition, the auctioneer pays out to the winners the sum of all edge weights in the perfect matching since every candidate can only stand in one position and every position is taken by one candidate. Thus, the auctioneer\"s worst-cast state and payment are the solution to the maximum weighted bipartite matching problem, which has known polynomial-time algorithms . Hence, the separation problem can be solved in polynomial time. Naturally, if the optimal solution to (4) gives a worst-case profit of c\u2217 > 0, there exists a matching. Thus, the matching problem can be solved in polynomial time also. 5. COMPLEXITY OF PAIR BETTING In this section we show that a slight change of the bidding language may bring about a dramatic change in the complexity of the optimal matching problem of the auctioneer. In particular, we show that finding the optimal match in the pair betting market is NP-hard for both divisible and indivisible orders. We then identify a polynomially-verifiable sufficient condition for deciding the existence of a match. The hardness results are surprising especially in light of the observation that a pair betting market has a seemingly more restrictive bidding language which only offers n(n\u22121) The notion of perfect matching in a bipartite graph, which we use only in this proof, should not be confused with the notion of matching bets which we use throughout the paper. securities. In contrast, the subset betting market enables traders to bet on an exponential number of securities and yet had a polynomial time solution for finding the optimal match. Our hope is that the comparison of the complexities of the subset and pair betting markets would offer insight into what makes a bidding language expressive while at the same time enabling an efficient matching solution. In all analysis that follows, we assume that traders submit unit orders in pair betting markets, that is qi = 1. A set of orders O received by the auctioneer in a pair betting market with unit orders can be represented by a directed graph, G(V, E), where the vertex set V contains candidates that traders bet on. An edge e \u2208 E, denoted (\u03b1, \u03b2, be), represents an order to buy 1 share of the security \u03b1 > \u03b2 at price be. All edges have equal weight of 1. We adopt the following notations throughout the paper: \u2022 G(V, E): original equally weighted directed graph for the set of unit orders O. \u2022 be: price of the order for edge e. \u2022 G\u2217 (V \u2217 , E\u2217 ); a weighted directed graph of accepted orders for optimal matching, where edge weight xe is the quantity of order e accepted by the auctioneer. xe = 1 for indivisible orders and 0 < xe \u2264 1 for divisible orders. \u2022 H(V, E): a generic weighted directed graph of accepted orders. \u2022 k(H): solution to the unweighted minimum feedback arc set problem on graph H. k(H) is the minimum number of edges to remove so that H becomes acyclic. \u2022 l(H): solution to the weighted minimum feedback arc set problem on graph H. l(H) is the minimum total weights for the set of edges which, when removed, leave H acyclic. \u2022 c(H): worst-case profit of the auctioneer if he accepts all orders in graph H. \u2022 : a sufficiently small positive real number. Where not stated, < 1/(2|E|) for a graph H(V, E). In other cases, the value is determined in context. A feedback arc set of a directed graph is a set of edges which, when removed from the graph, leave a directed acyclic graph (DAG). Unweighted minimum feedback arc set problem is to find a feedback arc set with the minimum cardinality, while weighted minimum feedback arc set problem seeks to find a feedback arc set with the minimum total edge weight. Both unweighted and weighted minimum feedback arc set problems have been shown to be NP-complete . We will use this result in our complexity analysis on pair betting markets. 5.1 Optimal Indivisible Matching The auctioneer\"s optimal indivisible matching problem is introduced in Definition 3 of Section 3. Assuming unit orders and considering the order graph G(V, E), we restate the auctioneer\"s optimal matching problem in a pair betting market as picking a subset of edges to accept such that 330 worst-case profit is maximized in the following optimization problem, max xe,c c (6) s.t. e be \u2212 Ie(s) xe \u2265 c, \u2200s \u2208 S xe \u2208 {0, 1}, \u2200e \u2208 E. Without lose of generality, we assume that there are no multi-edges in the order graph G. We show that the optimal matching problem for indivisible orders is NP-hard via a reduction from the unweighted minimum feedback arc set problem. The latter takes as input a directed graph, and asks what is the minimum number of edges to delete from the graph so as to be left with a DAG. Our hardness proof is based on the following lemmas. Lemma 2. Suppose the auctioneer accepts all edges in an equally weighted directed graph H(V, E) with edge price be = (1 \u2212 ) and edge weight xe = 1. Then the worst-case profit is equal to k(H) \u2212 |E|, where k(H) is the solution to the unweighted minimum feedback arc problem on H. Proof. If the order of an edge gets $1 payoff at the end of the market we call the edge a winning edge, otherwise it is called a losing edge. For any state s, all winning edges necessarily form a DAG. Conversely, for every DAG there is a state in which the DAG edges are winners (though the remaining edges in G are not necessarily losers). Suppose that in state s there are ws winning edges and ls = |E| \u2212 ws losing edges. Then, ls is the cardinality of a feedback arc set that consists of all losing edges in state s. The edges that remain after deleting the minimum feedback arc set form the maximum DAG for the graph H. Consider the state smax in which all edges of the maximum DAG are winners. This gives the maximum number of winning edges wmax. All other edges are necessarily losers in the state smax, since any edge which is not in the max DAG must form a cycle together with some of the DAG edges. The number of losing edges in state smax is the cardinality of the minimum feedback arc set of H, that is |E| \u2212 wmax = k(H). The profit of the auctioneer in a state s is profit(s) = e\u2208E be \u2212 w = (1 \u2212 )|E| \u2212 w \u2265 (1 \u2212 )|E| \u2212 wmax, where equality holds when s = smax. Thus, the worst-case profit is achieved at state smax, profit(smax) = (|E| \u2212 wmax) \u2212 |E| = k(H) \u2212 |E|. Consider the graph of accepted orders for optimal matching, G\u2217 (V \u2217 , E\u2217 ), which consists of the optimal subset of edges E\u2217 to be accepted by the auctioneer, that is edges with xe = 1 in the solution of the optimization problem (6). We have the following lemma. Lemma 3. If the edge prices are be = (1\u2212 ), then the optimal indivisible solution graph G\u2217 has the same unweighted minimum feedback arc set size as the graph of all orders G, that is k(G\u2217 ) = k(G). Furthermore, G\u2217 is the smallest such subgraph of G, i.e., it is the subgraph of G with the smallest number of edges, that has the same size of unweighted minimum feedback arc set as G. Proof. G\u2217 is a subgraph of G, hence the minimum number of edges to break cycles in G\u2217 is no more than that in G, namely k(G\u2217 ) \u2264 k(G). Suppose k(G\u2217 ) < k(G). Since both k(G\u2217 ) and k(G) are integers, for any < 1 |E| we have that k(G\u2217 ) \u2212 |E\u2217 | < k(G)\u2212 |E|. Hence by Lemma 2, the auctioneer has a higher worst-case profit by accepting G than accepting G\u2217 , which contradicts the optimality of G\u2217 . Finally, the worst-case profit k(G) \u2212 |E\u2217 | is maximized when |E\u2217 | is minimized. Hence, G\u2217 is the smallest subgraph of G such that k(G\u2217 ) = k(G). The above two lemmas prove that the maximum worstcase profit in the optimal indivisible matching is directly related to the size of the minimum feedback arc set. Thus computing each automatically gives the other, hence computing the maximum worst-case profit in the indivisible pair betting problem is NP-hard. Theorem 4. Computing the maximum worst-case profit in indivisible pair betting is NP-hard. Proof. By Lemma 3, the maximum worst-case profit which is the optimum to the mixed integer programming problem (6), is k(G) \u2212 |E\u2217 |, where |E\u2217 | is the number of accepted edges. Since k(G) is integer and |E\u2217 | \u2264 |E| < 1, solving (6) will automatically give us the cardinality of the minimum feedback arc set of G, k(G). Because the minimum feedback arc set problem is NP-complete , computing the maximum worst-case profit is NP-hard. Theorem 4 states that solving the optimization problem is hard, because even if the optimal set of orders are provided computing the optimal worst-case profit from accepting those orders is NP-hard. However, it does not imply whether the optimal matching problem, i.e. finding the optimal set of orders to accept, is NP-hard. It is possible to be able to determine which edges in a graph participating in the optimal match, yet unable to compute the corresponding worst-case profit. Next, we prove that the indivisible optimal matching problem is actually NP-hard. We will use the following short fact repeatedly. Lemma 5 (Edge removal lemma). Given a weighted graph H(V, E), removing a single edge e with weight xe from the graph decreases the weighted minimum feedback arc set solution l(H) by no more than xe and reduces the unweighted minimum feedback arc set solution k(H) by no more than 1. Proof. Suppose the weighted minimum feedback arc set for the graph H \u2212 {e} is F. Then F \u222a {e} is a feedback arc set for H, and has total edge weight l(H\u2212{e})+xe. Because l(H) is the solution to the weighted minimum feedback arc set problem on H, we have l(H) \u2264 l(H \u2212{e})+xe, implying that l(H \u2212 {e}) \u2265 l(H) \u2212 xe. Similarly, suppose the unweighted minimum feedback arc set for the graph H \u2212 {e} is F . Then F \u222a {e} is a feedback arc set for H, and has set cardinality k(H\u2212{e})+1. Because k(H) is the solution to the unweighted minimum feedback arc set problem on H, we have k(H) \u2264 k(H \u2212 {e}) + 1, giving that k(H \u2212 {e}) \u2265 k(H) \u2212 1. Theorem 6. Finding the optimal match in indivisible pair betting is NP-hard. 331 Proof. We reduce from the unweighted minimum feedback arc set problem again, although with a slightly more complex polynomial transformation involving multiple calls to the optimal match oracle. Consider an instance graph G of the minimum feedback arc set problem. We are interested in computing k(G), the size of the minimum feedback arc set of G. Suppose we have an oracle which solves the optimal matching problem. Denote by optimal match(G ) the output of the optimal matching oracle on graph G with prices be = (1\u2212 ) on all its edges. By Lemma 3, on input G , the oracle optimal match returns the subgraph of G with the smallest number of edges, that has the same size of minimum feedback arc set as G . The following procedure finds k(G) by using polynomially many calls to the optimal match oracle on a sequence of subgraphs of G. set G := G iterations := 0 while (G has nonempty edge set) reset G := optimal match(G ) if (G has nonempty edge set) increment iterations by 1 reset G by removing any edge e end if end while return (iterations) This procedure removes edges from the original graph G layer by layer until the graph is empty, while at the same time computing the minimum feedback arc set size k(G) of the original graph as the number of iterations. In each iteration, we start with a graph G and replace it with the smallest subgraph G that has the same k(G ). At this stage, removing an additional edge e necessarily results in k(G \u2212{e}) = k(G )\u22121, because k(G \u2212{e}) < k(G ) by the optimality of G , and k(G \u2212 {e}) \u2265 k(G ) \u2212 1 by the edgeremoval lemma. Therefore, in each iteration the cardinality of the minimum feedback arc set gets reduced exactly by 1. Hence the number of iterations is equal to k(G). Note that this procedure gives a polynomial transformation from the optimal matching problem to the unweighted minimum feedback arc set problem, which calls the optimal matching oracle exactly k(G) \u2264 |E| times, where |E| is the number of edges of G. Hence the optimal matching problem is NP-hard. 5.2 Optimal Divisible Matching When orders are divisible, the auctioneer\"s optimal matching problem is described in Definition 4 of Section 3. Assuming unit orders and considering the order graph G(V, E), we restate the auctioneer\"s optimal matching problem for divisible orders as choosing quantity of orders to accept, xe \u2208 , such that worst-case profit is maximized in the following linear programming problem, max xe,c c (7) s.t. e be \u2212 Ie(s) xe \u2265 c, \u2200s \u2208 S xe \u2208 , \u2200e \u2208 E. We still assume that there are no multi-edges in the order graph G. When orders are divisible, the auctioneer can be better off by accepting partial orders. Example 2 shows a situation when accepting partial orders generates higher worst-case profit than the optimal indivisible solution. Example 2. We show that the linear program (7) sometimes has a non-integer optimal solution. Figure 2: An order graph. Letters on edges represent order prices. Consider the graph in Figure 2. There are a total of five cycles in the graph: three four-edge cycles ABCD, ABEF, CDEF, and two six-edge cycles ABCDEF and ABEFCD. Suppose each edge has price b such that 4b \u2212 3 > 0 and 6b \u2212 5 < 0, namely b \u2208 (.75, .80), for example b = .78. With this, the optimal indivisible solution consists of at most one four-edge cycle, with worst case profit (4b\u22123). On the other hand, taking 1 fraction of each of the three four-edge cycles would yield higher worst-case profit of 3 (4b \u2212 3). Despite the potential profit increase for accepting divisible orders, the auctioneer\"s optimal matching problem remains to be NP-hard for divisible orders, which is presented below via several lemmas and theorems. Lemma 7. Suppose the auctioneer accept orders described by a weighted directed graph H(V, E) with edge weight xe to be the quantity accepted for edge order e. The worst-case profit for the auctioneer is c(H) = e\u2208E (be \u2212 1)xe + l(H). (8) Proof. For any state s, the winning edges form a DAG. Thus, the worst-case profit for the auctioneer achieves at the state(s) when the total quantity of losing orders is minimized. The minimum total quantity of losing orders is the solution to weighted minimal feedback arc set problem on H, that is l(H). Consider the graph of accepted orders for optimal divisible matching, G\u2217 (V \u2217 , E\u2217 ), which consists of the optimal subset of edges E\u2217 to be accepted by the auctioneer, with edge weight xe > 0 getting from the optimal solution of the linear program (7). We have the following lemmas. 332 Lemma 8. l(G\u2217 ) \u2264 k(G\u2217 ) \u2264 k(G). Proof. l(G\u2217 ) is the solution of the weighted minimum feedback arc set problem on G\u2217 , while k(G\u2217 ) is the solution of the unweighted minimum feedback arc set problem on G\u2217 . When all edge weights in G\u2217 are 1, l(G\u2217 ) = k(G\u2217 ). When xe\"s are less than 1, l(G\u2217 ) can be less than or equal to k(G\u2217 ). Since G\u2217 is a subgraph of G but possibly with different edge weights, k(G\u2217 ) \u2264 k(G). Hence, we have the above relation. Lemma 9. There exists some such that when all edge prices be\"s are (1 \u2212 ), l(G\u2217 ) = k(G). Proof. From lemma 8, l(G\u2217 ) \u2264 k(G). We know that the auctioneer\"s worst-case profit when accepting G\u2217 is c(G\u2217 ) = e\u2208E\u2217 (be \u2212 1)xe + l(G\u2217 ) = l(G\u2217 ) \u2212 e\u2208E\u2217 xe. When he accepts the original order graph G in full, his worstcase profit is c(G) = e\u2208E (be \u2212 1) + k(G) = k(G) \u2212 |E|. Suppose l(G\u2217 ) < k(G). If |E| \u2212 e\u2208E\u2217 xe = 0, it means that G\u2217 is G. Hence, l(G\u2217 ) = k(G) regardless of , which contradicts with the assumption l(G\u2217 ) < k(G). If |E| \u2212 e\u2208E\u2217 xe > 0, then when k(G) \u2212 l(G\u2217 |E| \u2212 e\u2208E\u2217 xe c(G) is strictly greater than c(G\u2217 ), contradicting with the optimality of c(G\u2217 ). Because xe\"s are less than 1, l(G\u2217 ) > k(G) is impossible. Thus, l(G\u2217 ) = k(G). Theorem 10. Finding the optimal worst-case profit in divisible pair betting is NP-hard. Proof. Given the optimal set of partial orders to accept for G when edge weights are (1 \u2212 ), if we can calculate the optimal worst-case profit, by lemma 9 we can solve the unweighted minimum feedback arc set problem on G, which is NP-hard. Hence, finding the optimal worst-case profit is NP-hard. Theorem 10 states that solving the linear program (7) is NP-hard. Similarly to the indivisible case, we still need to prove that just finding the optimal divisible match is hard, as opposed to being able to compute the optimal worstcase profit. Since in the divisible case the edges do not necessarily have unit weights, the proof in Theorem 6 does not apply directly. However, with an additional property of the divisible case, we can augment the procedure from the indivisible hardness proof to compute the unweighted minimum feedback arc set size k(G) here as well. First, note that the optimal divisible subgraph G\u2217 of a graph G is the weighted subgraph with minimum weighted feedback arc set size l(G\u2217 ) = k(G) and smallest sum of edge weights e\u2208E\u2217 xe, since its corresponding worst case profit is k(G) \u2212 e\u2208E\u2217 xe according to lemmas 7 and 9. Lemma 11. Suppose graph H satisfies l(H) = k(H) and we remove edge e from it with weight xe < 1. Then, k(H \u2212 {e}) = k(H). Proof. Assume the contrary, namely k(H\u2212{e}) < k(H). Then by Lemma 5, k(H \u2212 {e}) = k(H) \u2212 1. Since removing a single edge cannot reduce the minimum feedback arc set by more than the edge weight, l(H) \u2212 xe \u2264 l(H \u2212 {e}). (9) On the other hand H \u2212 {e} \u2282 H so we have, l(H \u2212 {e}) \u2264 k(H \u2212 {e}) = k(H) \u2212 1 = l(H) \u2212 1. (10) Combining (9) and (10), we get xe \u2265 1. The contradiction arises. Therefore, removing any edge with less than unit weight from an optimal divisible graph does not change k(H), the minimal feedback arc set size of the unweighted version of the graph. We now can augment the procedure for the indivisible case in Theorem 6, to prove hardness of the divisible version, as follows. Theorem 12. Finding the optimal match in divisible pair betting is NP-hard. Proof. We reduce from the unweighted minimum feedback arc set problem for graph G. Suppose we have an oracle for the optimal divisible problem called optimal divisible match, which on input graph H computes edge weights xe \u2208 (0, 1] for the optimal subgraph H\u2217 of H, satisfying l(H\u2217 ) = k(H). The following procedure outputs k(G). set G := G iterations := 0 while (G has nonempty edge set) reset G := optimal divisible match(G ) while (G has edges with weight < 1) remove an edge with weight < 1 from G reset G by setting all edge weights to 1 reset G := optimal divisible match(G ) end while if (G has nonempty edge set) increment iterations by 1 reset G by removing any edge e end if end while return (iterations) As in the proof of the corresponding Theorem 6 for the indivisible case, we compute k(G) by iteratively removing edges and recomputing the optimal divisible solution on the remaining subgraph, until all edges are deleted. In each iteration of the outer while loop, the minimum feedback arc set is reduced by 1, thus the number of iterations is equal to k(G). It remains to verify that each iteration reduces k(G) by exactly 1. Starting from a graph at the beginning of an iteration, we compute its optimal divisible subgraph. We then keep removing one non-unit weight edge at a time and recomputing the optimal divisible subgraph, until the latter contains only edges with unit weight. By Lemma 11 throughout the iteration so far the minimum feedback arc set of the corresponding unweighted graph remains unchanged. Once the oracle returns a graph G with unit edge weights, removing any edge would reduce the minimum feedback arc set: otherwise G is not optimal since G \u2212 {e} would have 333 the same minimum feedback arc set but smaller total edge weight. By Lemma 5 removing a single edge cannot reduce the minimum feedback arc set by more than one, thus as all edges have unit weight, k(G ) gets reduced by exactly one. k(G) is equal to the returned value from the procedure. Hence, the optimal matching problem for divisible orders is NP-hard. 5.3 Existence of a Match Knowing that the optimal matching problem is NP-hard for both indivisible and divisible orders in pair betting, we check whether the auctioneer can identify the existence of a match with ease. Lemma 13 states a sufficient condition for the matching problem with both indivisible and divisible orders. Lemma 13. A sufficient condition for the existence of a match for pair betting is that there exists a cycle C in G such that, e\u2208C be \u2265 |C| \u2212 1, (11) where |C| is the number of edges in the cycle C. Proof. The left-hand side of the inequality (11) represents the total payment that the auctioneer receives by accepting every unit orders in the cycle C in full. Because the direction of an edge represents predicted ordering of the two connected nodes in the final ranking, forming a cycle meaning that there is some logical contradiction on the predicted orderings of candidates. Hence, whichever state is realized, not all of the edges in the cycle can be winning edges. The worst-case for the auctioneer corresponds to a state where every edge in the cycle gets paid by $ 1 except one, with |C| \u2212 1 be the maximum payment to traders. Hence, if inequality (11) is satisfied, the auctioneer has non-negative worst-case profit by accepting the orders in the cycle. It can be shown that identifying such a non-negative worstcase profit cycle in an order graph G can be achieved in polynomial time. Lemma 14. It takes polynomial time to find a cycle in an order graph G(V, E) that has the highest worst-case profit, that is max C\u2208C e\u2208C be \u2212 (|C| \u2212 1) , where C is the set of all cycles in G. Proof. Because e\u2208C be \u2212 (|C| \u2212 1) = e\u2208C (be \u2212 1) + 1 = 1 \u2212 e\u2208C (1 \u2212 be), finding the cycle that gives the highest worst-case profit in the original order graph G is equivalent to finding the shortest cycle in a converted graph H(V, E), where H is achieved by setting the weight for edge e in G to be (1 \u2212 be). Finding the shortest cycle in graph H can be done within polynomial time by resorting to the shortest path problem. For any vertex v in V , we consider every neighbor vertex w such that (v, w) \u2208 E. We then find the shortest path from w to v, denoted as path(w, v). The shortest cycle that passes vertex v is found by choosing the w such that e(v,w) + path(w, v) is minimized. Comparing the shortest cycle found for every vertex, we then can determine the shortest overall cycle for the graph H. Because the short path problem can be solved in polynomial time , we can find the solution to our problem in polynomial time. If the worst-case profit for the optimal cycle is non-negative, we know that there exists a match in G. However, the condition in lemma 13 is not a necessary condition for the existence of a match. Even if all single cycles in the order graph have negative worst-case profit, the auctioneer may accept multiple interweaving cycles to have positive worstcase profit. Figure 1 exhibits such a situation. If the optimal indivisible match consists only of edge disjoint cycles, a natural greedy algorithm can find the cycle that gives the highest worst-case profit, remove its edges from the graph, and proceed until no more cycles exist. However, we show that such greedy algorithm can give a very poor approximation. n + 1 n + 1 n + 1 n + 1 n + 1 n + 1 Figure 3: Graph with n vertices and n + n edges on which the greedy algorithm finds only two cycles, the dotted cycle in the center and the unique remaining cycle. The labels in the faces give the number of edges in the corresponding cycle. Lemma 15. The greedy algorithm gives at most an O( n)approximation to the maximum number of disjoint cycles. Proof. Consider the graph in Figure 3 consisting of a cycle with n edges, each of which participates in another (otherwise disjoint) cycle with n + 1 edges. Suppose all edge weights are (1 \u2212 ). The maximum number of disjoint cycles is clearly n, taking all cycles with length n + 1. Because smaller cycles gives higher worst-case profit, the greedy algorithm would first select the cycle of length n, after which there would be only one remaining cycle of length n. Thus the total number of cycles selected by greedy is 2 and the approximation factor in this case is n/2. In light of Lemma 15, one may expect that greedy algorithms would give n-approximations at best. Approxima334 tion algorithms for finding the maximum number of edgedisjoint cycles have been considered by Krivelevich, Nutov and Yuster . Indeed, for the case of directed graphs, the authors show that a greedy algorithm gives a\u221a n-approximation . When the optimal match does not consist of edge-disjoint cycles as in the example of Figure 3, greedy algorithm trying to finding optimal single cycles fails obviously. 6. CONCLUSION We consider a permutation betting scenario, where traders wager on the final ordering of n candidates. While it is unnatural and intractable to allow traders to bet directly on the n! different final orderings, we propose two expressive betting languages, subset betting and pair betting. In a subset betting market, traders can bet either on a subset of positions that a candidate stands or on a subset of candidates who occupy a specific position in the final ordering. Pair betting allows traders bet on whether one given candidate ranks higher than another given candidate. We examine the auctioneer problem of matching orders without incurring risk. We find that in a subset betting market an auctioneer can find the optimal set and quantity of orders to accept such that his worst-case profit is maximized in polynomial time if orders are divisible. The complexity changes dramatically for pair betting. We prove that the optimal matching problem for the auctioneer is NP-hard for pair betting with both indivisible and divisible orders via reductions to the minimum feedback arc set problem. We identify a sufficient condition for the existence of a match, which can be verified in polynomial time. A natural greedy algorithm has been shown to give poor approximation for indivisible pair betting. Interesting open questions for our permutation betting include the computational complexity of optimal indivisible matching for subset betting and the necessary condition for the existence of a match in pair betting markets. We are interested in further exploring better approximation algorithms for pair betting markets. 7. ACKNOWLEDGMENTS We thank Ravi Kumar, Yishay Mansour, Amin Saberi, Andrew Tomkins, John Tomlin, and members of Yahoo! Research for valuable insights and discussions.", "body1": "Buying or selling a financial security in effect is a wager on the security\"s value. Consider buying a security at price fifty-two cents, that pays $ US Presidential election. However in many scenarios, even if no bilateral agreements exist among traders, multilateral agreements may be possible. We propose an exchange where traders have considerable flexibility to naturally and succinctly express their wagers, 326 and examine the computational complexity of the auctioneer\"s resulting matching problem of identifying bilateral and multilateral agreements. Traders may like to bet on arbitrary properties of the final ordering, for example candidate D will win, candidate D will finish in either first place or last place, candidate D will defeat candidate R, candidates D and R will both defeat candidate L, etc. Moreover, traders\" attention would be spread among n! There is a tradeoff between the expressiveness of the bidding language and the computational complexity of the matching problem. We consider permutation betting, or betting on the outcome of a competition among n candidates. For example, the candidates could be horses in a race and the outcome the list of horses in increasing order of their finishing times. In a typical horse race, people bet on properties of the outcome like horse A will win, horse A will show, or finish in either first or second place, or horses A and B will finish in first and second place, respectively. Ideally, we\"d like to allow traders to bet on any property of the final ordering they like, stated in exactly the language they prefer. We consider a framework where people propose to buy securities that pay $1 if and only if some property of the final ordering is true. In principle, we can do everything we want by explicitly offering n! A very simple bidding language might allow traders to bet only on who wins the competition, as is done in the win pool at racetracks. al. Hanson describes a market scoring rule mechanism which can allow betting on combinatorial number of outcomes. Market scoring rule mechanisms have a nice property that the worst-case loss of the market maker is bounded. Combinatorial auctions allow bidders to place distinct values on bundles of goods rather than just on individual goods. Uncertainty and risk are typically not considered and the central auctioneer problem is to maximize social welfare. In this section, we define the matching and optimal matching problems that an auctioneer needs to solve in a general permutation betting market. 3.1 Securities, Orders and Matching Problems Consider an event with n competing candidates where the outcome (state) is a ranking of the n candidates. Naturally, bi \u2208 (0, 1) since a unit of the security pays off at most $1 when the event is realized. The auctioneer can accept or reject each order, or in a divisible world accept a fraction of the order. There are two possible problems that the auctioneer may want to solve. Given a set of orders O, does there exist a set of xi \u2208 {0, 1}, i \u2208 O, with at least one xi = 1 such that (bi \u2212 Ii(s))qixi \u2265 0, \u2200s \u2208 S? Definition 2 (Existence of match, divisible orders). Given a set of orders O, does there exist a set of xi \u2208 , i \u2208 O, with at least one xi > 0 such that (bi \u2212 Ii(s))qixi \u2265 0, \u2200s \u2208 S? Given a set of orders O, choose xi \u2208 {0, 1} such that the following mixed integer programming problem achieves its optimality max xi,c c (3) s.t. Definition 4 (Optimal match, divisible orders). Given a set of orders O, choose xi \u2208 such that the following linear programming problem achieves its optimality max xi,c c (4) s.t. The variable c is the worst-case profit for the auctioneer. Note that, strictly speaking, the optimal matching problems do not require to solve the optimization problems (3) and (4), because only the optimal set of orders are needed. 3.2 Subset Betting A subset betting market allows two different types of bets. Traders can bet on a subset of positions a candidate may end up at, or they can bet on a subset of candidates that will occupy a particular position. For example, security \u03b1|{2, 4} pays $1 when candidate \u03b1 328 is ranked second or fourth. The auctioneer in a subset betting market faces a nontrivial matching problem, that is to determine which orders to accept among all submitted orders i \u2208 O. The following example illustrates the matching problem for indivisible orders in the subset-betting market. Example 1. 3.3 Pair Betting A pair betting market allows traders to bet on whether one candidate will rank higher than another candidate, in an outcome which is a permutation of n candidates. Traders place orders of the form buy qi shares of \u03b1 > \u03b2 at price per share no greater than bi. The same definitions for existence of a match and optimal match from Section 3.1 apply. The orders in the pair-betting market have a natural interpretation as a graph, where the candidates are nodes in the graph and each order which ranks a pair of candidates \u03b1 > \u03b2 is represented by a directed edge e = (\u03b1, \u03b2) with price be and weight qe. Assuming qe = 1 for all e, this is a cycle C with a total of |C| edges such that the worst-case profit for the auctioneer is e\u2208C be \u2212 (|C| \u2212 1) \u2265 0, since in the worst-case state the auctioneer needs to pay $,1 to every order in the cycle except one. The matching problems of the auctioneer in any permutation market, including the subset betting market have n! 329 Proof. We will show that the separation problem can be reduced to the maximum weighted bipartite matching1 problem . The left hand side in Eq. In this section we show that a slight change of the bidding language may bring about a dramatic change in the complexity of the optimal matching problem of the auctioneer. securities. In all analysis that follows, we assume that traders submit unit orders in pair betting markets, that is qi = 1. \u2022 be: price of the order for edge e. \u2022 G\u2217 (V \u2217 , E\u2217 ); a weighted directed graph of accepted orders for optimal matching, where edge weight xe is the quantity of order e accepted by the auctioneer. \u2022 H(V, E): a generic weighted directed graph of accepted orders. \u2022 k(H): solution to the unweighted minimum feedback arc set problem on graph H. k(H) is the minimum number of edges to remove so that H becomes acyclic. \u2022 l(H): solution to the weighted minimum feedback arc set problem on graph H. l(H) is the minimum total weights for the set of edges which, when removed, leave H acyclic. \u2022 c(H): worst-case profit of the auctioneer if he accepts all orders in graph H. \u2022 : a sufficiently small positive real number. A feedback arc set of a directed graph is a set of edges which, when removed from the graph, leave a directed acyclic graph (DAG). 5.1 Optimal Indivisible Matching The auctioneer\"s optimal indivisible matching problem is introduced in Definition 3 of Section 3. We show that the optimal matching problem for indivisible orders is NP-hard via a reduction from the unweighted minimum feedback arc set problem. Our hardness proof is based on the following lemmas. Lemma 2. Proof. The edges that remain after deleting the minimum feedback arc set form the maximum DAG for the graph H. Consider the state smax in which all edges of the maximum DAG are winners. Consider the graph of accepted orders for optimal matching, G\u2217 (V \u2217 , E\u2217 ), which consists of the optimal subset of edges E\u2217 to be accepted by the auctioneer, that is edges with xe = 1 in the solution of the optimization problem (6). We have the following lemma. Lemma 3. Suppose k(G\u2217 ) < k(G). The above two lemmas prove that the maximum worstcase profit in the optimal indivisible matching is directly related to the size of the minimum feedback arc set. Theorem 4. Theorem 4 states that solving the optimization problem is hard, because even if the optimal set of orders are provided computing the optimal worst-case profit from accepting those orders is NP-hard. Similarly, suppose the unweighted minimum feedback arc set for the graph H \u2212 {e} is F . 331 Proof. Suppose we have an oracle which solves the optimal matching problem. set G := G iterations := 0 while (G has nonempty edge set) reset G := optimal match(G ) if (G has nonempty edge set) increment iterations by 1 reset G by removing any edge e end if end while return (iterations) This procedure removes edges from the original graph G layer by layer until the graph is empty, while at the same time computing the minimum feedback arc set size k(G) of the original graph as the number of iterations. Note that this procedure gives a polynomial transformation from the optimal matching problem to the unweighted minimum feedback arc set problem, which calls the optimal matching oracle exactly k(G) \u2264 |E| times, where |E| is the number of edges of G. Hence the optimal matching problem is NP-hard. 5.2 Optimal Divisible Matching When orders are divisible, the auctioneer\"s optimal matching problem is described in Definition 4 of Section 3. Assuming unit orders and considering the order graph G(V, E), we restate the auctioneer\"s optimal matching problem for divisible orders as choosing quantity of orders to accept, xe \u2208 , such that worst-case profit is maximized in the following linear programming problem, max xe,c c (7) s.t. We still assume that there are no multi-edges in the order graph G. When orders are divisible, the auctioneer can be better off by accepting partial orders. Example 2. Figure 2: An order graph. Consider the graph in Figure 2. Suppose each edge has price b such that 4b \u2212 3 > 0 and 6b \u2212 5 < 0, namely b \u2208 (.75, .80), for example b = .78. Lemma 7. Consider the graph of accepted orders for optimal divisible matching, G\u2217 (V \u2217 , E\u2217 ), which consists of the optimal subset of edges E\u2217 to be accepted by the auctioneer, with edge weight xe > 0 getting from the optimal solution of the linear program (7). 332 Lemma 8. l(G\u2217 ) \u2264 k(G\u2217 ) \u2264 k(G). When xe\"s are less than 1, l(G\u2217 ) can be less than or equal to k(G\u2217 ). Suppose l(G\u2217 ) < k(G). Theorem 10 states that solving the linear program (7) is NP-hard. Lemma 11. Then by Lemma 5, k(H \u2212 {e}) = k(H) \u2212 1. Theorem 12. Proof. The following procedure outputs k(G). set G := G iterations := 0 while (G has nonempty edge set) reset G := optimal divisible match(G ) while (G has edges with weight < 1) remove an edge with weight < 1 from G reset G by setting all edge weights to 1 reset G := optimal divisible match(G ) end while if (G has nonempty edge set) increment iterations by 1 reset G by removing any edge e end if end while return (iterations) As in the proof of the corresponding Theorem 6 for the indivisible case, we compute k(G) by iteratively removing edges and recomputing the optimal divisible solution on the remaining subgraph, until all edges are deleted. It remains to verify that each iteration reduces k(G) by exactly 1. Once the oracle returns a graph G with unit edge weights, removing any edge would reduce the minimum feedback arc set: otherwise G is not optimal since G \u2212 {e} would have 333 the same minimum feedback arc set but smaller total edge weight. 5.3 Existence of a Match Knowing that the optimal matching problem is NP-hard for both indivisible and divisible orders in pair betting, we check whether the auctioneer can identify the existence of a match with ease. Lemma 13. Lemma 14. Finding the shortest cycle in graph H can be done within polynomial time by resorting to the shortest path problem. For any vertex v in V , we consider every neighbor vertex w such that (v, w) \u2208 E. We then find the shortest path from w to v, denoted as path(w, v). If the worst-case profit for the optimal cycle is non-negative, we know that there exists a match in G. However, the condition in lemma 13 is not a necessary condition for the existence of a match. However, we show that such greedy algorithm can give a very poor approximation. n + 1 n + 1 n + 1 n + 1 n + 1 n + 1 Figure 3: Graph with n vertices and n + n edges on which the greedy algorithm finds only two cycles, the dotted cycle in the center and the unique remaining cycle. Lemma 15. In light of Lemma 15, one may expect that greedy algorithms would give n-approximations at best. Approxima334 tion algorithms for finding the maximum number of edgedisjoint cycles have been considered by Krivelevich, Nutov and Yuster .", "body2": "In practice, this aggregate market assessment of the security\"s value is often more accurate than other forecasts relying on experts, polls, or statistical inference . For example, one trader willing to accept an x dollar loss if a Democrat does not win in return for a y dollar profit if a Democrat wins is matched up with a second trader willing to accept the opposite. For example, if one trader bets that the Democratic candidate will receive more votes than the Republican candidate, a second trader bets that the Republican candidate will receive more votes than the Libertarian candidate, and a third trader bets that the Libertarian candidate will receive more votes than the Democratic candidate, then, depending on the odds they each offer, there may be a three-way agreeable match even though no two-way matches exist. possible orderings of candidates after the final vote tally. securities, one for each possible final ordering, is both cumbersome for the traders and computationally infeasible even for modest sized n. independent choices, making the likelihood of two traders converging at the same time and place seem remote. We also provide a polynomiallyverifiable sufficient condition for the existence of a pairbetting match and show that a greedy algorithm offers poor approximation for indivisible pair bets. The final outcome or state s \u2208 S is an ordinal ranking of the n candidates. mutually exclusive and exhaustive permutations of candidates. Instead, we describe a central exchange where all bets on the outcome are processed together, thus aggregating liquidity and ensuring that informational inference happens automatically. We explore the tradeoff between the expressiveness of the bidding language and the computational complexity of the matching problem. The description of bets in terms of prices and shares is without loss of generality: we can also allow bets to be described in terms of odds, payoff vectors, or any of the diverse array of approaches practiced in financial and gambling circles. securities is infeasible in practice from a computational point of view as n grows. Our work is in direct analogy to work by Fortnow et. The authors show that divisible matching is co-NP-complete and indivisible matching is \u03a3p 2-complete. Hence, he bears risk and may incur loss. Research on bidding languages and winner determination in combinatorial auctions considers similar computational challenges in finding an allocation of items to bidders that maximizes the auctioneer\"s revenue. Combinatorial auctions allow bidders to place distinct values on bundles of goods rather than just on individual goods. Uncertainty and risk are considered and the auctioneer problem is to explore arbitrage opportunities and risklessly match up wagers. We then illustrate the problem definitions in the context of the subset-betting and pairbetting markets. Each order i \u2208 O is a triple (bi, qi, \u03c6i), where bi denotes how much the trader is willing to pay for a unit share of security \u03c6i and qi is the number of shares of the security he wants to purchase at price bi. Since order i is defined for a single security \u03c6i, we will omit the security variable whenever it is clear from the context. Further let Ii(s) be the indicator variable for whether order i is winning in state s, that is Ii(s) = 1 if the order is paid back $1 in state s and Ii(s) = 0 otherwise. Definition 1 (Existence of match, indivisible orders). (1) Similarly we can define the existence of a match with divisible orders. Definition 2 (Existence of match, divisible orders). Definition 3 (Optimal match, indivisible orders). i bi \u2212 Ii(s) qixi \u2265 c, \u2200s \u2208 S xi \u2208 {0, 1}, \u2200i \u2208 O. Definition 4 (Optimal match, divisible orders). i bi \u2212 Ii(s) qixi \u2265 c, \u2200s \u2208 S 0 \u2264 xi \u2264 1, \u2200i \u2208 O. The variable c is the worst-case profit for the auctioneer. The optimal worst-case profit may remain unknown. 3.2 Subset Betting A subset betting market allows two different types of bets. A security \u03b1|\u03a6 where \u03a6 is a subset of positions pays off $1 if candidate \u03b1 stands at a position that is an element of \u03a6 and it pays $0 otherwise. For instance, security {\u03b1, \u03b3}|2 pays off $1 when either candidate \u03b1 or candidate \u03b3 is ranked second. The auctioneer has to carefully choose which orders and what fractions of them to accept so as to be guaranteed a nonnegative profit in any future state. The following example illustrates the matching problem for indivisible orders in the subset-betting market. For indivisible orders, the auctioneer can either accept orders (2) and (4) and obtain profit vector c = (0.4, 0.4, 0.4, 0.4, 0.4, 0.4), or accept orders (2), (3), and (4) and has profit across state c = (0.2, 1.2, 0.2, 1.2, 0.2, 0.2). There are a total of N(N \u22121) different securities offered in the market, each corresponding to an ordered pair of candidates. Again the order can be either indivisible or divisible and the auctioneer needs to decide what fraction xi of each order to accept so as not to incur any loss, with .99 .99 .5 .5 .5 .99 .99 .99 .99 Figure 1: Every cycle has negative worst-case profit of \u22120.02 (for the cycles of length 4) or less (for the cycles of length 6), however accepting all edges in full gives a positive worst-case profit of 0.44. xi \u2208 {0, 1} for indivisible and xi \u2208 for divisible orders. The same definitions for existence of a match and optimal match from Section 3.1 apply. With this interpretation, it is tempting to assume that a necessary condition for a match is to have a cycle in the graph with a nonnegative worst-case profit. The edge labels in the figure are the prices be; both the optimal divisible and indivisible solution in this case accept all orders in full, xe = 1. The existence of a match and the optimal match problems with divisible orders in a subset betting market can both be solved in polynomial time. (4) asks whether there is a state or permutation s in which the profit is less than c, and can be rewritten as Ii(s)qixi < biqixi \u2212 c \u2200s \u2208 S. (5) Thus it suffices to show how to find efficiently a state s satisfying the above inequality (5) or verify that the opposite inequality holds for all states s. We will show that the separation problem can be reduced to the maximum weighted bipartite matching1 problem . Thus, the matching problem can be solved in polynomial time also. The hardness results are surprising especially in light of the observation that a pair betting market has a seemingly more restrictive bidding language which only offers n(n\u22121) The notion of perfect matching in a bipartite graph, which we use only in this proof, should not be confused with the notion of matching bets which we use throughout the paper. Our hope is that the comparison of the complexities of the subset and pair betting markets would offer insight into what makes a bidding language expressive while at the same time enabling an efficient matching solution. We adopt the following notations throughout the paper: \u2022 G(V, E): original equally weighted directed graph for the set of unit orders O. \u2022 be: price of the order for edge e. xe = 1 for indivisible orders and 0 < xe \u2264 1 for divisible orders. \u2022 H(V, E): a generic weighted directed graph of accepted orders. \u2022 k(H): solution to the unweighted minimum feedback arc set problem on graph H. k(H) is the minimum number of edges to remove so that H becomes acyclic. \u2022 l(H): solution to the weighted minimum feedback arc set problem on graph H. l(H) is the minimum total weights for the set of edges which, when removed, leave H acyclic. \u2022 c(H): worst-case profit of the auctioneer if he accepts all orders in graph H. In other cases, the value is determined in context. We will use this result in our complexity analysis on pair betting markets. e be \u2212 Ie(s) xe \u2265 c, \u2200s \u2208 S xe \u2208 {0, 1}, \u2200e \u2208 E. Without lose of generality, we assume that there are no multi-edges in the order graph G. The latter takes as input a directed graph, and asks what is the minimum number of edges to delete from the graph so as to be left with a DAG. Our hardness proof is based on the following lemmas. Then the worst-case profit is equal to k(H) \u2212 |E|, where k(H) is the solution to the unweighted minimum feedback arc problem on H. Then, ls is the cardinality of a feedback arc set that consists of all losing edges in state s. Thus, the worst-case profit is achieved at state smax, profit(smax) = (|E| \u2212 wmax) \u2212 |E| = k(H) \u2212 |E|. Consider the graph of accepted orders for optimal matching, G\u2217 (V \u2217 , E\u2217 ), which consists of the optimal subset of edges E\u2217 to be accepted by the auctioneer, that is edges with xe = 1 in the solution of the optimization problem (6). We have the following lemma. G\u2217 is a subgraph of G, hence the minimum number of edges to break cycles in G\u2217 is no more than that in G, namely k(G\u2217 ) \u2264 k(G). Hence, G\u2217 is the smallest subgraph of G such that k(G\u2217 ) = k(G). Thus computing each automatically gives the other, hence computing the maximum worst-case profit in the indivisible pair betting problem is NP-hard. Computing the maximum worst-case profit in indivisible pair betting is NP-hard. Because the minimum feedback arc set problem is NP-complete , computing the maximum worst-case profit is NP-hard. Given a weighted graph H(V, E), removing a single edge e with weight xe from the graph decreases the weighted minimum feedback arc set solution l(H) by no more than xe and reduces the unweighted minimum feedback arc set solution k(H) by no more than 1. Because l(H) is the solution to the weighted minimum feedback arc set problem on H, we have l(H) \u2264 l(H \u2212{e})+xe, implying that l(H \u2212 {e}) \u2265 l(H) \u2212 xe. Finding the optimal match in indivisible pair betting is NP-hard. We are interested in computing k(G), the size of the minimum feedback arc set of G. The following procedure finds k(G) by using polynomially many calls to the optimal match oracle on a sequence of subgraphs of G. Hence the number of iterations is equal to k(G). Note that this procedure gives a polynomial transformation from the optimal matching problem to the unweighted minimum feedback arc set problem, which calls the optimal matching oracle exactly k(G) \u2264 |E| times, where |E| is the number of edges of G. Hence the optimal matching problem is NP-hard. 5.2 Optimal Divisible Matching When orders are divisible, the auctioneer\"s optimal matching problem is described in Definition 4 of Section 3. e be \u2212 Ie(s) xe \u2265 c, \u2200s \u2208 S xe \u2208 , \u2200e \u2208 E. We still assume that there are no multi-edges in the order graph G. Example 2 shows a situation when accepting partial orders generates higher worst-case profit than the optimal indivisible solution. We show that the linear program (7) sometimes has a non-integer optimal solution. Letters on edges represent order prices. There are a total of five cycles in the graph: three four-edge cycles ABCD, ABEF, CDEF, and two six-edge cycles ABCDEF and ABEFCD. Despite the potential profit increase for accepting divisible orders, the auctioneer\"s optimal matching problem remains to be NP-hard for divisible orders, which is presented below via several lemmas and theorems. The minimum total quantity of losing orders is the solution to weighted minimal feedback arc set problem on H, that is l(H). We have the following lemmas. 332 Lemma 8. l(G\u2217 ) \u2264 k(G\u2217 ) \u2264 k(G). When all edge weights in G\u2217 are 1, l(G\u2217 ) = k(G\u2217 ). There exists some such that when all edge prices be\"s are (1 \u2212 ), l(G\u2217 ) = k(G). When he accepts the original order graph G in full, his worstcase profit is c(G) = e\u2208E (be \u2212 1) + k(G) = k(G) \u2212 |E|. Finding the optimal worst-case profit in divisible pair betting is NP-hard. Hence, finding the optimal worst-case profit is NP-hard. First, note that the optimal divisible subgraph G\u2217 of a graph G is the weighted subgraph with minimum weighted feedback arc set size l(G\u2217 ) = k(G) and smallest sum of edge weights e\u2208E\u2217 xe, since its corresponding worst case profit is k(G) \u2212 e\u2208E\u2217 xe according to lemmas 7 and 9. Assume the contrary, namely k(H\u2212{e}) < k(H). We now can augment the procedure for the indivisible case in Theorem 6, to prove hardness of the divisible version, as follows. Finding the optimal match in divisible pair betting is NP-hard. We reduce from the unweighted minimum feedback arc set problem for graph G. Suppose we have an oracle for the optimal divisible problem called optimal divisible match, which on input graph H computes edge weights xe \u2208 (0, 1] for the optimal subgraph H\u2217 of H, satisfying l(H\u2217 ) = k(H). The following procedure outputs k(G). In each iteration of the outer while loop, the minimum feedback arc set is reduced by 1, thus the number of iterations is equal to k(G). By Lemma 11 throughout the iteration so far the minimum feedback arc set of the corresponding unweighted graph remains unchanged. Hence, the optimal matching problem for divisible orders is NP-hard. Lemma 13 states a sufficient condition for the matching problem with both indivisible and divisible orders. A sufficient condition for the existence of a match for pair betting is that there exists a cycle C in G such that, e\u2208C be \u2265 |C| \u2212 1, (11) where |C| is the number of edges in the cycle C. It can be shown that identifying such a non-negative worstcase profit cycle in an order graph G can be achieved in polynomial time. It takes polynomial time to find a cycle in an order graph G(V, E) that has the highest worst-case profit, that is max C\u2208C e\u2208C be \u2212 (|C| \u2212 1) , where C is the set of all cycles in G. Because e\u2208C be \u2212 (|C| \u2212 1) = e\u2208C (be \u2212 1) + 1 = 1 \u2212 e\u2208C (1 \u2212 be), finding the cycle that gives the highest worst-case profit in the original order graph G is equivalent to finding the shortest cycle in a converted graph H(V, E), where H is achieved by setting the weight for edge e in G to be (1 \u2212 be). Finding the shortest cycle in graph H can be done within polynomial time by resorting to the shortest path problem. Comparing the shortest cycle found for every vertex, we then can determine the shortest overall cycle for the graph H. Because the short path problem can be solved in polynomial time , we can find the solution to our problem in polynomial time. If the optimal indivisible match consists only of edge disjoint cycles, a natural greedy algorithm can find the cycle that gives the highest worst-case profit, remove its edges from the graph, and proceed until no more cycles exist. However, we show that such greedy algorithm can give a very poor approximation. The labels in the faces give the number of edges in the corresponding cycle. The greedy algorithm gives at most an O( n)approximation to the maximum number of disjoint cycles. Because smaller cycles gives higher worst-case profit, the greedy algorithm would first select the cycle of length n, after which there would be only one remaining cycle of length n. Thus the total number of cycles selected by greedy is 2 and the approximation factor in this case is n/2. In light of Lemma 15, one may expect that greedy algorithms would give n-approximations at best. When the optimal match does not consist of edge-disjoint cycles as in the example of Figure 3, greedy algorithm trying to finding optimal single cycles fails obviously.", "introduction": "Buying or selling a financial security in effect is a wager on the security\"s value. For example, buying a stock is a bet that the stock\"s value is greater than its current price. Each trader evaluates his expected profit to decide the quantity to buy or sell according to his own information and subjective probability assessment. The collective interaction of all bets leads to an equilibrium that reflects an aggregation of all the traders\" information and beliefs. In practice, this aggregate market assessment of the security\"s value is often more accurate than other forecasts relying on experts, polls, or statistical inference . Consider buying a security at price fifty-two cents, that pays $ US Presidential election. The transaction is a commitment to accept a fifty-two cent loss if a Democrat does not win in return for a forty-eight cent profit if a Democrat does win. In this case of an event-contingent security, the price-the market\"s value of the security-corresponds directly to the estimated probability of the event. Almost all existing financial and betting exchanges pair up bilateral trading partners. For example, one trader willing to accept an x dollar loss if a Democrat does not win in return for a y dollar profit if a Democrat wins is matched up with a second trader willing to accept the opposite. However in many scenarios, even if no bilateral agreements exist among traders, multilateral agreements may be possible. For example, if one trader bets that the Democratic candidate will receive more votes than the Republican candidate, a second trader bets that the Republican candidate will receive more votes than the Libertarian candidate, and a third trader bets that the Libertarian candidate will receive more votes than the Democratic candidate, then, depending on the odds they each offer, there may be a three-way agreeable match even though no two-way matches exist. We propose an exchange where traders have considerable flexibility to naturally and succinctly express their wagers, 326 and examine the computational complexity of the auctioneer\"s resulting matching problem of identifying bilateral and multilateral agreements. In particular, we focus on a setting where traders bet on the outcome of a competition among n candidates. For example, suppose that there are n candidates in an election (or n horses in a race, etc.) possible orderings of candidates after the final vote tally. Traders may like to bet on arbitrary properties of the final ordering, for example candidate D will win, candidate D will finish in either first place or last place, candidate D will defeat candidate R, candidates D and R will both defeat candidate L, etc. The goal of the exchange is to search among all the offers to find two or more that together form an agreeable match. As we shall see, the matching problem can be set up as a linear or integer program, depending on whether orders are divisible or indivisible, respectively. Attempting to reduce the problem to a bilateral matching problem by explicitly creating n! securities, one for each possible final ordering, is both cumbersome for the traders and computationally infeasible even for modest sized n. Moreover, traders\" attention would be spread among n! independent choices, making the likelihood of two traders converging at the same time and place seem remote. There is a tradeoff between the expressiveness of the bidding language and the computational complexity of the matching problem. We want to offer traders the most expressive bidding language possible while maintaining computational feasibility. We explore two bidding languages that seem natural from a trader perspective. Subset betting, described in Section 3.2, allows traders to bet on which positions in the ranking a candidate will fall, for example candidate D will finish in position 1, 3-5, or 10. Symetrically, traders can also bet on which candidates will fall in a particular position. In Section 4, we derive a polynomial-time algorithm for matching (divisible) subset bets. The key to the result is showing that the exponentially big linear program has a corresponding separation problem that reduces to maximum weighted bipartite matching and consequently we can solve it in time polynomial in the number of orders. Pair betting, described in Section 3.3, allows traders to bet on the final ranking of any two candidates, for example candidate D will defeat candidate R. In Section 5, we show that optimal matching of (divisible or indivisible) pair bets is NP-hard, via a reduction from the unweighted minimum feedback arc set problem. We also provide a polynomiallyverifiable sufficient condition for the existence of a pairbetting match and show that a greedy algorithm offers poor approximation for indivisible pair bets.", "conclusion": "We consider a permutation betting scenario, where traders wager on the final ordering of n candidates.. While it is unnatural and intractable to allow traders to bet directly on the n!. different final orderings, we propose two expressive betting languages, subset betting and pair betting.. In a subset betting market, traders can bet either on a subset of positions that a candidate stands or on a subset of candidates who occupy a specific position in the final ordering.. Pair betting allows traders bet on whether one given candidate ranks higher than another given candidate.. We examine the auctioneer problem of matching orders without incurring risk.. We find that in a subset betting market an auctioneer can find the optimal set and quantity of orders to accept such that his worst-case profit is maximized in polynomial time if orders are divisible.. The complexity changes dramatically for pair betting.. We prove that the optimal matching problem for the auctioneer is NP-hard for pair betting with both indivisible and divisible orders via reductions to the minimum feedback arc set problem.. We identify a sufficient condition for the existence of a match, which can be verified in polynomial time.. A natural greedy algorithm has been shown to give poor approximation for indivisible pair betting.. Interesting open questions for our permutation betting include the computational complexity of optimal indivisible matching for subset betting and the necessary condition for the existence of a match in pair betting markets.. We are interested in further exploring better approximation algorithms for pair betting markets.. ACKNOWLEDGMENTS We thank Ravi Kumar, Yishay Mansour, Amin Saberi, Andrew Tomkins, John Tomlin, and members of Yahoo!. Research for valuable insights and discussions."}
{"id": "H-13", "keywords": ["web search", "summar", "snippet", "queri log"], "title": "The Influence of Caption Features on Clickthrough Patterns in Web Search", "abstract": "Web search engines present lists of captions, comprising ti- tle, snippet, and URL, to help users decide which search results to visit. Understanding the influence of features of these captions on Web search behavior may help validate algorithms and guidelines for their improved generation. In this paper we develop a methodology to use clickthrough logs from a commercial search engine to study user behavior when interacting with search result captions. The findings of our study suggest that relatively simple caption features such as the presence of all terms query terms, the readabil- ity of the snippet, and the length of the URL shown in the caption, can significantly influence users' Web search behav- ior.", "references": ["Improving web search ranking by incorporating user behavior information", "Learning user interaction models for predicting Web search result preferences", "A taxonomy of Web search", "What are you looking for? An eye-tracking study of information usage in Web search", "Optimizing search by showing results in context", "Summarizing text documents: Sentence selection and evaluation metrics", "Eye-tracking analysis of user behavior in WWW search", "Title extraction from bodies of HTML documents and its application to Web page retrieval", "Accurately interpreting clickthrough data as implicit feedback", "Automatic identification of user goals in Web search", "The automatic creation of literature abstracts", "WaveLens: A new view onto Internet search results", "Understanding user goals in Web search", "Web-page summarization using clickthrough data", "Advantages of query biased summaries in information retrieval", "A system for query-specific document summarization", "Finding relevant documents using top ranking sentences: An evaluation of two alternative schemes", "Optimizing web search using Web click-through data"], "full_text": "1. INTRODUCTION The major commercial Web search engines all present their results in much the same way. Each search result is described by a brief caption, comprising the URL of the associated Web page, a title, and a brief summary (or snippet) describing the contents of the page. Often the snippet is extracted from the Web page itself, but it may also be taken from external sources, such as the human-generated summaries found in Web directories. Figure 1 shows a typical Web search, with captions for the top three results. While the three captions share the same basic structure, their content differs in several respects. The snippet of the third caption is nearly twice as long as that of the first, while the snippet is missing entirely from the second caption. The title of the third caption contains all of the query terms in order, while the titles of the first and second captions contain only two of the three terms. One of the query terms is repeated in the first caption. All of the query terms appear in the URL of the third caption, while none appear in the URL of the first caption. The snippet of the first caption consists of a complete sentence that concisely describes the associated page, while the snippet of the third caption consists of two incomplete sentences that are largely unrelated to the overall contents of the associated page and to the apparent intent of the query. While these differences may seem minor, they may also have a substantial impact on user behavior. A principal motivation for providing a caption is to assist the user in determining the relevance of the associated page without actually having to click through to the result. In the case of a navigational query - particularly when the destination is well known - the URL alone may be sufficient to identify the desired page. But in the case of an informational query, the title and snippet may be necessary to guide the user in selecting a page for further study, and she may judge the relevance of a page on the basis of the caption alone. When this judgment is correct, it can speed the search process by allowing the user to avoid unwanted material. When it fails, the user may waste her time clicking through to an inappropriate result and scanning a page containing little or nothing of interest. Even worse, the user may be misled into skipping a page that contains desired information. All three of the results in figure 1 are relevant, with some limitations. The first result links to the main Yahoo Kids! homepage, but it is then necessary to follow a link in a menu to find the main page for games. Despite appearances, the second result links to a surprisingly large collection of online games, primarily with environmental themes. The third result might be somewhat disappointing to a user, since it leads to only a single game, hosted at the Centers for Disease Control, that could not reasonably be described as online. Unfortunately, these page characteristics are not entirely reflected in the captions. In this paper, we examine the influence of caption features on user\"s Web search behavior, using clickthroughs extracted from search engines logs as our primary investigative tool. Understanding this influence may help to validate algorithms and guidelines for the improved generation of the Figure 1: Top three results for the query: kids online games. captions themselves. In addition, these features can play a role in the process of inferring relevance judgments from user behavior . By better understanding their influence, better judgments may result. Different caption generation algorithms might select snippets of different lengths from different areas of a page. Snippets may be generated in a query-independent fashion, providing a summary of the page as a whole, or in a querydependent fashion, providing a summary of how the page relates to the query terms. The correct choice of snippet may depend on aspects of both the query and the result page. The title may be taken from the HTML header or extracted from the body of the document . For links that re-direct, it may be possible to display alternative URLs. Moreover, for pages listed in human-edited Web directories such as the Open Directory Project1 , it may be possible to display alternative titles and snippets derived from these listings. When these alternative snippets, titles and URLs are available, the selection of an appropriate combination for display may be guided by their features. A snippet from a Web directory may consist of complete sentences and be less fragmentary than an extracted snippet. A title extracted from the body may provide greater coverage of the query terms. A URL before re-direction may be shorter and provide a clearer idea of the final destination. The work reported in this paper was undertaken in the context of the Windows Live search engine. The image in figure 1 was captured from Windows Live and cropped to eliminate branding, advertising and navigational elements. The experiments reported in later sections are based on Windows Live query logs, result pages and relevance judgments collected as part of ongoing research into search engine performance . Nonetheless, given the similarity of caption formats across the major Web search engines we believe the results are applicable to these other engines. The query in www.dmoz.org figure 1 produces results with similar relevance on the other major search engines. This and other queries produce captions that exhibit similar variations. In addition, we believe our methodology may be generalized to other search applications when sufficient clickthrough data is available. 2. RELATED WORK While commercial Web search engines have followed similar approaches to caption display since their genesis, relatively little research has been published about methods for generating these captions and evaluating their impact on user behavior. Most related research in the area of document summarization has focused on newspaper articles and similar material, rather than Web pages, and has conducted evaluations by comparing automatically generated summaries with manually generated summaries. Most research on the display of Web results has proposed substantial interface changes, rather than addressing details of the existing interfaces. 2.1 Display of Web results Varadarajan and Hristidis are among the few who have attempted to improve directly upon the snippets generated by commercial search systems, without introducing additional changes to the interface. They generated snippets from spanning trees of document graphs and experimentally compared these snippets against the snippets generated for the same documents by the Google desktop search system and MSN desktop search system. They evaluated their method by asking users to compare snippets from the various sources. Cutrell and Guan conducted an eye-tracking study to investigate the influence of snippet length on Web search performance and found that the optimal snippet length varied according to the task type, with longer snippets leading to improved performance for informational tasks and shorter snippets for navigational tasks. Many researchers have explored alternative methods for displaying Web search results. Dumais et al. compared an interface typical of those used by major Web search engines with one that groups results by category, finding that users perform search tasks faster with the category interface. Paek et al. propose an interface based on a fisheye lens, in which mouse hovers and other events cause captions to zoom and snippets to expand with additional text. White et al. evaluated three alternatives to the standard Web search interface: one that displays expanded summaries on mouse hovers, one that displays a list of top ranking sentences extracted from the results taken as a group, and one that updates this list automatically through implicit feedback. They treat the length of time that a user spends viewing a summary as an implicit indicator of relevance. Their goal was to improve the ability of users to interact with a given result set, helping them to look beyond the first page of results and to reduce the burden of query re-formulation. 2.2 Document summarization Outside the narrow context of Web search considerable related research has been undertaken on the problem of document summarization. The basic idea of extractive summarization - creating a summary by selecting sentences or fragments - goes back to the foundational work of Luhn . Luhn\"s approach uses term frequencies to identify significant words within a document and then selects and extracts sentences that contain significant words in close proximity. A considerable fraction of later work may be viewed as extending and tuning this basic approach, developing improved methods for identifying significant words and selecting sentences. For example, a recent paper by Sun et al. describes a variant of Luhn\"s algorithm that uses clickthrough data to identify significant words. At its simplest, snippet generation for Web captions might also be viewed as following this approach, with query terms taking on the role of significant words. the annual Document Understanding Conference (DUC) series, conducted by the US National Institute of Standards and Technology, has provided a vehicle for evaluating much of the research in document summarization2 . Each year DUC defines a methodology for one or more experimental tasks, and supplies the necessary test documents, human-created summaries, and automatically extracted baseline summaries. The majority of participating systems use extractive summarization, but a number attempt natural language generation and other approaches. Evaluation at DUC is achieved through comparison with manually generated summaries. Over the years DUC has included both single-document summarization and multidocument summarization tasks. task is posed as taking place in a question answering context. Given a topic and 25 documents, participants were asked to generate a 250-word summary satisfying the information need enbodied in the topic. We view our approach of evaluating summarization through the analysis of Web logs as complementing the approach taken at DUC. A number of other researchers have examined the value of query-dependent summarization in a non-Web context. Tombros and Sanderson compared the performance of 20 subjects searching a collection of newspaper articles when duc.nist.gov guided by query-independent vs. query-dependent snippets. The query-independent snippets were created by extracting the first few sentences of the articles; the query-dependent snippets were created by selecting the highest scoring sentences under a measure biased towards sentences containing query terms. When query-dependent summaries were presented, subjects were better able to identify relevant documents without clicking through to the full text. Goldstein et al. describe another extractive system for generating query-dependent summaries from newspaper articles. In their system, sentences are ranked by combining statistical and linguistic features. They introduce normalized measures of recall and precision to facilitate evaluation. 2.3 Clickthroughs Queries and clickthroughs taken from the logs of commercial Web search engines have been widely used to improve the performance of these systems and to better understand how users interact with them. In early work, Broder examined the logs of the AltaVista search engine and identified three broad categories of Web queries: informational, navigational and transactional. Rose and Levinson conducted a similar study, developing a hierarchy of query goals with three top-level categories: informational, navigational and resource. Under their taxonomy, a transactional query as defined by Broder might fall under either of their three categories, depending on details of the desired transaction. Lee et al. used clickthrough patterns to automatically categorize queries into one of two categories: informational - for which multiple Websites may satisfy all or part of the user\"s need - and navigational - for which users have a particular Website in mind. Under their taxonomy, a transactional or resource query would be subsumed under one of these two categories. Agichtein et al. interpreted caption features, clickthroughs and other user behavior as implicit feedback to learn preferences and improve ranking in Web search. Xue et al. present several methods for associating queries with documents by analyzing clickthrough patterns and links between documents. Queries associated with documents in this way are treated as meta-data. In effect, they are added to the document content for indexing and ranking purposes. Of particular interest to us is the work of Joachims et al. and Granka et al. . They conducted eye-tracking studies and analyzed log data to determine the extent to which clickthrough data may be treated as implicit relevance judgments. They identified a trust bias, which leads users to prefer the higher ranking result when all other factors are equal. In addition, they explored techniques that treat clicks as pairwise preferences. For example, a click at position N + 1 - after skipping the result at position N - may be viewed as a preference for the result at position N+1 relative to the result at position N. These findings form the basis of the clickthrough inversion methodology we use to interpret user interactions with search results. Our examination of large search logs compliments their detailed analysis of a smaller number of participants. 3. CLICKTHROUGH INVERSIONS While other researchers have evaluated the display of Web search results through user studies - presenting users with a small number of different techniques and asking them to complete experimental tasks - we approach the problem by extracting implicit feedback from search engine logs. Examining user behavior in situ allows us to consider many more queries and caption characteristics, with the volume of available data compensating for the lack of a controlled lab environment. The problem remains of interpreting the information in these logs as implicit indicators of user preferences, and in this matter we are guided by the work of Joachims et al. . We consider caption pairs, which appear adjacent to one another in the result list. Our primary tool for examining the influence of caption features is a type of pattern observed with respect to these caption pairs, which we call a clickthrough inversion. A clickthrough inversion occurs at position N when the result at position N receives fewer clicks than the result at position N + 1. Following Joachims et al. , we interpret a clickthrough inversion as indicating a preference for the lower ranking result, overcoming any trust bias. For simplicity, in the remainder of this paper we refer to the higher ranking caption in a pair as caption A and the lower ranking caption as caption B. 3.1 Extracting clickthroughs For the experiments reported in this paper, we sampled a subset of the queries and clickthroughs from the logs of the Windows Live search engine over a period of 3-4 days on three separate occasions: once for results reported in section 3.3, once for a pilot of our main experiment, and once for the experiment itself (sections 4 and 5). For simplicity we restricted our sample to queries submitted to the US English interface and ignored any queries containing complex or non-alphanumeric terms (e.g. operators and phrases). At the end of each sampling period, we downloaded captions for the queries associated with the clickthrough sample. When identifying clickthroughs in search engine logs, we consider only the first clickthrough action taken by a user after entering a query and viewing the result page. Users are identified by IP address, which is a reasonably reliable method of eliminating multiple results from a single user, at the cost of falsely eliminating results from multiple users sharing the same address. By focusing on the initial clickthrough, we hope to capture a user\"s impression of the relative relevance within a caption pair when first encountered. If the user later clicks on other results or re-issues the same query, we ignore these actions. Any preference captured by a clickthrough inversion is therefore a preference among a group of users issuing a particular query, rather than a preference on the part of a single user. In the remainder of the paper, we use the term clickthrough to refer only to this initial action. Given the dynamic nature of the Web and the volumes of data involved, search engine logs are bound to contain considerable noise. For example, even over a period of hours or minutes the order of results for a given query can change, with some results dropping out of the top ten and new ones appearing. For this reason, we retained clickthroughs for a specific combination of a query and a result only if this result appears in a consistent position for at least 50% of the clickthroughs. Clickthroughs for the same result when it appeared at other positions were discarded. For similar reasons, if we did not detect at least ten clickthroughs for a particular query during the sampling period, no clickthroughs for that query were retained. 10 20 30 40 50 60 70 80 90 100 1 2 3 4 5 6 7 8 9 10 clickthroughpercent position a) craigslist 10 20 30 40 50 60 70 80 90 100 1 2 3 4 5 6 7 8 9 10 clickthroughpercent position b) periodic table of elements 10 20 30 40 50 60 70 80 90 100 1 2 3 4 5 6 7 8 9 10 clickthroughpercent position c) kids online games Figure 2: Clickthrough curves for three queries: a) a stereotypical navigational query, b) a stereotypical informational query, and c) a query exhibiting clickthrough inversions. The outcome at the end of each sampling period is a set of records, with each record describing the clickthroughs for a given query/result combination. Each record includes a query, a result position, a title, a snippet, a URL, the number of clickthroughs for this result, and the total number of clickthroughs for this query. We then processed this set to generate clickthrough curves and identify inversions. 3.2 Clickthrough curves It could be argued that under ideal circumstances, clickthrough inversions would not be present in search engine logs. A hypothetical perfect search engine would respond to a query by placing the result most likely to be relevant first in the result list. Each caption would appropriately summarize the content of the linked page and its relationship to the query, allowing users to make accurate judgments. Later results would complement earlier ones, linking to novel or supplementary material, and ordered by their interest to the greatest number of users. Figure 2 provides clickthrough curves for three example queries. For each example, we plot the percentage of clickthroughs against position for the top ten results. The first query (craigslist) is stereotypically navigational, showing a spike at the correct answer (www.craigslist.org). The second query is informational in the sense of Lee et al. (periodic table of elements). Its curve is flatter and less skewed toward a single result. For both queries, the number of clickthroughs is consistent with the result positions, with the percentage of clickthroughs decreasing monotonically as position increases, the ideal behavior. Regrettably, no search engine is perfect, and clickthrough inversions are seen for many queries. For example, for the third query (kids online games) the clickthrough curve exhibits a number of clickthrough inversions, with an apparent preference for the result at position 4. Several causes may be enlisted to explain the presence of an inversion in a clickthrough curve. The search engine may have failed in its primary goal, ranking more relevant results below less relevant results. Even when the relative ranking is appropriate, a caption may fail to reflect the content of the underlying page with respect to the query, leading the user to make an incorrect judgment. Before turning to the second case, we address the first, and examine the extent to which relevance alone may explain these inversions. 3.3 Relevance The simplest explanation for the presence of a clickthrough inversion is a relevance difference between the higher ranking member of caption pair and the lower ranking member. In order to examine the extent to which relevance plays a role in clickthrough inversions, we conducted an initial experiment using a set of 1,811 queries with associated judgments created as part of on-going work. Over a four-day period, we sampled the search engine logs and extracted over one hundred thousand clicks involving these queries. From these clicks we identified 355 clickthrough inversions, satisfying the criteria of section 3.1, where relevance judgments existed for both pages. The relevance judgments were made by independent assessors viewing the pages themselves, rather than the captions. Relevance was assessed on a 6-point scale. The outcome is presented in figure 3, which shows the explicit judgments for the 355 clickthrough inversions. In all of these cases, there were more clicks on the lower ranked member of the Relationship Number Percent rel(A) < rel(B) 119 33.5% rel(A) = rel(B) 134 37.7% rel(A) > rel(B) 102 28.7% Figure 3: Relevance relationships at clickthrough inversions. Compares relevance between the higher ranking member of a caption pair (rel(A)) to the relevance of the lower ranking member (rel(B)), where caption A received fewer clicks than caption B. pair (B). The figure shows the corresponding relevance judgments. For example, the first row rel(A) < rel(B), indicates that the higher ranking member of pair (A) was rated as less relevant than the lower ranking member of the pair (B). As we see in the figure, relevance alone appears inadequate to explain the majority of clickthrough inversions. For twothirds of the inversions (236), the page associated with caption A is at least as relevant as the page associated with caption B. For 28.7% of the inversions, A has greater relevance than B, which received the greater number of clickthroughs. 4. INFLUENCE OF CAPTION FEATURES Having demonstrated that clickthrough inversions cannot always be explained by relevance differences, we explore what features of caption pairs, if any, lead users to prefer one caption over another. For example, we may hypothesize that the absence of a snippet in caption A and the presence of a snippet in caption B (e.g. captions 2 and 3 in figure 1) leads users to prefer caption A. Nonetheless, due to competing factors, a large set of clickthrough inversions may also include pairs where the snippet is missing in caption B and not in caption A. However, if we compare a large set of clickthrough inversions to a similar set of pairs for which the clickthroughs are consistent with their ranking, we would expect to see relatively more pairs where the snippet was missing in caption A. 4.1 Evaluation methodology Following this line of reasoning, we extracted two sets of caption pairs from search logs over a three day period. The first is a set of nearly five thousand clickthrough inversions, extracted according to the procedure described in section 3.1. The second is a corresponding set of caption pairs that do not exhibit clickthrough inversions. In other words, for pairs in this set, the result at the higher rank (caption A) received more clickthroughs than the result at the lower rank (caption B). To the greatest extent possible, each pair in the second set was selected to correspond to a pair in the first set, in terms of result position and number of clicks on each result. We refer to the first set, containing clickthrough inversions, as the INV set; we refer to the second set, containing caption pairs for which the clickthroughs are consistent with their rank order, as the CON set. We extract a number of features characterizing snippets (described in detail in the next section) and compare the presence of each feature in the INV and CON sets. We describe the features as a hypothesized preference (e.g., a preference for captions containing a snippet). Thus, in either set, a given feature may be present in one of two forms: favoring the higher ranked caption (caption A) or favoring the lower ranked caption (caption B). For example, the abFeature Tag Description MissingSnippet snippet missing in caption A and present in caption B SnippetShort short snippet in caption A (< 25 characters) with long snippet (> 100 characters) in caption B TermMatchTitle title of caption A contains matches to fewer query terms than the title of caption B TermMatchTS title+snippet of caption A contains matches to fewer query terms than the title+snippet of caption B TermMatchTSU title+snippet+URL of caption A contains matches to fewer query terms than caption B TitleStartQuery title of caption B (but not A) starts with a phrase match to the query QueryPhraseMatch title+snippet+url contains the query as a phrase match MatchAll caption B contains one match to each term; caption A contains more matches with missing terms URLQuery caption B URL is of the form www.query.com where the query matches exactly with spaces removed URLSlashes caption A URL contains more slashes (i.e. a longer path length) than the caption B URL URLLenDIff caption A URL is longer than the caption B URL Official title or snippet of caption B (but not A) contains the term official (with stemming) Home title or snippet of caption B (but not A) contains the phrase home page Image title or snippet of caption B (but not A) contains a term suggesting the presence of an image gallery Readable caption B (but not A) passes a simple readability test Figure 4: Features measured in caption pairs (caption A and caption B), with caption A as the higher ranked result. These features are expressed from the perspective of the prevalent relationship predicted for clickthrough inversions. sence of a snippet in caption A favors caption B, and the absence of a snippet in caption B favors caption A. When the feature favors caption B (consistent with a clickthrough inversion) we refer to the caption pair as a positive pair. When the feature favors caption A, we refer to it as a negative pair. For missing snippets, a positive pair has the caption missing in caption A (but not B) and a negative pair has the caption missing in B (but not A). Thus, for a specific feature, we can construct four subsets: 1) INV+, the set of positive pairs from INV; 2) INV\u2212, the set of negative pairs from INV; 3) CON+; the set of positive pairs from CON; and 4) CON\u2212 the set of negative pairs from CON. The sets INV+, INV\u2212, CON+, and CON\u2212 will contain different subsets of INV and CON for each feature. When stating a feature corresponding to a hypothesized user preference, we follow the practice of stating the feature with the expectation that the size of INV+ relative to the size of INV\u2212 should be greater than the size of CON+ relative to the size of CON\u2212. For example, we state the missing snippet feature as snippet missing in caption A and present in caption B. This evaluation methodology allows us to construct a contingency table for each feature, with INV essentially forming the experimental group and CON the control group. We can then apply Pearson\"s chi-square test for significance. 4.2 Features Figure 4 lists the features tested. Many of the features on this list correspond to our own assumptions regarding the importance of certain caption characteristics: the presence of query terms, the inclusion of a snippet, and the importance of query term matches in the title. Other features suggested themselves during the examination of the snippets collected as part of the study described in section 3.3 and during a pilot of the evaluation methodology (section 4.1). For this pilot we collected INV and CON sets of similar sizes, and used these sets to evaluate a preliminary list of features and to establish appropriate parameters for the SnippetShort and Readable features. In the pilot, all of the features list in figure 4 were significant at the 95% level. A small number of other features were dropped after the pilot. These features all capture simple aspects of the captions. The first feature concerns the existence of a snippet and the second concerns the relative size of snippets. Apart from this first feature, we ignore pairs where one caption has a missing snippet. These pairs are not included in the sets constructed for the remaining features, since captions with missing snippets do not contain all the elements of a standard caption and we wanted to avoid their influence. The next six features concern the location and number of matching query terms. For the first five, a match for each query term is counted only once, additional matches for the same term are ignored. The MatchAll feature tests the idea that matching all the query terms exactly once is preferable to matching a subset of the terms many times with a least one query term unmatched. The next three features concern the URLs, capturing aspects of their length and complexity, and the last four features concern caption content. The first two of these content features (Official and Home) suggest claims about the importance or significance of the associated page. The third content feature (Image) suggests the presence of an image gallery, a popular genre of Web page. Terms represented by this feature include pictures, pics, and gallery. The last content feature (Readable) applies an ad-hoc readability metric to each snippet. Regular users of Web search engines may notice occasional snippets that consist of little more than lists of words and phrases, rather than a coherent description. We define our own metric, since the Flesch-Kincaid readability score and similar measures are intended for entire documents not text fragments. While the metric has not been experimentally validated, it does reflect our intuitions and observations regarding result snippets. In English, the 100 most frequent words represent about 48% of text, and we would expect readable prose, as opposed to a disjointed list of words, to contain these words in roughly this proportion. The Readable feature computes the percentage of these top-100 words appearing in each caption. If these words represent more than 40% of one caption and less than 10% of the other, the pair is included in the appropriate set. Feature Tag INV+ INV\u2212 %+ CON+ CON\u2212 %+ \u03c72 p-value MissingSnippet 185 121 60.4 144 133 51. SnippetShort 20 6 76.9 12 16 42. TermMatchTitle 800 559 58.8 660 700 48. < TermMatchTS 310 213 59.2 269 216 55. TermMatchTSU 236 138 63.1 189 149 55. 933 53. 45. < QueryPhraseMatch 465 346 57.3 427 422 50. MatchAll 8 2 80.0 1 4 20. URLQuery 277 188 59.5 159 315 33. < 55. 43. < 50. 43. < Official 215 142 60.2 133 215 38. < Home 62 49 55.8 64 82 43. Image 391 270 59.1 315 335 48. < Readable 52 43 54.7 31 48 39. Figure 5: Results corresponding to the features listed in figure 4 with \u03c72 and p-values (df = 1). Features supported at the 95% confidence level are bolded. The p-value for the MatchAll feature is computed using Fisher\"s Exact Test. 4.3 Results Figure 5 presents the results. Each row lists the size of the four sets (INV+, INV\u2212, CON+, and CON\u2212) for a given feature and indicates the percentage of positive pairs (%+) for INV and CON. In order to reject the null hypothesis, this percentage should be significantly greater for INV than CON. Except in one case, we applied the chi-squared test of independence to these sizes, with p-values shown in the last column. For the MatchAll feature, where the sum of the set sizes is 15, we applied Fisher\"s exact test. Features supported at the 95% confidence level are bolded. 5. COMMENTARY The results support claims that missing snippets, short snippets, missing query terms and complex URLs negatively impact clickthroughs. While this outcome may not be surprising, we are aware of no other work that can provide support for claims of this type in the context of a commercial Web search engine. This work was originally motivated by our desire to validate some simple guidelines for the generation of captionssummarizing opinions that we formulated while working on related issues. While our results do not direct address all of the many variables that influence users understanding of captions, they are consistent with the major guidelines. Further work is needed to provide additional support for the guidelines and to understand the relationships among variables. The first of these guidelines underscores the importance of displaying query terms in context: Whenever possible all of the query terms should appear in the caption, reflecting their relationship to the associated page. If a query term is missing from a caption, the user may have no idea why the result was returned. The results for the MatchAll feature directly support this guideline. The results for TermMatchTitle and TermMatchTSU confirm that matching more terms is desirable. Other features provide additional indirect support for this guideline, and none of the results are inconsistent with it. A second guideline speaks to the desirability of presenting the user with a readable snippet: When query terms are present in the title, they need not be repeated in the snippet. In particular, when a high-quality query-independent summary is available from an external source, such as a Web directory, it may be more appropriate to display this summary than a lower-quality query-dependent fragment selected on-the-fly. When titles are available from multiple sources -the header, the body, Web directories - a caption generation algorithm might a select a combination of title, snippet and URL that includes as many of the query terms as possible. When a title containing all query terms can be found, the algorithm might select a query-independent snippet. The MatchAll and Readable features directly support this guideline. Once again, other features provide indirect support, and none of the results are inconsistent with it. Finally, the length and complexity of a URL influences user behavior. When query terms appear in the URL they should highlighted or otherwise distinguished. When multiple URLs reference the same page (due to re-directions, etc.) the shortest URL should be preferred, provided that all query terms will still appear in the caption. In other words, URLs should be selected and displayed in a manner that emphasizes their relationship to the query. The three URL features, as well as TermMatchTSU, directly support this guideline. The influence of the Official and Image features led us to wonder what other terms are prevalent in the captions of clickthrough inversions. As an additional experiment, we treated each of the terms appearing in the INV and CON sets as a separate feature (case normalized), ranking them by their \u03c72 values. The results are presented in figure 6. Since we use the \u03c72 statistic as a divergence measure, rather than a significance test, no p-values are given. The final column of the table indicates the direction of the influence, whether the presence of the terms positively or negatively influence clickthroughs. The positive influence of official has already been observed (the difference in the \u03c72 value from that of figure 5 is due to stemming). \u2193 \u2193 \u2191 \u2191 \u2191 \u2191 \u2193 \u2191 \u2193 \u2191 Figure 6: Words exhibiting the greatest positive (\u2191) and negative (\u2193) influence on clickthrough patterns. feature appear in the top ten, but pictures and photos appear at positions 21 and 22. The high rank given to and may be related to readability (the term the appears in position 20). Most surprising to us is the negative influence of the terms: encyclopedia, wikipedia, free, and medlineplus. The first three terms appear in the title of Wikipedia articles3 and the last appears in the title of MedlinePlus articles4 These individual word-level features provide hints about issues. More detailed analyses and further experiments will be required to understand these features. 6. CONCLUSIONS Clickthrough inversions form an appropriate tool for assessing the influence of caption features. Using clickthrough inversions, we have demonstrated that relatively simple caption features can significantly influence user behavior. To our knowledge, this is first methodology validated for assessing the quality of Web captions through implicit feedback. In the future, we hope to substantially expand this work, considering more features over larger datasets. We also hope to directly address the goal of predicting relevance from clickthoughs and other information present in search engine logs. 7. ACKNOWLEDGMENTS This work was conducted while the first author was visiting Microsoft Research. The authors thank members of the Windows Live team for their comments and assistance, particularly Girish Kumar, Luke DeLorme, Rohit Wad and Ramez Naam.", "body1": "The major commercial Web search engines all present their results in much the same way. Figure 1 shows a typical Web search, with captions for the top three results. While these differences may seem minor, they may also have a substantial impact on user behavior. When it fails, the user may waste her time clicking through to an inappropriate result and scanning a page containing little or nothing of interest. All three of the results in figure 1 are relevant, with some limitations. In this paper, we examine the influence of caption features on user\"s Web search behavior, using clickthroughs extracted from search engines logs as our primary investigative tool. captions themselves. Snippets may be generated in a query-independent fashion, providing a summary of the page as a whole, or in a querydependent fashion, providing a summary of how the page relates to the query terms. When these alternative snippets, titles and URLs are available, the selection of an appropriate combination for display may be guided by their features. The work reported in this paper was undertaken in the context of the Windows Live search engine. While commercial Web search engines have followed similar approaches to caption display since their genesis, relatively little research has been published about methods for generating these captions and evaluating their impact on user behavior. 2.1 Display of Web results Varadarajan and Hristidis are among the few who have attempted to improve directly upon the snippets generated by commercial search systems, without introducing additional changes to the interface. Many researchers have explored alternative methods for displaying Web search results. White et al. Luhn\"s approach uses term frequencies to identify significant words within a document and then selects and extracts sentences that contain significant words in close proximity. A considerable fraction of later work may be viewed as extending and tuning this basic approach, developing improved methods for identifying significant words and selecting sentences. the annual Document Understanding Conference (DUC) series, conducted by the US National Institute of Standards and Technology, has provided a vehicle for evaluating much of the research in document summarization2 . Evaluation at DUC is achieved through comparison with manually generated summaries. A number of other researchers have examined the value of query-dependent summarization in a non-Web context. Tombros and Sanderson compared the performance of 20 subjects searching a collection of newspaper articles when duc.nist.gov guided by query-independent vs. query-dependent snippets. The query-independent snippets were created by extracting the first few sentences of the articles; the query-dependent snippets were created by selecting the highest scoring sentences under a measure biased towards sentences containing query terms. Goldstein et al. 2.3 Clickthroughs Queries and clickthroughs taken from the logs of commercial Web search engines have been widely used to improve the performance of these systems and to better understand how users interact with them. Lee et al. Agichtein et al. Of particular interest to us is the work of Joachims et al. While other researchers have evaluated the display of Web search results through user studies - presenting users with a small number of different techniques and asking them to complete experimental tasks - we approach the problem by extracting implicit feedback from search engine logs. The problem remains of interpreting the information in these logs as implicit indicators of user preferences, and in this matter we are guided by the work of Joachims et al. We consider caption pairs, which appear adjacent to one another in the result list. Our primary tool for examining the influence of caption features is a type of pattern observed with respect to these caption pairs, which we call a clickthrough inversion. 3.1 Extracting clickthroughs For the experiments reported in this paper, we sampled a subset of the queries and clickthroughs from the logs of the Windows Live search engine over a period of 3-4 days on three separate occasions: once for results reported in section 3.3, once for a pilot of our main experiment, and once for the experiment itself (sections 4 and 5). By focusing on the initial clickthrough, we hope to capture a user\"s impression of the relative relevance within a caption pair when first encountered. Given the dynamic nature of the Web and the volumes of data involved, search engine logs are bound to contain considerable noise. The outcome at the end of each sampling period is a set of records, with each record describing the clickthroughs for a given query/result combination. 3.2 Clickthrough curves It could be argued that under ideal circumstances, clickthrough inversions would not be present in search engine logs. Figure 2 provides clickthrough curves for three example queries. Several causes may be enlisted to explain the presence of an inversion in a clickthrough curve. In order to examine the extent to which relevance plays a role in clickthrough inversions, we conducted an initial experiment using a set of 1,811 queries with associated judgments created as part of on-going work. Relevance was assessed on a 6-point scale. pair (B). As we see in the figure, relevance alone appears inadequate to explain the majority of clickthrough inversions. Having demonstrated that clickthrough inversions cannot always be explained by relevance differences, we explore what features of caption pairs, if any, lead users to prefer one caption over another. The first is a set of nearly five thousand clickthrough inversions, extracted according to the procedure described in section 3.1. We extract a number of features characterizing snippets (described in detail in the next section) and compare the presence of each feature in the INV and CON sets. When the feature favors caption A, we refer to it as a negative pair. Thus, for a specific feature, we can construct four subsets: 1) INV+, the set of positive pairs from INV; 2) INV\u2212, the set of negative pairs from INV; 3) CON+; the set of positive pairs from CON; and 4) CON\u2212 the set of negative pairs from CON. When stating a feature corresponding to a hypothesized user preference, we follow the practice of stating the feature with the expectation that the size of INV+ relative to the size of INV\u2212 should be greater than the size of CON+ relative to the size of CON\u2212. This evaluation methodology allows us to construct a contingency table for each feature, with INV essentially forming the experimental group and CON the control group. 4.2 Features Figure 4 lists the features tested. For this pilot we collected INV and CON sets of similar sizes, and used these sets to evaluate a preliminary list of features and to establish appropriate parameters for the SnippetShort and Readable features. The first feature concerns the existence of a snippet and the second concerns the relative size of snippets. The next six features concern the location and number of matching query terms. The next three features concern the URLs, capturing aspects of their length and complexity, and the last four features concern caption content. The last content feature (Readable) applies an ad-hoc readability metric to each snippet. Feature Tag INV+ INV\u2212 %+ CON+ CON\u2212 %+ \u03c72 p-value MissingSnippet 185 121 60.4 144 133 51. SnippetShort 20 6 76.9 12 16 42. TermMatchTitle 800 559 58.8 660 700 48. < TermMatchTS 310 213 59.2 269 216 55. TermMatchTSU 236 138 63.1 189 149 55. 933 53. 45. < QueryPhraseMatch 465 346 57.3 427 422 50. MatchAll 8 2 80.0 1 4 20. URLQuery 277 188 59.5 159 315 33. < 55. 43. < 50. 43. < Official 215 142 60.2 133 215 38. < Home 62 49 55.8 64 82 43. Image 391 270 59.1 315 335 48. < Readable 52 43 54.7 31 48 39. Figure 5: Results corresponding to the features listed in figure 4 with \u03c72 and p-values (df = 1). 4.3 Results Figure 5 presents the results. The results support claims that missing snippets, short snippets, missing query terms and complex URLs negatively impact clickthroughs. Further work is needed to provide additional support for the guidelines and to understand the relationships among variables. The first of these guidelines underscores the importance of displaying query terms in context: Whenever possible all of the query terms should appear in the caption, reflecting their relationship to the associated page. A second guideline speaks to the desirability of presenting the user with a readable snippet: When query terms are present in the title, they need not be repeated in the snippet. Finally, the length and complexity of a URL influences user behavior. The influence of the Official and Image features led us to wonder what other terms are prevalent in the captions of clickthrough inversions. feature appear in the top ten, but pictures and photos appear at positions 21 and 22. Most surprising to us is the negative influence of the terms: encyclopedia, wikipedia, free, and medlineplus.", "body2": "Often the snippet is extracted from the Web page itself, but it may also be taken from external sources, such as the human-generated summaries found in Web directories. The snippet of the first caption consists of a complete sentence that concisely describes the associated page, while the snippet of the third caption consists of two incomplete sentences that are largely unrelated to the overall contents of the associated page and to the apparent intent of the query. When this judgment is correct, it can speed the search process by allowing the user to avoid unwanted material. Even worse, the user may be misled into skipping a page that contains desired information. Unfortunately, these page characteristics are not entirely reflected in the captions. Understanding this influence may help to validate algorithms and guidelines for the improved generation of the Figure 1: Top three results for the query: kids online games. Different caption generation algorithms might select snippets of different lengths from different areas of a page. Moreover, for pages listed in human-edited Web directories such as the Open Directory Project1 , it may be possible to display alternative titles and snippets derived from these listings. A URL before re-direction may be shorter and provide a clearer idea of the final destination. In addition, we believe our methodology may be generalized to other search applications when sufficient clickthrough data is available. Most research on the display of Web results has proposed substantial interface changes, rather than addressing details of the existing interfaces. Cutrell and Guan conducted an eye-tracking study to investigate the influence of snippet length on Web search performance and found that the optimal snippet length varied according to the task type, with longer snippets leading to improved performance for informational tasks and shorter snippets for navigational tasks. propose an interface based on a fisheye lens, in which mouse hovers and other events cause captions to zoom and snippets to expand with additional text. The basic idea of extractive summarization - creating a summary by selecting sentences or fragments - goes back to the foundational work of Luhn . Luhn\"s approach uses term frequencies to identify significant words within a document and then selects and extracts sentences that contain significant words in close proximity. At its simplest, snippet generation for Web captions might also be viewed as following this approach, with query terms taking on the role of significant words. The majority of participating systems use extractive summarization, but a number attempt natural language generation and other approaches. We view our approach of evaluating summarization through the analysis of Web logs as complementing the approach taken at DUC. A number of other researchers have examined the value of query-dependent summarization in a non-Web context. Tombros and Sanderson compared the performance of 20 subjects searching a collection of newspaper articles when duc.nist.gov guided by query-independent vs. query-dependent snippets. When query-dependent summaries were presented, subjects were better able to identify relevant documents without clicking through to the full text. They introduce normalized measures of recall and precision to facilitate evaluation. Under their taxonomy, a transactional query as defined by Broder might fall under either of their three categories, depending on details of the desired transaction. Under their taxonomy, a transactional or resource query would be subsumed under one of these two categories. In effect, they are added to the document content for indexing and ranking purposes. Our examination of large search logs compliments their detailed analysis of a smaller number of participants. Examining user behavior in situ allows us to consider many more queries and caption characteristics, with the volume of available data compensating for the lack of a controlled lab environment. . We consider caption pairs, which appear adjacent to one another in the result list. For simplicity, in the remainder of this paper we refer to the higher ranking caption in a pair as caption A and the lower ranking caption as caption B. Users are identified by IP address, which is a reasonably reliable method of eliminating multiple results from a single user, at the cost of falsely eliminating results from multiple users sharing the same address. In the remainder of the paper, we use the term clickthrough to refer only to this initial action. 10 20 30 40 50 60 70 80 90 100 1 2 3 4 5 6 7 8 9 10 clickthroughpercent position a) craigslist 10 20 30 40 50 60 70 80 90 100 1 2 3 4 5 6 7 8 9 10 clickthroughpercent position b) periodic table of elements 10 20 30 40 50 60 70 80 90 100 1 2 3 4 5 6 7 8 9 10 clickthroughpercent position c) kids online games Figure 2: Clickthrough curves for three queries: a) a stereotypical navigational query, b) a stereotypical informational query, and c) a query exhibiting clickthrough inversions. We then processed this set to generate clickthrough curves and identify inversions. Later results would complement earlier ones, linking to novel or supplementary material, and ordered by their interest to the greatest number of users. For example, for the third query (kids online games) the clickthrough curve exhibits a number of clickthrough inversions, with an apparent preference for the result at position 4. 3.3 Relevance The simplest explanation for the presence of a clickthrough inversion is a relevance difference between the higher ranking member of caption pair and the lower ranking member. The relevance judgments were made by independent assessors viewing the pages themselves, rather than the captions. Compares relevance between the higher ranking member of a caption pair (rel(A)) to the relevance of the lower ranking member (rel(B)), where caption A received fewer clicks than caption B. For example, the first row rel(A) < rel(B), indicates that the higher ranking member of pair (A) was rated as less relevant than the lower ranking member of the pair (B). For 28.7% of the inversions, A has greater relevance than B, which received the greater number of clickthroughs. 4.1 Evaluation methodology Following this line of reasoning, we extracted two sets of caption pairs from search logs over a three day period. We refer to the first set, containing clickthrough inversions, as the INV set; we refer to the second set, containing caption pairs for which the clickthroughs are consistent with their rank order, as the CON set. When the feature favors caption B (consistent with a clickthrough inversion) we refer to the caption pair as a positive pair. For missing snippets, a positive pair has the caption missing in caption A (but not B) and a negative pair has the caption missing in B (but not A). The sets INV+, INV\u2212, CON+, and CON\u2212 will contain different subsets of INV and CON for each feature. For example, we state the missing snippet feature as snippet missing in caption A and present in caption B. We can then apply Pearson\"s chi-square test for significance. Other features suggested themselves during the examination of the snippets collected as part of the study described in section 3.3 and during a pilot of the evaluation methodology (section 4.1). These features all capture simple aspects of the captions. These pairs are not included in the sets constructed for the remaining features, since captions with missing snippets do not contain all the elements of a standard caption and we wanted to avoid their influence. The MatchAll feature tests the idea that matching all the query terms exactly once is preferable to matching a subset of the terms many times with a least one query term unmatched. Terms represented by this feature include pictures, pics, and gallery. If these words represent more than 40% of one caption and less than 10% of the other, the pair is included in the appropriate set. The p-value for the MatchAll feature is computed using Fisher\"s Exact Test. Features supported at the 95% confidence level are bolded. While our results do not direct address all of the many variables that influence users understanding of captions, they are consistent with the major guidelines. Further work is needed to provide additional support for the guidelines and to understand the relationships among variables. Other features provide additional indirect support for this guideline, and none of the results are inconsistent with it. Once again, other features provide indirect support, and none of the results are inconsistent with it. The three URL features, as well as TermMatchTSU, directly support this guideline. \u2193 \u2193 \u2191 \u2191 \u2191 \u2191 \u2193 \u2191 \u2193 \u2191 Figure 6: Words exhibiting the greatest positive (\u2191) and negative (\u2193) influence on clickthrough patterns. The high rank given to and may be related to readability (the term the appears in position 20). More detailed analyses and further experiments will be required to understand these features.", "introduction": "The major commercial Web search engines all present their results in much the same way. Each search result is described by a brief caption, comprising the URL of the associated Web page, a title, and a brief summary (or snippet) describing the contents of the page. Often the snippet is extracted from the Web page itself, but it may also be taken from external sources, such as the human-generated summaries found in Web directories. Figure 1 shows a typical Web search, with captions for the top three results. While the three captions share the same basic structure, their content differs in several respects. The snippet of the third caption is nearly twice as long as that of the first, while the snippet is missing entirely from the second caption. The title of the third caption contains all of the query terms in order, while the titles of the first and second captions contain only two of the three terms. One of the query terms is repeated in the first caption. All of the query terms appear in the URL of the third caption, while none appear in the URL of the first caption. The snippet of the first caption consists of a complete sentence that concisely describes the associated page, while the snippet of the third caption consists of two incomplete sentences that are largely unrelated to the overall contents of the associated page and to the apparent intent of the query. While these differences may seem minor, they may also have a substantial impact on user behavior. A principal motivation for providing a caption is to assist the user in determining the relevance of the associated page without actually having to click through to the result. In the case of a navigational query - particularly when the destination is well known - the URL alone may be sufficient to identify the desired page. But in the case of an informational query, the title and snippet may be necessary to guide the user in selecting a page for further study, and she may judge the relevance of a page on the basis of the caption alone. When this judgment is correct, it can speed the search process by allowing the user to avoid unwanted material. When it fails, the user may waste her time clicking through to an inappropriate result and scanning a page containing little or nothing of interest. Even worse, the user may be misled into skipping a page that contains desired information. All three of the results in figure 1 are relevant, with some limitations. The first result links to the main Yahoo Kids! homepage, but it is then necessary to follow a link in a menu to find the main page for games. Despite appearances, the second result links to a surprisingly large collection of online games, primarily with environmental themes. The third result might be somewhat disappointing to a user, since it leads to only a single game, hosted at the Centers for Disease Control, that could not reasonably be described as online. Unfortunately, these page characteristics are not entirely reflected in the captions. In this paper, we examine the influence of caption features on user\"s Web search behavior, using clickthroughs extracted from search engines logs as our primary investigative tool. Understanding this influence may help to validate algorithms and guidelines for the improved generation of the Figure 1: Top three results for the query: kids online games. In addition, these features can play a role in the process of inferring relevance judgments from user behavior . By better understanding their influence, better judgments may result. Different caption generation algorithms might select snippets of different lengths from different areas of a page. Snippets may be generated in a query-independent fashion, providing a summary of the page as a whole, or in a querydependent fashion, providing a summary of how the page relates to the query terms. The correct choice of snippet may depend on aspects of both the query and the result page. The title may be taken from the HTML header or extracted from the body of the document . For links that re-direct, it may be possible to display alternative URLs. Moreover, for pages listed in human-edited Web directories such as the Open Directory Project1 , it may be possible to display alternative titles and snippets derived from these listings. When these alternative snippets, titles and URLs are available, the selection of an appropriate combination for display may be guided by their features. A snippet from a Web directory may consist of complete sentences and be less fragmentary than an extracted snippet. A title extracted from the body may provide greater coverage of the query terms. A URL before re-direction may be shorter and provide a clearer idea of the final destination. The work reported in this paper was undertaken in the context of the Windows Live search engine. The image in figure 1 was captured from Windows Live and cropped to eliminate branding, advertising and navigational elements. The experiments reported in later sections are based on Windows Live query logs, result pages and relevance judgments collected as part of ongoing research into search engine performance . Nonetheless, given the similarity of caption formats across the major Web search engines we believe the results are applicable to these other engines. The query in www.dmoz.org figure 1 produces results with similar relevance on the other major search engines. This and other queries produce captions that exhibit similar variations. In addition, we believe our methodology may be generalized to other search applications when sufficient clickthrough data is available.", "conclusion": "Clickthrough inversions form an appropriate tool for assessing the influence of caption features.. Using clickthrough inversions, we have demonstrated that relatively simple caption features can significantly influence user behavior.. To our knowledge, this is first methodology validated for assessing the quality of Web captions through implicit feedback.. In the future, we hope to substantially expand this work, considering more features over larger datasets.. We also hope to directly address the goal of predicting relevance from clickthoughs and other information present in search engine logs.. ACKNOWLEDGMENTS This work was conducted while the first author was visiting Microsoft Research.. The authors thank members of the Windows Live team for their comments and assistance, particularly Girish Kumar, Luke DeLorme, Rohit Wad and Ramez Naam."}
{"id": "I-7", "keywords": ["multiag system", "game theori", "commit", "extort"], "title": "Commitment and Extortion", "abstract": "Making commitments, e.g., through promises and threats, enables a player to exploit the strengths of his own strategic position as well as the weaknesses of that of his opponents. Which commitments a player can make with credibility depends on the circumstances. In some, a player can only commit to the performance of an ac-tion, in others, he can commit himself conditionally on the actions of the other players. Some situations even allow for commitments on commitments or for commitments to randomized actions. We explore the formal properties of these types of (conditional) com-mitment and their interrelationships. So as to preclude inconsis-tencies among conditional commitments, we assume an order in which the players make their commitments. Central to our analy-ses is the notion of an extortion, which we define, for a given order of the players, as a profile that contains, for each player, an optimal commitment given the commitments of the players that committed earlier. On this basis, we investigate for different commitment types whether it is advantageous to commit earlier rather than later, and how the outcomes obtained through extortions relate to backward induction and Pareto efficiency.", "references": ["Contextualizing commitment protocols", "Computing the optimal strategy to commit to", "A simplified bargaining model for the n-person cooperative game", "Games and Decisions: Introduction and Critical Survey", "Two-person cooperative games", "A Course in Game Theory", "How to commit to cooperation", "Leveled-commitment contracting. A backtracking instrument for multiagent systems", "The Strategy of Conflict", "Spieltheoretische Behandlung eines Oligopolmodells mit Nachfragetr\u00a8agheit", "An ontology for commitments in multiagent systems: Toward a unification of normative concepts", "Program equilibrium", "Commitment robust equilibria and endogenous timing", "The Theory of Games and Economic Behavior", "Marktform und Gleichgewicht", "Leadership with commitment to mixed strategies"], "full_text": "1. INTRODUCTION On one view, the least one may expect of game theory is that it provides an answer to the question which actions maximize an agent\"s expected utility in situations of interactive decision making. A slightly divergent view is expounded by Schelling when he states that strategy [. . . ] is not concerned with the efficient application of force but with the exploitation of potential force [9, page 5]. From this perspective, the formal model of a game in strategic form only outlines the strategic features of an interactive situation. Apart from merely choosing and performing an action from a set of actions, there may also be other courses open to an agent. E.g., the strategic lie of the land may be such that a promise, a threat, or a combination of both would be more conductive to his ends. The potency of a promise, however, essentially depends on the extent the promisee can be convinced of the promiser\"s resolve to see to its fulfillment. Likewise, a threat only succeeds in deterring an agent if the latter can be made to believe that the threatener is bound to execute the threat, should it be ignored. In this sense, promises and threats essentially involve a commitment on the part of the one who makes them, thus purposely restricting his freedom of choice. Promises and threats epitomize one of the fundamental and at first sight perhaps most surprising phenomena in game theory: it may occur that a player can improve his strategic position by limiting his own freedom of action. By commitments we will understand such limitations of one\"s action space. Action itself could be seen as the ultimate commitment. Performing a particular action means doing so to the exclusion of all other actions. Commitments come in different forms and it may depend on the circumstances which ones can and which ones cannot credibly be made. Besides simply committing to the performance of an action, an agent might make his commitment conditional on the actions of other agents, as, e.g., the kidnapper does, when he promises to set free a hostage on receiving a ransom, while threatening to cut off another toe, otherwise. Some situations even allow for commitments on commitments or for commitments to randomized actions. By focusing on the selection of actions rather than on commitments, it might seem that the conception of game theory as mere interactive decision theory is too narrow. In this respect, Schelling\"s view might seem to evince a more comprehensive understanding of what game theory tries to accomplish. One might object, that commitments could be seen as the actions of a larger game. In reply to this criticism Schelling remarks: While it is instructive and intellectually satisfying to see how such tactics as threats, commitments, and promises can be absorbed in an enlarged, abstract supergame (game in normal form), it should be emphasized that we cannot learn anything about those tactics by studying games that are already in normal form. [. . . ] What we want is a theory that systematizes the study of the various universal ingredients that make up the move-structure of games; too abstract a model will miss them. [9, pp. 156-7] 108 978-81--7-5 (RPS) IFAAMAS Our concern is with these commitment tactics, be it that our analysis is confined to situations in which the players can commit in a given order and where we assume the commitments the players can make are given. Despite Schelling\"s warning for too abstract a framework, our approach will be based on the formal notion of an extortion, which we will propose in Section 4 as a uniform tactic for a comprehensive class of situations in which commitments can be made sequentially. On this basis we tackle such issues as the usefulness of certain types of commitment in different situations (strategic games) or whether it is better to commit early rather than late. We also provide a framework for the assessment of more general game theoretic matters like the relationship of extortions to backward induction or Pareto efficiency. Insight into these matters has proved itself invaluable for a proper understanding of diplomatic policy during the Cold War. Nowadays, we believe, these issues are equally significant for applications and developments in such fields as multiagent systems, distributed computing and electronic markets. For example, commitments have been argued to be of importance for interacting software agents as well as for mechanism design. In the former setting, the inability to re-program a software agent on the fly can be seen as a commitment to its specification and thus exploited to strengthen its strategic position in a multiagent setting. A mechanism, on the other hand, could be seen as a set of commitments that steers the players\" behavior in a certain desired way (see, e.g., ). Our analysis is conceptually similar to that of Stackelberg or leadership games , which have been extensively studied in the economic literature (cf., ). These games analyze situations in which a leader commits to a pure or mixed strategy, and a number of followers, who then act simultaneously. Our approach, however, differs in that it is assumed that the players all move in a particular order-first, second, third and so on-and that it is specifically aimed at incorporating a wide range of possible commitments, in particular conditional commitments. After briefly discussing related work in Section 2, we present the formal game theoretic framework, in which we define the notions of a commitment type as well as conditional and unconditional commitments (Section 3). In Section 4 we propose the generic concept of an extortion, which for each commitment type captures the idea of an optimal commitment profile. We point out an equivalence between extortions and backward induction solutions, and investigate whether it is advantageous to commit earlier rather than later and how the outcomes obtained through extortions relate to Pareto efficiency. Section 5 briefly reviews some other commitment types, such as inductive, mixed and mixed conditional commitments. The paper concludes with an overview of the results and an outlook for future research in Section 6. 2. RELATED WORK Commitment is a central concept in game theory. The possibility to make commitments distinguishes cooperative from noncooperative game theory . Leadership games, as mentioned in the introduction, analyze commitments to pure or mixed strategies in what is essentially a two-player setting . Informally, Schelling has emphasized the importance of promises, threats and the like for a proper understanding of social interaction. On a more formal level, threats have also figured in bargaining theory. Nash\"s threat game and Harsanyi\"s rational threats are two important early examples. Also, commitments have played a significant role in the theory of equilibrium selection (see, e.g., . Over the last few years, game theory has become almost indispensable as a research tool for computer science and (multi)agent research. Commitments have by no means gone unnoticed (see, \u23a2\u23a2\u23a2\u23a2\u23a3 (1, 3) (3, 2) (0, 0) (2, 1) \u23a5\u23a5\u23a5\u23a5\u23a6 Figure 1: Committing to a dominated strategy can be advantageous. e.g., ). Recently, also the strategic aspects of commitments have attracted the attention of computer scientists. Thus, Conitzer and Sandholm have studied the computational complexity of computing the optimal strategy to commit to in normal form and Bayesian games. Sandholm and Lesser employ levelled commitments for the design of multiagent systems in which contractual agreements are not fully binding. Another connection between commitments and computer science has been pointed out by Samet and Tennenholtz . Their point of departure is the observation that programs can be used to formulate commitments that are conditional on the programs of other systems. Our approach is similar to the Stackleberg setting in that we assume an order in which the players commit. We, however, consider a number of different commitment types, among which conditional commitments, and propose a generic solution concept. 3. COMMITMENTS By committing, an agent can improve his strategic position. It may even be advantageous to commit to a strategy that is strongly dominated, i.e., one for which there is another strategy that yields a better payoff no matter how the other agents act. Consider for example the 2\u00d72 game in Figure 1, in which one player, Row, chooses rows and another, Col, chooses columns. The entries in the matrix indicate the payoffs to Row and Col, respectively. Then, top-left is the solution obtained by iterative elimination of strongly dominated strategies: for Row, playing top is always better than playing bottom, and assuming that Row will therefore never play bottom, left is always better than right for Col. However, if Row succeeds in convincing Col of his commitment to play bottom, the latter had better choose the right column. Thus, Row attains a payoff of two instead of one. Along a similar line of reasoning, however, Col would wish to commit to the left column, as convincing Row of this commitment guarantees him the most desirable outcome. If, on the other hand, both players actually commit themselves in this way but without convincing the other party of their having done so, the game ends in misery for both. Important types of commitments, however, cannot simply be analyzed as unconditional commitments to actions. The essence of a threat, for example, is deterrence. If successful, it is not carried out. (This is also the reason why the credibility of a threat is not necessarily undermined if its putting into effect means that the threatener is also harmed.) By contrast, promises are made to entice and, as such, meant to be fulfilled. Thus, both threats and promises would be strategically void if they were unconditional. Figure 2 shows an example, in which Col can guarantee himself a payoff of three by threatening to choose the right column if Row chooses top. (This will suffice to deter Row, and there is no need for an additional promise on the part of Col.) He cannot do so by merely committing unconditionally, and neither can Row if he were to commit first. In the case of conditional commitments, however, a particular kind of inconsistency can arise. It is not in general the case that any two commitments can both be credible. In a 2 \u00d7 2 game, it could occur that Row commits conditionally on playing top if the The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 109 \u23a2\u23a2\u23a2\u23a2\u23a3 (2, 2) (0, 0) (1, 3) (3, 1) \u23a5\u23a5\u23a5\u23a5\u23a6 Figure 2: The column player Col can guarantee himself a payoff of three by threatening to play right if the row player Row plays top. Col plays left, and bottom, otherwise. If now, Col simultaneously were able to commit to the conditional strategy to play right if Row plays top, and left, otherwise, there is no strategy profile that can be played without one of the players\" bluff being called. To get around this problem, one can write down conditional commitments in the form of rules and define appropriate fixed point constructions, as suggested by Samet and Tennenholtz . Since checking the semantic equivalence of two commitments (or commitment conditions) is undecidable in general, Tennenholtz bases his definition of program equilibrium on syntactic equivalence. We, by contrast, try to steer clear from fixed point constructions by assuming that the players make their commitment in a particular order. Each player can then make his commitments dependent on the actions of the players to commit after him, but not on the commitments of the players that committed before. On the issue how this order comes about we do not here enter. Rather, we assume it to be determined by the circumstances, which may force or permit some players to commit earlier and others later. We will find that it is not always beneficial to commit earlier than later or vice versa. Another point to heed is that we only consider the case in which the commitments are considered absolutely binding. We do not take into account commitments that can be violated. Intuitively, this could be understood as that the possibility of violation fatally undermines the credibility of the commitment. We also assume commitments to be complete, in the sense that they fully lay down a player\"s behavior in all foreseeable circumstances. These assumptions imply that the outcome of the game is entirely determined by the commitments the players make. Although these might be implausible assumptions for some situations, we had better study the idealized case first, before tackling the complications of the more general case. To make these concepts formally precise, we first have to fix some notation. 3.1 Strategic Games A strategic game is a tuple (N, (Ai)i\u2208N, (ui)i\u2208N), where N = {1, . . . , n} is a finite set of players, Ai is a set of actions available to player i and ui a real-valued utility function for player i on the set of (pure) strategy profiles S = A1\u00d7\u00b7 \u00b7 \u00b7\u00d7An. We call a game finite if for all players i the action set Ai is finite. A mixed strategy \u03c3i for a player i is a probability distribution over Ai. We write \u03a3i for the set of mixed strategies available to player i, and \u03a3 = \u03a31 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 \u03a3n for the set of mixed strategy profiles. We further have \u03c3(a) and \u03c3i(a) denote the probability of action a in mixed strategy profile \u03c3 or mixed strategy \u03c3i, respectively. In settings involving expected utility, we will generally assume that utility functions represent von Neumann-Morgenstern preferences. For a player i and (mixed) strategy profiles \u03c3 and \u03c4 we write \u03c3 i \u03c4 if ui (\u03c3) ui (\u03c4). 3.2 Conditional Commitments Relative to a strategic game (N, (Ai)i\u2208N, (ui)i\u2208N) and an ordering \u03c0 = (\u03c01, . . . , \u03c0n) of the players, we define the set F\u03c0i of (pure) conditional commitments of a player \u03c0i as the set of functions from A\u03c01 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 A\u03c0i\u22121 to A\u03c0i . For \u03c01 we have the set of conditional commitments coincide with A\u03c01 . By a conditional commitment profile f we understand any combination of conditional commitments in F\u03c01 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 F\u03c0n . Intuitively, \u03c0 reflects the sequential order in which the players can make their commitments, with \u03c0n committing first, \u03c0n\u22121 second, and so on. Each player can condition his action on the actions of all players that are to commit after him. In this manner, each conditional commitment profile f can be seen to determine a unique strategy profile, denoted by f , which will be played if all players stick to their conditional commitments. More formally, the strategy profile f = (f\u03c01 , . . . , f\u03c0n ) for a conditional commitment profile f is defined inductively as f\u03c01 =df. f\u03c01 f\u03c0i+1 =df. f\u03c0i+1 (f\u03c01 , . . . , f\u03c0i ). The sequence f\u03c01 , (f\u03c01 , f\u03c02 ), . . . , (f\u03c01 , . . . , f\u03c0n ) will be called the path of f . E.g., in the two-player game of Figure 2 and given the order (Row, Col), Row has two conditional commitments, top and bottom, which we will henceforth denote t and b. Col, on the other hand, has four conditional commitments, corresponding to the different functions mapping strategies of Row to those of Col. If we consider a conditional commitment f for Col such that f (t) = l and f (b) = r, then (t, f ) is a conditional commitment profile and (t, f ) = (t, f (t)) = (t, l). There is a natural way in which a strategic game G together with an ordering (\u03c01, . . . , \u03c0n) of the players can be interpreted as an extensive form game with perfect information (see, e.g., )1 , in which \u03c01 chooses his action first, \u03c02 second, and so on. Observe that under this assumption the strategies in the extensive form game and the conditional commitments in the strategic game G with ordering \u03c0 are mathematically the same objects. Applying backward induction to the extensive form game yields subgame perfect equilibria, which arguably provide appropriate solutions in this setting. From the perspective of conditional commitments, however, players move in reverse order. We will argue that under this interpretation other strategy profiles should be singled out as appropriate. To illustrate this point, consider once more the game in Figure 2 and observe that neither player can improve on the outcome obtained via iterated strong dominance by committing unconditionally to some strategy. Situations like this, in which players can make unconditional commitments in a fixed order, can fruitfully be analyzed as extensive form games, and the most lucrative unconditional commitment can be found through backward induction. Figure 3 shows the extensive form associated with the game of Figure 2. The strategies available to the row player are the same as in the strategic form: choosing the top or the bottom row. The strategies for the column player in the extensive game are given by the four functions that map strategies of the row player in the strategic game to one of his own. Transforming this extensive form back into a strategic game (see Figure 4), we find that there exists a second equilibrium besides the one found by means of backward induction. This equilibrium with outcome (1, 3), indicated by the thick lines in Figure 3, has been argued to be unacceptable in the sequential game as it would involve an incredible threat by Col: once Row has played top, Col finds himself confronted with a fait accompli. He had better make the best of a bad bargain and opt for the left column after all. This is in essence the line of thought Selten followed in his famous argument for subgame perfect equilibria . If, however, the strategies of Col in the extensive form are thought of as his conditional commitments he can make in case For a formal definition of a game in extensive form, the reader consult one of the standard textbooks, such as or . In this paper all formal definitions are based on strategic games and orderings of the players only. 110 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Row Col Figure 3: Extensive form obtained from the strategic game of Figure 2 when the row player chooses an action first. The backward induction solution is indicated by dashed lines, the conditional commitment solution by solid ones. (The horizontal dotted lines do not indicate information sets, but merely indicate which players are to move when.) he moves first, the situation is radically different. Thus we also assume that it is possible for Col to make credible the threat to choose the right column if Row were to play top, so as to ensure the latter is always better off to play the bottom row. If Col can make a conditional commitment of playing the right column if Row chooses top, and the left column otherwise, this leaves Row with the easy choice between a payoff of zero or one, and Col may expect a payoff of three. This line of reasoning can be generalized to yield an algorithm for finding optimal conditional commitments for general twoplayer games: 1. Find a strategy profile s = (s\u03c01 , s\u03c02 ) with maximum payoff to player \u03c02, and set f\u03c01 = s\u03c01 and f\u03c02 (s\u03c01 ) = s\u03c02 2. For each t\u03c01 \u2208 A\u03c01 with t\u03c01 s\u03c01 , find a strategy t\u03c02 \u2208 A\u03c02 that minimizes u\u03c01 (t\u03c01 , t\u03c02 ), and set f\u03c02 (t\u03c01 ) = t\u03c02 3. If u\u03c01 (t\u03c01 , f\u03c02 (t\u03c01 )) u\u03c01 (s\u03c01 , s\u03c02 ) for all t\u03c01 s\u03c01 , return f . 4. Otherwise, find the strategy profile (s\u03c01 , s\u03c02 ) with the highest payoff to \u03c02 among the ones that have not yet been considered. Set f\u03c01 = s\u03c01 and f\u03c02 (s\u03c01 ) = s\u03c02 , and continue with Step 2. Generalizing the idea underlying this algorithm, we present in Section 4 the concept of an extortion, which applies to games with any number of players. For any order of the players an extortion contains, for each player, an optimal commitment given the commitments of the players that committed earlier. 3.3 Commitment Types So far, we have distinguished between conditional and unconditional commitments. If made sequentially, both of them determine a unique strategy profile in a given strategic game. This notion of sequential commitment allows for generalization and gives rise to the following definition of a (sequential) commitment type. Definition 3.1. (Sequential commitment type) A (sequential) commitment type \u03c4 associates with each strategic game G and each ordering \u03c0 of its players, a tuple X\u03c01 , . . . , X\u03c0n , \u03c6 , where X\u03c01 , . . . , X\u03c0n are (abstract) sets of commitments and \u03c6 is a function mapping each profile in X = X\u03c01 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 X\u03c0n to a (mixed) strategy profile of G. A commitment type X\u03c01 , . . . , X\u03c0n , \u03c6 is finite whenever X\u03c0i is finite for each i with 1 i n. Thus, the type of unconditional commitments associates with a game and an ordering \u03c0 of its players the tuple S \u03c01 , . . . , S \u03c0n , id , \u23a2\u23a2\u23a2\u23a2\u23a3 (2, 2) (2, 2) (0, 0) (0, 0) (1, 3) (3, 1) (1, 3) (3, 1) \u23a5\u23a5\u23a5\u23a5\u23a6 Figure 4: The strategic game corresponding to the extensive form of Figure 3 where id is the identity function. Similarly, F\u03c01 , . . . , F\u03c0n , is the tuple associated with the same game by the type of (pure) conditional commitments. 4. EXTORTIONS In the introduction, we argued informally how players could improve their position by conditionally committing. How well they can do, could be analyzed by means of an extensive game with the actions of each player being defined as the possible commitments he can make. Here, we introduce for each commitment type a corresponding notion of extortion, which is defined relative to a strategic game and an ordering of the players. Extortions are meant to capture the concept of a profile that contains, for each player, an optimal commitment given the commitments of the players that committed earlier. A complicating factor is that in finding a player\"s optimal commitment, one should not only take into account how such a commitment affects other players\" actions, but also how it enables them to make their commitments. Definition 4.1. (Extortions) Let G be a strategic game, \u03c0 an ordering of its players, and \u03c4 a commitment type. Let \u03c4(G, \u03c0) be given by X\u03c01 , . . . , X\u03c0n , \u03c6 . A \u03c4-extortion of order 0 is any commitment profile x \u2208 X\u03c01 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 X\u03c0n . For m > 0, a commitment profile x \u2208 X\u03c01 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 X\u03c0n is a \u03c4-extortion of order m in G given \u03c0 if x is an \u03c4-extortion of order m \u2212 1 with \u03c6 y\u03c01 , . . . , y\u03c0m , x\u03c0m+1 , . . . , x\u03c0n \u03c0m \u03c6 x\u03c01 , . . . , x\u03c0m , x\u03c0m+1 , . . . , x\u03c0n for all commitment profiles g in X with (y\u03c01 , . . . , y\u03c0m , x\u03c0m+1 , . . . , x\u03c0n ) a \u03c4-extortion of order m \u2212 1. A \u03c4-extortion is a commitment profile that is a \u03c4-extortion of order m for all m with 0 m n. Furthermore, we say that a (mixed) strategy profile \u03c3 is \u03c4-extortionable if there is some \u03c4-extortion x with \u03c6(x) = s. Thus, an extortion of order 1 is a commitment profile in which player \u03c01, makes a commitment that maximizes his payoff, given fixed commitments of the other players. An extortion of order m is an extortion of order m \u2212 1 that maximizes player m\"s payoff, given fixed commitments of the players \u03c0m+1 through \u03c0n. For the type of conditional commitments we have that any conditional commitment profile f is an extortion of order 0 and an extortion of an order m greater than 0 is any extortion of order m \u2212 1 for which: g\u03c01 , . . . , g\u03c0m , f\u03c0m+1 , . . . , f\u03c0n \u03c0m f\u03c01 , . . . , f\u03c0m , f\u03c0m+1 , . . . , f\u03c0n , for each conditional commitment profile g such that g\u03c01 , . . . , g\u03c0m , f\u03c0m+1 , . . . , f\u03c0n an extortion of order m \u2212 1. To illustrate the concept of an extortion for conditional commitments consider the three-player game in Figure 5 and assume \u23a2\u23a2\u23a2\u23a2\u23a3 (1, 4, 0) (1, 4, 0) (3, 3, 2) (0, 0, 2) \u23a5\u23a5\u23a5\u23a5\u23a6 \u23a2\u23a2\u23a2\u23a2\u23a3 (4, 1, 1) (4, 0, 0) (3, 3, 2) (0, 0, 2) \u23a5\u23a5\u23a5\u23a5\u23a6 Figure 5: A three-player strategic game The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 111 \u239c\u239c\u239c\u239c\u239c\u239c\u239c\u239d \u239f\u239f\u239f\u239f\u239f\u239f\u239f\u23a0 \u239c\u239c\u239c\u239c\u239c\u239c\u239c\u239d \u239f\u239f\u239f\u239f\u239f\u239f\u239f\u23a0 \u239c\u239c\u239c\u239c\u239c\u239c\u239c\u239d \u239f\u239f\u239f\u239f\u239f\u239f\u239f\u23a0 \u239c\u239c\u239c\u239c\u239c\u239c\u239c\u239d \u239f\u239f\u239f\u239f\u239f\u239f\u239f\u23a0 Row Col Mat \u239c\u239c\u239c\u239c\u239c\u239c\u239c\u239d \u239f\u239f\u239f\u239f\u239f\u239f\u239f\u23a0 \u239c\u239c\u239c\u239c\u239c\u239c\u239c\u239d \u239f\u239f\u239f\u239f\u239f\u239f\u239f\u23a0 \u239c\u239c\u239c\u239c\u239c\u239c\u239c\u239d \u239f\u239f\u239f\u239f\u239f\u239f\u239f\u23a0 \u239c\u239c\u239c\u239c\u239c\u239c\u239c\u239d \u239f\u239f\u239f\u239f\u239f\u239f\u239f\u23a0 \u239c\u239c\u239c\u239c\u239c\u239c\u239c\u239d \u239f\u239f\u239f\u239f\u239f\u239f\u239f\u23a0 \u239c\u239c\u239c\u239c\u239c\u239c\u239c\u239d \u239f\u239f\u239f\u239f\u239f\u239f\u239f\u23a0 \u239c\u239c\u239c\u239c\u239c\u239c\u239c\u239d \u239f\u239f\u239f\u239f\u239f\u239f\u239f\u23a0 \u239c\u239c\u239c\u239c\u239c\u239c\u239c\u239d \u239f\u239f\u239f\u239f\u239f\u239f\u239f\u23a0 Row Col Mat \u239c\u239c\u239c\u239c\u239c\u239c\u239c\u239d \u239f\u239f\u239f\u239f\u239f\u239f\u239f\u23a0 \u239c\u239c\u239c\u239c\u239c\u239c\u239c\u239d \u239f\u239f\u239f\u239f\u239f\u239f\u239f\u23a0 \u239c\u239c\u239c\u239c\u239c\u239c\u239c\u239d \u239f\u239f\u239f\u239f\u239f\u239f\u239f\u23a0 \u239c\u239c\u239c\u239c\u239c\u239c\u239c\u239d \u239f\u239f\u239f\u239f\u239f\u239f\u239f\u23a0 Figure 6: A conditional extortion f of order 1 (left) and an extortion g of order 3 (right). (Row, Col, Mat) to be the order in which the players commit. Figure 6 depicts the possible conditional commitments of the players in extensive forms, with the left branch corresponding to Row\"s strategy of playing the top row. Let f and g be the conditional commitment strategies indicated by the thick lines in the left and right figures respectively. Both f and g are extortions of order 1. In both f and g Row guarantees himself the higher payoff given the conditional commitments of Mat and Col. Only g, however, is also an extortion of order 2. To appreciate that f is not, consider the conditional commitment profile h in which Row chooses top and Col chooses right no matter how Row decides, i.e., h is such that hRow = t and hCol(t) = hCol(b) = r. Then, (hRow, hCol, fMat) is also an extortion of order 1, but yields Col a higher payoff than f does. We leave it to the reader to check that, by contrast, g is an extortion of order 3, and therewith an extortion per se. 4.1 Promises and Threats One way of understanding conditional extortions is by conceiving of them as combinations of precisely one promise and a number of threats. From the strategy profiles that can still be realized given the conditional commitments of players that have committed before him, a player tries to enforce the strategy profile that yields him as much payoff as possible. Hence, he chooses his commitment so as to render deviations from the path that leads to this strategy profile as unattractive as possible (\u2018threats\") and the desired strategy profile as appealing as possible (\u2018promises\") for the relevant players. If (s\u03c01 , . . . , s\u03c0n ) is such a desirable strategy profile for player \u03c0i and f\u03c0i his conditional commitment, the value of f\u03c0i (s\u03c01 , . . . , s\u03c0i\u22121 ) could be taken as his promise, whereas the values of f\u03c0i for all other (t\u03c01 , . . . , t\u03c0i\u22121 ) could be seen as constituting his threats. The higher the payoff is to the other players in a strategy profile a player aims for, the easier it is for him to formulate an effective threat. However, making appropriate threats in this respect does not merely come down to minimizing the payoffs to players to commit later wherever possible. A player should also take into account the commitments, promises and threats the following players can make on the basis of his and his predecessors\" commitments. This is what makes extortionate reasoning sometimes so complicated, especially in situations with more than two players. For example, in the game of Figure 5, there is no conditional extortion that ensures Mat a payoff of two. To appreciate this, consider the possible commitments Mat can make in case Row plays top and Col plays left (tl) and in case Row plays top and Col plays right (tr). If Mat commits to the right matrix in both cases, he virtually promises Row a payoff of four, leaving himself with a payoff of at most one. Otherwise, he puts Col in a position to deter Row from choosing bottom by threatening to choose the right column if the latter does so. Again, Mat cannot expect a payoff higher than one. In short, no matter how Mat conditionally commits, he will either enable Col to threaten Row into playing top or fail to lure Row into playing the bottom row. 4.2 Benign Backward Induction The solutions extortions provide can also be obtained by modeling the situation as an extensive form game and applying a backward inductive type of argument. The actions of the players in any such extensive form game are then given by their conditional commitments, which they then choose sequentially. For higher types of commitment, such as conditional commitments, such \u2018metagames\", however, grow exponentially in the number of strategies available to the players and are generally much larger than the original game. The correspondence between the backward induction solutions in the meta-game and the extortions of the original strategic game rather signifies that the concept of an extortion is defined properly. First we define the concept of benign backward induction in general relative to a game in strategic form together with an ordering of the players. Intuitively it reflects the idea that each player chooses for each possible combination of actions of his predecessors the action that yields the highest payoff, given that his successors do similarly. The concept is called benign backward induction, because it implies that a player, when indifferent between a number of actions, chooses the one that benefits his predecessors most. For an ordering \u03c0 of the players, we have \u03c0R denote its reversal (\u03c0n, . . . , \u03c01). Definition 4.2. (Benign backward induction) Let G be a strategic game and \u03c0 an ordering of its players. A benign backward induction of order 0 is any conditional commitment profile f subject to \u03c0. For m > 0, a conditional commitment strategy profile f is a benign backward induction (solution) of order m if f is a benign backward induction of order m \u2212 1 and (g\u03c0R , . . . , g\u03c0R m+1 , g\u03c0R , . . . , g\u03c0R ) \u03c0R (g\u03c0R , . . . , g\u03c0R m+1 , f\u03c0R , . . . , f\u03c0R for any backward induction (g\u03c0R ,..., g\u03c0R m+1 , g\u03c0R ,..., g\u03c0R ) of order m\u22121. A conditional commitment profile f is a benign backward induction if it is a benign backward induction of order k for each k with 0 k n. For games with a finite action set for each player, the following result follows straightforwardly from Kuhn\"s Theorem (cf. [6, p. 99]). In particular, this result holds if the players\" actions are commitments of a finite type. Fact 4.3. For each finite game and each ordering of the players, benign backward inductions exist. For each game, each ordering of its players and each commitment type, we can define another game G\u2217 with the the actions of each player i given by his \u03c4-commitments Xi in G. The utility 112 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) of a strategy profile (x\u03c01 , . . . , x\u03c0n ) for a player i in G\u2217 can then be equated to his utility of the strategy profile \u03c6(x\u03c0n , . . . , x\u03c01 ) in G. We now find that the extortions of G can be retrieved as the paths of the benign backward induction solutions of the game G\u2217 for the ordering \u03c0R of the players, provided that the commitment type is finite. Theorem 4.4. Let G = (N, (Ai)i\u2208N, (ui)i\u2208N) be a game and \u03c0 an ordering of its players with which the finite commitment type \u03c4 associates the tuple X\u03c01 , . . . , X\u03c0n , \u03c6 . Let further G\u2217 N, (X\u03c0i )i\u2208N, (u\u2217 \u03c0i )i\u2208N , where u\u2217 \u03c0i (x\u03c0n , . . . , x\u03c01 ) = u\u03c0i (\u03c6(x\u03c01 , . . . , x\u03c0n )), for each \u03c4-commitment profile (x\u03c01 , . . . , x\u03c0n ). Then, a \u03c0commitment profile (x\u03c01 , . . . , x\u03c0n ) is a \u03c4-extortion in G given \u03c0 if and only if there is some benign backward induction f in G\u2217 given \u03c0R with f = (x\u03c0n , . . . , x\u03c01 ). Proof. Assume that f is a benign backward induction in G\u2217 relative to \u03c0R . Then, f = (x\u03c0n , . . . , x\u03c01 ), for some commitment profile (x\u03c01 , . . . , x\u03c0n ) of G relative to \u03c0. We show by induction that (x\u03c01 , . . . , x\u03c0n ) is an extortion of order m, for all m with 0 m n. For m = 0, the proof is trivial. For the induction step, consider an arbitrary commitment profile (y\u03c01 , . . . , y\u03c0n ) such that (y\u03c01 , . . . , y\u03c0m , x\u03c0m+1 , . . . , x\u03c0n ) is an extortion of order m \u2212 1. In virtue of the induction hypothesis, there is a benign backward induction g of order m \u2212 1 in G\u2217 with g = (x\u03c0n , . . . , x\u03c0m+1 , y\u03c0m , . . . , y\u03c01 ). As f is also a benign backward induction of order m: (g\u03c0n , . . . , g\u03c01 ) \u2217 \u03c0m (g\u03c0n , . . . , g\u03c0m+1 , f\u03c0m , . . . , f\u03c01 ) . Hence, (x\u03c0n , . . . , x\u03c0m+1 , y\u03c0m , . . . , y\u03c01 ) \u2217 \u03c0m (x\u03c0n , . . . , x\u03c01 ). By definition of u\u2217 \u03c0m , then also: \u03c6(y\u03c01 , . . . , y\u03c0m , x\u03c0m+1 , . . . , x\u03c0n ) \u03c0m \u03c6(x\u03c01 , . . . , x\u03c0n ). We may conclude that x is an extortion of order m. For the only if direction, assume that x is an extortion of G given \u03c0. We prove that there is a benign backward induction f (\u2217) in G\u2217 for \u03c0R with f (\u2217) = x. In virtue of Fact 4.3, there is a benign backward induction h in G\u2217 given \u03c0R . Now define f (\u2217) in such a way that f (\u2217) \u03c0i (z\u03c0n , . . . , z\u03c0i\u22121 ) = x\u03c0i , if (z\u03c0n , . . . , z\u03c0i\u22121 ) = (x\u03c0n , . . . , x\u03c0i\u22121 ), and f (\u2217) \u03c0i (z\u03c0n , . . . , z\u03c0i\u22121 ) = h\u03c0i (z\u03c0n , . . . , z\u03c0i\u22121 ), otherwise. We prove by induction on m, that f (\u2217) is a benign backward induction of order m, for each m with 0 m n. The basis is trivial. So assume that f (\u2217) is a backward induction of order m \u2212 1 in G\u2217 given \u03c0R and consider an arbitrary benign backward induction g of order m \u2212 1 in G\u2217 given \u03c0R . Let g be given by (y\u03c0n , . . . , y\u03c01 ). Either (y\u03c0n , . . . , y\u03c0m+1 ) = (x\u03c0n , . . . , x\u03c0m+1 ), or this is not the case. If the latter, it can readily be appreciated that: (g\u03c0n , . . . , g\u03c0m+1 , f (\u2217) \u03c0m , . . . , f (\u2217) \u03c01 ) = (g\u03c0n , . . . , g\u03c0m+1 , h\u03c0m , . . . , h\u03c01 ) . Having assumed that h is a benign backward induction, subsequently, (g\u03c0n , . . . , g\u03c01 ) \u2217 m (g\u03c0n , . . . , g\u03c0m+1 , h\u03c0m , . . . , h\u03c01 ) , and (g\u03c0n , . . . , g\u03c01 ) \u2217 m (g\u03c0n , . . . , g\u03c0m+1 , f (\u2217) \u03c0m , . . . , f (\u2217) \u03c01 ) . Hence, f (\u2217) is a benign backward induction of order m. In the former case the reasoning is slightly different. Then, (g\u03c0n , . . . , g\u03c01 ) = (x\u03c0n , . . . , x\u03c0m+1 , y\u03c0m , . . . , y\u03c01 ). It follows that: (g\u03c0n , . . . , g\u03c0m+1 , f (\u2217) \u03c0m , . . . , f (\u2217) \u03c01 ) = (f (\u2217) \u03c0n , . . . , f (\u2217) \u03c01 ) = (x\u03c0n , . . . , x\u03c01 ). In virtue of the induction hypothesis, (y\u03c01 , . . . , y\u03c0n ) is an extortion of order m \u2212 1 in G given \u03c0. As the reasoning takes place under the assumption that x is an extortion in G given \u03c0, we also have: \u03c6(y\u03c01 , . . . , y\u03c0m , x\u03c0m+1 , . . . , x\u03c0n ) \u03c0m \u03c6(x\u03c01 , . . . , x\u03c0n ). Then, (x\u03c0n , . . . , x\u03c0m+1 , y\u03c0m , . . . , y\u03c01 , ) \u2217 \u03c0m (x\u03c0n , . . . , x\u03c01 )., by definition of u\u2217 . We may conclude that: (g\u03c0n , . . . , g\u03c01 ) \u2217 \u03c0m (g\u03c0n , . . . , g\u03c0m+1 , f (\u2217) \u03c0m , . . . , f (\u2217) \u03c01 ) , signifying that f (\u2217) is a benign backward induction of order m. As an immediate consequence of Theorem 4.4 and Fact 4.3 we also have the following result. Corollary 4.5. Let \u03c4 be a finite commitment type. Then, \u03c4-extortions exist for each strategic game and for each ordering of the players. 4.3 Commitment Order In the case of unconditional commitments, it is not always favorable to be the first to commit. This is well illustrated by the familiar game rock-paper-scissors. If, on the other hand, the players are in a position to make conditional commitments in this particular game, moving first is an advantage. Rather, we find that it can never harm to move first in a two-player game with conditional commitments. Theorem 4.6. Let G be a two-player strategic game involving player i. Further let f be an extortion of G in which i commits first, and g an extortion in which i commits second. Then, g i f . Proof sketch. Let f be a conditional extortion in G given \u03c0. It suffices to show that there is some conditional extortion h of order 1 in G given \u03c0 with h = f . Assume for a contradiction that there is no such extortion of order 1 in G given \u03c0 . Then there must be some b\u2217 \u2208 Aj such that f \u227aj b\u2217 , a , for all a \u2208 Ai. (Otherwise we could define (gj, gi) such that gj = fj(fi), gi(gj) = fi, and for any other b \u2208 Aj, gi(b) = a\u2217 , where a\u2217 is an action in Ai such that (b, a\u2217 ) j f . Then g would be an extortion of order 1 in G given \u03c0 with g .) Now consider a conditional commitment profile h for G and \u03c0 such that hj(a) = b\u2217 , for all a \u2208 Ai. Let further hi be such that (a, hj) i (hi, hj) , for all a \u2208 Ai. Then, h is an extortion of order 1 in G given \u03c0. Observe that (hi, hj) = (fi , b\u2217 ). Hence, f \u227aj h , contradicting the assumption that f is an extortion in G given \u03c0. Theorem 4.6 does not generalize to games with more than two players. Consider the three-player game in Figure 7, with extensive forms as in Figure 8. Here, Row and Mat have identical preferences. The latter\"s extortionate powers relative Col, however, are very weak if he is to commit first: any conditional commitment he makes puts Col in a situation in which she can enforce a payoff of two, leaving Mat and Row in the cold with a payoff of one. However, if Mat is last to commit and Row first, then the latter can exploit his strategic powers, threaten Col so that she plays left, and guarantee both himself and Mat a payoff of two. 4.4 Pareto Efficiency Another issue concerns the Pareto efficiency of the strategy profiles extortionable through conditional commitments. We say that a strategy profile s (weakly) Pareto dominates another strategy profile t if t i s for all players i and s it for some. Moreover, a strategy profile s is (weakly) Pareto efficient if it is not (weakly) Pareto dominated by any other strategy profile. We extend this terminology to conditional commitment profiles by saying that a conditional commitment profile f is (weakly) Pareto efficient or (weakly) Pareto dominates another conditional commitment profile if f is or does so. We now have the following result. \u23a2\u23a2\u23a2\u23a2\u23a3 (0, 1, 0) (0, 0, 0) (0, 0, 0) (1, 2, 1) \u23a5\u23a5\u23a5\u23a5\u23a6 \u23a2\u23a2\u23a2\u23a2\u23a3 (2, 1, 2) (0, 0, 0) (0, 0, 0) (1, 2, 1) \u23a5\u23a5\u23a5\u23a5\u23a6 Figure 7: A three-person game. The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 113 \u239c\u239c\u239c\u239c\u239c\u239c\u239c\u239d \u239f\u239f\u239f\u239f\u239f\u239f\u239f\u23a0 \u239c\u239c\u239c\u239c\u239c\u239c\u239c\u239d \u239f\u239f\u239f\u239f\u239f\u239f\u239f\u23a0 \u239c\u239c\u239c\u239c\u239c\u239c\u239c\u239d \u239f\u239f\u239f\u239f\u239f\u239f\u239f\u23a0 \u239c\u239c\u239c\u239c\u239c\u239c\u239c\u239d \u239f\u239f\u239f\u239f\u239f\u239f\u239f\u23a0 Row Col Mat \u239c\u239c\u239c\u239c\u239c\u239c\u239c\u239d \u239f\u239f\u239f\u239f\u239f\u239f\u239f\u23a0 \u239c\u239c\u239c\u239c\u239c\u239c\u239c\u239d \u239f\u239f\u239f\u239f\u239f\u239f\u239f\u23a0 \u239c\u239c\u239c\u239c\u239c\u239c\u239c\u239d \u239f\u239f\u239f\u239f\u239f\u239f\u239f\u23a0 \u239c\u239c\u239c\u239c\u239c\u239c\u239c\u239d \u239f\u239f\u239f\u239f\u239f\u239f\u239f\u23a0 \u239c\u239c\u239c\u239c\u239c\u239c\u239c\u239d \u239f\u239f\u239f\u239f\u239f\u239f\u239f\u23a0 \u239c\u239c\u239c\u239c\u239c\u239c\u239c\u239d \u239f\u239f\u239f\u239f\u239f\u239f\u239f\u23a0 \u239c\u239c\u239c\u239c\u239c\u239c\u239c\u239d \u239f\u239f\u239f\u239f\u239f\u239f\u239f\u23a0 \u239c\u239c\u239c\u239c\u239c\u239c\u239c\u239d \u239f\u239f\u239f\u239f\u239f\u239f\u239f\u23a0 Row Col Mat \u239c\u239c\u239c\u239c\u239c\u239c\u239c\u239d \u239f\u239f\u239f\u239f\u239f\u239f\u239f\u23a0 \u239c\u239c\u239c\u239c\u239c\u239c\u239c\u239d \u239f\u239f\u239f\u239f\u239f\u239f\u239f\u23a0 \u239c\u239c\u239c\u239c\u239c\u239c\u239c\u239d \u239f\u239f\u239f\u239f\u239f\u239f\u239f\u23a0 \u239c\u239c\u239c\u239c\u239c\u239c\u239c\u239d \u239f\u239f\u239f\u239f\u239f\u239f\u239f\u23a0 Figure 8: It is not always better to commit early than late, even in the case of conditional or inductive commitments. Theorem 4.7. In each game, Pareto efficient conditional extortions exist. Moreover, any strategy profile that Pareto dominates an extortion is also extortionable through a conditional commitment. Proof sketch. Since, in virtue of Fact 4.5, extortions generally exists in each game, it suffices to recognize that the second claim holds. Let s be the strategy profile (s\u03c01 , . . . , s\u03c0n ). Let further the conditional extortion f be Pareto dominated by s. An extortion g with g = s can then be constructed by adopting all threats of f while promising g . I.e., for all players \u03c0i we have g\u03c0i (s\u03c01 , . . . , s\u03c0i\u22121 ) = si and g\u03c0i (t\u03c01 , . . . , t\u03c0n ) = f\u03c0i (t\u03c01 , . . . , t\u03c0n ), for all other t\u03c01 , . . . , t\u03c0n . As s Pareto dominates f , the threats of f remain effective as threats of g given that s is being promised. This result hints at a difference between (benign) backward induction and extortions. In general, solutions of benign backward inductions can be Pareto dominated by outcomes that are no benign backward induction solutions. Therefore, although every extortion can be seen as a benign backward induction in a larger game, it is not the case that all formal properties of extortions are shared by benign backward inductions in general. 5. OTHER COMMITMENT TYPES Conditional and unconditional commitments are only two possible commitment types. The definition also provides for types of commitment that allow for committing on commitments, thus achieving a finer adjustment of promises and threats. Similarly, it subsumes commitments on and to mixed strategies. In this section we comment on some of these possibilities. 5.1 Inductive Commitments Apart from making commitments conditional on the actions of the players to commit later, one could also commit on the commitments of the following players. Informally, such commitments would have the form of if you only dare to commit in such and such a way, then I do such and such, otherwise I promise to act so and so. For a strategic game G and an ordering \u03c0 of the players, we define the inductive commitments of the players inductively. The inductive commitments available to \u03c01 coincide with the actions that are available to him. An inductive commitment for player \u03c0i+1 is a function mapping each profile of inductive commitments of players \u03c01 through \u03c0i to one of his basic actions. Formally we define the type of inductive commitments F\u03c01 , . . . , F\u03c0n , such that for each player \u03c0i in a game G and given \u03c0: F\u03c01 =df. A\u03c01 F\u03c0i+1 =df. A F\u03c01 \u00d7\u00b7\u00b7\u00b7\u00d7F\u03c0i \u03c0i+1 Let f\u03c0i = f\u03c0i f\u03c01 , . . . , f\u03c0i\u22121 , for each player \u03c0i and have f denote the pure strategy profile f\u03c01 , . . . , f\u03c0n Inductive commitments have a greater extortionate power than conditional commitments. To appreciate this, consider once more the game in Figure 5. We found that the strategy profile in which Row chooses bottom and Col and Mat both choose left is not extortionable through conditional commitments. By means of inductive commitments, however, this is possible. Let f be the inductive commitment profile such that fRow is Row choosing the bottom row (b), fCol is the column player choosing the left column (l) no matter how Row decides, and fMat is defined such that: fMat fRow, fCol = \u23aa\u23aa\u23a8 \u23aa\u23aa\u23a9 r if fRow = t and fCol (b) = r, l otherwise. Instead of showing formally that f is an inductive extortion of the strategy profile (b, l, l), we point out informally how this can be done. We argued that in order to exact a payoff of two by means of a conditional extortion, Mat would have to lure Row into choosing the bottom row without at the same time putting Col in a position to successfully threaten Row not to choose top. This, we found, is an impossibility if the players can only make conditional commitments. By contrast, if Mat can commit to commitments, he can undermine Col\"s efforts to threaten Row by playing the right matrix, if Col were to do so. Yet, Mat can still force Row to choose the bottom row, in case Col desists form making this threat. As can readily be observed, in any game, the inductive commitments of the first two players to commit coincide with their conditional commitments. Hence, as an immediate consequence of Theorem 4.6, it can never harm to be the first to commit to an inductive commitment in the two player case. Similarly, we find that the game depicted in Figure 7 also serves as an example showing that, in case there are more than two players, it is not always better to commit to an inductive commitment early. In this example the strategic position of Mat is so weak if he is to commit first, that even the possibility to commit inductively does not strengthen it, whereas, in a similar fashion as with conditional commitments, Row can enforce a payoff of two to both himself and Mat if he is the first to commit. 5.2 Mixed Commitments Types So far we have merely considered commitments to and on pure strategies. A natural extension would be also to consider commitments to and on mixed strategies. We distinguish between conditional, unconditional as well as inductive mixed commitments. We find that they are generally quite incomparable with their pure counterparts: in some situations a player can achieve more using a mixed commitment, in another using a pure commitment type. A complicating factor with mixed commitment types is that they 114 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) can result in a mixed strategy profile being played. This makes that the distinction between promises and threats, as delineated in Section 4.1, gets blurred for mixed commitment types. The type of mixed unconditional commitments associates with each game G and ordering \u03c0 of its players the tuple \u03a3\u03c01 , . . . , \u03a3\u03c0n , id . The two-player case has been extensively studied (e.g., ). As a matter of fact, von Neumann\"s famous minimax theorem shows that for two-player zero-sum games, it does not matter which player commits first. If the second player to commit plays a mixed strategy that ensures his security level, the first player to commit can do no better than to do so as well . In the game of Figure 5 we found that, with conditional commitments, Mat is unable to enforce an outcome that awards him a payoff of two. Recall that the reason of this failure is that any effort to deter Row from choosing the top row is flawed, as it would put Col in an excellent position to threaten Row not to choose the bottom row. If Mat has inductive commitments at his disposal, however, this is a possibility. We now find that in case the players can dispose of unconditional mixed strategies, Mat is in a much similar position. He could randomize uniformly between the left and right matrix. Then, Row\"s expected utility is 21 if he plays the top row, no matter how Col randomizes. The expected payoff of Col does not exceed 21 , either, in case Row chooses top. By purely committing to the left column, Col player entices Row to play bottom, as his expected utility then amounts to 3. This ensures an expected utility of three for Col as well. However, a player is not always better off with unconditional mixed commitments than with pure conditional commitments. For an example, consider the game in Figure 2. Using pure conditional commitments, he can ensure a payoff of three, whereas with unconditional mixed commitments 21 would be the most he could achieve. Neither is it in general advantageous to commit first to a mixed strategy in a three-player game. To appreciate this, consider once more the game in Figure 7. Again committing to a mixed strategy will not achieve much for Mat if he is to move first, and as before the other players have no reason to commit to anything other than a pure strategy. This holds for all players if Row commits first, Col second and Mat last, be it that in this case Mat obtains the best payoff he can get. Analogous to conditional and inductive commitments one can also define the types of mixed conditional and mixed inductive commitments. With the former, a player can condition his mixed strategies on the mixed strategies of the players to commit after him. These tend to be very large objects and, knowing little about them yet, we shelve their formal analysis for future research. Conceptually, it might not be immediately clear how such mixed conditional commitments can be made with credibility. For one, when one\"s commitments are conditional on a particular mixed strategy being played, how can it be recognized that it was in fact this mixed strategy that was played rather than another one? If this proves to be impossible, how can one know how his conditional commitments is to be effectuated? A possible answer would be, that all depends on the circumstances in which the commitments were made. E.g., if the different agents can submit their mixed conditional commitments to an independent party, the latter can execute the randomizations and determine the unique mixed strategy profile that their commitments induce. 6. SUMMARY AND CONCLUSION In some situations agents can strengthen their strategic position by committing themselves to a particular course of action. There are various types of commitment, e.g., pure, mixed and conditional. Which type of commitment an agent is in a position in to make essentially depends on the situation under consideration. If the agents commit in a particular order, there is a tactic common to making commitments of any type, which we have formalized by means the concept of an extortion. This generic concept of extortion can be analyzed in abstracto. Moreover, on its basis the various commitment types can be compared formally and systematically. We have seen that the type of commitment an agent can make has a profound impact on what an agent can achieve in a gamelike situation. In some situations a player is much helped if he is in a position to commit conditionally, whereas in others mixed commitments would be more profitable. This raises the question as to the characteristic formal features of the situations in which it is advantageous for a player to be able to make commitments of a particular type. Another issue which we leave for future research is the computational complexity of finding an extortion for the different commitment types.", "body1": "On one view, the least one may expect of game theory is that it provides an answer to the question which actions maximize an agent\"s expected utility in situations of interactive decision making. A slightly divergent view is expounded by Schelling when he states that strategy [. The potency of a promise, however, essentially depends on the extent the promisee can be convinced of the promiser\"s resolve to see to its fulfillment. Commitments come in different forms and it may depend on the circumstances which ones can and which ones cannot credibly be made. By focusing on the selection of actions rather than on commitments, it might seem that the conception of game theory as mere interactive decision theory is too narrow. Nowadays, we believe, these issues are equally significant for applications and developments in such fields as multiagent systems, distributed computing and electronic markets. Our analysis is conceptually similar to that of Stackelberg or leadership games , which have been extensively studied in the economic literature (cf., ). After briefly discussing related work in Section 2, we present the formal game theoretic framework, in which we define the notions of a commitment type as well as conditional and unconditional commitments (Section 3). Commitment is a central concept in game theory. Over the last few years, game theory has become almost indispensable as a research tool for computer science and (multi)agent research. e.g., ). By committing, an agent can improve his strategic position. Important types of commitments, however, cannot simply be analyzed as unconditional commitments to actions. (This is also the reason why the credibility of a threat is not necessarily undermined if its putting into effect means that the threatener is also harmed.) In the case of conditional commitments, however, a particular kind of inconsistency can arise. To get around this problem, one can write down conditional commitments in the form of rules and define appropriate fixed point constructions, as suggested by Samet and Tennenholtz . Since checking the semantic equivalence of two commitments (or commitment conditions) is undecidable in general, Tennenholtz bases his definition of program equilibrium on syntactic equivalence. Another point to heed is that we only consider the case in which the commitments are considered absolutely binding. 3.1 Strategic Games A strategic game is a tuple (N, (Ai)i\u2208N, (ui)i\u2208N), where N = {1, . 3.2 Conditional Commitments Relative to a strategic game (N, (Ai)i\u2208N, (ui)i\u2208N) and an ordering \u03c0 = (\u03c01, . Intuitively, \u03c0 reflects the sequential order in which the players can make their commitments, with \u03c0n committing first, \u03c0n\u22121 second, and so on. The sequence f\u03c01 , (f\u03c01 , f\u03c02 ), . There is a natural way in which a strategic game G together with an ordering (\u03c01, . To illustrate this point, consider once more the game in Figure 2 and observe that neither player can improve on the outcome obtained via iterated strong dominance by committing unconditionally to some strategy. Figure 3 shows the extensive form associated with the game of Figure 2. 110 The Sixth Intl. This line of reasoning can be generalized to yield an algorithm for finding optimal conditional commitments for general twoplayer games: 1. 4. 3.3 Commitment Types So far, we have distinguished between conditional and unconditional commitments. Definition 3.1. Thus, the type of unconditional commitments associates with a game and an ordering \u03c0 of its players the tuple S \u03c01 , . In the introduction, we argued informally how players could improve their position by conditionally committing. Definition 4.1. Thus, an extortion of order 1 is a commitment profile in which player \u03c01, makes a commitment that maximizes his payoff, given fixed commitments of the other players. For the type of conditional commitments we have that any conditional commitment profile f is an extortion of order 0 and an extortion of an order m greater than 0 is any extortion of order m \u2212 1 for which: g\u03c01 , . To illustrate the concept of an extortion for conditional commitments consider the three-player game in Figure 5 and assume \u23a2\u23a2\u23a2\u23a2\u23a3 (1, 4, 0) (1, 4, 0) (3, 3, 2) (0, 0, 2) \u23a5\u23a5\u23a5\u23a5\u23a6 \u23a2\u23a2\u23a2\u23a2\u23a3 (4, 1, 1) (4, 0, 0) (3, 3, 2) (0, 0, 2) \u23a5\u23a5\u23a5\u23a5\u23a6 Figure 5: A three-player strategic game The Sixth Intl. Figure 6 depicts the possible conditional commitments of the players in extensive forms, with the left branch corresponding to Row\"s strategy of playing the top row. In both f and g Row guarantees himself the higher payoff given the conditional commitments of Mat and Col. Only g, however, is also an extortion of order 2. 4.1 Promises and Threats One way of understanding conditional extortions is by conceiving of them as combinations of precisely one promise and a number of threats. For example, in the game of Figure 5, there is no conditional extortion that ensures Mat a payoff of two. 4.2 Benign Backward Induction The solutions extortions provide can also be obtained by modeling the situation as an extensive form game and applying a backward inductive type of argument. Definition 4.2. For games with a finite action set for each player, the following result follows straightforwardly from Kuhn\"s Theorem (cf. For each game, each ordering of its players and each commitment type, we can define another game G\u2217 with the the actions of each player i given by his \u03c4-commitments Xi in G. The utility 112 The Sixth Intl. Theorem 4.4. Proof. Hence, (x\u03c0n , . For the only if direction, assume that x is an extortion of G given \u03c0. Either (y\u03c0n , . Having assumed that h is a benign backward induction, subsequently, (g\u03c0n , . In virtue of the induction hypothesis, (y\u03c01 , . Then, (x\u03c0n , . Corollary 4.5. 4.3 Commitment Order In the case of unconditional commitments, it is not always favorable to be the first to commit. Theorem 4.6. Proof sketch. (Otherwise we could define (gj, gi) such that gj = fj(fi), gi(gj) = fi, and for any other b \u2208 Aj, gi(b) = a\u2217 , where a\u2217 is an action in Ai such that (b, a\u2217 ) j f . Theorem 4.6 does not generalize to games with more than two players. 4.4 Pareto Efficiency Another issue concerns the Pareto efficiency of the strategy profiles extortionable through conditional commitments. The Sixth Intl. Theorem 4.7. This result hints at a difference between (benign) backward induction and extortions. Conditional and unconditional commitments are only two possible commitment types. For a strategic game G and an ordering \u03c0 of the players, we define the inductive commitments of the players inductively. Instead of showing formally that f is an inductive extortion of the strategy profile (b, l, l), we point out informally how this can be done. As can readily be observed, in any game, the inductive commitments of the first two players to commit coincide with their conditional commitments. 5.2 Mixed Commitments Types So far we have merely considered commitments to and on pure strategies. A complicating factor with mixed commitment types is that they 114 The Sixth Intl. The type of mixed unconditional commitments associates with each game G and ordering \u03c0 of its players the tuple \u03a3\u03c01 , . In the game of Figure 5 we found that, with conditional commitments, Mat is unable to enforce an outcome that awards him a payoff of two. However, a player is not always better off with unconditional mixed commitments than with pure conditional commitments. These tend to be very large objects and, knowing little about them yet, we shelve their formal analysis for future research. Conceptually, it might not be immediately clear how such mixed conditional commitments can be made with credibility.", "body2": "On one view, the least one may expect of game theory is that it provides an answer to the question which actions maximize an agent\"s expected utility in situations of interactive decision making. E.g., the strategic lie of the land may be such that a promise, a threat, or a combination of both would be more conductive to his ends. Performing a particular action means doing so to the exclusion of all other actions. Some situations even allow for commitments on commitments or for commitments to randomized actions. Insight into these matters has proved itself invaluable for a proper understanding of diplomatic policy during the Cold War. A mechanism, on the other hand, could be seen as a set of commitments that steers the players\" behavior in a certain desired way (see, e.g., ). Our approach, however, differs in that it is assumed that the players all move in a particular order-first, second, third and so on-and that it is specifically aimed at incorporating a wide range of possible commitments, in particular conditional commitments. The paper concludes with an overview of the results and an outlook for future research in Section 6. Also, commitments have played a significant role in the theory of equilibrium selection (see, e.g., . Commitments have by no means gone unnoticed (see, \u23a2\u23a2\u23a2\u23a2\u23a3 (1, 3) (3, 2) (0, 0) (2, 1) \u23a5\u23a5\u23a5\u23a5\u23a6 Figure 1: Committing to a dominated strategy can be advantageous. We, however, consider a number of different commitment types, among which conditional commitments, and propose a generic solution concept. If, on the other hand, both players actually commit themselves in this way but without convincing the other party of their having done so, the game ends in misery for both. If successful, it is not carried out. (This will suffice to deter Row, and there is no need for an additional promise on the part of Col.) He cannot do so by merely committing unconditionally, and neither can Row if he were to commit first. If now, Col simultaneously were able to commit to the conditional strategy to play right if Row plays top, and left, otherwise, there is no strategy profile that can be played without one of the players\" bluff being called. To get around this problem, one can write down conditional commitments in the form of rules and define appropriate fixed point constructions, as suggested by Samet and Tennenholtz . We will find that it is not always beneficial to commit earlier than later or vice versa. To make these concepts formally precise, we first have to fix some notation. For a player i and (mixed) strategy profiles \u03c3 and \u03c4 we write \u03c3 i \u03c4 if ui (\u03c3) ui (\u03c4). By a conditional commitment profile f we understand any combination of conditional commitments in F\u03c01 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 F\u03c0n . , f\u03c0i ). If we consider a conditional commitment f for Col such that f (t) = l and f (b) = r, then (t, f ) is a conditional commitment profile and (t, f ) = (t, f (t)) = (t, l). We will argue that under this interpretation other strategy profiles should be singled out as appropriate. Situations like this, in which players can make unconditional commitments in a fixed order, can fruitfully be analyzed as extensive form games, and the most lucrative unconditional commitment can be found through backward induction. In this paper all formal definitions are based on strategic games and orderings of the players only. If Col can make a conditional commitment of playing the right column if Row chooses top, and the left column otherwise, this leaves Row with the easy choice between a payoff of zero or one, and Col may expect a payoff of three. If u\u03c01 (t\u03c01 , f\u03c02 (t\u03c01 )) u\u03c01 (s\u03c01 , s\u03c02 ) for all t\u03c01 s\u03c01 , return f . For any order of the players an extortion contains, for each player, an optimal commitment given the commitments of the players that committed earlier. This notion of sequential commitment allows for generalization and gives rise to the following definition of a (sequential) commitment type. , X\u03c0n , \u03c6 is finite whenever X\u03c0i is finite for each i with 1 i n. , F\u03c0n , is the tuple associated with the same game by the type of (pure) conditional commitments. A complicating factor is that in finding a player\"s optimal commitment, one should not only take into account how such a commitment affects other players\" actions, but also how it enables them to make their commitments. A \u03c4-extortion is a commitment profile that is a \u03c4-extortion of order m for all m with 0 m n. Furthermore, we say that a (mixed) strategy profile \u03c3 is \u03c4-extortionable if there is some \u03c4-extortion x with \u03c6(x) = s. An extortion of order m is an extortion of order m \u2212 1 that maximizes player m\"s payoff, given fixed commitments of the players \u03c0m+1 through \u03c0n. , f\u03c0n an extortion of order m \u2212 1. (Row, Col, Mat) to be the order in which the players commit. Both f and g are extortions of order 1. We leave it to the reader to check that, by contrast, g is an extortion of order 3, and therewith an extortion per se. This is what makes extortionate reasoning sometimes so complicated, especially in situations with more than two players. In short, no matter how Mat conditionally commits, he will either enable Col to threaten Row into playing top or fail to lure Row into playing the bottom row. , \u03c01). A conditional commitment profile f is a benign backward induction if it is a benign backward induction of order k for each k with 0 k n. For each finite game and each ordering of the players, benign backward inductions exist. , x\u03c01 ) in G. We now find that the extortions of G can be retrieved as the paths of the benign backward induction solutions of the game G\u2217 for the ordering \u03c0R of the players, provided that the commitment type is finite. , x\u03c01 ). , f\u03c01 ) . We may conclude that x is an extortion of order m. , y\u03c01 ). , h\u03c01 ) . , x\u03c0n ). , f (\u2217) \u03c01 ) , signifying that f (\u2217) is a benign backward induction of order m. As an immediate consequence of Theorem 4.4 and Fact 4.3 we also have the following result. Then, \u03c4-extortions exist for each strategic game and for each ordering of the players. Rather, we find that it can never harm to move first in a two-player game with conditional commitments. Then, g i f . Then there must be some b\u2217 \u2208 Aj such that f \u227aj b\u2217 , a , for all a \u2208 Ai. Hence, f \u227aj h , contradicting the assumption that f is an extortion in G given \u03c0. However, if Mat is last to commit and Row first, then the latter can exploit his strategic powers, threaten Col so that she plays left, and guarantee both himself and Mat a payoff of two. \u23a2\u23a2\u23a2\u23a2\u23a3 (0, 1, 0) (0, 0, 0) (0, 0, 0) (1, 2, 1) \u23a5\u23a5\u23a5\u23a5\u23a6 \u23a2\u23a2\u23a2\u23a2\u23a3 (2, 1, 2) (0, 0, 0) (0, 0, 0) (1, 2, 1) \u23a5\u23a5\u23a5\u23a5\u23a6 Figure 7: A three-person game. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 113 \u239c\u239c\u239c\u239c\u239c\u239c\u239c\u239d \u239f\u239f\u239f\u239f\u239f\u239f\u239f\u23a0 \u239c\u239c\u239c\u239c\u239c\u239c\u239c\u239d \u239f\u239f\u239f\u239f\u239f\u239f\u239f\u23a0 \u239c\u239c\u239c\u239c\u239c\u239c\u239c\u239d \u239f\u239f\u239f\u239f\u239f\u239f\u239f\u23a0 \u239c\u239c\u239c\u239c\u239c\u239c\u239c\u239d \u239f\u239f\u239f\u239f\u239f\u239f\u239f\u23a0 Row Col Mat \u239c\u239c\u239c\u239c\u239c\u239c\u239c\u239d \u239f\u239f\u239f\u239f\u239f\u239f\u239f\u23a0 \u239c\u239c\u239c\u239c\u239c\u239c\u239c\u239d \u239f\u239f\u239f\u239f\u239f\u239f\u239f\u23a0 \u239c\u239c\u239c\u239c\u239c\u239c\u239c\u239d \u239f\u239f\u239f\u239f\u239f\u239f\u239f\u23a0 \u239c\u239c\u239c\u239c\u239c\u239c\u239c\u239d \u239f\u239f\u239f\u239f\u239f\u239f\u239f\u23a0 \u239c\u239c\u239c\u239c\u239c\u239c\u239c\u239d \u239f\u239f\u239f\u239f\u239f\u239f\u239f\u23a0 \u239c\u239c\u239c\u239c\u239c\u239c\u239c\u239d \u239f\u239f\u239f\u239f\u239f\u239f\u239f\u23a0 \u239c\u239c\u239c\u239c\u239c\u239c\u239c\u239d \u239f\u239f\u239f\u239f\u239f\u239f\u239f\u23a0 \u239c\u239c\u239c\u239c\u239c\u239c\u239c\u239d \u239f\u239f\u239f\u239f\u239f\u239f\u239f\u23a0 Row Col Mat \u239c\u239c\u239c\u239c\u239c\u239c\u239c\u239d \u239f\u239f\u239f\u239f\u239f\u239f\u239f\u23a0 \u239c\u239c\u239c\u239c\u239c\u239c\u239c\u239d \u239f\u239f\u239f\u239f\u239f\u239f\u239f\u23a0 \u239c\u239c\u239c\u239c\u239c\u239c\u239c\u239d \u239f\u239f\u239f\u239f\u239f\u239f\u239f\u23a0 \u239c\u239c\u239c\u239c\u239c\u239c\u239c\u239d \u239f\u239f\u239f\u239f\u239f\u239f\u239f\u23a0 Figure 8: It is not always better to commit early than late, even in the case of conditional or inductive commitments. Moreover, any strategy profile that Pareto dominates an extortion is also extortionable through a conditional commitment. As s Pareto dominates f , the threats of f remain effective as threats of g given that s is being promised. Therefore, although every extortion can be seen as a benign backward induction in a larger game, it is not the case that all formal properties of extortions are shared by benign backward inductions in general. Informally, such commitments would have the form of if you only dare to commit in such and such a way, then I do such and such, otherwise I promise to act so and so. Let f be the inductive commitment profile such that fRow is Row choosing the bottom row (b), fCol is the column player choosing the left column (l) no matter how Row decides, and fMat is defined such that: fMat fRow, fCol = \u23aa\u23aa\u23a8 \u23aa\u23aa\u23a9 r if fRow = t and fCol (b) = r, l otherwise. Yet, Mat can still force Row to choose the bottom row, in case Col desists form making this threat. In this example the strategic position of Mat is so weak if he is to commit first, that even the possibility to commit inductively does not strengthen it, whereas, in a similar fashion as with conditional commitments, Row can enforce a payoff of two to both himself and Mat if he is the first to commit. We find that they are generally quite incomparable with their pure counterparts: in some situations a player can achieve more using a mixed commitment, in another using a pure commitment type. This makes that the distinction between promises and threats, as delineated in Section 4.1, gets blurred for mixed commitment types. If the second player to commit plays a mixed strategy that ensures his security level, the first player to commit can do no better than to do so as well . This ensures an expected utility of three for Col as well. With the former, a player can condition his mixed strategies on the mixed strategies of the players to commit after him. These tend to be very large objects and, knowing little about them yet, we shelve their formal analysis for future research. E.g., if the different agents can submit their mixed conditional commitments to an independent party, the latter can execute the randomizations and determine the unique mixed strategy profile that their commitments induce.", "introduction": "On one view, the least one may expect of game theory is that it provides an answer to the question which actions maximize an agent\"s expected utility in situations of interactive decision making. A slightly divergent view is expounded by Schelling when he states that strategy [. is not concerned with the efficient application of force but with the exploitation of potential force [9, page 5]. From this perspective, the formal model of a game in strategic form only outlines the strategic features of an interactive situation. Apart from merely choosing and performing an action from a set of actions, there may also be other courses open to an agent. E.g., the strategic lie of the land may be such that a promise, a threat, or a combination of both would be more conductive to his ends. The potency of a promise, however, essentially depends on the extent the promisee can be convinced of the promiser\"s resolve to see to its fulfillment. Likewise, a threat only succeeds in deterring an agent if the latter can be made to believe that the threatener is bound to execute the threat, should it be ignored. In this sense, promises and threats essentially involve a commitment on the part of the one who makes them, thus purposely restricting his freedom of choice. Promises and threats epitomize one of the fundamental and at first sight perhaps most surprising phenomena in game theory: it may occur that a player can improve his strategic position by limiting his own freedom of action. By commitments we will understand such limitations of one\"s action space. Action itself could be seen as the ultimate commitment. Performing a particular action means doing so to the exclusion of all other actions. Commitments come in different forms and it may depend on the circumstances which ones can and which ones cannot credibly be made. Besides simply committing to the performance of an action, an agent might make his commitment conditional on the actions of other agents, as, e.g., the kidnapper does, when he promises to set free a hostage on receiving a ransom, while threatening to cut off another toe, otherwise. Some situations even allow for commitments on commitments or for commitments to randomized actions. By focusing on the selection of actions rather than on commitments, it might seem that the conception of game theory as mere interactive decision theory is too narrow. In this respect, Schelling\"s view might seem to evince a more comprehensive understanding of what game theory tries to accomplish. One might object, that commitments could be seen as the actions of a larger game. In reply to this criticism Schelling remarks: While it is instructive and intellectually satisfying to see how such tactics as threats, commitments, and promises can be absorbed in an enlarged, abstract supergame (game in normal form), it should be emphasized that we cannot learn anything about those tactics by studying games that are already in normal form. What we want is a theory that systematizes the study of the various universal ingredients that make up the move-structure of games; too abstract a model will miss them. 156-7] 108 978-81--7-5 (RPS) IFAAMAS Our concern is with these commitment tactics, be it that our analysis is confined to situations in which the players can commit in a given order and where we assume the commitments the players can make are given. Despite Schelling\"s warning for too abstract a framework, our approach will be based on the formal notion of an extortion, which we will propose in Section 4 as a uniform tactic for a comprehensive class of situations in which commitments can be made sequentially. On this basis we tackle such issues as the usefulness of certain types of commitment in different situations (strategic games) or whether it is better to commit early rather than late. We also provide a framework for the assessment of more general game theoretic matters like the relationship of extortions to backward induction or Pareto efficiency. Insight into these matters has proved itself invaluable for a proper understanding of diplomatic policy during the Cold War. Nowadays, we believe, these issues are equally significant for applications and developments in such fields as multiagent systems, distributed computing and electronic markets. For example, commitments have been argued to be of importance for interacting software agents as well as for mechanism design. In the former setting, the inability to re-program a software agent on the fly can be seen as a commitment to its specification and thus exploited to strengthen its strategic position in a multiagent setting. A mechanism, on the other hand, could be seen as a set of commitments that steers the players\" behavior in a certain desired way (see, e.g., ). Our analysis is conceptually similar to that of Stackelberg or leadership games , which have been extensively studied in the economic literature (cf., ). These games analyze situations in which a leader commits to a pure or mixed strategy, and a number of followers, who then act simultaneously. Our approach, however, differs in that it is assumed that the players all move in a particular order-first, second, third and so on-and that it is specifically aimed at incorporating a wide range of possible commitments, in particular conditional commitments. After briefly discussing related work in Section 2, we present the formal game theoretic framework, in which we define the notions of a commitment type as well as conditional and unconditional commitments (Section 3). In Section 4 we propose the generic concept of an extortion, which for each commitment type captures the idea of an optimal commitment profile. We point out an equivalence between extortions and backward induction solutions, and investigate whether it is advantageous to commit earlier rather than later and how the outcomes obtained through extortions relate to Pareto efficiency. Section 5 briefly reviews some other commitment types, such as inductive, mixed and mixed conditional commitments. The paper concludes with an overview of the results and an outlook for future research in Section 6.", "conclusion": "In some situations agents can strengthen their strategic position by committing themselves to a particular course of action.. There are various types of commitment, e.g., pure, mixed and conditional.. Which type of commitment an agent is in a position in to make essentially depends on the situation under consideration.. If the agents commit in a particular order, there is a tactic common to making commitments of any type, which we have formalized by means the concept of an extortion.. This generic concept of extortion can be analyzed in abstracto.. Moreover, on its basis the various commitment types can be compared formally and systematically.. We have seen that the type of commitment an agent can make has a profound impact on what an agent can achieve in a gamelike situation.. In some situations a player is much helped if he is in a position to commit conditionally, whereas in others mixed commitments would be more profitable.. This raises the question as to the characteristic formal features of the situations in which it is advantageous for a player to be able to make commitments of a particular type.. Another issue which we leave for future research is the computational complexity of finding an extortion for the different commitment types."}
{"id": "I-29", "keywords": ["multi-agent schedul", "agent architectur"], "title": "Distributed Management of Flexible Times Schedules", "abstract": "We consider the problem of managing schedules in an un- certain, distributed environment. We assume a team of col- laborative agents, each responsible for executing a portion of a globally pre-established schedule, but none possessing a global view of either the problem or solution. The goal is to maximize the joint quality obtained from the activities executed by all agents, given that, during execution, unex- pected events will force changes to some prescribed activi- ties and reduce the utility of executing others. We describe an agent architecture for solving this problem that couples two basic mechanisms: (1) a \"flexible times\" representation of the agent's schedule (using a Simple Temporal Network) and (2) an incremental rescheduling procedure. The former hedges against temporal uncertainty by allowing execution to proceed from a set of feasible solutions, and the latter acts to revise the agent's schedule when execution is forced out- side of this set of solutions or when execution events reduce the expected value of this feasible solution set. Basic coordi- nation with other agents is achieved simply by communicat- ing schedule changes to those agents with inter-dependent activities. Then, as time permits, the core local problem solving infra-structure is used to drive an inter-agent option generation and query process, aimed at identifying opportu- nities for solution improvement through joint change. Using a simulator to model the environment, we compare the per- formance of our multi-agent system with that of an expected optimal (but non-scalable) centralized MDP solver.", "references": ["teams language specification v. 1.06", "Gaining efficiency and flexibility in the simple temporal problem", "Temporal constraint networks", "TEAMS: A framework for environment centered analysis & design of coordination mechanisms", "Designing a family of coordination algorithms", "Design-To-Time Real-Time Scheduling", "Algorithms for a temporal decoupling problem in multi-agent planning", "Interleaving temporal planning and execution in robotics domains", "Remote agent: To boldly go where no AI system has gone before", "On-line planning and scheduling of high-speed manufacturing", "Enabling fast flexible planning through incremental temporal reasoning with conflict extraction", "Slack-based heuristics for constraint satisfaction scheduling", "Criteria-directed heuristic task scheduling"], "full_text": "1. INTRODUCTION The practical constraints of many application environments require distributed management of executing plans and schedules. Such factors as geographical separation of executing agents, limitations on communication bandwidth, constraints relating to chain of command and the high tempo of execution dynamics may all preclude any single agent from obtaining a complete global view of the problem, and hence necessitate collaborative yet localized planning and scheduling decisions. In this paper, we consider the problem of managing and executing schedules in an uncertain and distributed environment as defined by the DARPA Coordinators program. We assume a team of collaborative agents, each responsible for executing a portion of a globally preestablished schedule, but none possessing a global view of either the problem or solution. The team goal is to maximize the total quality of all activities executed by all agents, given that unexpected events will force changes to pre-scheduled activities and alter the utility of executing others as execution unfolds. To provide a basis for distributed coordination, each agent is aware of dependencies between its scheduled activities and those of other agents. Each agent is also given a pre-computed set of local contingency (fall-back) options. Central to our approach to solving this multi-agent problem is an incremental flexible-times scheduling framework. In a flexible-times representation of an agent\"s schedule, the execution intervals associated with scheduled activities are not fixed, but instead are allowed to float within imposed time and activity sequencing constraints. This representation allows the explicit use of slack as a hedge against simple forms of executional uncertainty (e.g., activity durations), and its underlying implementation as a Simple Temporal Network (STN) model provides efficient updating and consistency enforcement mechanisms. The advantages of flexible times frameworks have been demonstrated in various centralized planning and scheduling contexts (e.g., ). However their use in distributed problem solving settings has been quite sparse ( is one exception), and prior approaches to multi-agent scheduling (e.g., ) have generally operated with fixed-times representations of agent schedules. We define an agent architecture centered around incremental management of a flexible times schedule. The underlying STN-based representation is used (1) to loosen the coupling between executor and scheduler threads, (2) to retain a basic ability to absorb unexpected executional delays (or speedups), and (3) to provide a basic criterion for detecting the need for schedule change. Local change is ac484 978-81--7-5 (RPS) IFAAMAS Figure 1: A two agent C TAEMS problem. complished by an incremental scheduler, designed to maximize quality while attempting to minimize schedule change. To this schedule management infra-structure, we add two mechanisms for multi-agent coordination. Basic coordination with other agents is achieved by simple communication of local schedule changes to other agents with interdependent activities. Layered over this is a non-local option generation and evaluation process (similar in some respects to ), aimed at identification of opportunities for global improvement through joint changes to the schedules of multiple agents. This latter process uses analysis of detected conflicts in the STN as a basis for generating options. The remainder of the paper is organized as follows. We begin by briefly summarizing the general distributed scheduling problem of interest in our work. Next, we introduce the agent architecture we have developed to solve this problem and sketch its operation. In the following sections, we describe the components of the architecture in more detail, considering in turn issues relating to executing agent schedules, incrementally revising agent schedules and coordinating schedule changes among multiple agents. We then give some experimental results to indicate current system performance. Finally we conclude with a brief discussion of current research plans. 2. THE COORDINATORS PROBLEM As indicated above the distributed schedule management problem that we address in this paper is that put forth by the DARPA Coordinators program. The Coordinators problem is concerned generally with the collaborative execution of a joint mission by a team of agents in a highly dynamic environment. A mission is formulated as a network of tasks, which are distributed among the agents by the MASS simulator such that no agent has a complete, objective view of the whole problem. Instead, each agent receives only a subjective view containing just the portion of the task network that relates to ground tasks that it is responsible for and any remote tasks that have interdependencies with these local tasks. A pre-computed initial schedule is also distributed to the agents, and each agent\"s schedule indicates which of its local tasks should be executed and when. Each task has an associated quality value which accrues if it is successfully executed within its constraints, and the overall goal is to maximize the quality obtained during execution. Figure 2: Subjective view for Agent 2. As execution proceeds, agents must react to unexpected results (e.g., task delays, failures) and changes to the mission (e.g., new tasks, deadline changes) generated by the simulator, recognize when scheduled tasks are no longer feasible or desirable, and coordinate with each other to take corrective, quality-maximizing rescheduling actions that keep execution of the overall mission moving forward. Problems are formally specified using a version of the TAEMS language (Task Analysis, Environment Modeling and Simulation) called C TAEMS . Within C TAEMS, tasks are represented hierarchically, as shown in the example in Figure 1. At the highest, most abstract level, the root of the tree is a special task called the task group. On successive levels, tasks constitute aggregate activities, which can be decomposed into sets of subtasks and/or primitive activities, termed methods. Methods appear at the leaf level of C TAEMS task structures and are those that are directly executable in the world. Each declared method m can only be executed by a specified agent (denoted by ag : AgentN in Figure 1) and each agent can be executing at most one method at any given time (i.e. agents are unit-capacity resources). Method durations and quality are typically specified as discrete probability distributions, and hence known with certainty only after they have been executed.1 It is also possible for a method to fail unexpectedly in execution, in which case the reported quality is zero. For each task, a quality accumulation function qaf is defined, which specifies when and how a task accumulates quality as its subtasks (methods) are executed. For example, a task with a min qaf will accrue the quality of its child with lowest quality if all its children execute and accumulate positive quality. Tasks with sum or max qafs acquire quality as soon as one child executes with positive quality; as their qaf names suggest, their respective values ultimately will be the total or maximum quality of all children that executed. A sync-sum task will accrue quality only for those children that commence execution concurrently with the first child that executes, while an exactly-one task accrues quality only if precisely one of its children executes. Inter-dependencies between tasks/methods in the problem are modeled via non-local effects (nles). Two types of nles can be specified: hard and soft. Hard nles express For simplicity, Figures 1 and 2 show only fixed values for method quality and duration. The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 485 causal preconditions: for example, the enables nle in Figure 1 stipulates that the target method M5 can not be executed until the source M4 accumulates quality. Soft nles, which include facilitates and hinders, are not required constraints; however, when they are in play, they amplify (or dampen) the quality and duration of the target task. Any given task or method a can also be constrained by an earliest start time and a deadline, specifying the window in which a can be feasibly executed. a may also inherit these constraints from ancestor tasks at any higher level in the task structure, and its effective execution window will be defined by the tightest of these constraints. Figure 1 shows the complete objective view of a simple 2 agent problem. Figure 2 shows the subjective view available to agent 2 for the same problem. In what follows, we will sometimes use the term activity to refer generically to both task and method nodes. 3. OVERVIEW OF APPROACH Our solution framework combines two basic principles for coping with the problem of managing multi-agent schedules in an uncertain and time stressed execution environment. First is the use of a STN-based flexible times representation of solution constraints, which allows execution to be driven by a set of schedules rather than a single point solution. This provides a basic hedge against temporal uncertainty and can be used to modulate the need for solution revision. The second principle is to first respond locally to exceptional events, and then, as time permits, explore nonlocal options (i.e., options involving change by 2 or more agents) for global solution improvement. This provides a means for keeping pace with execution, and for tying the amount of effort spent in more global multi-agent solution improvement to the time available. Both local and non-local problem solving time is further minimized by the use of a core incremental scheduling procedure. Figure 3: Agent Architecture. Our solution framework is made concrete in the agent architecture depicted in Figure 3. In its most basic form, an agent comprises four principal components - an Executor, a Scheduler, a Distributed State Manager (DSM), and an Options Manager - all of which share a common model of the current problem and solution state that couples a domainlevel representation of the subjective c taems task structure to an underlying STN. At any point during operation, the currently installed schedule dictates the timing and sequence of domain-level activities that will be initiated by the agent. The Executor, running in its own thread, continually monitors the enabling conditions of various pending activities, and activates the next pending activity as soon as all of its causal and temporal constraints are satisfied. When execution results are received back from the environment (MASS) and/or changes to assumed external constraints are received from other agents, the agent\"s model of current state is updated. In cases where this update leads to inconsistency in the STN or it is otherwise recognized that the current local schedule might now be improved, the Scheduler, running on a separate thread, is invoked to revise the current solution and install a new schedule. Whenever local schedule constraints change either in response to a current state update or through manipulation by the Scheduler, the DSM is invoked to communicate these changes to interested agents (i.e., those agents that share dependencies and have overlapping subjective views). After responding locally to a given state update and communicating consequences, the agent will use any remaining computation time to explore possibilities for improvement through joint change. The Option Manager utilizes the Scheduler (in this case in hypothetical mode) to generate one or more non-local options, i.e., identifying changes to the schedule of one or more other agents that will enable the local agent to raise the quality of its schedule. These options are formulated and communicated as queries to the appropriate remote agents, who in turn hypothetically evaluate the impact of proposed changes from their local perspective. In those cases where global improvement is verified, joint changes are committed to. In the following sections we consider the mechanics of these components in more detail. 4. THE SCHEDULER As indicated above, our agent scheduler operates incrementally. Incremental scheduling frameworks are ideally suited for domains requiring tight scheduler-execution coupling: rather than recomputing a new schedule in response to every change, they respond quickly to execution events by localizing changes and making adjustments to the current schedule to accommodate the event. There is an inherent bias toward schedule stability which provides better support for the continuity in execution. This latter property is also advantageous in multi-agent settings, since solution stability tends to minimize the ripple across different agents\" schedules. The coupling of incremental scheduling with flexible times scheduling adds additional leverage in an uncertain, multiagent execution environment. As mentioned earlier, slack can be used as a hedge against uncertain method execution times. It also provides a basis for softening the impact of inter-dependencies across agents. In this section, we summarize the core scheduler that we have developed to solve the Coordinators problem. In subsequent sections we discuss its use in managing execution and coordinating with other agents. 4.1 STN Solution Representation To maintain the range of admissible values for the start and end times of various methods in a given agent\"s sched486 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) ule, all problem and scheduling constraints impacting these times are encoded in an underlying Simple Temporal Network (STN). An STN represents temporal constraints as a graph G < N, E >, where nodes in N represent the set of time points of interest, and edges in E are distances between pairs of time points in N. A special time point, called calendar zero grounds the network and has the value 0. Constraints on activities (e.g. release time, due time, duration) and relationships between activities (e.g. parentchild relation, enables) are uniformly represented as temporal constraints (i.e., edges) between relevant start and finish time points. An agent\"s schedule is designated as a total ordering of selected methods by posting precedence constraints between the end and start points of each ordered pair. As new methods are inserted into a schedule or external state updates require adjustments to existing constraints (e.g., substitution of an actual duration constraint, tightening of a deadline), the network propagates constraints and maintains lower and upper bounds on all time points in the network. This is accomplished efficiently via the use of a standard all-pairs shortest path algorithm; in our implementation, we take advantage of an incremental procedure based on . As bounds are updated, a consistency check is made for the presence of negative cycles, and the absence of any such cycle ensures the continued temporal feasibility of the network (and hence the schedule). Otherwise a conflict has been detected, and some amount of constraint retraction is necessary to restore feasibility. 4.2 Maintaining High-Quality Schedules The scheduler consists of two basic components: a quality propagator and an activity allocator that work in a tightly integrated loop. The quality propagator analyzes the activity hierarchy and collects a set of methods that (if scheduled) would maximize the quality of the agent\"s local problem. The methods are collected without regard for resource contention; in essence, the quality propagator optimally solves a relaxed problem where agents are capable of performing an infinite number of activities at once. The allocator selects methods from this list and attempts to install them in the agent\"s schedule. Failure to do so reinvokes the quality propagator with the problematic activity excluded. The Quality Propagator - The quality propagator performs the following actions on the C TAEMS task structure: \u2022 Computes the quality of all activities in the task structure: The expected quality qual(m) of a method m is computed from the probability distribution of the execution outcomes. The quality qual(t) of a task t is computed by applying its qaf to the assessed quality of its children. \u2022 Generates a list of contributors for each task: methods that, if scheduled, will maximize the quality obtained by the task. \u2022 Generates a list of activators for each task: methods that, if scheduled, are sufficient to qualify the task as scheduled. Methods in the activators list are chosen to minimize demands on the agent\"s timeline without regard to quality. The first time the quality propagator is invoked, the qualities of all tasks and methods are calculated and the initial lists of contributors and activators are determined. Subsequent calls to the propagator occur as the allocator installs methods on the agent\"s timeline: failure of the allocator to install a method causes the propagator to recompute a new list of contributors and activators. The Activity Allocator - The activity allocator seeks to install the contributors of the taskgroup identified by the quality propagator onto the agent\"s timeline. Any currently scheduled methods that do not appear in the contributors list are first unscheduled and removed from the timeline. The contributors are then preprocessed using a quality-centric heuristic to create an agenda sorted in decreasing quality order. In addition, methods associated with a and task (i.e., min, sumand) are grouped consecutively within the agenda. Since an and task accumulates quality only if all its children are scheduled, this biases the scheduling process towards failing early (and regenerating contributors) when the methods chosen for the and cannot together be allocated. The allocator iteratively pops the first method mnew from the agenda and attempts to install it. This entails first checking that all activities that enable mnew have been scheduled, while attempting to install any enabler that is not. If any of the enabler activities fails to install, the allocation pass fails. When successful, the enables constraints linking the enabler activities to mnew are activated. The STN rejects an infeasible enabler constraint by returning a conflict. In this event any enabler activities it has scheduled are uninstalled and the allocator returns failure. Once scheduling of enablers is ensured, a feasible slot on the agent\"s timeline within mnew\"s time window is sought and the allocator attempts to insert mnew between two currently scheduled methods. At the STN level, mnew\"s insertion breaks the sequencing constraint between the two extant timeline methods and attempts to insert two new sequencing constraints that chain mnew to these methods. If these insertions succeed, the routine returns success, otherwise the two extant timeline methods are relinked and allocation attempts the next possible slot for mnew insertion. 5. THE DYNAMICS OF EXECUTION Maintaining a flexible-times schedule enables us to use a conflict-driven approach to schedule repair: Rather than reacting to every event in the execution that may impact the existing schedule by computing an updated solution, the STN can absorb any change that does not cause a conflict. Consequently, computation (producing a new schedule) and communication costs (informing other agents of changes that affect them) are minimized. One basic mechanism needed to model execution in the STN is a dynamic model for current time. We employ a model proposed by that establishes a \u2018current-time\" time point and includes a link between it and the calendar-zero time point. As each method is scheduled, a simple precedence constraint between the current-time time point and the method is established. When the scheduler receives a current time update, the link between calendar-zero and current-time is modified to reflect this new time, and the constraint propagates to all scheduled methods. A second issue concerns synchronization between the executor and the scheduler, as producer and consumer of the schedule running on different threads within a given agent. This coordination must be robust despite the fact that the The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 487 executor needs to start methods for execution in real-time even while the scheduler may be reassessing the schedule to maximize quality, and/or transmitting a revised schedule. If the executor, for example, slates a method for execution based on current time while the scheduler is instantiating a revised schedule in which that method is no longer nextto-be-executed, an inconsistent state may arise within the agent architecture. This is addressed in part by introducing a freeze window; a specified short (and adjustable) time period beyond current time within which any activity slated as eligible to start in the current schedule cannot be rescheduled by the scheduler. The scheduler is triggered in response to various environmental messages. There are two types of environmental message classes that we discuss here as execution dynamics: 1) feedback as a result of method execution - both the agent\"s own and that of other agents, and 2) changes in the C TAEMS model corresponding to a set of simulatordirected evolutions of the problem and environment. Such messages are termed updates and are treated by the scheduler as directives to permanently modify parameters in its model. We discuss these update types in turn here and defer until later the discussion of queries to the scheduler, a \"what-if\" mode initiated by a remote agent that is pursuing higher global quality. Whether it is invoked via an update or a query, the scheduler\"s response is an option; essentially a complete schedule of activities the agent can execute along with associated quality metrics. We define a local option as a valid schedule for an agent\"s activities, which does not require change to any other agent\"s schedule. The overarching design for handling execution dynamics aims at anytime scheduling behavior in which a local option maximizing the local view of quality is returned quickly, possibly followed by globally higher quality schedules that entail inter-agent coordination if available scheduler cycles permit. As such, the default scheduling mode for updates is to seek the highest quality local option according to the scheduler\"s search strategy, instantiate the option as its current schedule, and notify the executor of the revision. 5.1 Responding to Activity Execution As suggested earlier, a committed schedule consists of a sequence of methods, each with a designated [est, lst] start time window (as provided by the underlying STN representation). The executor is free to execute a method any time within its start time window, once any additional enabling conditions have been confirmed. These scheduled start time windows are established using the expected duration of each scheduled method (derived from associated method duration distributions during schedule construction). Of course as execution unfolds, actual method durations may deviate from these expectations. In these cases, the flexibility retained in the schedule can be used to absorb some of this unpredictability and modulate invocation of a schedule revision process. Consider the case of a method completion message, one of the environmental messages that could be communicated to the scheduler as an execution state update. If the completion time is coincident with the expected duration (i.e., it completes exactly as expected), then the scheduler\"s response is to simply mark it as \u2018completed\" and the agent can proceed to communicate the time at which it has accumulated quality to any remote agents linked to this method. However if the method completes with a duration shorter than expected a rescheduling action might be warranted. The posting of the actual duration in the STN introduces no potential for conflict in this case, either with the latest start times (lsts) of local or remote methods that depend on this method as an enabler, or to successively scheduled methods on the agent\"s timeline. However, it may present a possibility for exploiting the unanticipated scheduling slack. The flexible times representation afforded by the STN provides a quick means of assessing whether the next method on the timeline can begin immediate execution instead of waiting for its previously established earliest start time (est). If indeed the est of the next scheduled method can spring back to current-time once the actual duration constraint is substituted for the expected duration constraint, then the schedule can be left intact and simply communicated back to the executor. If alternatively, other problem constraints prevent this relaxation of the est, then there is forced idle time that may be exploited by revising the schedule, and the scheduler is invoked (always respecting the freeze period). If the method completes later than expected, then there is no need for rescheduling under flexible times scheduling unless 1) the method finishes later than the lst of the subsequent scheduled activity, or 2) it finishes later than its deadline. Thus we only invoke the scheduler if, upon posting the late finish in the STN, a constraint violation occurs. In the latter case no quality is accrued and rescheduling is mandated even if there are no conflicts with subsequent scheduled activities. Other execution status updates the agent may receive include: \u2022 method start - If a method sent for execution is started within its [est, lst] window, the response is to mark it as \"executing\". A method cannot start earlier than when it is transmitted by the executor but it is possible for it to start later than requested. If the posted start time causes an inconsistency in the STN (e.g. because the expected method duration can no longer be accommodated) the duration constraint in the STN is shortened based on the known distribution until either consistency is restored or rescheduling is mandated. \u2022 method failure - Any method under execution may fail unexpectedly, garnering no quality for the agent. At this point rescheduling is mandated as the method may enable other activities or significantly impact quality in the absence of local repair. Again, the executor will proceed with execution of the next method if its start time arrives before the revised schedule is committed, and the scheduler accommodates this by respecting the freeze window. \u2022 current time advances An update on \"current time\" may arrive either alone or as part of any of the previously discussed updates. If, when updating the currenttime link in the STN (as described above), a conflict results, the execution state is inconsistent with the schedule. In this case, the scheduler proceeds as if execution were consistent with its expectations, subject to possible later updates. 488 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 5.2 Responding to Model Updates The agent can also dynamically receive changes to the agent\"s underlying C TAEMS model. Dynamic revisions in the outcome distributions for methods already in an agent\"s subjective view may impact the assessed quality and/or duration values that shaped the current schedule. Similarly, dynamic revisions in the designated release times and deadlines for methods and tasks already in an agent\"s subjective view can invalidate an extant schedule or present opportunities to boost quality. It is also possible during execution to receive updates in which new methods and possibly entire task structures are given to the agent for inclusion in its subjective view. Model changes that involve temporal constraints are handled in much the same fashion as described for method starts and completions, i.e, rescheduling is required only when the posting of the revised constraints leads to an STN conflict. In the case of non-temporal model changes, rescheduling action is currently always initiated. 6. INTER-AGENT COORDINATION Having responded locally to an unexpected execution result or model change, it is necessary to communicate the consequences to agents with inter-dependent activities so that they can align their decisions accordingly. Responses that look good locally may have a sub-optimal global effect once alignments are made, and hence agents must have the ability to seek mutually beneficial joint schedule changes. In this section we summarize the coordination mechanisms provided in the agent architecture to address these issues. 6.1 Communicating Non-Local Constraints A basic means of coordination with other agents is provided by the Distributed State Mechanism (DSM), which is responsible for communicating changes made to the model or schedule of a given agent to other interested agents. More specifically, the DSM of a given agent acts to push any changes made to the time bounds, quality, or status of a local task/method to all the other agents that have that same task/method as a remote node in their subjective views. A recipient agent treats any communicated changes as additional forms of updates, in this case an update that modifies the current constraints associated with non-local (but inter-dependent) tasks or methods. These changes are handled identically to updates reflecting schedule execution results, potentially triggering the local scheduler if the need to reschedule is detected. 6.2 Generating Non-Local Options As mentioned in the previous section, the agent\"s first response to any given query or update (either from execution or from another agent) is to generate one or more local options. Such options represent local schedule changes that are consistent with all currently known constraints originating from other agents\" schedules, and hence can be implemented without interaction with other agents. In many cases, however, a larger-scoped change to the schedules of two or more agents can produce a higher-quality response. Exploration of opportunities for such coordinated action by two or more agents is the responsibility of the Options Manager. Running in lower priority mode than the Executor and Scheduler, the Options Manager initiates a non-local option generation and evaluation process in response to any local schedule change made by the agent if computation time constraints permits. Generally speaking, a non-local option identifies certain relaxations (to one or more constraints imposed by methods that are scheduled by one or more remote agents) that enable the generation of a higher quality local schedule. When found, a non-local option is used by a coordinating agent to formulate queries to any other involved agents in order to determine the impact of such constraint relaxations on their local schedules. If the combined quality change reported back from a set of one or more relevant queries is a net gain, then the issuing agent signals to the other involved agents to commit to this joint set of schedule changes. The Option Manager currently employs two basic search strategies for generating non-local options, each exploiting the local scheduler in hypothetical mode. Optimistic Synchronization - Optimistic synchronization is a non-local option generation strategy where search is used to explore the impact on quality if optimistic assumptions are made about currently unscheduled remote enablers. More specifically, the strategy looks for would be contributor methods that are currently unscheduled due to the fact that one or more remote enabling (source) tasks or methods are not currently scheduled. For each such local method, the set of remote enablers are hypothetically activated, and the scheduler attempts to construct a new local schedule under these optimistic assumptions. If successful, a non-local option is generated, specifying the value of the new, higher quality local schedule, the temporal constraints on the local target activity, and the set of must-schedule enabler activities that must be scheduled by remote agents in order to achieve this local quality. The needed queries requesting the quality impact of scheduling these activities are then formulated and sent to the relevant remote agents. To illustrate, consider again the example in Figure 1. The maximum quality that Agent1 can contribute to the task group is 15 (by scheduling M1, M2 and M3). Assume that this is Agent1\"s current schedule. Given this state, the maximum quality that Agent2 can contribute to the task group is 10, and the total task group quality would then be 15 + 10 = 25. Using optimistic synchronization, Agent2 will generate a non-local option that indicates that if M5 becomes enabled, both M5 and M6 would be scheduled, and the quality contributed by Agent2 to the task group would become 30. Agent2 sends a must schedule M4 query to Agent1. Because of the time window constraints, Agent1 must remove M3 from its schedule to get M4 on, resulting in a new lower quality schedule of 5. However, when Agent2 receives this option response from Agent1, it determines that the total quality accumulated for the task group would be 5 + 30 = 35, a net gain of 10. Hence, Agent 2 signals to Agent1 to commit to this non-local option. Conflict-Driven Relaxation - A second strategy for generating non-local options, referred to as Conflict-Directed Relaxation, utilizes analysis of STN conflicts to identify and prioritize external constraints to relax in the event that a particular method that would increase local quality is found to be unschedulable. Recall that if a method cannot be feasibly inserted into the schedule, an attempt to do so will generate a negative cycle. Given this cycle, the mechanism proceeds in three steps. First, the constraints involved in the cycle are collected. Second, by virtue of the connections in the STN to the domain-level C TAEMS model, this set is filtered to identify the subset associated with remote nodes. Third, constraints in this subset are selectively retracted to The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 489 Figure 4: A high quality task is added to the task structure of Agent2. Figure 5: If M4, M5 and M7 are scheduled, a conflict is detected by the STN. determine if STN consistency is restored. If successful, a non-local option is generated indicating which remote constraint(s) must be relaxed and by how much to allow installation of the new, higher quality local schedule. To illustrate this strategy, consider Figure 5 where Agent1 has M1, M2 and M4 on its timeline, and therefore est(M4) = 21. Agent2 has M5 and M6 on its timeline, with est(M5) = 31 (M6 could be scheduled before or after M5). Suppose that Agent2 receives a new task M7 with deadline 55 (see Figure 4). If Agent2 could schedule M7, the quality contributed by Agent2 to the task group would be 70. However, an attempt to schedule M7 together with M5 and M6 leads to a conflict, since the est(M7) = 46, dur(M7) = 10 and lft(M7) = 55 (see Figure 5). Conflict-directed relaxation by Agent 2 suggests relaxing the lft(M4) by 1 tick to 30, and this query is communicated to Agent 1. In fact, by retracting either method M1 or M2 from the schedule this relaxation can be accommodated with no quality loss to Agent1 (due to the min qaf). Upon communication of this fact Agent 2 signals to commit. 7. EXPERIMENTAL RESULTS An initial version of the agent described in this paper was developed in collaboration with SRI International and subjected to the independently conducted Coordinators programmatic evaluation. problem instances randomly generated by a scenario generator that was configured to produce scenarios of varying Problem Class Description Agent Class Quality OD \u2018Only Dynamics\". No NLEs. 97.9% (390 probs) Actual task duration & quality vary according to distribution. INT \u2018Interdependent\". Frequent & 100% (360 probs) random (esp. facilitates) CHAINS Activities chained together 99.5% (360 probs) via sequences of enables NLEs (1-4 chains/prob) TT \u2018Temporal Tightness\". Release - 94.9% (360 probs) Deadline windows preclude preferred high quality (longest duration) tasks from all being scheduled. SYNC Problems contain range of 97.1% (360 probs) different Sync sum tasks NTA \u2018New Task Arrival\". cTaems 99.0% (360 probs) model is augmented with new tasks dynamically during run. OVERALL Avg: 98.1% ( probs) Std dev: 6.96 Table 1: Performance of year 1 agent over Coordinators evaluation. \u2018Agent Quality\" is % of \u2018optimal\" durations within six experiment classes. These classes, summarized in Table 1, were designed to evaluate key aspects of a set of Coordinators distributed scheduling agents, such as their ability to handle unexpected execution results, chains of nle\"s involving multiple agents, and effective scheduling of new activities that arise unexpectedly at some point during the problem run. Year 1 evaluation problems were constrained to be small enough (3 -10 agents, 50 - 100 methods) such that comparison against an optimal centralized solver was feasible. The evaluation team employed an MDP-based solver capable of unrolling the entire search space for these problems, choosing for an agent at each execution decision point the activity most likely to produce maximum global quality. This established a challenging benchmark for the distributed agent systems to compare against. The hardware configuration used by the evaluators instantiated and ran one agent per machine, dedicating a separate machine to the MASS simulator. As reported in Table 1, the year 1 prototype agent clearly compares favorably to the benchmark on all classes, coming within 2% problems. These results are particularly notable given that each agent\"s STN-based scheduler does very little reasoning over the success probability of the activity sequences it selects to execute. Only simple tactics were adopted to explicitly address such uncertainty, such as the use of expected durations and quality for activities and a policy of excluding from consideration those activities with failure likelihood of >75%. The very respectable agent performance can be at least partially credited to the fact that the flexible times representation employed by the scheduler affords it an important buffer against the uncertainty of execution and exogenous events. The agent turns in its lowest performance on the TT (Temporal Tightness) experiment classes, and an examination of the agent trace logs reveals possible reasons. In about half of the TT problems the year 1 agent under-performs on, the specified time windows within which an agent\"s ac490 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) tivities must be scheduled are so tight that any scheduled activity which executes with a longer duration than the expected value, causes a deadline failure. This constitutes a case where more sophisticated reasoning over success probability would benefit this agent. The other half of underperforming TT problems involve activities that depend on facilitation relationships in order to fit in their time windows (recall that facilitation increases quality and decreases duration). The limited facilitates reasoning performed by the year 1 scheduler sometimes causes failures to install a heavily facilitated initial schedule. Even when such activities are successfully installed they tend to be prone to deadline failures -If a source-side activity(s) either fails or exceeds its expected duration the resulting longer duration of the target activity can violate its time window deadline. 8. STATUS AND DIRECTIONS Our current research efforts are aimed at extending the capabilities of the Year 1 agent and scaling up to significantly larger problems. Year 2 programmatic evaluation goals call for solving problems on the order of 100 agents and 10,000 methods. This scale places much higher computational demands on all of the agent\"s components. We have recently completed a re-implementation of the prototype agent designed to address some recognized performance issues. In addition to verifying that the performance on Year 1 problems is matched or exceeded, we have recently run some successful tests with the agent on a few 100 agent problems. To fully address various scale up issues, we are investigating a number of more advanced coordination mechanisms. To provide more global perspective to local scheduling decisions, we are introducing mechanisms for computing, communicating and using estimates of the non-local impact of remote nodes. To better address the problem of establishing inter-agent synchronization points, we expanding the use of task owners and qaf-specifc protocols as a means for directing coordination activity. Finally, we plan to explore the use of more advanced STN-driven coordination mechanisms, including the use of temporal decoupling to insulate the actions of inter-dependent agents and the introduction of probability sensitive contingency schedules.", "body1": "The practical constraints of many application environments require distributed management of executing plans and schedules. In a flexible-times representation of an agent\"s schedule, the execution intervals associated with scheduled activities are not fixed, but instead are allowed to float within imposed time and activity sequencing constraints. We define an agent architecture centered around incremental management of a flexible times schedule. To this schedule management infra-structure, we add two mechanisms for multi-agent coordination. The remainder of the paper is organized as follows. As indicated above the distributed schedule management problem that we address in this paper is that put forth by the DARPA Coordinators program. As execution proceeds, agents must react to unexpected results (e.g., task delays, failures) and changes to the mission (e.g., new tasks, deadline changes) generated by the simulator, recognize when scheduled tasks are no longer feasible or desirable, and coordinate with each other to take corrective, quality-maximizing rescheduling actions that keep execution of the overall mission moving forward. Problems are formally specified using a version of the TAEMS language (Task Analysis, Environment Modeling and Simulation) called C TAEMS . On successive levels, tasks constitute aggregate activities, which can be decomposed into sets of subtasks and/or primitive activities, termed methods. For each task, a quality accumulation function qaf is defined, which specifies when and how a task accumulates quality as its subtasks (methods) are executed. Inter-dependencies between tasks/methods in the problem are modeled via non-local effects (nles). The Sixth Intl. Figure 1 shows the complete objective view of a simple 2 agent problem. Our solution framework combines two basic principles for coping with the problem of managing multi-agent schedules in an uncertain and time stressed execution environment. First is the use of a STN-based flexible times representation of solution constraints, which allows execution to be driven by a set of schedules rather than a single point solution. Our solution framework is made concrete in the agent architecture depicted in Figure 3. When execution results are received back from the environment (MASS) and/or changes to assumed external constraints are received from other agents, the agent\"s model of current state is updated. After responding locally to a given state update and communicating consequences, the agent will use any remaining computation time to explore possibilities for improvement through joint change. As indicated above, our agent scheduler operates incrementally. The coupling of incremental scheduling with flexible times scheduling adds additional leverage in an uncertain, multiagent execution environment. 4.1 STN Solution Representation To maintain the range of admissible values for the start and end times of various methods in a given agent\"s sched486 The Sixth Intl. parentchild relation, enables) are uniformly represented as temporal constraints (i.e., edges) between relevant start and finish time points. The methods are collected without regard for resource contention; in essence, the quality propagator optimally solves a relaxed problem where agents are capable of performing an infinite number of activities at once. \u2022 Generates a list of contributors for each task: methods that, if scheduled, will maximize the quality obtained by the task. \u2022 Generates a list of activators for each task: methods that, if scheduled, are sufficient to qualify the task as scheduled. The first time the quality propagator is invoked, the qualities of all tasks and methods are calculated and the initial lists of contributors and activators are determined. Subsequent calls to the propagator occur as the allocator installs methods on the agent\"s timeline: failure of the allocator to install a method causes the propagator to recompute a new list of contributors and activators. The Activity Allocator - The activity allocator seeks to install the contributors of the taskgroup identified by the quality propagator onto the agent\"s timeline. The allocator iteratively pops the first method mnew from the agenda and attempts to install it. In this event any enabler activities it has scheduled are uninstalled and the allocator returns failure. Maintaining a flexible-times schedule enables us to use a conflict-driven approach to schedule repair: Rather than reacting to every event in the execution that may impact the existing schedule by computing an updated solution, the STN can absorb any change that does not cause a conflict. One basic mechanism needed to model execution in the STN is a dynamic model for current time. This coordination must be robust despite the fact that the The Sixth Intl. The scheduler is triggered in response to various environmental messages. Whether it is invoked via an update or a query, the scheduler\"s response is an option; essentially a complete schedule of activities the agent can execute along with associated quality metrics. 5.1 Responding to Activity Execution As suggested earlier, a committed schedule consists of a sequence of methods, each with a designated [est, lst] start time window (as provided by the underlying STN representation). However if the method completes with a duration shorter than expected a rescheduling action might be warranted. The posting of the actual duration in the STN introduces no potential for conflict in this case, either with the latest start times (lsts) of local or remote methods that depend on this method as an enabler, or to successively scheduled methods on the agent\"s timeline. The flexible times representation afforded by the STN provides a quick means of assessing whether the next method on the timeline can begin immediate execution instead of waiting for its previously established earliest start time (est). If indeed the est of the next scheduled method can spring back to current-time once the actual duration constraint is substituted for the expected duration constraint, then the schedule can be left intact and simply communicated back to the executor. If the method completes later than expected, then there is no need for rescheduling under flexible times scheduling unless 1) the method finishes later than the lst of the subsequent scheduled activity, or 2) it finishes later than its deadline. In the latter case no quality is accrued and rescheduling is mandated even if there are no conflicts with subsequent scheduled activities. Other execution status updates the agent may receive include: \u2022 method start - If a method sent for execution is started within its [est, lst] window, the response is to mark it as \"executing\". \u2022 method failure - Any method under execution may fail unexpectedly, garnering no quality for the agent. \u2022 current time advances An update on \"current time\" may arrive either alone or as part of any of the previously discussed updates. 488 The Sixth Intl. Having responded locally to an unexpected execution result or model change, it is necessary to communicate the consequences to agents with inter-dependent activities so that they can align their decisions accordingly. 6.1 Communicating Non-Local Constraints A basic means of coordination with other agents is provided by the Distributed State Mechanism (DSM), which is responsible for communicating changes made to the model or schedule of a given agent to other interested agents. More specifically, the DSM of a given agent acts to push any changes made to the time bounds, quality, or status of a local task/method to all the other agents that have that same task/method as a remote node in their subjective views. 6.2 Generating Non-Local Options As mentioned in the previous section, the agent\"s first response to any given query or update (either from execution or from another agent) is to generate one or more local options. Exploration of opportunities for such coordinated action by two or more agents is the responsibility of the Options Manager. Optimistic Synchronization - Optimistic synchronization is a non-local option generation strategy where search is used to explore the impact on quality if optimistic assumptions are made about currently unscheduled remote enablers. To illustrate, consider again the example in Figure 1. Conflict-Driven Relaxation - A second strategy for generating non-local options, referred to as Conflict-Directed Relaxation, utilizes analysis of STN conflicts to identify and prioritize external constraints to relax in the event that a particular method that would increase local quality is found to be unschedulable. Third, constraints in this subset are selectively retracted to The Sixth Intl. determine if STN consistency is restored. To illustrate this strategy, consider Figure 5 where Agent1 has M1, M2 and M4 on its timeline, and therefore est(M4) = 21. However, an attempt to schedule M7 together with M5 and M6 leads to a conflict, since the est(M7) = 46, dur(M7) = 10 and lft(M7) = 55 (see Figure 5). An initial version of the agent described in this paper was developed in collaboration with SRI International and subjected to the independently conducted Coordinators programmatic evaluation. INT \u2018Interdependent\". OVERALL Avg: 98.1% ( probs) Std dev: 6.96 Table 1: Performance of year 1 agent over Coordinators evaluation. As reported in Table 1, the year 1 prototype agent clearly compares favorably to the benchmark on all classes, coming within 2% problems. The agent turns in its lowest performance on the TT (Temporal Tightness) experiment classes, and an examination of the agent trace logs reveals possible reasons.", "body2": "Central to our approach to solving this multi-agent problem is an incremental flexible-times scheduling framework. However their use in distributed problem solving settings has been quite sparse ( is one exception), and prior approaches to multi-agent scheduling (e.g., ) have generally operated with fixed-times representations of agent schedules. complished by an incremental scheduler, designed to maximize quality while attempting to minimize schedule change. This latter process uses analysis of detected conflicts in the STN as a basis for generating options. Finally we conclude with a brief discussion of current research plans. Figure 2: Subjective view for Agent 2. As execution proceeds, agents must react to unexpected results (e.g., task delays, failures) and changes to the mission (e.g., new tasks, deadline changes) generated by the simulator, recognize when scheduled tasks are no longer feasible or desirable, and coordinate with each other to take corrective, quality-maximizing rescheduling actions that keep execution of the overall mission moving forward. At the highest, most abstract level, the root of the tree is a special task called the task group. Method durations and quality are typically specified as discrete probability distributions, and hence known with certainty only after they have been executed.1 It is also possible for a method to fail unexpectedly in execution, in which case the reported quality is zero. A sync-sum task will accrue quality only for those children that commence execution concurrently with the first child that executes, while an exactly-one task accrues quality only if precisely one of its children executes. Hard nles express For simplicity, Figures 1 and 2 show only fixed values for method quality and duration. a may also inherit these constraints from ancestor tasks at any higher level in the task structure, and its effective execution window will be defined by the tightest of these constraints. In what follows, we will sometimes use the term activity to refer generically to both task and method nodes. Our solution framework combines two basic principles for coping with the problem of managing multi-agent schedules in an uncertain and time stressed execution environment. Figure 3: Agent Architecture. The Executor, running in its own thread, continually monitors the enabling conditions of various pending activities, and activates the next pending activity as soon as all of its causal and temporal constraints are satisfied. Whenever local schedule constraints change either in response to a current state update or through manipulation by the Scheduler, the DSM is invoked to communicate these changes to interested agents (i.e., those agents that share dependencies and have overlapping subjective views). In the following sections we consider the mechanics of these components in more detail. This latter property is also advantageous in multi-agent settings, since solution stability tends to minimize the ripple across different agents\" schedules. In subsequent sections we discuss its use in managing execution and coordinating with other agents. release time, due time, duration) and relationships between activities (e.g. The quality propagator analyzes the activity hierarchy and collects a set of methods that (if scheduled) would maximize the quality of the agent\"s local problem. The quality qual(t) of a task t is computed by applying its qaf to the assessed quality of its children. \u2022 Generates a list of contributors for each task: methods that, if scheduled, will maximize the quality obtained by the task. Methods in the activators list are chosen to minimize demands on the agent\"s timeline without regard to quality. The first time the quality propagator is invoked, the qualities of all tasks and methods are calculated and the initial lists of contributors and activators are determined. Subsequent calls to the propagator occur as the allocator installs methods on the agent\"s timeline: failure of the allocator to install a method causes the propagator to recompute a new list of contributors and activators. Since an and task accumulates quality only if all its children are scheduled, this biases the scheduling process towards failing early (and regenerating contributors) when the methods chosen for the and cannot together be allocated. The STN rejects an infeasible enabler constraint by returning a conflict. If these insertions succeed, the routine returns success, otherwise the two extant timeline methods are relinked and allocation attempts the next possible slot for mnew insertion. Consequently, computation (producing a new schedule) and communication costs (informing other agents of changes that affect them) are minimized. A second issue concerns synchronization between the executor and the scheduler, as producer and consumer of the schedule running on different threads within a given agent. This is addressed in part by introducing a freeze window; a specified short (and adjustable) time period beyond current time within which any activity slated as eligible to start in the current schedule cannot be rescheduled by the scheduler. We discuss these update types in turn here and defer until later the discussion of queries to the scheduler, a \"what-if\" mode initiated by a remote agent that is pursuing higher global quality. As such, the default scheduling mode for updates is to seek the highest quality local option according to the scheduler\"s search strategy, instantiate the option as its current schedule, and notify the executor of the revision. If the completion time is coincident with the expected duration (i.e., it completes exactly as expected), then the scheduler\"s response is to simply mark it as \u2018completed\" and the agent can proceed to communicate the time at which it has accumulated quality to any remote agents linked to this method. However if the method completes with a duration shorter than expected a rescheduling action might be warranted. However, it may present a possibility for exploiting the unanticipated scheduling slack. The flexible times representation afforded by the STN provides a quick means of assessing whether the next method on the timeline can begin immediate execution instead of waiting for its previously established earliest start time (est). If alternatively, other problem constraints prevent this relaxation of the est, then there is forced idle time that may be exploited by revising the schedule, and the scheduler is invoked (always respecting the freeze period). Thus we only invoke the scheduler if, upon posting the late finish in the STN, a constraint violation occurs. In the latter case no quality is accrued and rescheduling is mandated even if there are no conflicts with subsequent scheduled activities. because the expected method duration can no longer be accommodated) the duration constraint in the STN is shortened based on the known distribution until either consistency is restored or rescheduling is mandated. Again, the executor will proceed with execution of the next method if its start time arrives before the revised schedule is committed, and the scheduler accommodates this by respecting the freeze window. In this case, the scheduler proceeds as if execution were consistent with its expectations, subject to possible later updates. In the case of non-temporal model changes, rescheduling action is currently always initiated. In this section we summarize the coordination mechanisms provided in the agent architecture to address these issues. 6.1 Communicating Non-Local Constraints A basic means of coordination with other agents is provided by the Distributed State Mechanism (DSM), which is responsible for communicating changes made to the model or schedule of a given agent to other interested agents. These changes are handled identically to updates reflecting schedule execution results, potentially triggering the local scheduler if the need to reschedule is detected. In many cases, however, a larger-scoped change to the schedules of two or more agents can produce a higher-quality response. The Option Manager currently employs two basic search strategies for generating non-local options, each exploiting the local scheduler in hypothetical mode. The needed queries requesting the quality impact of scheduling these activities are then formulated and sent to the relevant remote agents. Hence, Agent 2 signals to Agent1 to commit to this non-local option. Second, by virtue of the connections in the STN to the domain-level C TAEMS model, this set is filtered to identify the subset associated with remote nodes. Figure 5: If M4, M5 and M7 are scheduled, a conflict is detected by the STN. If successful, a non-local option is generated indicating which remote constraint(s) must be relaxed and by how much to allow installation of the new, higher quality local schedule. If Agent2 could schedule M7, the quality contributed by Agent2 to the task group would be 70. Upon communication of this fact Agent 2 signals to commit. 97.9% (390 probs) Actual task duration & quality vary according to distribution. cTaems 99.0% (360 probs) model is augmented with new tasks dynamically during run. The hardware configuration used by the evaluators instantiated and ran one agent per machine, dedicating a separate machine to the MASS simulator. The very respectable agent performance can be at least partially credited to the fact that the flexible times representation employed by the scheduler affords it an important buffer against the uncertainty of execution and exogenous events. Even when such activities are successfully installed they tend to be prone to deadline failures -If a source-side activity(s) either fails or exceeds its expected duration the resulting longer duration of the target activity can violate its time window deadline.", "introduction": "The practical constraints of many application environments require distributed management of executing plans and schedules. Such factors as geographical separation of executing agents, limitations on communication bandwidth, constraints relating to chain of command and the high tempo of execution dynamics may all preclude any single agent from obtaining a complete global view of the problem, and hence necessitate collaborative yet localized planning and scheduling decisions. In this paper, we consider the problem of managing and executing schedules in an uncertain and distributed environment as defined by the DARPA Coordinators program. We assume a team of collaborative agents, each responsible for executing a portion of a globally preestablished schedule, but none possessing a global view of either the problem or solution. The team goal is to maximize the total quality of all activities executed by all agents, given that unexpected events will force changes to pre-scheduled activities and alter the utility of executing others as execution unfolds. To provide a basis for distributed coordination, each agent is aware of dependencies between its scheduled activities and those of other agents. Each agent is also given a pre-computed set of local contingency (fall-back) options. Central to our approach to solving this multi-agent problem is an incremental flexible-times scheduling framework. In a flexible-times representation of an agent\"s schedule, the execution intervals associated with scheduled activities are not fixed, but instead are allowed to float within imposed time and activity sequencing constraints. This representation allows the explicit use of slack as a hedge against simple forms of executional uncertainty (e.g., activity durations), and its underlying implementation as a Simple Temporal Network (STN) model provides efficient updating and consistency enforcement mechanisms. The advantages of flexible times frameworks have been demonstrated in various centralized planning and scheduling contexts (e.g., ). However their use in distributed problem solving settings has been quite sparse ( is one exception), and prior approaches to multi-agent scheduling (e.g., ) have generally operated with fixed-times representations of agent schedules. We define an agent architecture centered around incremental management of a flexible times schedule. The underlying STN-based representation is used (1) to loosen the coupling between executor and scheduler threads, (2) to retain a basic ability to absorb unexpected executional delays (or speedups), and (3) to provide a basic criterion for detecting the need for schedule change. Local change is ac484 978-81--7-5 (RPS) IFAAMAS Figure 1: A two agent C TAEMS problem. complished by an incremental scheduler, designed to maximize quality while attempting to minimize schedule change. To this schedule management infra-structure, we add two mechanisms for multi-agent coordination. Basic coordination with other agents is achieved by simple communication of local schedule changes to other agents with interdependent activities. Layered over this is a non-local option generation and evaluation process (similar in some respects to ), aimed at identification of opportunities for global improvement through joint changes to the schedules of multiple agents. This latter process uses analysis of detected conflicts in the STN as a basis for generating options. The remainder of the paper is organized as follows. We begin by briefly summarizing the general distributed scheduling problem of interest in our work. Next, we introduce the agent architecture we have developed to solve this problem and sketch its operation. In the following sections, we describe the components of the architecture in more detail, considering in turn issues relating to executing agent schedules, incrementally revising agent schedules and coordinating schedule changes among multiple agents. We then give some experimental results to indicate current system performance. Finally we conclude with a brief discussion of current research plans.", "conclusion": "Our current research efforts are aimed at extending the capabilities of the Year 1 agent and scaling up to significantly larger problems.. Year 2 programmatic evaluation goals call for solving problems on the order of 100 agents and 10,000 methods.. This scale places much higher computational demands on all of the agent\"s components.. We have recently completed a re-implementation of the prototype agent designed to address some recognized performance issues.. In addition to verifying that the performance on Year 1 problems is matched or exceeded, we have recently run some successful tests with the agent on a few 100 agent problems.. To fully address various scale up issues, we are investigating a number of more advanced coordination mechanisms.. To provide more global perspective to local scheduling decisions, we are introducing mechanisms for computing, communicating and using estimates of the non-local impact of remote nodes.. To better address the problem of establishing inter-agent synchronization points, we expanding the use of task owners and qaf-specifc protocols as a means for directing coordination activity.. Finally, we plan to explore the use of more advanced STN-driven coordination mechanisms, including the use of temporal decoupling to insulate the actions of inter-dependent agents and the introduction of probability sensitive contingency schedules."}
{"id": "I-70", "keywords": ["ontolog", "cooper", "emerg behavior"], "title": "A Multi-Agent System for Building Dynamic Ontologies", "abstract": "Ontologies building from text is still a time-consuming task which justifies the growth of Ontology Learning. Our system named Dynamo is designed along this domain but following an original approach based on an adaptive multi-agent architecture. In this paper we present a distributed hierarchical clustering algorithm, core of our approach. It is evaluated and compared to a more conventional centralized algorithm. We also present how it has been improved using a multi-criteria approach. With those results in mind, we discuss the limits of our system and add as perspectives the modifications required to reach a complete ontology building solution.", "references": ["Construction of a regional ontology from text and its use within a documentary system", "Text analysis for ontology and terminology engineering", "Collaborative ontology building with wiki", "Agent-Oriented Methodologies", "Background and foreground knowledge in dynamic ontology construction", "A corpus-based conceptual clustering method for verb frames and ontology acquisition", "Ontology Engineering: a Survey and a Return on Experience", "Living Design for Open Computational Systems", "A Theory of emergent computation based on cooperative self-organization for adaptive artificial systems", "Dynamic ontologies on the web", "Terminology extraction from text to build an ontology in surgical intensive care", "Reconciling Ontological Differences by Assistant Agents", "Ontology learning for the Semantic Web", "Mining Ontologies from Text", "Foundations of Statistical Natural Language Processing", "Dynamic decentralized any-time hierarchical clustering"], "full_text": "1. INTRODUCTION Nowadays, it is well established that ontologies are needed for semantic web, knowledge management, B2B... For knowledge management, ontologies are used to annotate documents and to enhance the information retrieval. But building an ontology manually is a slow, tedious, costly, complex and time consuming process. Currently, a real challenge lies in building them automatically or semi-automatically and keeping them up to date. It would mean creating dynamic ontologies and it justifies the emergence of ontology learning techniques . Our research focuses on Dynamo (an acronym of DYNAMic Ontologies), a tool based on an adaptive multi-agent system to construct and maintain an ontology from a domain specific set of texts. Our aim is not to build an exhaustive, general hierarchical ontology but a domain specific one. We propose a semi-automated tool since an external resource is required: the \"ontologist\". An ontologist is a kind of cognitive engineer, or analyst, who is using information from texts and expert interviews to design ontologies. In the multi-agent field, ontologies generally enable agents to understand each other . They\"re sometimes used to ease the ontology building process, in particular for collaborative contexts , but they rarely represent the ontology itself . Most works interested in the construction of ontologies propose the refinement of ontologies. This process consists in using an existing ontology and building a new one from it. This approach is different from our approach because Dynamo starts from scratch. Researchers, working on the construction of ontologies from texts, claim that the work to be automated requires external resources such as a dictionary , or web access . In our work, we propose an interaction between the ontologist and the system, our external resource lies both in the texts and the ontologist. This paper first presents, in section 2, the big picture of the Dynamo system. In particular the motives that led to its creation and its general architecture. Then, in section 3 we discuss the distributed clustering algorithm used in Dynamo and compare it to a more classic centralized approach. Section 4 is dedicated to some enhancement of the agents behavior that got designed by taking into account criteria ignored by clustering. And finally, in section 5, we discuss the limitations of our approach and explain how it will be addressed in further work. 2. DYNAMO OVERVIEW 2.1 Ontology as a Multi-Agent System Dynamo aims at reducing the need for manual actions in processing the text analysis results and at suggesting a concept network kick-off in order to build ontologies more efficiently. The chosen approach is completely original to our knowledge and uses an adaptive multi-agent system. This choice comes from the qualities offered by multi-agent system: they can ease the interactive design of a system (in our case, a conceptual network), they allow its incremental building by progressively taking into account new data (coming from text analysis and user interaction), and last but not least they can be easily distributed across a computer network. Dynamo takes a syntactical and terminological analysis of texts as input. It uses several criteria based on statistics computed from the linguistic contexts of terms to create and position the concepts. As output, Dynamo provides to the analyst a hierarchical organization of concepts (the multi-agent system itself) that can be validated, refined of modified, until he/ 978-81--7-5 (RPS) IFAAMAS the semantic network. An ontology can be seen as a stable map constituted of conceptual entities, represented here by agents, linked by labelled relations. Thus, our approach considers an ontology as a type of equilibrium between its concept-agents where their forces are defined by their potential relationships. The ontology modification is a perturbation of the previous equilibrium by the appearance or disappearance of agents or relationships. In this way, a dynamic ontology is a self-organizing process occurring when new texts are included into the corpus, or when the ontologist interacts with it. To support the needed flexibility of such a system we use a selforganizing multi-agent system based on a cooperative approach . We followed the ADELFE method proposed to drive the design of this kind of multi-agent system. It justifies how we designed some of the rules used by our agents in order to maximize the cooperation degree within Dynamo\"s multi-agent system. 2.2 Proposed Architecture In this section, we present our system architecture. It addresses the needs of Knowledge Engineering in the context of dynamic ontology management and maintenance when the ontology is linked to a document collection. The Dynamo system consists of three parts (cf. figure 1): \u2022 a term network, obtained thanks to a term extraction tool used to preprocess the textual corpus, \u2022 a multi-agent system which uses the term network to make a hierarchical clustering in order to obtain a taxonomy of concepts, \u2022 an interface allowing the ontologist to visualize and control the clustering process. ?? Ontologist Interface System Concept Agent Term Term network Terms Extraction Tool Figure 1: System architecture The term extractor we use is Syntex, a software that has efficiently been used for ontology building tasks . We mainly selected it because of its robustness and the great amount of information extracted. In particular, it creates a \"Head-Expansion\" network which has already proven to be interesting for a clustering system . In such a network, each term is linked to its head term1 and i.e. the maximum sub-phrase located as head of the term its expansion term2 , and also to all the terms for which it is a head or an expansion term. For example, \"knowledge engineering from text\" has \"knowledge engineering\" as head term and \"text\" as expansion term. Moreover, \"knowledge engineering\" is composed of \"knowledge\" as head term and \"engineering\" as expansion term. With Dynamo, the term network obtained as the output of the extractor is stored in a database. For each term pair, we assume that it is possible to compute a similarity value in order to make a clustering . Because of the nature of the data, we are only focusing on similarity computation between objects described thanks to binary variables, that means that each item is described by the presence or absence of a characteristic set . In the case of terms we are generally dealing with their usage contexts. With Syntex, those contexts are identified by terms and characterized by some syntactic relations. The Dynamo multi-agent system implements the distributed clustering algorithm described in detail in section 3 and the rules described in section 4. It is designed to be both the system producing the resulting structure and the structure itself. It means that each agent represent a class in the taxonomy. Then, the system output is the organization obtained from the interaction between agents, while taking into account feedback coming from the ontologist when he/she modifies the taxonomy given his needs or expertise. 3. DISTRIBUTED CLUSTERING This section presents the distributed clustering algorithm used in Dynamo. For the sake of understanding, and because of its evaluation in section 3.1, we recall the basic centralized algorithm used for a hierarchical ascending clustering in a non metric space, when a symmetrical similarity measure is available (which is the case of the measures used in our system). Algorithm 1: Centralized hierarchical ascending clustering algorithm Data: List L of items to organize as a hierarchy Result: Root R of the hierarchy while length(L) > 1 do max \u2190 0; A \u2190 nil; B \u2190 nil; for i \u2190 1 to length(L) do I \u2190 L[i]; for j \u2190 i + 1 to length(L) do J \u2190 L[j]; sim \u2190 similarity(I, J); if sim > max then max \u2190 sim; A \u2190 I; B \u2190 J; end end end remove(A, L); remove(B, L); append((A, B), L); end R \u2190 L; In algorithm 1, for each clustering step, the pair of the most similar elements is determined. Those two elements are grouped in a cluster, and the resulting class is appended to the list of remaining elements. This algorithm stops when the list has only one element left. i.e. the maximum sub-phrase located as tail of the term The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) The hierarchy resulting from algorithm 1 is always a binary tree because of the way grouping is done. Moreover grouping the most similar elements is equivalent to moving them away from the least similar ones. Our distributed algorithm is designed relying on those two facts. It is executed concurrently in each of the agents of the system. Note that, in the following of this paper, we used for both algorithms an Anderberg similarity (with \u03b1 = 0.75) and an average link clustering strategy . Those choices have an impact on the resulting tree, but they impact neither the global execution of the algorithm nor its complexity. We now present the distributed algorithm used in our system. It is bootstrapped in the following way: \u2022 a TOP agent having no parent is created, it will be the root of the resulting taxonomy, \u2022 an agent is created for each term to be positioned in the taxonomy, they all have TOP as parent. Once this basic structure is set, the algorithm runs until it reaches equilibrium and then provides the resulting taxonomy. Ak\u22121 Ak AnA2A1 ...... ...... A1 Figure 2: Distributed classification: Step 1 The process first step (figure 2) is triggered when an agent (here Ak) has more than one brother (since we want to obtain a binary tree). Then it sends a message to its parent P indicating its most dissimilar brother (here A1). Then P receives the same kind of message from each of its children. In the following, this kind of message will be called a \"vote\". Ak\u22121 Ak AnA2A1 P\" ...... ...... P\" P\" Figure 3: Distributed clustering: Step 2 Next, when P has got messages from all its children, it starts the second step (figure 3). Thanks to the received messages indicating the preferences of its children, P can determine three sub-groups among its children: \u2022 the child which got the most \"votes\" by its brothers, that is the child being the most dissimilar from the greatest number of its brothers. In case of a draw, one of the winners is chosen randomly (here A1), \u2022 the children that allowed the \"election\" of the first group, that is the agents which chose their brother of the first group as being the most dissimilar one (here Ak to An), \u2022 the remaining children (here A2 to Ak\u22121). Then P creates a new agent P (having P as parent) and asks agents from the second group (here agents Ak to An) to make it their new parent. Ak\u22121 Ak AnA2A1 P\" ...... ...... Figure 4: Distributed clustering: Step 3 Finally, step 3 (figure 4) is trivial. The children rejected by P (here agent A2 to An) take its message into account and choose P as their new parent. The hierarchy just created a new intermediate level. Note that this algorithm generally converges, since the number of brothers of an agent drops. When an agent has only one remaining brother, its activity stops (although it keeps processing messages coming from its children). However in a few cases we can reach a \"circular conflict\" in the voting procedure when for example A votes against B, B against C and C against A. With the current system no decision can be taken. The current procedure should be improved to address this, probably using a ranked voting method. 3.1 Quantitative Evaluation Now, we evaluate the properties of our distributed algorithm. It requires to begin with a quantitative evaluation, based on its complexity, while comparing it with the algorithm 1 from the previous section. Its theoretical complexity is calculated for the worst case, by considering the similarity computation operation as elementary. For the distributed algorithm, the worst case means that for each run, only a two-item group can be created. Under those conditions, for a given dataset of n items, we can determine the amount of similarity computations. For algorithm 1, we note l = length(L), then the most enclosed \"for\" loop is run l \u2212 i times. And its body has the only similarity computation, so its cost is l\u2212i. The second \"for\" loop is ran l times for i ranging from 1 to l. Then its cost is Pl i=1(l \u2212 i) which can be simplified in l\u00d7(l\u22121) . Finally for each run of the \"while\" loop, l is decreased from n to 1 which gives us t1(n) as the amount of similarity computations for algorithm 1: t1(n) = nX l=1 l \u00d7 (l \u2212 1) (1) For the distributed algorithm, at a given step, each one of the l agents evaluates the similarity with its l \u22121 brothers. So each steps has a l \u00d7 (l \u2212 1) cost. Then, groups are created and another vote occurs with l decreased by one (since we assume worst case, only groups of size 2 or l \u22121 are built). Since l is equal to n on first run, we obtain tdist(n) as the amount of similarity computations for the distributed algorithm: tdist(n) = nX l=1 l \u00d7 (l \u2212 1) (2) Both algorithms then have an O(n3 ) complexity. But in the worst case, The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) ementary operations done by the centralized algorithm. This gap comes from the local decision making in each agent. Because of this, the similarity computations are done twice for each agent pair. We could conceive that an agent sends its computation result to its peer. But, 10 20 30 40 50 60 70 80 90 100 Amountofcomparisons Amount of input terms 1. Distributed algorithm (on average, with min and max) 2. Logarithmic polynomial 3. Centralized algorithm Figure 5: Experimental results In a second step, the average complexity of the algorithm has been determined by experiments. The multi-agent system has been executed with randomly generated input data sets ranging from ten to one hundred terms. The given value is the average of comparisons made for one hundred of runs without any user interaction. It results in the plots of figure 5. The algorithm is then more efficient on average than the centralized algorithm, and its average complexity is below the worst case. It can be explained by the low probability that a data set forces the system to create only minimal groups (two items) or maximal (n \u2212 1 elements) for each step of reasoning. Curve number 2 represents the logarithmic polynomial minimizing the error with curve number 1. The highest degree term of this polynomial is in n2 log(n), then our distributed algorithm has a O(n2 log(n)) complexity on average. Finally, let\"s note the reduced variation of the average performances with the maximum and the minimum. In the worst case for 100 terms, the variation is of 1,960.75 for an average of 40,550.10 (around 5%) which shows the good stability of the system. 3.2 Qualitative Evaluation Although the quantitative results are interesting, the real advantage of this approach comes from more qualitative characteristics that we will present in this section. All are advantages obtained thanks to the use of an adaptive multi-agent system. The main advantage to the use of a multi-agent system for a clustering task is to introduce dynamic in such a system. The ontologist can make modifications and the hierarchy adapts depending on the request. It is particularly interesting in a knowledge engineering context. Indeed, the hierarchy created by the system is meant to be modified by the ontologist since it is the result of a statistic computation. During the necessary look at the texts to examine the usage contexts of terms , the ontologist will be able to interpret the real content and to revise the system proposal. It is extremely difficult to realize this with a centralized \"black-box\" approach. In most cases, one has to find which reasoning step generated the error and to manually modify the resulting class. Unfortunately, in this case, all the reasoning steps that occurred after the creation of the modified class are lost and must be recalculated by taking the modification into account. That is why a system like ASIUM tries to soften the problem with a system-user collaboration by showing to the ontologist the created classes after each step of reasoning. But, the ontologist can make a mistake, and become aware of it too late. Figure 6: Concept agent tree after autonomous stabilization of the system In order to illustrate our claims, we present an example thanks to a few screenshots from the working prototype tested on a medical related corpus. By using test data and letting the system work by itself, we obtain the hierarchy from figure 6 after stabilization. It is clear that the concept described by the term \"l\u00e9sion\" (lesion) is misplaced. It happens that the similarity computations place it closer to \"femme\" (woman) and \"chirurgien\" (surgeon) than to \"infection\", \"gastro-ent\u00e9rite\" (gastro-enteritis) and \"h\u00e9patite\" (hepatitis). This wrong position for \"lesion\" is explained by the fact that without ontologist input the reasoning is only done on statistics criteria. Figure 7: Concept agent tree after ontologist modification Then, the ontologist replaces the concept in the right branch, by affecting \"ConceptAgent:8\" as its new parent. The name \"ConceptAgent:X\" is automatically given to a concept agent that is not described by a term. The system reacts by itself and refines the clustering hierarchy to obtain a binary tree by creating \"ConceptAgent:11\". The new stable state if the one of figure 7. This system-user coupling is necessary to build an ontology, but no particular adjustment to the distributed algorithm principle is needed since each agent does an autonomous local processing and communicates with its neighborhood by messages. Moreover, this algorithm can de facto be distributed on a computer network. The communication between agents is then done by sending messages and each one keeps its decision autonomy. Then, a system modification to make it run networked would not require to adjust the algorithm. On the contrary, it would only require to rework the communication layer and the agent creation process since in our current implementation those are not networked. 4. MULTI-CRITERIA HIERARCHY In the previous sections, we assumed that similarity can be computed for any term pair. But, as soon as one uses real data this property is not verified anymore. Some terms do not have any similarity value with any extracted term. Moreover for leaf nodes it is sometimes interesting to use other means to position them in the hierarchy. For this low level structuring, ontologists generally base The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) their choices on simple heuristics. Using this observation, we built a new set of rules, which are not based on similarity to support low level structuring. 4.1 Adding Head Coverage Rules In this case, agents can act with a very local point of view simply by looking at the parent/child relation. Each agent can try to determine if its parent is adequate. It is possible to guess this because each concept agent is described by a set of terms and thanks to the \"Head-Expansion\" term network. In the following TX will be the set of terms describing concept agent X and head(TX ) the set of all the terms that are head of at least one element of TX . Thanks to those two notations we can describe the parent adequacy function a(P, C) between a parent P and a child C: a(P, C) = |TP \u2229 head(TC )| |TP \u222a head(TC )| (3) Then, the best parent for C is the P agent that maximizes a(P, C). An agent unsatisfied by its parent can then try to find a better one by evaluating adequacy with candidates. We designed a complementary algorithm to drive this search: When an agent C is unsatisfied by its parent P, it evaluates a(Bi, C) with all its brothers (noted Bi) the one maximizing a(Bi, C) is then chosen as the new parent. Figure 8: Concept agent tree after autonomous stabilization of the system without head coverage rule We now illustrate this rule behavior with an example. Figure 8 shows the state of the system after stabilization on test data. We can notice that \"h\u00e9patite viral\" (viral hepatitis) is still linked to the taxonomy root. It is caused by the fact that there is no similarity value between the \"viral hepatitis\" term and any of the term of the other concept agents. Figure 9: Concept agent tree after activation of the head coverage rule After activating the head coverage rule and letting the system stabilize again we obtain figure 9. We can see that \"viral hepatitis\" slipped through the branch leading to \"hepatitis\" and chose it as its new parent. It is a sensible default choice since \"viral hepatitis\" is a more specific term than \"hepatitis\". This rule tends to push agents described by a set of term to become leafs of the concept tree. It addresses our concern to improve the low level structuring of our taxonomy. But obviously our agents lack a way to backtrack in case of modifications in the taxonomy which would make them be located in the wrong branch. That is one of the point where our system still has to be improved by adding another set of rules. 4.2 On Using Several Criteria In the previous sections and examples, we only used one algorithm at a time. The distributed clustering algorithm tends to introduce new layers in the taxonomy, while the head coverage algorithm tends to push some of the agents toward the leafs of the taxonomy. It obviously raises the question on how to deal with multiple criteria in our taxonomy building, and how agents determine their priorities at a given time. The solution we chose came from the search for minimizing non cooperation within the system in accordance with the ADELFE method. Each agent computes three non cooperation degrees and chooses its current priority depending on which degree is the highest. For a given agent A having a parent P, a set of brothers Bi and which received a set of messages Mk having the priority pk the three non cooperation degrees are: \u2022 \u03bcH (A) = 1 \u2212 a(P, A), is the \"head coverage\" non cooperation degree, determined by the head coverage of the parent, \u2022 \u03bcB(A) = max(1 \u2212 similarity(A, Bi)), is the \"brotherhood\" non cooperation degree, determined by the worst brother of A regarding similarities, \u2022 \u03bcM (A) = max(pk), is the \"message\" non cooperation degree, determined by the most urgent message received. Then, the non cooperation degree \u03bc(A) of agent A is: \u03bc(A) = max(\u03bcH (A), \u03bcB(A), \u03bcM (A)) (4) Then, we have three cases determining which kind of action A will choose: \u2022 if \u03bc(A) = \u03bcH (A) then A will use the head coverage algorithm we detailed in the previous subsection \u2022 if \u03bc(A) = \u03bcB(A) then A will use the distributed clustering algorithm (see section 3) \u2022 if \u03bc(A) = \u03bcM (A) then A will process Mk immediately in order to help its sender Those three cases summarize the current activities of our agents: they have to find the best parent for them (\u03bc(A) = \u03bcH (A)), improve the structuring through clustering (\u03bc(A) = \u03bcB(A)) and process other agent messages (\u03bc(A) = \u03bcM (A)) in order to help them fulfill their own goals. 4.3 Experimental Complexity Revisited We evaluated the experimental complexity of the whole multiagent system when all the rules are activated. In this case, the metric used is the number of messages exchanged in the system. Once again the system has been executed with input data sets ranging from ten to one hundred terms. The given value is the average of message amount sent in the system as a whole for one hundred runs without user interaction. It results in the plots of figure 10. Curve number 1 represents the average of the value obtained. Curve number 2 represents the average of the value obtained when only the distributed clustering algorithm is activated, not the full rule set. Curve number 3 represents the polynomial minimizing the error with curve number 1. The highest degree term of this polynomial is in n3 , then our multi-agent system has a O(n3 ) The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 10 20 30 40 50 60 70 80 90 100 Amountofmessages Amount of input terms 1. Dynamo, all rules (on average, with min and max) 2. Distributed clustering only (on average) 2. Cubic polynomial Figure 10: Experimental results on average. Moreover, let\"s note the very small variation of the average performances with the maximum and the minimum. In the worst case for 100 terms, the variation is of 126.73 for an average of 20,737.03 (around 0.6%) which proves the excellent stability of the system. Finally the extra head coverage rules are a real improvement on the distributed algorithm alone. They introduce more constraints and stability point is reached with less interactions and decision making by the agents. It means that less messages are exchanged in the system while obtaining a tree of higher quality for the ontologist. 5. DISCUSSION & PERSPECTIVES 5.1 Current Limitation of our Approach The most important limitation of our current algorithm is that the result depends on the order the data gets added. When the system works by itself on a fixed data set given during initialization, the final result is equivalent to what we could obtain with a centralized algorithm. On the contrary, adding a new item after a first stabilization has an impact on the final result. Figure 11: Concept agent tree after autonomous stabilization of the system To illustrate our claims, we present another example of the working system. By using test data and letting the system work by itself, we obtain the hierarchy of figure 11 after stabilization. Figure 12: Concept agent tree after taking in account \"hepatitis\" Then, the ontologist interacts with the system and adds a new concept described by the term \"hepatitis\" and linked to the root. The system reacts and stabilizes, we then obtain figure 12 as a result. \"hepatitis\" is located in the right branch, but we have not obtained the same organization as the figure 6 of the previous example. We need to improve our distributed algorithm to allow a concept to move along a branch. We are currently working on the required rules, but the comparison with centralized algorithm will become very difficult. In particular since they will take into account criteria ignored by the centralized algorithm. 5.2 Pruning for Ontologies Building In section 3, we presented the distributed clustering algorithm used in the Dynamo system. Since this work was first based on this algorithm, it introduced a clear bias toward binary trees as a result. But we have to keep in mind that we are trying to obtain taxonomies which are more refined and concise. Although the head coverage rule is an improvement because it is based on how the ontologists generally work, it only addresses low level structuring but not the intermediate levels of the tree. By looking at figure 7, it is clear that some pruning could be done in the taxonomy. In particular, since \"l\u00e9sion\" moved, \"ConceptAgent:9\" could be removed, it is not needed anymore. Moreover the branch starting with \"ConceptAgent:8\" clearly respects the constraint to make a binary tree, but it would be more useful to the user in a more compact and meaningful form. In this case \"ConceptAgent:10\" and \"ConceptAgent:11\" could probably be merged. Currently, our system has the necessary rules to create intermediate levels in the taxonomy, or to have concepts shifting towards the leaf. As we pointed, it is not enough, so new rules are needed to allow removing nodes from the tree, or move them toward the root. Most of the work needed to develop those rules consists in finding the relevant statistic information that will support the ontologist. 6. CONCLUSION After being presented as a promising solution, ensuring model quality and their terminological richness, ontology building from textual corpus analysis is difficult and costly. It requires analyst supervising and taking in account the ontology aim. Using natural languages processing tools ease the knowledge localization in texts through language uses. That said, those tools produce a huge amount of lexical or grammatical data which is not trivial to examine in order to define conceptual elements. Our contribution lies in this step of the modeling process from texts, before any attempts to normalize or formalize the result. We proposed an approach based on an adaptive multi-agent system to provide the ontologist with a first taxonomic structure of concepts. Our system makes use of a terminological network resulting from an analysis made by Syntex. The current state of our software allows to produce simple structures, to propose them to the ontologist and to make them evolve depending on the modifications he made. Performances of the system are interesting and some aspects are even comparable to their centralized counterpart. Its strengths are mostly qualitative since it allows more subtle user interactions and a progressive adaptation to new linguistic based information. From the point of view of ontology building, this work is a first step showing the relevance of our approach. It must continue, both to ensure a better robustness during classification, and to obtain richer structures semantic wise than simple trees. From this improvements we are mostly focusing on the pruning to obtain better taxonomies. We\"re currently working on the criterion to trigger the complementary actions of the structure changes applied by our clustering algorithm. In other words this algorithm introduces inThe Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) termediate levels, and we need to be able to remove them if necessary, in order to reach a dynamic equilibrium. Also from the multi-agent engineering point of view, their use in a dynamic ontology context has shown its relevance. This dynamic ontologies can be seen as complex problem solving, in such a case self-organization through cooperation has been an efficient solution. And, more generally it\"s likely to be interesting for other design related tasks, even if we\"re focusing only on knowledge engineering in this paper. Of course, our system still requires more evaluation and validation work to accurately determine the advantages and flaws of this approach. We\"re planning to work on such benchmarking in the near future.", "body1": "Nowadays, it is well established that ontologies are needed for semantic web, knowledge management, B2B... For knowledge management, ontologies are used to annotate documents and to enhance the information retrieval. Our research focuses on Dynamo (an acronym of DYNAMic Ontologies), a tool based on an adaptive multi-agent system to construct and maintain an ontology from a domain specific set of texts. Our aim is not to build an exhaustive, general hierarchical ontology but a domain specific one. In the multi-agent field, ontologies generally enable agents to understand each other . This paper first presents, in section 2, the big picture of the Dynamo system. 2.1 Ontology as a Multi-Agent System Dynamo aims at reducing the need for manual actions in processing the text analysis results and at suggesting a concept network kick-off in order to build ontologies more efficiently. As output, Dynamo provides to the analyst a hierarchical organization of concepts (the multi-agent system itself) that can be validated, refined of modified, until he/ 978-81--7-5 (RPS) IFAAMAS the semantic network. An ontology can be seen as a stable map constituted of conceptual entities, represented here by agents, linked by labelled relations. We followed the ADELFE method proposed to drive the design of this kind of multi-agent system. 2.2 Proposed Architecture In this section, we present our system architecture. The Dynamo system consists of three parts (cf. ?? With Dynamo, the term network obtained as the output of the extractor is stored in a database. The Dynamo multi-agent system implements the distributed clustering algorithm described in detail in section 3 and the rules described in section 4. This section presents the distributed clustering algorithm used in Dynamo. Algorithm 1: Centralized hierarchical ascending clustering algorithm Data: List L of items to organize as a hierarchy Result: Root R of the hierarchy while length(L) > 1 do max \u2190 0; A \u2190 nil; B \u2190 nil; for i \u2190 1 to length(L) do I \u2190 L[i]; for j \u2190 i + 1 to length(L) do J \u2190 L[j]; sim \u2190 similarity(I, J); if sim > max then max \u2190 sim; A \u2190 I; B \u2190 J; end end end remove(A, L); remove(B, L); append((A, B), L); end R \u2190 L; In algorithm 1, for each clustering step, the pair of the most similar elements is determined. i.e. We now present the distributed algorithm used in our system. Once this basic structure is set, the algorithm runs until it reaches equilibrium and then provides the resulting taxonomy. Ak\u22121 Ak AnA2A1 ...... ...... A1 Figure 2: Distributed classification: Step 1 The process first step (figure 2) is triggered when an agent (here Ak) has more than one brother (since we want to obtain a binary tree). P\" P\" Figure 3: Distributed clustering: Step 2 Next, when P has got messages from all its children, it starts the second step (figure 3). Ak\u22121 Ak AnA2A1 P\" ...... ...... Figure 4: Distributed clustering: Step 3 Finally, step 3 (figure 4) is trivial. Note that this algorithm generally converges, since the number of brothers of an agent drops. Its theoretical complexity is calculated for the worst case, by considering the similarity computation operation as elementary. For algorithm 1, we note l = length(L), then the most enclosed \"for\" loop is run l \u2212 10 20 30 40 50 60 70 80 90 100 Amountofcomparisons Amount of input terms 1. It results in the plots of figure 5. The main advantage to the use of a multi-agent system for a clustering task is to introduce dynamic in such a system. Figure 6: Concept agent tree after autonomous stabilization of the system In order to illustrate our claims, we present an example thanks to a few screenshots from the working prototype tested on a medical related corpus. Figure 7: Concept agent tree after ontologist modification Then, the ontologist replaces the concept in the right branch, by affecting \"ConceptAgent:8\" as its new parent. Moreover, this algorithm can de facto be distributed on a computer network. In the previous sections, we assumed that similarity can be computed for any term pair. 4.1 Adding Head Coverage Rules In this case, agents can act with a very local point of view simply by looking at the parent/child relation. An agent unsatisfied by its parent can then try to find a better one by evaluating adequacy with candidates. Figure 8: Concept agent tree after autonomous stabilization of the system without head coverage rule We now illustrate this rule behavior with an example. Figure 9: Concept agent tree after activation of the head coverage rule After activating the head coverage rule and letting the system stabilize again we obtain figure 9. This rule tends to push agents described by a set of term to become leafs of the concept tree. 4.2 On Using Several Criteria In the previous sections and examples, we only used one algorithm at a time. The solution we chose came from the search for minimizing non cooperation within the system in accordance with the ADELFE method. 4.3 Experimental Complexity Revisited We evaluated the experimental complexity of the whole multiagent system when all the rules are activated. Curve number 2 represents the average of the value obtained when only the distributed clustering algorithm is activated, not the full rule set. Finally the extra head coverage rules are a real improvement on the distributed algorithm alone. 5.1 Current Limitation of our Approach The most important limitation of our current algorithm is that the result depends on the order the data gets added. Figure 12: Concept agent tree after taking in account \"hepatitis\" Then, the ontologist interacts with the system and adds a new concept described by the term \"hepatitis\" and linked to the root. The system reacts and stabilizes, we then obtain figure 12 as a result. But we have to keep in mind that we are trying to obtain taxonomies which are more refined and concise. By looking at figure 7, it is clear that some pruning could be done in the taxonomy. Moreover the branch starting with \"ConceptAgent:8\" clearly respects the constraint to make a binary tree, but it would be more useful to the user in a more compact and meaningful form. Currently, our system has the necessary rules to create intermediate levels in the taxonomy, or to have concepts shifting towards the leaf. Most of the work needed to develop those rules consists in finding the relevant statistic information that will support the ontologist.", "body2": "It would mean creating dynamic ontologies and it justifies the emergence of ontology learning techniques . Our research focuses on Dynamo (an acronym of DYNAMic Ontologies), a tool based on an adaptive multi-agent system to construct and maintain an ontology from a domain specific set of texts. An ontologist is a kind of cognitive engineer, or analyst, who is using information from texts and expert interviews to design ontologies. In our work, we propose an interaction between the ontologist and the system, our external resource lies both in the texts and the ontologist. And finally, in section 5, we discuss the limitations of our approach and explain how it will be addressed in further work. It uses several criteria based on statistics computed from the linguistic contexts of terms to create and position the concepts. As output, Dynamo provides to the analyst a hierarchical organization of concepts (the multi-agent system itself) that can be validated, refined of modified, until he/ 978-81--7-5 (RPS) IFAAMAS the semantic network. To support the needed flexibility of such a system we use a selforganizing multi-agent system based on a cooperative approach . It justifies how we designed some of the rules used by our agents in order to maximize the cooperation degree within Dynamo\"s multi-agent system. It addresses the needs of Knowledge Engineering in the context of dynamic ontology management and maintenance when the ontology is linked to a document collection. figure 1): \u2022 a term network, obtained thanks to a term extraction tool used to preprocess the textual corpus, \u2022 a multi-agent system which uses the term network to make a hierarchical clustering in order to obtain a taxonomy of concepts, \u2022 an interface allowing the ontologist to visualize and control the clustering process. Moreover, \"knowledge engineering\" is composed of \"knowledge\" as head term and \"engineering\" as expansion term. With Syntex, those contexts are identified by terms and characterized by some syntactic relations. Then, the system output is the organization obtained from the interaction between agents, while taking into account feedback coming from the ontologist when he/she modifies the taxonomy given his needs or expertise. For the sake of understanding, and because of its evaluation in section 3.1, we recall the basic centralized algorithm used for a hierarchical ascending clustering in a non metric space, when a symmetrical similarity measure is available (which is the case of the measures used in our system). This algorithm stops when the list has only one element left. Those choices have an impact on the resulting tree, but they impact neither the global execution of the algorithm nor its complexity. It is bootstrapped in the following way: \u2022 a TOP agent having no parent is created, it will be the root of the resulting taxonomy, \u2022 an agent is created for each term to be positioned in the taxonomy, they all have TOP as parent. Once this basic structure is set, the algorithm runs until it reaches equilibrium and then provides the resulting taxonomy. Ak\u22121 Ak AnA2A1 ...... ...... Ak\u22121 Ak AnA2A1 P\" ...... ...... Then P creates a new agent P (having P as parent) and asks agents from the second group (here agents Ak to An) to make it their new parent. Ak\u22121 Ak AnA2A1 P\" ...... ...... The hierarchy just created a new intermediate level. It requires to begin with a quantitative evaluation, based on its complexity, while comparing it with the algorithm 1 from the previous section. Under those conditions, for a given dataset of n items, we can determine the amount of similarity computations. But, it would simply move the problem by generating more communication in the system. The given value is the average of comparisons made for one hundred of runs without any user interaction. All are advantages obtained thanks to the use of an adaptive multi-agent system. But, the ontologist can make a mistake, and become aware of it too late. This wrong position for \"lesion\" is explained by the fact that without ontologist input the reasoning is only done on statistics criteria. This system-user coupling is necessary to build an ontology, but no particular adjustment to the distributed algorithm principle is needed since each agent does an autonomous local processing and communicates with its neighborhood by messages. On the contrary, it would only require to rework the communication layer and the agent creation process since in our current implementation those are not networked. Using this observation, we built a new set of rules, which are not based on similarity to support low level structuring. Thanks to those two notations we can describe the parent adequacy function a(P, C) between a parent P and a child C: a(P, C) = |TP \u2229 head(TC )| |TP \u222a head(TC )| (3) Then, the best parent for C is the P agent that maximizes a(P, C). We designed a complementary algorithm to drive this search: When an agent C is unsatisfied by its parent P, it evaluates a(Bi, C) with all its brothers (noted Bi) the one maximizing a(Bi, C) is then chosen as the new parent. It is caused by the fact that there is no similarity value between the \"viral hepatitis\" term and any of the term of the other concept agents. It is a sensible default choice since \"viral hepatitis\" is a more specific term than \"hepatitis\". That is one of the point where our system still has to be improved by adding another set of rules. It obviously raises the question on how to deal with multiple criteria in our taxonomy building, and how agents determine their priorities at a given time. Then, the non cooperation degree \u03bc(A) of agent A is: \u03bc(A) = max(\u03bcH (A), \u03bcB(A), \u03bcM (A)) (4) Then, we have three cases determining which kind of action A will choose: \u2022 if \u03bc(A) = \u03bcH (A) then A will use the head coverage algorithm we detailed in the previous subsection \u2022 if \u03bc(A) = \u03bcB(A) then A will use the distributed clustering algorithm (see section 3) \u2022 if \u03bc(A) = \u03bcM (A) then A will process Mk immediately in order to help its sender Those three cases summarize the current activities of our agents: they have to find the best parent for them (\u03bc(A) = \u03bcH (A)), improve the structuring through clustering (\u03bc(A) = \u03bcB(A)) and process other agent messages (\u03bc(A) = \u03bcM (A)) in order to help them fulfill their own goals. Curve number 1 represents the average of the value obtained. In the worst case for 100 terms, the variation is of 126.73 for an average of 20,737.03 (around 0.6%) which proves the excellent stability of the system. It means that less messages are exchanged in the system while obtaining a tree of higher quality for the ontologist. By using test data and letting the system work by itself, we obtain the hierarchy of figure 11 after stabilization. Figure 12: Concept agent tree after taking in account \"hepatitis\" Then, the ontologist interacts with the system and adds a new concept described by the term \"hepatitis\" and linked to the root. Since this work was first based on this algorithm, it introduced a clear bias toward binary trees as a result. Although the head coverage rule is an improvement because it is based on how the ontologists generally work, it only addresses low level structuring but not the intermediate levels of the tree. In particular, since \"l\u00e9sion\" moved, \"ConceptAgent:9\" could be removed, it is not needed anymore. In this case \"ConceptAgent:10\" and \"ConceptAgent:11\" could probably be merged. As we pointed, it is not enough, so new rules are needed to allow removing nodes from the tree, or move them toward the root. Most of the work needed to develop those rules consists in finding the relevant statistic information that will support the ontologist.", "introduction": "Nowadays, it is well established that ontologies are needed for semantic web, knowledge management, B2B... For knowledge management, ontologies are used to annotate documents and to enhance the information retrieval. But building an ontology manually is a slow, tedious, costly, complex and time consuming process. Currently, a real challenge lies in building them automatically or semi-automatically and keeping them up to date. It would mean creating dynamic ontologies and it justifies the emergence of ontology learning techniques . Our research focuses on Dynamo (an acronym of DYNAMic Ontologies), a tool based on an adaptive multi-agent system to construct and maintain an ontology from a domain specific set of texts. Our aim is not to build an exhaustive, general hierarchical ontology but a domain specific one. We propose a semi-automated tool since an external resource is required: the \"ontologist\". An ontologist is a kind of cognitive engineer, or analyst, who is using information from texts and expert interviews to design ontologies. In the multi-agent field, ontologies generally enable agents to understand each other . They\"re sometimes used to ease the ontology building process, in particular for collaborative contexts , but they rarely represent the ontology itself . Most works interested in the construction of ontologies propose the refinement of ontologies. This process consists in using an existing ontology and building a new one from it. This approach is different from our approach because Dynamo starts from scratch. Researchers, working on the construction of ontologies from texts, claim that the work to be automated requires external resources such as a dictionary , or web access . In our work, we propose an interaction between the ontologist and the system, our external resource lies both in the texts and the ontologist. This paper first presents, in section 2, the big picture of the Dynamo system. In particular the motives that led to its creation and its general architecture. Then, in section 3 we discuss the distributed clustering algorithm used in Dynamo and compare it to a more classic centralized approach. Section 4 is dedicated to some enhancement of the agents behavior that got designed by taking into account criteria ignored by clustering. And finally, in section 5, we discuss the limitations of our approach and explain how it will be addressed in further work.", "conclusion": "After being presented as a promising solution, ensuring model quality and their terminological richness, ontology building from textual corpus analysis is difficult and costly.. It requires analyst supervising and taking in account the ontology aim.. Using natural languages processing tools ease the knowledge localization in texts through language uses.. That said, those tools produce a huge amount of lexical or grammatical data which is not trivial to examine in order to define conceptual elements.. Our contribution lies in this step of the modeling process from texts, before any attempts to normalize or formalize the result.. We proposed an approach based on an adaptive multi-agent system to provide the ontologist with a first taxonomic structure of concepts.. Our system makes use of a terminological network resulting from an analysis made by Syntex.. The current state of our software allows to produce simple structures, to propose them to the ontologist and to make them evolve depending on the modifications he made.. Performances of the system are interesting and some aspects are even comparable to their centralized counterpart.. Its strengths are mostly qualitative since it allows more subtle user interactions and a progressive adaptation to new linguistic based information.. From the point of view of ontology building, this work is a first step showing the relevance of our approach.. It must continue, both to ensure a better robustness during classification, and to obtain richer structures semantic wise than simple trees.. From this improvements we are mostly focusing on the pruning to obtain better taxonomies.. We\"re currently working on the criterion to trigger the complementary actions of the structure changes applied by our clustering algorithm.. In other words this algorithm introduces inThe Sixth Intl.. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) termediate levels, and we need to be able to remove them if necessary, in order to reach a dynamic equilibrium.. Also from the multi-agent engineering point of view, their use in a dynamic ontology context has shown its relevance.. This dynamic ontologies can be seen as complex problem solving, in such a case self-organization through cooperation has been an efficient solution.. And, more generally it\"s likely to be interesting for other design related tasks, even if we\"re focusing only on knowledge engineering in this paper.. Of course, our system still requires more evaluation and validation work to accurately determine the advantages and flaws of this approach.. We\"re planning to work on such benchmarking in the near future."}
{"id": "J-50", "keywords": ["commun complex", "vote"], "title": "Communication Complexity of Common Voting Rules", "abstract": "We determine the communication complexity of the common voting rules. The rules (sorted by their communication complexity from low to high) are plurality, plurality with runoff, single transferable vote (STV), Condorcet, approval, Bucklin, cup, maximin, Borda, Copeland, and ranked pairs. For each rule, we first give a deterministic communication protocol and an upper bound on the number of bits communicated in it; then, we give a lower bound on (even the nondeterministic) communication requirements of the voting rule. The bounds match for all voting rules except STV and maximin.", "references": ["Ascending auctions with package bidding", "Single transferable vote resists strategic voting", "The computational difficulty of manipulating an election", "Preference elicitation and query learning", "Preference elicitation in combinatorial auctions: Extended abstract", "How many candidates are needed to make elections hard to manipulate?", "Complexity of manipulating elections with few candidates", "Vote elicitation: Complexity and strategy-proofness", "On ascending auctions for heterogeneous objects", "Manipulation of voting schemes", "Effectiveness of query types and policies for preference elicitation in combinatorial auctions", "Communication Complexity", "Applying learning algorithms to preference elicitation", "The communication requirements of efficient allocations and supporting prices", "iBundle: An efficient ascending price bundle auction", "An implementation of the contract net protocol based on marginal cost calculations", "Preference elicitation in combinatorial auctions", "Towards a characterization of polynomial preference elicitation with value queries in combinatorial auctions", "Strategy-proofness and Arrow's conditions: existence and correspondence theorems for voting procedures and social welfare functions", "The communication requirements of social choice rules and supporting budget sets", "AkBA: A progressive, anonymous-price combinatorial auction", "Some complexity questions related to distributed computing", "On polynomial-time preference elicitation with value queries"], "full_text": "1. INTRODUCTION One key factor in the practicality of any preference aggregation rule is its communication burden. To successfully aggregate the agents\" preferences, it is usually not necessary for all the agents to report all of their preference information. Clever protocols that elicit the agents\" preferences partially and sequentially have the potential to dramatically reduce the required communication. This has at least the following advantages: \u2022 It can make preference aggregation feasible in settings where the total amount of preference information is too large to communicate. \u2022 Even when communicating all the preference information is feasible, reducing the communication requirements lessens the burden placed on the agents. This is especially true when the agents, rather than knowing all their preferences in advance, need to invest effort (such as computation or information gathering) to determine their preferences . \u2022 It preserves (some of) the agents\" privacy. Most of the work on reducing the communication burden in preference aggregation has focused on resource allocation settings such as combinatorial auctions, in which an auctioneer auctions off a number of (possibly distinct) items in a single event. Because in a combinatorial auction, bidders can have separate valuations for each of an exponential number of possible bundles of items, this is a setting in which reducing the communication burden is especially crucial. This can be accomplished by supplementing the auctioneer with an elicitor that incrementally elicits parts of the bidders\" preferences on an as-needed basis, based on what the bidders have revealed about their preferences so far, as suggested by Conen and Sandholm . For example, the elicitor can ask for a bidder\"s value for a specific bundle (value queries), which of two bundles the bidder prefers (order queries), which bundle he ranks kth or what the rank of a given bundle is (rank queries), which bundle he would purchase given a particular vector of prices (demand queries), etc.-until (at least) the final allocation can be determined. Experimentally, this yields drastic savings in preference revelation . Furthermore, if the agents\" valuation functions are drawn from certain natural subclasses, the elicitation problem can be solved using only polynomially many queries even in the worst case . For a review of preference elicitation in combinatorial auctions, see . Ascending combinatorial auctions are a well-known special form of preference elicitation, where the elicitor asks demand queries with increasing prices . Finally, resource 78 allocation problems have also been studied from a communication complexity viewpoint, thereby deriving lower bounds on the required communication. For example, Nisan and Segal show that exponential communication is required even to obtain a surplus greater than that obtained by auctioning off all objects as a single bundle . Segal also studies social choice rules in general, and shows that for a large class of social choice rules, supporting budget sets must be revealed such that if every agent prefers the same outcome in her budget set, this proves the optimality of that outcome. Segal then uses this characterization to prove bounds on the communication required in resource allocation as well as matching settings . In this paper, we will focus on the communication requirements of a generally applicable subclass of social choice rules, commonly known as voting rules. In a voting setting, there is a set of candidate outcomes over which the voters express their preferences by submitting a vote (typically, a ranking of the candidates), and the winner (that is, the chosen outcome) is determined based on these votes. The communication required by voting rules can be large either because the number of voters is large (such as, for example, in national elections), or because the number of candidates is large (for example, the agents can vote over allocations of a number of resources), or both. Prior work has studied elicitation in voting, studying how computationally hard it is to decide whether a winner can be determined with the information elicited so far, as well as how hard it is to find the optimal sequence of queries given perfect suspicions about the voters\" preferences. In addition, that paper discusses strategic (game-theoretic) issues introduced by elicitation. In contrast, in this paper, we are concerned with the worst-case number of bits that must be communicated to execute a given voting rule, when nothing is known in advance about the voters\" preferences. We determine the communication complexity of the common voting rules. For each rule, we first give an upper bound on the (deterministic) communication complexity by providing a communication protocol for it and analyzing how many bits need to be transmitted in this protocol. (Segal\"s results do not apply to most voting rules because most voting rules are not intersectionmonotonic (or even monotonic).1 ) For many of the voting rules under study, it turns out that one cannot do better than simply letting each voter immediately communicate all her (potentially relevant) information. However, for some rules (such as plurality with runoff, STV and cup) there is a straightforward multistage communication protocol that, with some analysis, can be shown to significantly outperform the immediate communication of all (potentially relevant) information. Finally, for some rules (such as the Condorcet and Bucklin rules), we need to introduce a more complex communication protocol to achieve the best possible upper For two of the rules that we study that are intersectionmonotonic, namely the approval and Condorcet rules, Segal\"s results can in fact be used to give alternative proofs of our lower bounds. We only give direct proofs for these rules here because 1) these direct proofs are among the easier ones in this paper, 2) the alternative proofs are nontrivial even given Segal\"s results, and 3) a space constraint applies. However, we hope to also include the alternative proofs in a later version. bound. After obtaining the upper bounds, we show that they are tight by giving matching lower bounds on (even the nondeterministic) communication complexity of each voting rule. There are two exceptions: STV, for which our upper and lower bounds are apart by a factor log m; and maximin, for which our best deterministic upper bound is also a factor log m above the (nondeterministic) lower bound, although we give a nondeterministic upper bound that matches the lower bound. 2. REVIEW OF VOTING RULES In this section, we review the common voting rules that we study in this paper. A voting rule2 is a function mapping a vector of the n voters\" votes (i.e. preferences over candidates) to one of the m candidates (the winner) in the candidate set C. In some cases (such as the Condorcet rule), the rule may also declare that no winner exists. We do not concern ourselves with what happens in case of a tie between candidates (our lower bounds hold regardless of how ties are broken, and the communication protocols used for our upper bounds do not attempt to break the ties). All of the rules that we study are rank-based rules, which means that a vote is defined as an ordering of the candidates (with the exception of the plurality rule, for which a vote is a single candidate, and the approval rule, for which a vote is a subset of the candidates). We will consider the following voting rules. (For rules that define a score, the candidate with the highest score wins.) \u2022 scoring rules. Let \u03b1 = \u03b11, . . . , \u03b1m be a vector of integers such that \u03b11 \u2265 \u03b12 . . . \u2265 \u03b1m. For each voter, a candidate receives \u03b11 points if it is ranked first by the voter, \u03b12 if it is ranked second etc. The score s\u03b1 of a candidate is the total number of points the candidate receives. The Borda rule is the scoring rule with \u03b1 = m\u22121, m\u22122, . . . , 0 . The plurality rule is the scoring rule with \u03b1 = 1, 0, . . . , 0 . \u2022 single transferable vote (STV). The rule proceeds through a series of m \u2212 1 rounds. In each round, the candidate with the lowest plurality score (that is, the least number of voters ranking it first among the remaining candidates) is eliminated (and each of the votes for that candidate transfer to the next remaining candidate in the order given in that vote). The winner is the last remaining candidate. \u2022 plurality with run-off. In this rule, a first round eliminates all candidates except the two with the highest plurality scores. Votes are transferred to these as in the STV rule, and a second round determines the winner from these two. \u2022 approval. Each voter labels each candidate as either approved or disapproved. The candidate approved by the greatest number of voters wins. \u2022 Condorcet. For any two candidates i and j, let N(i, j) be the number of voters who prefer i to j. If there is a candidate i that is preferred to any other candidate by a majority of the voters (that is, N(i, j) > N(j, i) for all j = i-that is, i wins every pairwise election), then candidate i wins. The term voting protocol is often used to describe the same concept, but we seek to draw a sharp distinction between the rule mapping preferences to outcomes, and the communication/elicitation protocol used to implement this rule. 79 \u2022 maximin (aka. Simpson). The maximin score of i is s(i) = minj=i N(i, j)-that is, i\"s worst performance in a pairwise election. The candidate with the highest maximin score wins. \u2022 Copeland. For any two distinct candidates i and j, let C(i, j) = 1 if N(i, j) > N(j, i), C(i, j) = 1/2 if N(i, j) = N(j, i) and C(i, j) = 0 if N(i, j) < N(j, i). The Copeland score of candidate i is s(i) = j=i C(i, j). \u2022 cup (sequential binary comparisons). The cup rule is defined by a balanced3 binary tree T with one leaf per candidate, and an assignment of candidates to leaves (each leaf gets one candidate). Each non-leaf node is assigned the winner of the pairwise election of the node\"s children; the candidate assigned to the root wins. \u2022 Bucklin. For any candidate i and integer l, let B(i, l) be the number of voters that rank candidate i among the top l candidates. The winner is arg mini(min{l : B(i, l) > n/2}). That is, if we say that a voter approves her top l candidates, then we repeatedly increase l by 1 until some candidate is approved by more than half the voters, and this candidate is the winner. \u2022 ranked pairs. This rule determines an order on all the candidates, and the winner is the candidate at the top of this order. Sort all ordered pairs of candidates (i, j) by N(i, j), the number of voters who prefer i to j. Starting with the pair (i, j) with the highest N(i, j), we lock in the result of their pairwise election (i j). Then, we move to the next pair, and we lock the result of their pairwise election. We continue to lock every pairwise result that does not contradict the ordering established so far. We emphasize that these definitions of voting rules do not concern themselves with how the votes are elicited from the voters; all the voting rules, including those that are suggestively defined in terms of rounds, are in actuality just functions mapping the vector of all the voters\" votes to a winner. Nevertheless, there are always many different ways of eliciting the votes (or the relevant parts thereof) from the voters. For example, in the plurality with runoff rule, one way of eliciting the votes is to ask every voter to declare her entire ordering of the candidates up front. Alternatively, we can first ask every voter to declare only her most preferred candidate; then, we will know the two candidates in the runoff, and we can ask every voter which of these two candidates she prefers. Thus, we distinguish between the voting rule (the mapping from vectors of votes to outcomes) and the communication protocol (which determines how the relevant parts of the votes are actually elicited from the voters). The goal of this paper is to give efficient communication protocols for the voting rules just defined, and to prove that there do not exist any more efficient communication protocols. It is interesting to note that the choice of the communication protocol may affect the strategic behavior of the voters. Multistage communication protocols may reveal to the voters some information about how the other voters are voting (for example, in the two-stage communication protocol just given for plurality with runoff, in the second stage voters Balanced means that the difference in depth between two leaves can be at most one. will know which two candidates have the highest plurality scores). In general, when the voters receive such information, it may give them incentives to vote differently than they would have in a single-stage communication protocol in which all voters declare their entire votes simultaneously. Of course, even the single-stage communication protocol is not strategy-proof4 for any reasonable voting rule, by the Gibbard-Satterthwaite theorem . However, this does not mean that we should not be concerned about adding even more opportunities for strategic voting. In fact, many of the communication protocols introduced in this paper do introduce additional opportunities for strategic voting, but we do not have the space to discuss this here. (In prior work , we do give an example where an elicitation protocol for the approval voting rule introduces strategic voting, and give principles for designing elicitation protocols that do not introduce strategic problems.) Now that we have reviewed voting rules, we move on to a brief review of communication complexity theory. 3. REVIEW OF SOME COMMUNICATION COMPLEXITY THEORY In this section, we review the basic model of a communication problem and the lower-bounding technique of constructing a fooling set. (The basic model of a communication problem is due to Yao ; for an overview see Kushilevitz and Nisan .) Each player 1 \u2264 i \u2264 n knows (only) input xi. Together, they seek to compute f(x1, x2, . . . , xn). In a deterministic protocol for computing f, in each stage, one of the players announces (to all other players) a bit of information based on her own input and the bits announced so far. Eventually, the communication terminates and all players know f(x1, x2, . . . , xn). The goal is to minimize the worst-case (over all input vectors) number of bits sent. The deterministic communication complexity of a problem is the worstcase number of bits sent in the best (correct) deterministic protocol for it. In a nondeterministic protocol, the next bit to be sent can be chosen nondeterministically. For the purposes of this paper, we will consider a nondeterministic protocol correct if for every input vector, there is some sequence of nondeterministic choices the players can make so that the players know the value of f when the protocol terminates. The nondeterministic communication complexity of a problem is the worst-case number of bits sent in the best (correct) nondeterministic protocol for it. We are now ready to give the definition of a fooling set. Definition 1. A fooling set is a set of input vectors {(x1 1, x1 2, . . . , x1 n), (x2 1, x2 2, . . . , x2 n), . . . , (xk 1 , xk 2 , . . . , xk n) such that for any i, f(xi 1, xi 2, . . . , xi n) = f0 for some constant f0, but for any i = j, there exists some vector (r1, r2, . . . , rn) \u2208 {i, j}n such that f(xr1 1 , xr2 2 , . . . , xrn n ) = f0. (That is, we can mix the inputs from the two input vectors to obtain a vector with a different function value.) It is known that if a fooling set of size k exists, then log k is a lower bound on the communication complexity (even the nondeterministic communication complexity) . A strategy-proof protocol is one in which it is in the players\" best interest to report their preferences truthfully. 80 For the purposes of this paper, f is the voting rule that maps the votes to the winning candidate, and xi is voter i\"s vote (the information that the voting rule would require from the voter if there were no possibility of multistage communication-i.e. the most preferred candidate (plurality), the approved candidates (approval), or the ranking of all the candidates (all other protocols)). However, when we derive our lower bounds, f will only signify whether a distinguished candidate a wins. (That is, f is 1 if a wins, and 0 otherwise.) This will strengthen our lower bound results (because it implies that even finding out whether one given candidate wins is hard).5 Thus, a fooling set in our context is a set of vectors of votes so that a wins (does not win) with each of them; but for any two different vote vectors in the set, there is a way of taking some voters\" votes from the first vector and the others\" votes from the second vector, so that a does not win (wins). To simplify the proofs of our lower bounds, we make assumptions such as the number of voters n is odd in many of these proofs. Therefore, technically, we do not prove the lower bound for (number of candidates, number of voters) pairs (m, n) that do not satisfy these assumptions (for example, if we make the above assumption, then we technically do not prove the lower bound for any pair (m, n) in which n is even). Nevertheless, we always prove the lower bound for a representative set of (m, n) pairs. For example, for every one of our lower bounds it is the case that for infinitely many values of m, there are infinitely many values of n such that the lower bound is proved for the pair (m, n). 4. RESULTS We are now ready to present our results. For each voting rule, we first give a deterministic communication protocol for determining the winner to establish an upper bound. Then, we give a lower bound on the nondeterministic communication complexity (even on the complexity of deciding whether a given candidate wins, which is an easier question). The lower bounds match the upper bounds in all but two cases: the STV rule (upper bound O(n(log m)2 ); lower bound \u2126(n log m)) and the maximin rule (upper bound O(nm log m), although we do give a nondeterministic protocol that is O(nm); lower bound \u2126(nm)). When we discuss a voting rule in which the voters rank the candidates, we will represent a ranking in which candidate c1 is ranked first, c2 is ranked second, etc. as c1 c2 . . . cm. One possible concern is that in the case where ties are possible, it may require much communication to verify whether a specific candidate a is among the winners, but little communication to produce one of the winners. However, all the fooling sets we use in the proofs have the property that if a wins, then a is the unique winner. Therefore, in these fooling sets, if one knows any one of the winners, then one knows whether a is a winner. Thus, computing one of the winners requires at least as much communication as verifying whether a is among the winners. In general, when a communication problem allows multiple correct answers for a given vector of inputs, this is known as computing a relation rather than a function . However, as per the above, we can restrict our attention to a subset of the domain where the voting rule truly is a (single-valued) function, and hence lower bounding techniques for functions rather than relations will suffice. Sometimes for the purposes of a proof the internal ranking of a subset of the candidates does not matter, and in this case we will not specify it. For example, if S = {c2, c3}, then c1 S c4 indicates that either the ranking c1 c2 c3 c4 or the ranking c1 c3 c2 c4 can be used for the proof. We first give a universal upper bound. Theorem 1. The deterministic communication complexity of any rank-based voting rule is O(nm log m). Proof. This bound is achieved by simply having everyone communicate their entire ordering of the candidates (indicating the rank of an individual candidate requires only O(log m) bits, so each of the n voters can simply indicate the rank of each of the m candidates). The next lemma will be useful in a few of our proofs. Lemma 1. If m divides n, then log(n!)\u2212m log((n/m)!) \u2265 n(log m \u2212 1)/2. Proof. If n/m = 1 (that is, n = m), then this expression simplifies to log(n!). We have log(n!) = i=1 log i \u2265 x=1 log(i)dx, which, using integration by parts, is equal to n log n \u2212 (n \u2212 1) > n(log n \u2212 1) = n(log m \u2212 1) > n(log m \u2212 1)/2. So, we can assume that n/m \u2265 2. We observe that log(n!) = i=1 log i = n/m\u22121 i=0 j=1 log(im+j) \u2265 n/m\u22121 i=1 j=1 log(im) = m n/m\u22121 i=1 log(im), and that m log((n/m)!) = m n/m i=1 log(i). Therefore, log(n!) \u2212 m log((n/m)!) \u2265 m n/m\u22121 i=1 log(im) \u2212 n/m i=1 log(i) = m(( n/m\u22121 i=1 log(im/i))\u2212log(n/m)) = m((n/m\u2212 1) log m\u2212log n+log m) = n log m\u2212m log n. Now, using the fact that n/m \u2265 2, we have m log n = n(m/n) log m(n/m) = n(m/n)(log m + log(n/m)) \u2264 n(1/2)(log m + log 2). Thus, log(n!) \u2212 m log((n/m)!) \u2265 n log m \u2212 m log n \u2265 n log m \u2212 n(1/2)(log m + log 2) = n(log m \u2212 1)/2. Theorem 2. The deterministic communication complexity of the plurality rule is O(n log m). Proof. Indicating one of the candidates requires only O(log m) bits, so each voter can simply indicate her most preferred candidate. Theorem 3. The nondeterministic communication complexity of the plurality rule is \u2126(n log m) (even to decide whether a given candidate a wins). Proof. We will exhibit a fooling set of size n ! (( n )!)m where n = (n\u22121)/2. Taking the logarithm of this gives log(n !)\u2212 m log((n /m)!), so the result follows from Lemma 1. The fooling set will consist of all vectors of votes satisfying the following constraints: \u2022 For any 1 \u2264 i \u2264 n , voters 2i\u22121 and 2i vote the same. 81 \u2022 Every candidate receives equally many votes from the first 2n = n \u2212 1 voters. \u2022 The last voter (voter n) votes for a. Candidate a wins with each one of these vote vectors because of the extra vote for a from the last voter. Given that m divides n , let us see how many vote vectors there are in the fooling set. We need to distribute n voter pairs evenly over m candidates, for a total of n /m voter pairs per candidate; and there are precisely n ! (( n )!)m ways of doing this.6 All that remains to show is that for any two distinct vectors of votes in the fooling set, we can let each of the voters vote according to one of these two vectors in such a way that a loses. Let i be a number such that the two vote vectors disagree on the candidate for which voters 2i \u2212 1 and 2i vote. Without loss of generality, suppose that in the first vote vector, these voters do not vote for a (but for some other candidate, b, instead). Now, construct a new vote vector by taking votes 2i \u2212 1 and 2i from the first vote vector, and the remaining votes from the second vote vector. Then, b receives 2n /m + 2 votes in this newly constructed vote vector, whereas a receives at most 2n /m+1 votes. So, a is not the winner in the newly constructed vote vector, and hence we have a correct fooling set. Theorem 4. The deterministic communication complexity of the plurality with runoff rule is O(n log m). Proof. First, let every voter indicate her most preferred candidate using log m bits. After this, the two candidates in the runoff are known, and each voter can indicate which one she prefers using a single additional bit. Theorem 5. The nondeterministic communication complexity of the plurality with runoff rule is \u2126(n log m) (even to decide whether a given candidate a wins). Proof. We will exhibit a fooling set of size n ! (( n )!)m where m = m/2 and n = (n \u2212 2)/4. Taking the logarithm of this gives log(n !) \u2212 m log((n /m )!), so the result follows from Lemma 1. Divide the candidates into m pairs: (c1, d1), (c2, d2), . . . , (cm , dm ) where c1 = a and d1 = b. The fooling set will consist of all vectors of votes satisfying the following constraints: \u2022 For any 1 \u2264 i \u2264 n , voters 4i \u2212 3 and 4i \u2212 2 rank the candidates ck(i) a C \u2212 {a, ck(i)}, for some candidate ck(i). (If ck(i) = a then the vote is simply a C \u2212 {a}.) \u2022 For any 1 \u2264 i \u2264 n , voters 4i \u2212 1 and 4i rank the candidates dk(i) a C \u2212 {a, dk(i)} (that is, their most preferred candidate is the candidate that is paired with the candidate that the previous two voters vote for). An intuitive proof of this is the following. We can count the number of permutations of n elements as follows. First, divide the elements into m buckets of size n /m, so that if x is placed in a lower-indexed bucket than y, then x will be indexed lower in the eventual permutation. Then, decide on the permutation within each bucket (for which there are (n /m)! choices per bucket). It follows that n ! equals the number of ways to divide n elements into m buckets of size n /m, times ((n /m)!)m \u2022 Every candidate is ranked at the top of equally many of the first 4n = n \u2212 2 votes. \u2022 Voter 4n +1 = n\u22121 ranks the candidates a C\u2212{a}. \u2022 Voter 4n + 2 = n ranks the candidates b C \u2212 {b}. Candidate a wins with each one of these vote vectors: because of the last two votes, candidates a and b are one vote ahead of all the other candidates and continue to the runoff, and at this point all the votes that had another candidate ranked at the top transfer to a, so that a wins the runoff. Given that m divides n , let us see how many vote vectors there are in the fooling set. We need to distribute n groups of four voters evenly over the m pairs of candidates, and (as in the proof of Theorem 3) there are n ! (( n )!)m ways of doing this. All that remains to show is that for any two distinct vectors of votes in the fooling set, we can let each of the voters vote according to one of these two vectors in such a way that a loses. Let i be a number such that ck(i) is not the same in both of these two vote vectors, that is, c1 k(i) (ck(i) in the first vote vector) is not equal to c2 k(i) (ck(i) in the second vote vector). Without loss of generality, suppose c1 k(i) = a. Now, construct a new vote vector by taking votes 4i \u2212 3, 4i \u2212 2, 4i \u2212 1, 4i from the first vote vector, and the remaining votes from the second vote vector. In this newly constructed vote vector, c1 k(i) and d1 k(i) each receive 4n /m+2 votes in the first round, whereas a receives at most 4n /m+1 votes. So, a does not continue to the runoff in the newly constructed vote vector, and hence we have a correct fooling set. Theorem 6. The nondeterministic communication complexity of the Borda rule is \u2126(nm log m) (even to decide whether a given candidate a wins). Proof. We will exhibit a fooling set of size (m !)n where m = m\u22122 and n = (n\u22122)/4. This will prove the theorem because m ! is \u2126(m log m), so that log((m !)n ) = n log(m !) is \u2126(nm log m). For every vector (\u03c01, \u03c02, . . . , \u03c0n ) consisting of n orderings of all candidates other than a and another fixed candidate b (technically, the orderings take the form of a one-to-one function \u03c0i : {1, 2, . . . , m } \u2192 C \u2212 {a, b} with \u03c0i(j) = c indicating that candidate c is the jth in the order represented by \u03c0i), let the following vector of votes be an element of the fooling set: \u2022 For 1 \u2264 i \u2264 n , let voters 4i \u2212 3 and 4i \u2212 2 rank the candidates a b \u03c0i(1) \u03c0i(2) . . . \u03c0i(m ). \u2022 For 1 \u2264 i \u2264 n , let voters 4i \u2212 1 and 4i rank the candidates \u03c0i(m ) \u03c0i(m \u2212 1) . . . \u03c0i(1) b a. \u2022 Let voter 4n + 1 = n \u2212 1 rank the candidates a b \u03c00(1) \u03c00(2) . . . \u03c00(m ) (where \u03c00 is an arbitrary order of the candidates other than a and b which is the same for every element of the fooling set). \u2022 Let voter 4n + 2 = n rank the candidates \u03c00(m ) \u03c00(m \u2212 1) . . . \u03c00(1) a b. We observe that this fooling set has size (m !)n , and that candidate a wins in each vector of votes in the fooling set (to 82 see why, we observe that for any 1 \u2264 i \u2264 n , votes 4i\u22123 and 4i \u2212 2 rank the candidates in the exact opposite way from votes 4i \u2212 1 and 4i, which under the Borda rule means they cancel out; and the last two votes give one more point to a than to any other candidate-besides b who gets two fewer points than a). All that remains to show is that for any two distinct vectors of votes in the fooling set, we can let each of the voters vote according to one of these two vectors in such a way that a loses. Let the first vote vector correspond to the vector (\u03c01 1, \u03c01 2, . . . , \u03c01 n ), and let the second vote vector correspond to the vector (\u03c02 1, \u03c02 2, . . . , \u03c02 n ). For some i, we must have \u03c01 i = \u03c02 i , so that for some candidate c /\u2208 {a, b}, (\u03c01 i )\u22121 (c) < (\u03c02 i )\u22121 (c) (that is, c is ranked higher in \u03c01 i than in \u03c02 i ). Now, construct a new vote vector by taking votes 4i\u22123 and 4i\u22122 from the first vote vector, and the remaining votes from the second vote vector. a\"s Borda score remains unchanged. However, because c is ranked higher in \u03c01 i than in \u03c02 i , c receives at least 2 more points from votes 4i\u22123 and 4i \u2212 2 in the newly constructed vote vector than it did in the second vote vector. It follows that c has a higher Borda score than a in the newly constructed vote vector. So, a is not the winner in the newly constructed vote vector, and hence we have a correct fooling set. Theorem 7. The nondeterministic communication complexity of the Copeland rule is \u2126(nm log m) (even to decide whether a given candidate a wins). Proof. We will exhibit a fooling set of size (m !)n where m = (m \u2212 2)/2 and n = (n \u2212 2)/2. This will prove the theorem because m ! is \u2126(m log m), so that log((m !)n ) = n log(m !) is \u2126(nm log m). We write the set of candidates as the following disjoint union: C = {a, b} \u222a L \u222a R where L = {l1, l2, . . . , lm } and R = {r1, r2, . . . , rm }. For every vector (\u03c01, \u03c02, . . . , \u03c0n ) consisting of n permutations of the integers 1 through m (\u03c0i : {1, 2, . . . , m } \u2192 {1, 2, . . . , m }), let the following vector of votes be an element of the fooling set: \u2022 For 1 \u2264 i \u2264 n , let voter 2i \u2212 1 rank the candidates a b l\u03c0i(1) r\u03c0i(1) l\u03c0i(2) r\u03c0i(2) . . . l\u03c0i(m ) r\u03c0i(m ). \u2022 For 1 \u2264 i \u2264 n , let voter 2i rank the candidates r\u03c0i(m ) l\u03c0i(m ) r\u03c0i(m \u22121) l\u03c0i(m \u22121) . . . r\u03c0i(1) l\u03c0i(1) b a. \u2022 Let voter n \u2212 1 = 2n + 1 rank the candidates a b l1 r1 l2 r2 . . . lm rm . \u2022 Let voter n = 2n +2 rank the candidates rm lm rm \u22121 lm \u22121 . . . r1 l1 a b. We observe that this fooling set has size (m !)n , and that candidate a wins in each vector of votes in the fooling set (every pair of candidates is tied in their pairwise election, with the exception that a defeats b, so that a wins the election by half a point). All that remains to show is that for any two distinct vectors of votes in the fooling set, we can let each of the voters vote according to one of these two vectors in such a way that a loses. Let the first vote vector correspond to the vector (\u03c01 1, \u03c01 2, . . . , \u03c01 n ), and let the second vote vector correspond to the vector (\u03c02 1, \u03c02 2, . . . , \u03c02 n ). For some i, we must have \u03c01 i = \u03c02 i , so that for some j \u2208 {1, 2, . . . , m }, we have (\u03c01 i )\u22121 (j) < (\u03c02 i )\u22121 (j). Now, construct a new vote vector by taking vote 2i\u22121 from the first vote vector, and the remaining votes from the second vote vector. a\"s Copeland score remains unchanged. Let us consider the score of lj. We first observe that the rank of lj in vote 2i \u2212 1 in the newly constructed vote vector is at least 2 higher than it was in the second vote vector, because (\u03c01 i )\u22121 (j) < (\u03c02 i )\u22121 (j). Let D1 (lj) be the set of candidates in L \u222a R that voter 2i \u2212 1 ranked lower than lj in the first vote vector (D1 (lj) = {c \u2208 L \u222a R : lj 2i\u22121 c}), and let D2 (lj) be the set of candidates in L \u222a R that voter 2i \u2212 1 ranked lower than lj in the second vote vector (D2 (lj) = {c \u2208 L \u222a R : lj 2i\u22121 c}). Then, it follows that in the newly constructed vote vector, lj defeats all the candidates in D1 (lj) \u2212 D2 (lj) in their pairwise elections (because lj receives an extra vote in each one of these pairwise elections relative to the second vote vector), and loses to all the candidates in D2 (lj) \u2212 D1 (lj) (because lj loses a vote in each one of these pairwise elections relative to the second vote vector), and ties with everyone else. But |D1 (lj)|\u2212|D2 (lj)| \u2265 2, and hence |D1 (lj) \u2212 D2 (lj)| \u2212 |D2 (lj) \u2212 D1 (lj)| \u2265 2. Hence, in the newly constructed vote vector, lj has at least two more pairwise wins than pairwise losses, and therefore has at least 1 more point than if lj had tied all its pairwise elections. Thus, lj has a higher Copeland score than a in the newly constructed vote vector. So, a is not the winner in the newly constructed vote vector, and hence we have a correct fooling set. Theorem 8. The nondeterministic communication complexity of the maximin rule is O(nm). Proof. The nondeterministic protocol will guess which candidate w is the winner, and, for each other candidate c, which candidate o(c) is the candidate against whom c receives its lowest score in a pairwise election. Then, let every voter communicate the following: \u2022 for each candidate c = w, whether she prefers c to w; \u2022 for each candidate c = w, whether she prefers c to o(c). We observe that this requires the communication of 2n(m\u2212 1) bits. If the guesses were correct, then, letting N(d, e) be the number of voters preferring candidate d to candidate e, we should have N(c, o(c)) < N(w, c ) for any c = w, c = w, which will prove that w wins the election. Theorem 9. The nondeterministic communication complexity of the maximin rule is \u2126(nm) (even to decide whether a given candidate a wins). Proof. We will exhibit a fooling set of size 2n m where m = m \u2212 2 and n = (n \u2212 1)/4. Let b be a candidate other than a. For every vector (S1, S2, . . . , Sn ) consisting of n subsets Si \u2286 C \u2212 {a, b}, let the following vector of votes be an element of the fooling set: \u2022 For 1 \u2264 i \u2264 n , let voters 4i \u2212 3 and 4i \u2212 2 rank the candidates Si a C \u2212 (Si \u222a {a, b}) b. \u2022 For 1 \u2264 i \u2264 n , let voters 4i \u2212 1 and 4i rank the candidates b C \u2212 (Si \u222a {a, b}) a Si. 83 \u2022 Let voter 4n + 1 = n rank the candidates a b C \u2212 {a, b}. We observe that this fooling set has size (2m )n = 2n m and that candidate a wins in each vector of votes in the fooling set (in every one of a\"s pairwise elections, a is ranked higher than its opponent by 2n +1 = (n+1)/2 > n/2 votes). All that remains to show is that for any two distinct vectors of votes in the fooling set, we can let each of the voters vote according to one of these two vectors in such a way that a loses. Let the first vote vector correspond to the vector (S1 1 , S1 2 , . . . , S1 n ), and let the second vote vector correspond to the vector (S2 1 , S2 2 , . . . , S2 n ). For some i, we must have S1 i = S2 i , so that either S1 i S2 i or S2 i S1 i . Without loss of generality, suppose S1 i S2 i , and let c be some candidate in S1 i \u2212 S2 i . Now, construct a new vote vector by taking votes 4i \u2212 3 and 4i \u2212 2 from the first vote vector, and the remaining votes from the second vote vector. In this newly constructed vote vector, a is ranked higher than c by only 2n \u22121 voters, for the following reason. Whereas voters 4i\u22123 and 4i \u2212 2 do not rank c higher than a in the second vote vector (because c /\u2208 S2 i ), voters 4i \u2212 3 and 4i \u2212 2 do rank c higher than a in the first vote vector (because c \u2208 S1 i ). Moreover, in every one of b\"s pairwise elections, b is ranked higher than its opponent by at least 2n voters. So, a has a lower maximin score than b, therefore a is not the winner in the newly constructed vote vector, and hence we have a correct fooling set. Theorem 10. The deterministic communication complexity of the STV rule is O(n(log m)2 ). Proof. Consider the following communication protocol. Let each voter first announce her most preferred candidate (O(n log m) communication). In the remaining rounds, we will keep track of each voter\"s most preferred candidate among the remaining candidates, which will be enough to implement the rule. When candidate c is eliminated, let each of the voters whose most preferred candidate among the remaining candidates was c announce their most preferred candidate among the candidates remaining after c\"s elimination. If candidate c was the ith candidate to be eliminated (that is, there were m \u2212 i + 1 candidates remaining before c\"s elimination), it follows that at most n/(m \u2212 i + 1) voters had candidate c as their most preferred candidate among the remaining candidates, and thus the number of bits to be communicated after the elimination of the ith candidate is O((n/(m\u2212i+1)) log m).7 Thus, the total communication in this communication protocol is O(n log m + m\u22121 i=1 (n/(m \u2212 i + 1)) log m). Of course, m\u22121 i=1 1/(m \u2212 i + 1) = i=2 1/i, which is O(log m). Substituting into the previous expression, we find that the communication complexity is O(n(log m)2 ). Theorem 11. The nondeterministic communication complexity of the STV rule is \u2126(n log m) (even to decide whether a given candidate a wins). Proof. We omit this proof because of space constraint. Actually, O((n/(m \u2212 i + 1)) log(m \u2212 i + 1)) is also correct, but it will not improve the bound. Theorem 12. The deterministic communication complexity of the approval rule is O(nm). Proof. Approving or disapproving of a candidate requires only one bit of information, so every voter can simply approve or disapprove of every candidate for a total communication of nm bits. Theorem 13. The nondeterministic communication complexity of the approval rule is \u2126(nm) (even to decide whether a given candidate a wins). Proof. We will exhibit a fooling set of size 2n m where m = m \u2212 1 and n = (n \u2212 1)/4. For every vector (S1, S2, . . . , Sn ) consisting of n subsets Si \u2286 C \u2212 {a}, let the following vector of votes be an element of the fooling set: \u2022 For 1 \u2264 i \u2264 n , let voters 4i \u2212 3 and 4i \u2212 2 approve Si \u222a {a}. \u2022 For 1 \u2264 i \u2264 n , let voters 4i \u2212 1 and 4i approve C \u2212 (Si \u222a {a}). \u2022 Let voter 4n + 1 = n approve {a}. We observe that this fooling set has size (2m )n = 2n m and that candidate a wins in each vector of votes in the fooling set (a is approved by 2n + 1 voters, whereas each other candidate is approved by only 2n voters). All that remains to show is that for any two distinct vectors of votes in the fooling set, we can let each of the voters vote according to one of these two vectors in such a way that a loses. Let the first vote vector correspond to the vector (S1 1 , S1 2 , . . . , S1 n ), and let the second vote vector correspond to the vector (S2 1 , S2 2 , . . . , S2 n ). For some i, we must have S1 i = S2 i , so that either S1 i S2 i or S2 i S1 i . Without loss of generality, suppose S1 i S2 i , and let b be some candidate in S1 i \u2212 S2 i . Now, construct a new vote vector by taking votes 4i \u2212 3 and 4i \u2212 2 from the first vote vector, and the remaining votes from the second vote vector. In this newly constructed vote vector, a is still approved by 2n + 1 votes. However, b is approved by 2n + 2 votes, for the following reason. Whereas voters 4i\u22123 and 4i\u22122 do not approve b in the second vote vector (because b /\u2208 S2 i ), voters 4i \u2212 3 and 4i \u2212 2 do approve b in the first vote vector (because b \u2208 S1 i ). It follows that b\"s score in the newly constructed vote vector is b\"s score in the second vote vector (2n ), plus two. So, a is not the winner in the newly constructed vote vector, and hence we have a correct fooling set. Interestingly, an \u2126(m) lower bound can be obtained even for the problem of finding a candidate that is approved by more than one voter . Theorem 14. The deterministic communication complexity of the Condorcet rule is O(nm). Proof. We maintain a set of active candidates S which is initialized to C. At each stage, we choose two of the active candidates (say, the two candidates with the lowest indices), and we let each voter communicate which of the two candidates she prefers. (Such a stage requires the communication of n bits, one per voter.) The candidate preferred by fewer 84 voters (the loser of the pairwise election) is removed from S. (If the pairwise election is tied, both candidates are removed.) After at most m \u2212 1 iterations, only one candidate is left (or zero candidates are left, in which case there is no Condorcet winner). Let a be the remaining candidate. To find out whether candidate a is the Condorcet winner, let each voter communicate, for every candidate c = a, whether she prefers a to c. (This requires the communication of at most n(m \u2212 1) bits.) This is enough to establish whether a won each of its pairwise elections (and thus, whether a is the Condorcet winner). Theorem 15. The nondeterministic communication complexity of the Condorcet rule is \u2126(nm) (even to decide whether a given candidate a wins). Proof. We will exhibit a fooling set of size 2n m where m = m \u2212 1 and n = (n \u2212 1)/2. For every vector (S1, S2, . . . , Sn ) consisting of n subsets Si \u2286 C \u2212 {a}, let the following vector of votes be an element of the fooling set: \u2022 For 1 \u2264 i \u2264 n , let voter 2i \u2212 1 rank the candidates Si a C \u2212 Si. \u2022 For 1 \u2264 i \u2264 n , let voter 2i rank the candidates C \u2212 Si a Si. \u2022 Let voter 2n +1 = n rank the candidates a C \u2212{a}. We observe that this fooling set has size (2m )n = 2n m and that candidate a wins in each vector of votes in the fooling set (a wins each of its pairwise elections by a single vote). All that remains to show is that for any two distinct vectors of votes in the fooling set, we can let each of the voters vote according to one of these two vectors in such a way that a loses. Let the first vote vector correspond to the vector (S1 1 , S1 2 , . . . , S1 n ), and let the second vote vector correspond to the vector (S2 1 , S2 2 , . . . , S2 n ). For some i, we must have S1 i = S2 i , so that either S1 i S2 i or S2 i S1 i . Without loss of generality, suppose S1 i S2 i , and let b be some candidate in S1 i \u2212 S2 i . Now, construct a new vote vector by taking vote 2i \u2212 1 from the first vote vector, and the remaining votes from the second vote vector. In this newly constructed vote vector, b wins its pairwise election against a by one vote (vote 2i \u2212 1 ranks b above a in the newly constructed vote vector because b \u2208 S1 i , whereas in the second vote vector vote 2i \u2212 1 ranked a above b because b /\u2208 S2 i ). So, a is not the Condorcet winner in the newly constructed vote vector, and hence we have a correct fooling set. Theorem 16. The deterministic communication complexity of the cup rule is O(nm). Proof. Consider the following simple communication protocol. First, let all the voters communicate, for every one of the matchups in the first round, which of its two candidates they prefer. After this, the matchups for the second round are known, so let all the voters communicate which candidate they prefer in each matchup in the second round-etc. Because communicating which of two candidates is preferred requires only one bit per voter, and because there are only m \u2212 1 matchups in total, this communication protocol requires O(nm) communication. Theorem 17. The nondeterministic communication complexity of the cup rule is \u2126(nm) (even to decide whether a given candidate a wins). Proof. We will exhibit a fooling set of size 2n m where m = (m \u2212 1)/2 and n = (n \u2212 7)/2. Given that m + 1 is a power of 2, so that one candidate gets a bye (that is, does not face an opponent) in the first round, let a be the candidate with the bye. Of the m first-round matchups, let lj denote the one (left) candidate in the jth matchup, and let rj be the other (right) candidate. Let L = {lj : 1 \u2264 j \u2264 m } and R = {rj : 1 \u2264 j \u2264 m }, so that C = L \u222a R \u222a {a}. . . . l r l r l r m\"1 1 2 2 m\" Figure 1: The schedule for the cup rule used in the proof of Theorem 17. For every vector (S1, S2, . . . , Sn ) consisting of n subsets Si \u2286 R, let the following vector of votes be an element of the fooling set: \u2022 For 1 \u2264 i \u2264 n , let voter 2i \u2212 1 rank the candidates Si L a R \u2212 Si. \u2022 For 1 \u2264 i \u2264 n , let voter 2i rank the candidates R \u2212 Si L a Si. \u2022 Let voters 2n +1 = n\u22126, 2n +2 = n\u22125, 2n +3 = n\u22124 rank the candidates L a R. \u2022 Let voters 2n + 4 = n \u2212 3, 2n + 5 = n \u2212 2 rank the candidates a r1 l1 r2 l2 . . . rm lm . \u2022 Let voters 2n + 6 = n \u2212 1, 2n + 7 = n rank the candidates rm lm rm \u22121 lm \u22121 . . . r1 l1 a. We observe that this fooling set has size (2m )n = 2n m Also, candidate a wins in each vector of votes in the fooling set, for the following reasons. Each candidate rj defeats its opponent lj in the first round. (For any 1 \u2264 i \u2264 n , the net effect of votes 2i \u2212 1 and 2i on the pairwise election between rj and lj is zero; votes n \u2212 6, n \u2212 5, n \u2212 4 prefer lj to rj, but votes n \u2212 3, n \u2212 2, n \u2212 1, n all prefer rj to lj.) Moreover, a defeats every rj in their pairwise election. (For any 1 \u2264 i \u2264 n , the net effect of votes 2i \u2212 1 and 2i on the pairwise election between a and rj is zero; votes n \u2212 1, n prefer rj to a, but votes n \u2212 6, n \u2212 5, n \u2212 4, n \u2212 3, n \u2212 2 all prefer a to rj.) It follows that a will defeat all the candidates that it faces. All that remains to show is that for any two distinct vectors of votes in the fooling set, we can let each of the voters vote according to one of these two vectors in such a way that a loses. Let the first vote vector correspond to the vector 85 (S1 1 , S1 2 , . . . , S1 n ), and let the second vote vector correspond to the vector (S2 1 , S2 2 , . . . , S2 n ). For some i, we must have S1 i = S2 i , so that either S1 i S2 i or S2 i S1 i . Without loss of generality, suppose S1 i S2 i , and let rj be some candidate in S1 i \u2212 S2 i . Now, construct a new vote vector by taking vote 2i from the first vote vector, and the remaining votes from the second vote vector. We note that, whereas in the second vote vector vote 2i preferred rj to lj (because rj \u2208 R\u2212S2 i ), in the newly constructed vote vector this is no longer the case (because rj \u2208 S1 i ). It follows that, whereas in the second vote vector, rj defeated lj in the first round by one vote, in the newly constructed vote vector, lj defeats rj in the first round. Thus, at least one lj advances to the second round after defeating its opponent rj. Now, we observe that in the newly constructed vote vector, any lk wins its pairwise election against any rq with q = k. This is because among the first 2n votes, at least n \u2212 1 prefer lk to rq; votes n \u2212 6, n \u2212 5, n \u2212 4 prefer lk to rq; and, because q = k, either votes n \u2212 3, n \u2212 2 prefer lk to rq (if k < q), or votes n \u2212 1, n prefer lk to rq (if k > q). Thus, at least n + 4 = (n + 1)/2 > n/2 votes prefer lk to rq. Moreover, any lk wins its pairwise election against a. This is because only votes n \u2212 3 and n \u2212 2 prefer a to lk. It follows that, after the first round, any surviving candidate lk can only lose a matchup against another surviving lk , so that one of the lk must win the election. So, a is not the winner in the newly constructed vote vector, and hence we have a correct fooling set. Theorem 18. The deterministic communication complexity of the Bucklin rule is O(nm). Proof. Let l be the minimum integer for which there is a candidate who is ranked among the top l candidates by more than half the votes. We will do a binary search for l. At each point, we will have a lower bound lL which is smaller than l (initialized to 0), and an upper bound lH which is at least l (initialized to m). While lH \u2212 lL > 1, we continue by finding out whether (lH \u2212 l)/2 is smaller than l, after which we can update the bounds. To find out whether a number k is smaller than l, we determine every voter\"s k most preferred candidates. Every voter can communicate which candidates are among her k most preferred candidates using m bits (for each candidate, indicate whether the candidate is among the top k or not), but because the binary search requires log m iterations, this gives us an upper bound of O((log m)nm), which is not strong enough. However, if lL < k < lH , and we already know a voter\"s lL most preferred candidates, as well as her lH most preferred candidates, then the voter no longer needs to communicate whether the lL most preferred candidates are among her k most preferred candidates (because they must be), and she no longer needs to communicate whether the m\u2212lH least preferred candidates are among her k most preferred candidates (because they cannot be). Thus the voter needs to communicate only m\u2212lL \u2212(m\u2212lH ) = lH \u2212lL bits in any given stage. Because each stage, lH \u2212 lL is (roughly) halved, each voter in total communicates only (roughly) m + m/2 + m/4 + . . . \u2264 2m bits. Theorem 19. The nondeterministic communication complexity of the Bucklin rule is \u2126(nm) (even to decide whether a given candidate a wins). Proof. We will exhibit a fooling set of size 2n m where m = (m\u22121)/2 and n = n/2. We write the set of candidates as the following disjoint union: C = {a} \u222a L \u222a R where L = {l1, l2, . . . , lm } and R = {r1, r2, . . . , rm }. For any subset S \u2286 {1, 2, . . . , m }, let L(S) = {li : i \u2208 S} and let R(S) = {ri : i \u2208 S}. For every vector (S1, S2, . . . , Sn ) consisting of n sets Si \u2286 {1, 2, . . . , m }, let the following vector of votes be an element of the fooling set: \u2022 For 1 \u2264 i \u2264 n , let voter 2i \u2212 1 rank the candidates L(Si) R \u2212 R(Si) a L \u2212 L(Si) R(Si). \u2022 For 1 \u2264 i \u2264 n , let voter 2i rank the candidates L \u2212 L(Si) R(Si) a L(Si) R \u2212 R(Si). We observe that this fooling set has size (2m )n = 2n m and that candidate a wins in each vector of votes in the fooling set, for the following reason. Each candidate in C \u2212 {a} is ranked among the top m candidates by exactly half the voters (which is not enough to win). Thus, we need to look at the voters\" top m +1 candidates, and a is ranked m +1th by all voters. All that remains to show is that for any two distinct vectors of votes in the fooling set, we can let each of the voters vote according to one of these two vectors in such a way that a loses. Let the first vote vector correspond to the vector (S1 1 , S1 2 , . . . , S1 n ), and let the second vote vector correspond to the vector (S2 1 , S2 2 , . . . , S2 n ). For some i, we must have S1 i = S2 i , so that either S1 i S2 i or S2 i S1 i . Without loss of generality, suppose S1 i S2 i , and let j be some integer in S1 i \u2212 S2 i . Now, construct a new vote vector by taking vote 2i \u2212 1 from the first vote vector, and the remaining votes from the second vote vector. In this newly constructed vote vector, a is still ranked m + 1th by all votes. However, lj is ranked among the top m candidates by n + 1 = n/2 + 1 votes. This is because whereas vote 2i \u2212 1 does not rank lj among the top m candidates in the second vote vector (because j /\u2208 S2 i , we have lj /\u2208 L(S2 i )), vote 2i \u2212 1 does rank lj among the top m candidates in the first vote vector (because j \u2208 S1 i , we have lj \u2208 L(S1 i )). So, a is not the winner in the newly constructed vote vector, and hence we have a correct fooling set. Theorem 20. The nondeterministic communication complexity of the ranked pairs rule is \u2126(nm log m) (even to decide whether a given candidate a wins). Proof. We omit this proof because of space constraint. 5. DISCUSSION One key obstacle to using voting for preference aggregation is the communication burden that an election places on the voters. By lowering this burden, it may become feasible to conduct more elections over more issues. In the limit, this could lead to a shift from representational government to a system in which most issues are decided by referenda-a veritable e-democracy. In this paper, we analyzed the communication complexity of the common voting rules. Knowing which voting rules require little communication is especially important when the issue to be voted on is of low enough importance that the following is true: the parties involved are willing to accept a rule that tends 86 to produce outcomes that are slightly less representative of the voters\" preferences, if this rule reduces the communication burden on the voters significantly. The following table summarizes the results we obtained. Rule Lower bound Upper bound plurality \u2126(n log m) O(n log m) plurality w/ runoff \u2126(n log m) O(n log m) STV \u2126(n log m) O(n(log m)2) Condorcet \u2126(nm) O(nm) approval \u2126(nm) O(nm) Bucklin \u2126(nm) O(nm) cup \u2126(nm) O(nm) maximin \u2126(nm) O(nm) Borda \u2126(nm log m) O(nm log m) Copeland \u2126(nm log m) O(nm log m) ranked pairs \u2126(nm log m) O(nm log m) Communication complexity of voting rules, sorted from low to high. All of the upper bounds are deterministic (with the exception of maximin, for which the best deterministic upper bound we proved is O(nm log m)). All of the lower bounds hold even for nondeterministic communication and even just for determining whether a given candidate a is the winner. One area of future research is to study what happens when we restrict our attention to communication protocols that do not reveal any strategically useful information. This restriction may invalidate some of the upper bounds that we derived using multistage communication protocols. Also, all of our bounds are worst-case bounds. It may be possible to outperform these bounds when the distribution of votes has additional structure. When deciding which voting rule to use for an election, there are many considerations to take into account. The voting rules that we studied in this paper are the most common ones that have survived the test of time. One way to select among these rules is to consider recent results on complexity. The table above shows that from a communication complexity perspective, plurality, plurality with runoff, and STV are preferable. However, plurality has the undesirable property that it is computationally easy to manipulate by voting strategically . Plurality with runoff is NP-hard to manipulate by a coalition of weighted voters, or by an individual that faces correlated uncertainty about the others\" votes . STV is NP-hard to manipulate in those settings as well , but also by an individual with perfect knowledge of the others\" votes (when the number of candidates is unbounded) . Therefore, STV is more robust, although it may require slightly more worst-case communication as per the table above. Yet other selection criteria are the computational complexity of determining whether enough information has been elicited to declare a winner, and that of determining the optimal sequence of queries .", "body1": "One key factor in the practicality of any preference aggregation rule is its communication burden. \u2022 Even when communicating all the preference information is feasible, reducing the communication requirements lessens the burden placed on the agents. \u2022 It preserves (some of) the agents\" privacy. Most of the work on reducing the communication burden in preference aggregation has focused on resource allocation settings such as combinatorial auctions, in which an auctioneer auctions off a number of (possibly distinct) items in a single event. Experimentally, this yields drastic savings in preference revelation . Ascending combinatorial auctions are a well-known special form of preference elicitation, where the elicitor asks demand queries with increasing prices . In this paper, we will focus on the communication requirements of a generally applicable subclass of social choice rules, commonly known as voting rules. In contrast, in this paper, we are concerned with the worst-case number of bits that must be communicated to execute a given voting rule, when nothing is known in advance about the voters\" preferences. bound. In this section, we review the common voting rules that we study in this paper. We will consider the following voting rules. The plurality rule is the scoring rule with \u03b1 = 1, 0, . \u2022 single transferable vote (STV). \u2022 plurality with run-off. \u2022 approval. \u2022 Condorcet. 79 \u2022 maximin (aka. \u2022 Copeland. \u2022 cup (sequential binary comparisons). \u2022 Bucklin. \u2022 ranked pairs. We emphasize that these definitions of voting rules do not concern themselves with how the votes are elicited from the voters; all the voting rules, including those that are suggestively defined in terms of rounds, are in actuality just functions mapping the vector of all the voters\" votes to a winner. Multistage communication protocols may reveal to the voters some information about how the other voters are voting (for example, in the two-stage communication protocol just given for plurality with runoff, in the second stage voters Balanced means that the difference in depth between two leaves can be at most one. will know which two candidates have the highest plurality scores). Of course, even the single-stage communication protocol is not strategy-proof4 for any reasonable voting rule, by the Gibbard-Satterthwaite theorem . In this section, we review the basic model of a communication problem and the lower-bounding technique of constructing a fooling set. Eventually, the communication terminates and all players know f(x1, x2, . Definition 1. 80 For the purposes of this paper, f is the voting rule that maps the votes to the winning candidate, and xi is voter i\"s vote (the information that the voting rule would require from the voter if there were no possibility of multistage communication-i.e. To simplify the proofs of our lower bounds, we make assumptions such as the number of voters n is odd in many of these proofs. We are now ready to present our results. When we discuss a voting rule in which the voters rank the candidates, we will represent a ranking in which candidate c1 is ranked first, c2 is ranked second, etc. One possible concern is that in the case where ties are possible, it may require much communication to verify whether a specific candidate a is among the winners, but little communication to produce one of the winners. We first give a universal upper bound. Theorem 1. Proof. The next lemma will be useful in a few of our proofs. Lemma 1. Proof. Therefore, log(n!) Theorem 3. \u2022 The last voter (voter n) votes for a. Candidate a wins with each one of these vote vectors because of the extra vote for a from the last voter. Without loss of generality, suppose that in the first vote vector, these voters do not vote for a (but for some other candidate, b, instead). The fooling set will consist of all vectors of votes satisfying the following constraints: \u2022 For any 1 \u2264 i \u2264 n , voters 4i \u2212 3 and 4i \u2212 2 rank the candidates ck(i) a C \u2212 {a, ck(i)}, for some candidate ck(i). An intuitive proof of this is the following. \u2022 Voter 4n + 2 = n ranks the candidates b C \u2212 {b}. Candidate a wins with each one of these vote vectors: because of the last two votes, candidates a and b are one vote ahead of all the other candidates and continue to the runoff, and at this point all the votes that had another candidate ranked at the top transfer to a, so that a wins the runoff. Given that m divides n , let us see how many vote vectors there are in the fooling set. \u2022 For 1 \u2264 i \u2264 n , let voters 4i \u2212 1 and 4i rank the candidates \u03c0i(m ) \u03c0i(m \u2212 1) . \u2022 Let voter 4n + 1 = n \u2212 1 rank the candidates a b \u03c00(1) \u03c00(2) . \u2022 Let voter 4n + 2 = n rank the candidates \u03c00(m ) \u03c00(m \u2212 1) . We observe that this fooling set has size (m ! \u2022 For 1 \u2264 i \u2264 n , let voter 2i rank the candidates r\u03c0i(m ) l\u03c0i(m ) r\u03c0i(m \u22121) l\u03c0i(m \u22121) . \u2022 Let voter n \u2212 1 = 2n + 1 rank the candidates a b l1 r1 l2 r2 . \u2022 Let voter n = 2n +2 rank the candidates rm lm rm \u22121 lm \u22121 . Now, construct a new vote vector by taking vote 2i\u22121 from the first vote vector, and the remaining votes from the second vote vector. Theorem 9. 83 \u2022 Let voter 4n + 1 = n rank the candidates a b C \u2212 {a, b}. We observe that this fooling set has size (2m )n = 2n m and that candidate a wins in each vector of votes in the fooling set (in every one of a\"s pairwise elections, a is ranked higher than its opponent by 2n +1 = (n+1)/2 > n/2 votes). All that remains to show is that for any two distinct vectors of votes in the fooling set, we can let each of the voters vote according to one of these two vectors in such a way that a loses. Theorem 10. Let each voter first announce her most preferred candidate (O(n log m) communication). Actually, O((n/(m \u2212 i + 1)) log(m \u2212 i + 1)) is also correct, but it will not improve the bound. Theorem 12. Theorem 13. \u2022 Let voter 4n + 1 = n approve {a}. We observe that this fooling set has size (2m )n = 2n m and that candidate a wins in each vector of votes in the fooling set (a is approved by 2n + 1 voters, whereas each other candidate is approved by only 2n voters). It follows that b\"s score in the newly constructed vote vector is b\"s score in the second vote vector (2n ), plus two. Interestingly, an \u2126(m) lower bound can be obtained even for the problem of finding a candidate that is approved by more than one voter . Theorem 14. \u2022 Let voter 2n +1 = n rank the candidates a C \u2212{a}. We observe that this fooling set has size (2m )n = 2n m and that candidate a wins in each vector of votes in the fooling set (a wins each of its pairwise elections by a single vote). Without loss of generality, suppose S1 i S2 i , and let b be some candidate in S1 i \u2212 S2 i . Theorem 17. . For every vector (S1, S2, . \u2022 Let voters 2n +1 = n\u22126, 2n +2 = n\u22125, 2n +3 = n\u22124 rank the candidates L a R. \u2022 Let voters 2n + 4 = n \u2212 3, 2n + 5 = n \u2212 2 rank the candidates a r1 l1 r2 l2 . \u2022 Let voters 2n + 6 = n \u2212 1, 2n + 7 = n rank the candidates rm lm rm \u22121 lm \u22121 . We observe that this fooling set has size (2m )n = 2n m Also, candidate a wins in each vector of votes in the fooling set, for the following reasons. To find out whether a number k is smaller than l, we determine every voter\"s k most preferred candidates. Every voter can communicate which candidates are among her k most preferred candidates using m bits (for each candidate, indicate whether the candidate is among the top k or not), but because the binary search requires log m iterations, this gives us an upper bound of O((log m)nm), which is not strong enough. We observe that this fooling set has size (2m )n = 2n m and that candidate a wins in each vector of votes in the fooling set, for the following reason. Without loss of generality, suppose S1 i S2 i , and let j be some integer in S1 i \u2212 S2 i .", "body2": "This has at least the following advantages: \u2022 It can make preference aggregation feasible in settings where the total amount of preference information is too large to communicate. This is especially true when the agents, rather than knowing all their preferences in advance, need to invest effort (such as computation or information gathering) to determine their preferences . \u2022 It preserves (some of) the agents\" privacy. For example, the elicitor can ask for a bidder\"s value for a specific bundle (value queries), which of two bundles the bidder prefers (order queries), which bundle he ranks kth or what the rank of a given bundle is (rank queries), which bundle he would purchase given a particular vector of prices (demand queries), etc.-until (at least) the final allocation can be determined. For a review of preference elicitation in combinatorial auctions, see . Segal then uses this characterization to prove bounds on the communication required in resource allocation as well as matching settings . In addition, that paper discusses strategic (game-theoretic) issues introduced by elicitation. However, we hope to also include the alternative proofs in a later version. There are two exceptions: STV, for which our upper and lower bounds are apart by a factor log m; and maximin, for which our best deterministic upper bound is also a factor log m above the (nondeterministic) lower bound, although we give a nondeterministic upper bound that matches the lower bound. All of the rules that we study are rank-based rules, which means that a vote is defined as an ordering of the candidates (with the exception of the plurality rule, for which a vote is a single candidate, and the approval rule, for which a vote is a subset of the candidates). , 0 . The winner is the last remaining candidate. Votes are transferred to these as in the STV rule, and a second round determines the winner from these two. The candidate approved by the greatest number of voters wins. The term voting protocol is often used to describe the same concept, but we seek to draw a sharp distinction between the rule mapping preferences to outcomes, and the communication/elicitation protocol used to implement this rule. The candidate with the highest maximin score wins. The Copeland score of candidate i is s(i) = j=i C(i, j). Each non-leaf node is assigned the winner of the pairwise election of the node\"s children; the candidate assigned to the root wins. That is, if we say that a voter approves her top l candidates, then we repeatedly increase l by 1 until some candidate is approved by more than half the voters, and this candidate is the winner. We continue to lock every pairwise result that does not contradict the ordering established so far. It is interesting to note that the choice of the communication protocol may affect the strategic behavior of the voters. Multistage communication protocols may reveal to the voters some information about how the other voters are voting (for example, in the two-stage communication protocol just given for plurality with runoff, in the second stage voters Balanced means that the difference in depth between two leaves can be at most one. In general, when the voters receive such information, it may give them incentives to vote differently than they would have in a single-stage communication protocol in which all voters declare their entire votes simultaneously. Now that we have reviewed voting rules, we move on to a brief review of communication complexity theory. In a deterministic protocol for computing f, in each stage, one of the players announces (to all other players) a bit of information based on her own input and the bits announced so far. We are now ready to give the definition of a fooling set. A strategy-proof protocol is one in which it is in the players\" best interest to report their preferences truthfully. This will strengthen our lower bound results (because it implies that even finding out whether one given candidate wins is hard).5 Thus, a fooling set in our context is a set of vectors of votes so that a wins (does not win) with each of them; but for any two different vote vectors in the set, there is a way of taking some voters\" votes from the first vector and the others\" votes from the second vector, so that a does not win (wins). For example, for every one of our lower bounds it is the case that for infinitely many values of m, there are infinitely many values of n such that the lower bound is proved for the pair (m, n). The lower bounds match the upper bounds in all but two cases: the STV rule (upper bound O(n(log m)2 ); lower bound \u2126(n log m)) and the maximin rule (upper bound O(nm log m), although we do give a nondeterministic protocol that is O(nm); lower bound \u2126(nm)). cm. For example, if S = {c2, c3}, then c1 S c4 indicates that either the ranking c1 c2 c3 c4 or the ranking c1 c3 c2 c4 can be used for the proof. We first give a universal upper bound. The deterministic communication complexity of any rank-based voting rule is O(nm log m). This bound is achieved by simply having everyone communicate their entire ordering of the candidates (indicating the rank of an individual candidate requires only O(log m) bits, so each of the n voters can simply indicate the rank of each of the m candidates). The next lemma will be useful in a few of our proofs. \u2265 n(log m \u2212 1)/2. = m n/m i=1 log(i). The deterministic communication complexity of the plurality rule is O(n log m). Indicating one of the candidates requires only O(log m) bits, so each voter can simply indicate her most preferred candidate. The nondeterministic communication complexity of the plurality rule is \u2126(n log m) (even to decide whether a given candidate a wins). 81 \u2022 Every candidate receives equally many votes from the first 2n = n \u2212 1 voters. \u2022 The last voter (voter n) votes for a. Let i be a number such that the two vote vectors disagree on the candidate for which voters 2i \u2212 1 and 2i vote. The deterministic communication complexity of the plurality with runoff rule is O(n log m). The nondeterministic communication complexity of the plurality with runoff rule is \u2126(n log m) (even to decide whether a given candidate a wins). , (cm , dm ) where c1 = a and d1 = b. \u2022 For any 1 \u2264 i \u2264 n , voters 4i \u2212 1 and 4i rank the candidates dk(i) a C \u2212 {a, dk(i)} (that is, their most preferred candidate is the candidate that is paired with the candidate that the previous two voters vote for). \u2022 Voter 4n +1 = n\u22121 ranks the candidates a C\u2212{a}. \u2022 Voter 4n + 2 = n ranks the candidates b C \u2212 {b}. Candidate a wins with each one of these vote vectors: because of the last two votes, candidates a and b are one vote ahead of all the other candidates and continue to the runoff, and at this point all the votes that had another candidate ranked at the top transfer to a, so that a wins the runoff. The nondeterministic communication complexity of the Borda rule is \u2126(nm log m) (even to decide whether a given candidate a wins). \u03c0i(m ). \u03c0i(1) b a. \u03c00(m ) (where \u03c00 is an arbitrary order of the candidates other than a and b which is the same for every element of the fooling set). \u03c00(1) a b. The nondeterministic communication complexity of the Copeland rule is \u2126(nm log m) (even to decide whether a given candidate a wins). l\u03c0i(m ) r\u03c0i(m ). r\u03c0i(1) l\u03c0i(1) b a. lm rm . r1 l1 a b. , m }, we have (\u03c01 i )\u22121 (j) < (\u03c02 i )\u22121 (j). The nondeterministic communication complexity of the maximin rule is O(nm). If the guesses were correct, then, letting N(d, e) be the number of voters preferring candidate d to candidate e, we should have N(c, o(c)) < N(w, c ) for any c = w, c = w, which will prove that w wins the election. The nondeterministic communication complexity of the maximin rule is \u2126(nm) (even to decide whether a given candidate a wins). \u2022 For 1 \u2264 i \u2264 n , let voters 4i \u2212 1 and 4i rank the candidates b C \u2212 (Si \u222a {a, b}) a Si. 83 \u2022 Let voter 4n + 1 = n rank the candidates a b C \u2212 {a, b}. We observe that this fooling set has size (2m )n = 2n m and that candidate a wins in each vector of votes in the fooling set (in every one of a\"s pairwise elections, a is ranked higher than its opponent by 2n +1 = (n+1)/2 > n/2 votes). So, a has a lower maximin score than b, therefore a is not the winner in the newly constructed vote vector, and hence we have a correct fooling set. The deterministic communication complexity of the STV rule is O(n(log m)2 ). Consider the following communication protocol. The nondeterministic communication complexity of the STV rule is \u2126(n log m) (even to decide whether a given candidate a wins). We omit this proof because of space constraint. Actually, O((n/(m \u2212 i + 1)) log(m \u2212 i + 1)) is also correct, but it will not improve the bound. The deterministic communication complexity of the approval rule is O(nm). Approving or disapproving of a candidate requires only one bit of information, so every voter can simply approve or disapprove of every candidate for a total communication of nm bits. The nondeterministic communication complexity of the approval rule is \u2126(nm) (even to decide whether a given candidate a wins). \u2022 For 1 \u2264 i \u2264 n , let voters 4i \u2212 1 and 4i approve C \u2212 (Si \u222a {a}). \u2022 Let voter 4n + 1 = n approve {a}. Whereas voters 4i\u22123 and 4i\u22122 do not approve b in the second vote vector (because b /\u2208 S2 i ), voters 4i \u2212 3 and 4i \u2212 2 do approve b in the first vote vector (because b \u2208 S1 i ). So, a is not the winner in the newly constructed vote vector, and hence we have a correct fooling set. Interestingly, an \u2126(m) lower bound can be obtained even for the problem of finding a candidate that is approved by more than one voter . The deterministic communication complexity of the Condorcet rule is O(nm). The nondeterministic communication complexity of the Condorcet rule is \u2126(nm) (even to decide whether a given candidate a wins). \u2022 For 1 \u2264 i \u2264 n , let voter 2i rank the candidates C \u2212 Si a Si. \u2022 Let voter 2n +1 = n rank the candidates a C \u2212{a}. For some i, we must have S1 i = S2 i , so that either S1 i S2 i or S2 i S1 i . The deterministic communication complexity of the cup rule is O(nm). Because communicating which of two candidates is preferred requires only one bit per voter, and because there are only m \u2212 1 matchups in total, this communication protocol requires O(nm) communication. The nondeterministic communication complexity of the cup rule is \u2126(nm) (even to decide whether a given candidate a wins). Let L = {lj : 1 \u2264 j \u2264 m } and R = {rj : 1 \u2264 j \u2264 m }, so that C = L \u222a R \u222a {a}. l r l r l r m\"1 1 2 2 m\" Figure 1: The schedule for the cup rule used in the proof of Theorem 17. \u2022 For 1 \u2264 i \u2264 n , let voter 2i rank the candidates R \u2212 Si L a Si. \u2022 Let voters 2n +1 = n\u22126, 2n +2 = n\u22125, 2n +3 = n\u22124 rank the candidates L a R. rm lm . r1 l1 a. It follows that a will defeat all the candidates that it faces. The deterministic communication complexity of the Bucklin rule is O(nm). While lH \u2212 lL > 1, we continue by finding out whether (lH \u2212 l)/2 is smaller than l, after which we can update the bounds. To find out whether a number k is smaller than l, we determine every voter\"s k most preferred candidates. The nondeterministic communication complexity of the Bucklin rule is \u2126(nm) (even to decide whether a given candidate a wins). \u2022 For 1 \u2264 i \u2264 n , let voter 2i rank the candidates L \u2212 L(Si) R(Si) a L(Si) R \u2212 R(Si). The nondeterministic communication complexity of the ranked pairs rule is \u2126(nm log m) (even to decide whether a given candidate a wins).", "introduction": "One key factor in the practicality of any preference aggregation rule is its communication burden. To successfully aggregate the agents\" preferences, it is usually not necessary for all the agents to report all of their preference information. Clever protocols that elicit the agents\" preferences partially and sequentially have the potential to dramatically reduce the required communication. This has at least the following advantages: \u2022 It can make preference aggregation feasible in settings where the total amount of preference information is too large to communicate. \u2022 Even when communicating all the preference information is feasible, reducing the communication requirements lessens the burden placed on the agents. This is especially true when the agents, rather than knowing all their preferences in advance, need to invest effort (such as computation or information gathering) to determine their preferences . \u2022 It preserves (some of) the agents\" privacy. Most of the work on reducing the communication burden in preference aggregation has focused on resource allocation settings such as combinatorial auctions, in which an auctioneer auctions off a number of (possibly distinct) items in a single event. Because in a combinatorial auction, bidders can have separate valuations for each of an exponential number of possible bundles of items, this is a setting in which reducing the communication burden is especially crucial. This can be accomplished by supplementing the auctioneer with an elicitor that incrementally elicits parts of the bidders\" preferences on an as-needed basis, based on what the bidders have revealed about their preferences so far, as suggested by Conen and Sandholm . For example, the elicitor can ask for a bidder\"s value for a specific bundle (value queries), which of two bundles the bidder prefers (order queries), which bundle he ranks kth or what the rank of a given bundle is (rank queries), which bundle he would purchase given a particular vector of prices (demand queries), etc.-until (at least) the final allocation can be determined. Experimentally, this yields drastic savings in preference revelation . Furthermore, if the agents\" valuation functions are drawn from certain natural subclasses, the elicitation problem can be solved using only polynomially many queries even in the worst case . For a review of preference elicitation in combinatorial auctions, see . Ascending combinatorial auctions are a well-known special form of preference elicitation, where the elicitor asks demand queries with increasing prices . Finally, resource 78 allocation problems have also been studied from a communication complexity viewpoint, thereby deriving lower bounds on the required communication. For example, Nisan and Segal show that exponential communication is required even to obtain a surplus greater than that obtained by auctioning off all objects as a single bundle . Segal also studies social choice rules in general, and shows that for a large class of social choice rules, supporting budget sets must be revealed such that if every agent prefers the same outcome in her budget set, this proves the optimality of that outcome. Segal then uses this characterization to prove bounds on the communication required in resource allocation as well as matching settings . In this paper, we will focus on the communication requirements of a generally applicable subclass of social choice rules, commonly known as voting rules. In a voting setting, there is a set of candidate outcomes over which the voters express their preferences by submitting a vote (typically, a ranking of the candidates), and the winner (that is, the chosen outcome) is determined based on these votes. The communication required by voting rules can be large either because the number of voters is large (such as, for example, in national elections), or because the number of candidates is large (for example, the agents can vote over allocations of a number of resources), or both. Prior work has studied elicitation in voting, studying how computationally hard it is to decide whether a winner can be determined with the information elicited so far, as well as how hard it is to find the optimal sequence of queries given perfect suspicions about the voters\" preferences. In addition, that paper discusses strategic (game-theoretic) issues introduced by elicitation. In contrast, in this paper, we are concerned with the worst-case number of bits that must be communicated to execute a given voting rule, when nothing is known in advance about the voters\" preferences. We determine the communication complexity of the common voting rules. For each rule, we first give an upper bound on the (deterministic) communication complexity by providing a communication protocol for it and analyzing how many bits need to be transmitted in this protocol. (Segal\"s results do not apply to most voting rules because most voting rules are not intersectionmonotonic (or even monotonic).1 ) For many of the voting rules under study, it turns out that one cannot do better than simply letting each voter immediately communicate all her (potentially relevant) information. However, for some rules (such as plurality with runoff, STV and cup) there is a straightforward multistage communication protocol that, with some analysis, can be shown to significantly outperform the immediate communication of all (potentially relevant) information. Finally, for some rules (such as the Condorcet and Bucklin rules), we need to introduce a more complex communication protocol to achieve the best possible upper For two of the rules that we study that are intersectionmonotonic, namely the approval and Condorcet rules, Segal\"s results can in fact be used to give alternative proofs of our lower bounds. We only give direct proofs for these rules here because 1) these direct proofs are among the easier ones in this paper, 2) the alternative proofs are nontrivial even given Segal\"s results, and 3) a space constraint applies. However, we hope to also include the alternative proofs in a later version. After obtaining the upper bounds, we show that they are tight by giving matching lower bounds on (even the nondeterministic) communication complexity of each voting rule. There are two exceptions: STV, for which our upper and lower bounds are apart by a factor log m; and maximin, for which our best deterministic upper bound is also a factor log m above the (nondeterministic) lower bound, although we give a nondeterministic upper bound that matches the lower bound.", "conclusion": "One key obstacle to using voting for preference aggregation is the communication burden that an election places on the voters.. By lowering this burden, it may become feasible to conduct more elections over more issues.. In the limit, this could lead to a shift from representational government to a system in which most issues are decided by referenda-a veritable e-democracy.. In this paper, we analyzed the communication complexity of the common voting rules.. Knowing which voting rules require little communication is especially important when the issue to be voted on is of low enough importance that the following is true: the parties involved are willing to accept a rule that tends 86 to produce outcomes that are slightly less representative of the voters\" preferences, if this rule reduces the communication burden on the voters significantly.. The following table summarizes the results we obtained.. Rule Lower bound Upper bound plurality \u2126(n log m) O(n log m) plurality w/ runoff \u2126(n log m) O(n log m) STV \u2126(n log m) O(n(log m)2) Condorcet \u2126(nm) O(nm) approval \u2126(nm) O(nm) Bucklin \u2126(nm) O(nm) cup \u2126(nm) O(nm) maximin \u2126(nm) O(nm) Borda \u2126(nm log m) O(nm log m) Copeland \u2126(nm log m) O(nm log m) ranked pairs \u2126(nm log m) O(nm log m) Communication complexity of voting rules, sorted from low to high.. All of the upper bounds are deterministic (with the exception of maximin, for which the best deterministic upper bound we proved is O(nm log m)).. All of the lower bounds hold even for nondeterministic communication and even just for determining whether a given candidate a is the winner.. One area of future research is to study what happens when we restrict our attention to communication protocols that do not reveal any strategically useful information.. This restriction may invalidate some of the upper bounds that we derived using multistage communication protocols.. Also, all of our bounds are worst-case bounds.. It may be possible to outperform these bounds when the distribution of votes has additional structure.. When deciding which voting rule to use for an election, there are many considerations to take into account.. The voting rules that we studied in this paper are the most common ones that have survived the test of time.. One way to select among these rules is to consider recent results on complexity.. The table above shows that from a communication complexity perspective, plurality, plurality with runoff, and STV are preferable.. However, plurality has the undesirable property that it is computationally easy to manipulate by voting strategically .. Plurality with runoff is NP-hard to manipulate by a coalition of weighted voters, or by an individual that faces correlated uncertainty about the others\" votes .. STV is NP-hard to manipulate in those settings as well , but also by an individual with perfect knowledge of the others\" votes (when the number of candidates is unbounded) .. Therefore, STV is more robust, although it may require slightly more worst-case communication as per the table above.. Yet other selection criteria are the computational complexity of determining whether enough information has been elicited to declare a winner, and that of determining the optimal sequence of queries ."}
{"id": "H-96", "keywords": ["implicit relev feedback", "relev feedback"], "title": "A Study of Factors Affecting the Utility of Implicit Relevance Feedback", "abstract": "Implicit relevance feedback (IRF) is the process by which a search system unobtrusively gathers evidence on searcher interests from their interaction with the system. IRF is a new method of gathering information on user interest and, if IRF is to be used in operational IR systems, it is important to establish when it performs well and when it performs poorly. In this paper we investigate how the use and effectiveness of IRF is affected by three factors: search task complexity, the search experience of the user and the stage in the search. Our findings suggest that all three of these factors contribute to the utility of IRF.", "references": ["Searchers' assessments of task complexity for web searching", "Experimental components for the evaluation of interactive information retrieval systems", "Strategic help for user interfaces for information retrieval", "Research methods in librarianship: Techniques and interpretation", "The ostensive model of developing information needs", "Relevance feedback and other query modification techniques", "Implicit feedback for inferring user preference", "A case for interaction: A study of interactive information retrieval behavior and effectiveness", "Statistics using ranks: A unified approach", "Information filtering based on user behavior analysis and best match text retrieval", "Improving retrieval performance by relevance feedback", "Nonparametric statistics for the behavioural sciences", "Implicit feedback for interactive information retrieval", "An implicit feedback approach for interactive information retrieval", "A simulated study of implicit feedback models", "The impact of fluid documents on reading and browsing: An observational study"], "full_text": "1. INTRODUCTION Information Retrieval (IR) systems are designed to help searchers solve problems. In the traditional interaction metaphor employed by Web search systems such as Yahoo! and MSN Search, the system generally only supports the retrieval of potentially relevant documents from the collection. However, it is also possible to offer support to searchers for different search activities, such as selecting the terms to present to the system or choosing which search strategy to adopt ; both of which can be problematic for searchers. As the quality of the query submitted to the system directly affects the quality of search results, the issue of how to improve search queries has been studied extensively in IR research . Techniques such as Relevance Feedback (RF) have been proposed as a way in which the IR system can support the iterative development of a search query by suggesting alternative terms for query modification. However, in practice RF techniques have been underutilised as they place an increased cognitive burden on searchers to directly indicate relevant results . Implicit Relevance Feedback (IRF) has been proposed as a way in which search queries can be improved by passively observing searchers as they interact. IRF has been implemented either through the use of surrogate measures based on interaction with documents (such as reading time, scrolling or document retention) or using interaction with browse-based result interfaces . IRF has been shown to display mixed effectiveness because the factors that are good indicators of user interest are often erratic and the inferences drawn from user interaction are not always valid . In this paper we present a study into the use and effectiveness of IRF in an online search environment. The study aims to investigate the factors that affect IRF, in particular three research questions: (i) is the use of and perceived quality of terms generated by IRF affected by the search task? (ii) is the use of and perceived quality of terms generated by IRF affected by the level of search experience of system users? (iii) is IRF equally used and does it generate terms that are equally useful at all search stages? This study aims to establish when, and under what circumstances, IRF performs well in terms of its use and the query modification terms selected as a result of its use. The main experiment from which the data are taken was designed to test techniques for selecting query modification terms and techniques for displaying retrieval results . In this paper we use data derived from that experiment to study factors affecting the utility of IRF. 2. STUDY In this section we describe the user study conducted to address our research questions. 2.1 Systems Our study used two systems both of which suggested new query terms to the user. One system suggested terms based on the user\"s interaction (IRF), the other used Explicit RF (ERF) asking the user to explicitly indicate relevant material. Both systems used the same term suggestion algorithm, , and used a common interface. 2.1.1 Interface Overview In both systems, retrieved documents are represented at the interface by their full-text and a variety of smaller, query-relevant representations, created at retrieval time. We used the Web as the test collection in this study and Google1 as the underlying search engine. Document representations include the document title and a summary of the document; a list of top-ranking sentences (TRS) extracted from the top documents retrieved, scored in relation to the query, a sentence in the document summary, and each summary sentence in the context it occurs in the document (i.e., with the preceding and following sentence). Each summary sentence and top-ranking sentence is regarded as a representation of the document. The default display contains the list of top-ranking sentences and the list of the first ten document titles. Interacting with a representation guides searchers to a different representation from the same document, e.g., moving the mouse over a document title displays a summary of the document. This presentation of progressively more information from documents to aid relevance assessments has been shown to be effective in earlier work . In Appendix A we show the complete interface to the IRF system with the document representations marked and in Appendix B we show a fragment from the ERF interface with the checkboxes used by searchers to indicate relevant information. Both systems provide an interactive query expansion feature by suggesting new query terms to the user. The searcher has the responsibility for choosing which, if any, of these terms to add to the query. The searcher can also add or remove terms from the query at will. 2.1.2 Explicit RF system This version of the system implements explicit RF. Next to each document representation are checkboxes that allow searchers to mark individual representations as relevant; marking a representation is an indication that its contents are relevant. Only the representations marked relevant by the user are used for suggesting new query terms. This system was used as a baseline against which the IRF system could be compared. 2.1.3 Implicit RF system This system makes inferences about searcher interests based on the information with which they interact. As described in Section 2.1.1 interacting with a representation highlights a new representation from the same document. To the searcher this is a way they can find out more information from a potentially interesting source. To the implicit RF system each interaction with a representation is interpreted as an implicit indication of interest in that representation; interacting with a representation is assumed to be an indication that its contents are relevant. The query modification terms are selected using the same algorithm as in the Explicit RF system. Therefore the only difference between the systems is how relevance is communicated to the system. The results of the main experiment indicated that these two systems were comparable in terms of effectiveness. 2.2 Tasks Search tasks were designed to encourage realistic search behaviour by our subjects. The tasks were phrased in the form of simulated work task situations , i.e., short search scenarios that were designed to reflect real-life search situations and allow subjects to develop personal assessments of relevance. We devised six search topics (i.e., applying to university, allergies in the workplace, art galleries in Rome, Third Generation mobile phones, Internet music piracy and petrol prices) based on pilot testing with a small representative group of subjects. These subjects were not involved in the main experiment. For each topic, three versions of each work task situation were devised, each version differing in their predicted level of task complexity. As described in task complexity is a variable that affects subject perceptions of a task and their interactive behaviour, e.g., subjects perform more filtering activities with highly complex search tasks. By developing tasks of different complexity we can assess how the nature of the task affects the subjects\" interactive behaviour and hence the evidence supplied to IRF algorithms. Task complexity was varied according to the methodology described in , specifically by varying the number of potential information sources and types of information required, to complete a task. In our pilot tests (and in a posteriori analysis of the main experiment results) we verified that subjects reporting of individual task complexity matched our estimation of the complexity of the task. Subjects attempted three search tasks: one high complexity, one moderate complexity and one low complexity2 . They were asked to read the task, place themselves in the situation it described and find the information they felt was required to complete the task. Figure 1 shows the task statements for three levels of task complexity for one of the six search topics. HC Task: High Complexity Whilst having dinner with an American colleague, they comment on the high price of petrol in the UK compared to other countries, despite large volumes coming from the same source. Unaware of any major differences, you decide to find out how and why petrol prices vary worldwide. MC Task: Moderate Complexity Whilst out for dinner one night, one of your friends\" guests is complaining about the price of petrol and the factors that cause it. Throughout the night they seem to be complaining about everything they can, reducing the credibility of their earlier statements so you decide to research which factors actually are important in determining the price of petrol in the UK. LC Task: Low Complexity While out for dinner one night, your friend complains about the rising price of petrol. However, as you have not been driving for long, you are unaware of any major changes in price. You decide to find out how the price of petrol has changed in the UK in recent years. Figure 1. Varying task complexity (Petrol Prices topic). 2.3 Subjects 156 volunteers expressed an interest in participating in our study. 48 subjects were selected from this set with the aim of populating two groups, each with 24 subjects: inexperienced (infrequent/ inexperienced searchers) and experienced (frequent/ experienced searchers). Subjects were not chosen and classified into their groups until they had completed an entry questionnaire that asked them about their search experience and computer use. The average age of the subjects was 22.83 years (maximum 51, minimum 18, \u03c3 = 5.23 years) and 75% had a university diploma or a higher degree. 47.91% of subjects had, or were pursuing, a qualification in a discipline related to Computer Science. The subjects were a mixture of students, researchers, academic staff and others, with different levels of computer and search experience. The subjects were divided into the two groups depending on their search experience, how often they searched and the types of searches they performed. All were familiar with Web searching, and some with searching in other domains. 2.4 Methodology The experiment had a factorial design; with 2 levels of search experience, 3 experimental systems (although we only report on the findings from the ERF and IRF systems) and 3 levels of search task complexity. Subjects attempted one task of each complexity, The main experiment from which these results are drawn had a third comparator system which had a different interface. Each subject carried out three tasks, one on each system. We only report on the results from the ERF and IRF systems as these are the only pertinent ones for this paper. switched systems after each task and used each system once. The order in which systems were used and search tasks attempted was randomised according to a Latin square experimental design. Questionnaires used Likert scales, semantic differentials and openended questions to elicit subject opinions . System logging was also used to record subject interaction. A tutorial carried out prior to the experiment allowed subjects to use a non-feedback version of the system to attempt a practice task before using the first experimental system. Experiments lasted between oneand-a-half and two hours, dependent on variables such as the time spent completing questionnaires. Subjects were offered a 5 minute break after the first hour. In each experiment: i. the subject was welcomed and asked to read an introduction to the experiments and sign consent forms. This set of instructions was written to ensure that each subject received precisely the same information. ii. the subject was asked to complete an introductory questionnaire. This contained questions about the subject\"s education, general search experience, computer experience and Web search experience. iii. the subject was given a tutorial on the interface, followed by a training topic on a version of the interface with no RF. iv. the subject was given three task sheets and asked to choose one task from the six topics on each sheet. No guidelines were given to subjects when choosing a task other than they could not choose a task from any topic more than once. Task complexity was rotated by the experimenter so each subject attempted one high complexity task, one moderate complexity task and one low complexity task. v. the subject was asked to perform the search and was given 15 minutes to search. The subject could terminate a search early if they were unable to find any more information they felt helped them complete the task. vi. after completion of the search, the subject was asked to complete a post-search questionnaire. vii. the remaining tasks were attempted by the subject, following steps v. and vi. viii. the subject completed a post-experiment questionnaire and participated in a post-experiment interview. Subjects were told that their interaction may be used by the IRF system to help them as they searched. They were not told which behaviours would be used or how it would be used. We now describe the findings of our analysis. 3. FINDINGS In this section we use the data derived from the experiment to answer our research questions about the effect of search task complexity, search experience and stage in search on the use and effectiveness of IRF. We present our findings per research question. Due to the ordinal nature of much of the data non-parametric statistical testing is used in this analysis and the level of significance is set to p < .05, unless otherwise stated. We use the method proposed by to determine the significance of differences in multiple comparisons and that of to test for interaction effects between experimental variables, the occurrence of which we report where appropriate. All Likert scales and semantic differentials were on a 5-point scale where a rating closer to 1 signifies more agreement with the attitude statement. The category labels HC, MC and LC are used to denote the high, moderate and low complexity tasks respectively. The highest, or most positive, values in each table are shown in bold. Our analysis uses data from questionnaires, post-experiment interviews and background system logging on the ERF and IRF systems. 3.1 Search Task Searchers attempted three search tasks of varying complexity, each on a different experimental system. In this section we present an analysis on the use and usefulness of IRF for search tasks of different complexities. We present our findings in terms of the RF provided by subjects and the terms recommended by the systems. 3.1.1 Feedback We use questionnaires and system logs to gather data on subject perceptions and provision of RF for different search tasks. In the postsearch questionnaire subjects were asked about how RF was conveyed using differentials to elicit their opinion on: 1. the value of the feedback technique: How you conveyed relevance to the system (i.e. ticking boxes or viewing information) was: easy / difficult, effective/ ineffective, useful\"/not useful. 2. the process of providing the feedback: How you conveyed relevance to the system made you feel: comfortable/uncomfortable, in control/not in control. The average obtained differential values are shown in Table 1 for IRF and each task category. The value corresponding to the differential All represents the mean of all differentials for a particular attitude statement. This gives some overall understanding of the subjects\" feelings which can be useful as the subjects may not answer individual differentials very precisely. The values for ERF are included for reference in this table and all other tables and figures in the Findings section. Since the aim of the paper is to investigate situations in which IRF might perform well, not a direct comparison between IRF and ERF, we make only limited comparisons between these two types of feedback. Table 1. Subject perceptions of RF method (lower = better). Each cell in Table 1 summarises the subject responses for 16 tasksystem pairs (16 subjects who ran a high complexity (HC) task on the ERF system, 16 subjects who ran a medium complexity (MC) task on the ERF system, etc). Kruskal-Wallis Tests were applied to each differential for each type of RF3 . Subject responses suggested that Since this analysis involved many differentials, we use a Bonferroni correction to control the experiment-wise error rate and set the alpha level (\u03b1) for both statements 1. and 2. respectively, i.e., .05 divided by the number of differentials. This correction reduces the number of Type I errors i.e., rejecting null hypotheses that are true. Explicit RF Implicit RF Differential HC MC LC HC MC LC Easy 2.78 2.47 2.12 1.86 1.81 1.93 Effective 2.94 2.68 2.44 2.04 2.41 2.66 Useful 2.76 2.51 2.16 1.91 2.37 2.56 All (1) 2.83 2.55 2.24 1.94 2.20 2.38 Comfortable 2.27 2.28 2.35 2.11 2.15 2.16 In control 2.01 1.97 1.93 2.73 2.68 2.61 All (2) 2.14 2.13 2.14 2.42 2.42 2.39 IRF was most effective and useful for more complex search tasks4 and that the differences in all pair-wise comparisons between tasks were significant5 . Subject perceptions of IRF elicited using the other differentials did not appear to be affected by the complexity of the search task6 . To determine whether a relationship exists between the effectiveness and usefulness of the IRF process and task complexity we applied Spearman\"s Rank Order Correlation Coefficient to participant responses. The results of this analysis suggest that the effectiveness of IRF and usefulness of IRF are both related to task complexity; as task complexity increases subject preference for IRF also increases7 On the other hand, subjects felt ERF was more effective and useful for low complexity tasks8 . Their verbal reporting of ERF, where perceived utility and effectiveness increased as task complexity decreased, supports this finding. In tasks of lower complexity the subjects felt they were better able to provide feedback on whether or not documents were relevant to the task. We analyse interaction logs generated by both interfaces to investigate the amount of RF subjects provided. To do this we use a measure of search precision that is the proportion of all possible document representations that a searcher assessed, divided by the total number they could assess. In ERF this is the proportion of all possible representations that were marked relevant by the searcher, i.e., those representations explicitly marked relevant. In IRF this is the proportion of representations viewed by a searcher over all possible representations that could have been viewed by the searcher. This proportion measures the searcher\"s level of interaction with a document, we take it to measure the user\"s interest in the document: the more document representations viewed the more interested we assume a user is in the content of the document. There are a maximum of 14 representations per document: 4 topranking sentences, 1 title, 1 summary, 4 summary sentences and 4 summary sentences in document context. Since the interface shows document representations from the top-30 documents, there are 420 representations that a searcher can assess. Table 2 shows proportion of representations provided as RF by subjects. Table 2. Feedback and documents viewed. Explicit RF Implicit RF Measure HC MC LC HC MC LC Proportion Feedback 2.14 2.39 2.65 21.50 19.36 15.32 Documents Viewed 10.63 10.43 10.81 10.84 12.19 14.81 For IRF there is a clear pattern: as complexity increases the subjects viewed fewer documents but viewed more representations for each document. This suggests a pattern where users are investigating retrieved documents in more depth. It also means that the amount of effective: \u03c72 (2) = 11.62, p = .003; useful: \u03c72 (2) = 12.43, p = .002 Dunn\"s post-hoc tests (multiple comparison using rank sums); all Z \u2265 2.88, all p \u2264 .002 all \u03c72 (2) \u2264 2.85, all p \u2265 .24 (Kruskal-Wallis Tests) effective: all r \u2265 0.644, p \u2264 .002; useful: all r \u2265 0.541, p \u2264 .009 effective: \u03c72 (2) = 7.01, p = .03; useful: \u03c72 (2) = 6.59, p = .037 (Kruskal-Wallis Test); all pair-wise differences significant, all Z \u2265 2.34, all p \u2264 .01 (Dunn\"s post-hoc tests) feedback varies based on the complexity of the search task. Since IRF is based on the interaction of the searcher, the more they interact, the more feedback they provide. This has no effect on the number of RF terms chosen, but may affect the quality of the terms selected. Correlation analysis revealed a strong negative correlation between the number of documents viewed and the amount of feedback searchers provide9 ; as the number of documents viewed increases the proportion of feedback falls (searchers view less representations of each document). This may be a natural consequence of their being less time to view documents in a time constrained task environment but as we will show as complexity changes, the nature of information searchers interact with also appears to change. In the next section we investigate the effect of task complexity on the terms chosen as a result of IRF. 3.1.2 Terms The same RF algorithm was used to select query modification terms in all systems . We use subject opinions of terms recommended by the systems as a measure of the effectiveness of IRF with respect to the terms generated for different search tasks. To test this, subjects were asked to complete two semantic differentials that completed the statement: The words chosen by the system were: relevant/irrelevant and useful/not useful. Table 3 presents average responses grouped by search task. Table 3. Subject perceptions of system terms (lower = better). Explicit RF Implicit RF Differential HC MC LC HC MC LC Relevant 2.50 2.46 2.41 1.94 2.35 2.68 Useful 2.61 2.61 2.59 2.06 2.54 2.70 Kruskal-Wallis Tests were applied within each type of RF. The results indicate that the relevance and usefulness of the terms chosen by IRF is affected by the complexity of the search task; the terms chosen are more relevant and useful when the search task is more complex. 10 Relevant here, was explained as being related to their task whereas useful was for terms that were seen as being helpful in the search task. For ERF, the results indicate that the terms generated are perceived to be more relevant and useful for less complex search tasks; although differences between tasks were not significant11 . This suggests that subject perceptions of the terms chosen for query modification are affected by task complexity. Comparison between ERF and IRF shows that subject perceptions also vary for different types of RF12 As well as using data on relevance and utility of the terms chosen, we used data on term acceptance to measure the perceived value of the terms suggested. Explicit and Implicit RF systems made recommendations about which terms could be added to the original search query. In Table 4 we show the proportion of the top six terms r = \u22120.696, p = .001 (Pearson\"s Correlation Coefficient) 10 relevant: \u03c72 (2) = 13.82, p = .001; useful: \u03c72 (2) = 11.04, p = .004; \u03b1 = .025 11 all \u03c72 (2) \u2264 2.28, all p \u2265 .32 (Kruskal-Wallis Test) 12 all T(16) \u2265 102, all p \u2264 .021, (Wilcoxon Signed-Rank Test) 13 that were shown to the searcher that were added to the search query, for each type of task and each type of RF. Table 4. Term Acceptance (percentage of top six terms). Explicit RF Implicit RFProportion of terms HC MC LC HC MC LC Accepted 65.31 67.32 68.65 67.45 67.24 67.59 The average number of terms accepted from IRF is approximately the same across all search tasks and generally the same as that of ERF14 As Table 2 shows, subjects marked fewer documents relevant for highly complex tasks . Therefore, when task complexity increases the ERF system has fewer examples of relevant documents and the expansion terms generated may be poorer. This could explain the difference in the proportion of recommended terms accepted in ERF as task complexity increases. For IRF there is little difference in how many of the recommended terms were chosen by subjects for each level of task complexity15 . Subjects may have perceived IRF terms as more useful for high complexity tasks but this was not reflected in the proportion of IRF terms accepted. Differences may reside in the nature of the terms accepted; future work will investigate this issue. 3.1.3 Summary In this section we have presented an investigation on the effect of search task complexity on the utility of IRF. From the results there appears to be a strong relation between the complexity of the task and the subject interaction: subjects preferring IRF for highly complex tasks. Task complexity did not affect the proportion of terms accepted in either RF method, despite there being a difference in how relevant and useful subjects perceived the terms to be for different complexities; complexity may affect term selection in ways other than the proportion of terms accepted. 3.2 Search Experience Experienced searchers may interact differently and give different types of evidence to RF than inexperienced searchers. As such, levels of search experience may affect searchers\" use and perceptions of IRF. In our experiment subjects were divided into two groups based on their level of search experience, the frequency with which they searched and the types of searches they performed. In this section we use their perceptions and logging to address the next research question; the relationship between the usefulness and use of IRF and the search experience of experimental subjects. The data are the same as that analysed in the previous section, but here we focus on search experience rather than the search task. 3.2.1 Feedback We analyse the results from the attitude statements described at the beginning of Section 3.1.1. (i.e., How you conveyed relevance to the system was\u2026 and How you conveyed relevance to the system made you feel\u2026). These differentials elicited opinion from experimental subjects about the RF method used. In Table 5 we show the mean average responses for inexperienced and experienced subject groups on ERF and IRF; 24 subjects per cell. 13 This was the smallest number of query modification terms that were offered in both systems. 14 all T(16) \u2265 80, all p \u2264 .31, (Wilcoxon Signed-Rank Test) 15 ERF: \u03c72 (2) = 3.67, p = .16; IRF: \u03c72 (2) = 2.55, p = .28 (KruskalWallis Tests) Table 5. Subject perceptions of RF method (lower = better). The results demonstrate a strong preference in inexperienced subjects for IRF; they found it more easy and effective than experienced subjects. 16 The differences for all other IRF differentials were not statistically significant. For all differentials, apart from in control, inexperienced subjects generally preferred IRF over ERF17 Inexperienced subjects also felt that IRF was more difficult to control than experienced subjects18 . As these subjects have less search experience they may be less able to understand RF processes and may be more comfortable with the system gathering feedback implicitly from their interaction. Experienced subjects tended to like ERF more than inexperienced subjects and felt more comfortable with this feedback method19 . It appears from these results that experienced subjects found ERF more useful and were more at ease with the ERF process. In a similar way to Section 3.1.1 we analysed the proportion of feedback that searchers provided to the experimental systems. Our analysis suggested that search experience does not affect the amount of feedback subjects provide20 3.2.2 Terms We used questionnaire responses to gauge subject opinion on the relevance and usefulness of the terms from the perspective of experienced and inexperienced subjects. Table 6 shows the average differential responses obtained from both subject groups. Table 6. Subject perceptions of system terms (lower = better). Explicit RF Implicit RF Differential Inexp. Exp. Inexp. Exp. Relevant 2.58 2.44 2.33 2.21 Useful 2.88 2.63 2.33 2.23 The differences between subject groups were significant21 Experienced subjects generally reacted to the query modification terms chosen by the system more positively than inexperienced 16 easy: U(24) = 391, p = .016; effective: U(24) = 399, p = .011; \u03b1 = (Mann-Whitney Tests) 17 all T(24) \u2265 231, all p \u2264 .001 (Wilcoxon Signed-Rank Test) 18 U(24) = 390, p = .018; \u03b1 = (Mann-Whitney Test) 19 T(24) = 222, p = .020 (Wilcoxon Signed-Rank Test) 20 ERF: all U(24) \u2264 319, p \u2265 .26, IRF: all U(24) \u2264 313, p \u2265 .30 (MannWhitney Tests) 21 ERF: all U(24) \u2265 388, p \u2264 .020, IRF: all U(24) \u2265 384, p \u2264 .024 Explicit RF Implicit RF Differential Inexp. Exp. Inexp. Exp. Easy 2.46 2.46 1.84 1.98 Effective 2.75 2.63 2.32 2.43 Useful 2.50 2.46 2.28 2.27 All (1) 2.57 2.52 2.14 2.23 Comfortable 2.46 2.14 2.05 2.24 In control 1.96 1.98 2.73 2.64 All (2) 2.21 2.06 2.39 2.44 subjects. This finding was supported by the proportion of query modification terms these subjects accepted. In the same way as in Section 3.1.2, we analysed the number of query modification terms recommended by the system that were used by experimental subjects. Table 7 shows the average number of accepted terms per subject group. Table 7. Term Acceptance (percentage of top six terms). Explicit RF Implicit RFProportion of terms Inexp. Exp. Inexp. Exp. Accepted 63.76 70.44 64.43 71.35 Our analysis of the data show that differences between subject groups for each type of RF are significant; experienced subjects accepted more expansion terms regardless of type of RF. However, the differences between the same groups for different types of RF are not significant; subjects chose roughly the same percentage of expansion terms offered irrespective of the type of RF22 3.2.3 Summary In this section we have analysed data gathered from two subject groups - inexperienced searchers and experienced searchers - on how they perceive and use IRF. The results indicate that inexperienced subjects found IRF more easy and effective than experienced subjects, who in turn found the terms chosen as a result of IRF more relevant and useful. We also showed that inexperienced subjects generally accepted less recommended terms than experienced subjects, perhaps because they were less comfortable with RF or generally submitted shorter search queries. Search experience appears to affect how subjects use the terms recommended as a result of the RF process. 3.3 Search Stage From our observations of experimental subjects as they searched we conjectured that RF may be used differently at different times during a search. To test this, our third research question concerned the use and usefulness of IRF during the course of a search. In this section we investigate whether the amount of RF provided by searchers or the proportion of terms accepted are affected by how far through their search they are. For the purposes of this analysis a search begins when a subject poses the first query to the system and progresses until they terminate the search or reach the maximum allowed time for a search task of 15 minutes. We do not divide tasks based on this limit as subjects often terminated their search in less than 15 minutes. In this section we use data gathered from interaction logs and subject opinions to investigate the extent to which RF was used and the extent to which it appeared to benefit our experimental subjects at different stages in their search 3.3.1 Feedback The interaction logs for all searches on the Explicit RF and Implicit RF were analysed and each search is divided up into nine equal length time slices. This number of slices gave us an equal number per stage and was a sufficient level of granularity to identify trends in the results. Slices 1 - 3 correspond to the start of the search, 4 - 6 to the middle of the search and 7 - 9 to the end. In Figure 2 we plot the measure of precision described in Section 3.1.1 (i.e., the proportion of all possible representations that were provided as RF) at each of the 22 IRF: U(24) = 403, p = .009, ERF: U(24) = 396, p = .013 nine slices, per search task, averaged across all subjects; this allows us to see how the provision of RF was distributed during a search. The total amount of feedback for a single RF method/task complexity pairing across all nine slices corresponds to the value recorded in the first row of Table 2 (e.g., the sum of the RF for IRF/HC across all nine slices of Figure 2 is 21.50%). To simplify the statistical analysis and comparison we use the grouping of start, middle and end. 0 1 2 3 4 5 6 7 8 9 0.5 1.5 2.5 3.5 4.5 Slice Search\"precision\"(%oftotalrepsprovidedasRF) Explicit RF/HC Explicit RF/MC Explicit RF/LC Implicit RF/HC Implicit RF/MC Implicit RF/LC Figure 2. Distribution of RF provision per search task. Figure 2 appears to show the existence of a relationship between the stage in the search and the amount of relevance information provided to the different types of feedback algorithm. These are essentially differences in the way users are assessing documents. In the case of ERF subjects provide explicit relevance assessments throughout most of the search, but there is generally a steep increase in the end phase towards the completion of the search23 When using the IRF system, the data indicates that at the start of the search subjects are providing little relevance information24 , which corresponds to interacting with few document representations. At this stage the subjects are perhaps concentrating more on reading the retrieved results. Implicit relevance information is generally offered extensively in the middle of the search as they interact with results and it then tails off towards the end of the search. This would appear to correspond to stages of initial exploration, detailed analysis of document representations and storage and presentation of findings. Figure 2 also shows the proportion of feedback for tasks of different complexity. The results appear to show a difference25 in how IRF is used that relates to the complexity of the search task. More specifically, as complexity increases it appears as though subjects take longer to reach their most interactive point. This suggests that task complexity affects how IRF is distributed during the search and that they may be spending more time initially interpreting search results for more complex tasks. 23 IRF: all Z \u2265 1.87, p \u2264 .031, ERF: start vs. end Z = 2.58, p = .005 (Dunn\"s post-hoc tests). 24 Although increasing toward the end of the start stage. 25 Although not statistically significant; \u03c72 (2) = 3.54, p = .17 (Friedman Rank Sum Test) 3.3.2 Terms The terms recommended by the system are chosen based on the frequency of their occurrence in the relevant items. That is, nonstopword, non-query terms occurring frequently in search results regarded as relevant are likely to be recommended to the searcher for query modification. Since there is a direct association between the RF and the terms selected we use the number of terms accepted by searchers at different points in the search as an indication of how effective the RF has been up until the current point in the search. In this section we analysed the average number of terms from the top six terms recommended by Explicit RF and Implicit RF over the course of a search. The average proportion of the top six recommended terms that were accepted at each stage are shown in Table 8; each cell contains data from all 48 subjects. Table 8. Term Acceptance (proportion of top six terms). Explicit RF Implicit RFProportion of terms start middle end start middle end Accepted 66.87 66.98 67.34 61.85 68.54 73.22 The results show an apparent association between the stage in the search and the number of feedback terms subjects accept. Search stage affects term acceptance in IRF but not in ERF26 . The further into a search a searcher progresses, the more likely they are to accept terms recommended via IRF (significantly more than ERF27 ). A correlation analysis between the proportion of terms accepted at each search stage and cumulative RF (i.e., the sum of all precision at each slice in Figure 2 up to and including the end of the search stage) suggests that in both types of RF the quality of system terms improves as more RF is provided28 3.3.3 Summary The results from this section indicate that the location in a search affects the amount of feedback given by the user to the system, and hence the amount of information that the RF mechanism has to decide which terms to offer the user. Further, trends in the data suggest that the complexity of the task affects how subjects provide IRF and the proportion of system terms accepted. 4. DISCUSSION AND IMPLICATIONS In this section we discuss the implications of the findings presented in the previous section for each research question. 4.1 Search Task The results of our study showed that ERF was preferred for less complex tasks and IRF for more complex tasks. From observations and subject comments we perceived that when using ERF systems subjects generally forgot to provide the feedback but also employed different criteria during the ERF process (i.e., they were assessing relevance rather than expressing an interest). When the search was more complex subjects rarely found results they regarded as completely relevant. Therefore they struggled to find relevant 26 ERF: \u03c72 (2) = 2.22, p = .33; IRF: \u03c72 (2) = 7.73, p = .021 (Friedman Rank Sum Tests); IRF: all pair-wise comparisons significant at Z \u2265 1.77, all p \u2264 .038 (Dunn\"s post-hoc tests) 27 all T(48) \u2265 786, all p \u2264 .002, (Wilcoxon Signed-Rank Test) 28 IRF: r = .712, p < .001, ERF: r = .695, p = .001 (Pearson Correlation Coefficient) information and were unable to communicate RF to the search system. In these situations subjects appeared to prefer IRF as they do not need to make a relevance decision to obtain the benefits of RF, i.e., term suggestions, whereas in ERF they do. The association between RF method and task complexity has implications for the design of user studies of RF systems and the RF systems themselves. It implies that in the design of user studies involving ERF or IRF systems care should be taken to include tasks of varying complexities, to avoid task bias. Also, in the design of search systems it implies that since different types of RF may be appropriate for different task complexities then a system that could automatically detect complexity could use both ERF and IRF simultaneously to benefit the searcher. For example, on the IRF system we noticed that as task complexity falls search behaviour shifts from results interface to retrieved documents. Monitoring such interaction across a number of studies may lead to a set of criteria that could help IR systems automatically detect task complexity and tailor support to suit. 4.2 Search Experience We analysed the affect of search experience on the utility of IRF. Our analysis revealed a general preference across all subjects for IRF over ERF. That is, the average ratings assigned to IRF were generally more positive than those assigned to ERF. However, IRF was generally liked by both subject groups (perhaps because it removed the burden of providing relevance information) and ERF was generally preferred by experienced subjects more than inexperienced subjects (perhaps because it allowed them to specify which results were used by the system when generating term recommendations). All subjects felt more in control with ERF than IRF, but for inexperienced subjects this did not appear to affect their overall preferences29 . These subjects may understand the RF process less, but may be more willing to sacrifice control over feedback in favour of IRF, a process that they perceive more positively. 4.3 Search Stage We also analysed the effects of search stage on the use and usefulness of IRF. Through analysis of this nature we can build a more complete picture of how searchers used RF and how this varies based on the RF method. The results suggest that IRF is used more in the middle of the search than at the beginning or end, whereas ERF is used more towards the end. The results also show the effects of task complexity on the IRF process and how rapidly subjects reach their most interactive point. Without an analysis of this type it would not have been possible to establish the existence of such patterns of behaviour. The findings suggest that searchers interact differently for IRF and ERF. Since ERF is not traditionally used until toward the end of the search it may be possible to incorporate both IRF and ERF into the same IR system, with IRF being used to gather evidence until subjects decide to use ERF. The development of such a system represents part of our ongoing work in this area. 5. CONCLUSIONS In this paper we have presented an investigation of Implicit Relevance Feedback (IRF). We aimed to answer three research questions about factors that may affect the provision and usefulness of IRF. These factors were search task complexity, the subjects\" search experience and the stage in the search. Our overall conclusion was that all factors 29 This may also be true for experienced subjects, but the data we have is insufficient to draw this conclusion. appear to have some effect on the use and effectiveness of IRF, although the interaction effects between factors are not statistically significant. Our conclusions per each research question are: (i) IRF is generally more useful for complex search tasks, where searchers want to focus on the search task and get new ideas for their search from the system, (ii) IRF is preferred to ERF overall and generally preferred by inexperienced subjects wanting to reduce the burden of providing RF, and (iii) within a single search session IRF is affected by temporal location in a search (i.e., it is used in the middle, not the beginning or end) and task complexity. Studies of this nature are important to establish the circumstances where a promising technique such as IRF are useful and those when it is not. It is only after such studies have been run and analysed in this way can we develop an understanding of IRF that allow it to be successfully implemented in operational IR systems.", "body1": "Information Retrieval (IR) systems are designed to help searchers solve problems. As the quality of the query submitted to the system directly affects the quality of search results, the issue of how to improve search queries has been studied extensively in IR research . Implicit Relevance Feedback (IRF) has been proposed as a way in which search queries can be improved by passively observing searchers as they interact. In this paper we present a study into the use and effectiveness of IRF in an online search environment. In this section we describe the user study conducted to address our research questions. 2.1 Systems Our study used two systems both of which suggested new query terms to the user. Document representations include the document title and a summary of the document; a list of top-ranking sentences (TRS) extracted from the top documents retrieved, scored in relation to the query, a sentence in the document summary, and each summary sentence in the context it occurs in the document (i.e., with the preceding and following sentence). This presentation of progressively more information from documents to aid relevance assessments has been shown to be effective in earlier work . 2.1.2 Explicit RF system This version of the system implements explicit RF. 2.1.3 Implicit RF system This system makes inferences about searcher interests based on the information with which they interact. 2.2 Tasks Search tasks were designed to encourage realistic search behaviour by our subjects. For each topic, three versions of each work task situation were devised, each version differing in their predicted level of task complexity. Subjects attempted three search tasks: one high complexity, one moderate complexity and one low complexity2 . MC Task: Moderate Complexity Whilst out for dinner one night, one of your friends\" guests is complaining about the price of petrol and the factors that cause it. LC Task: Low Complexity While out for dinner one night, your friend complains about the rising price of petrol. 2.3 Subjects 156 volunteers expressed an interest in participating in our study. The average age of the subjects was 22.83 years (maximum 51, minimum 18, \u03c3 = 5.23 years) and 75% had a university diploma or a higher degree. 2.4 Methodology The experiment had a factorial design; with 2 levels of search experience, 3 experimental systems (although we only report on the findings from the ERF and IRF systems) and 3 levels of search task complexity. Questionnaires used Likert scales, semantic differentials and openended questions to elicit subject opinions . A tutorial carried out prior to the experiment allowed subjects to use a non-feedback version of the system to attempt a practice task before using the first experimental system. This contained questions about the subject\"s education, general search experience, computer experience and Web search experience. iii. iv. vi. vii. viii. Subjects were told that their interaction may be used by the IRF system to help them as they searched. We now describe the findings of our analysis. In this section we use the data derived from the experiment to answer our research questions about the effect of search task complexity, search experience and stage in search on the use and effectiveness of IRF. 3.1 Search Task Searchers attempted three search tasks of varying complexity, each on a different experimental system. 3.1.1 Feedback We use questionnaires and system logs to gather data on subject perceptions and provision of RF for different search tasks. The average obtained differential values are shown in Table 1 for IRF and each task category. Each cell in Table 1 summarises the subject responses for 16 tasksystem pairs (16 subjects who ran a high complexity (HC) task on the ERF system, 16 subjects who ran a medium complexity (MC) task on the ERF system, etc). Explicit RF Implicit RF Differential HC MC LC HC MC LC Easy 2.78 2.47 2.12 1.86 1.81 1.93 Effective 2.94 2.68 2.44 2.04 2.41 2.66 Useful 2.76 2.51 2.16 1.91 2.37 2.56 All (1) 2.83 2.55 2.24 1.94 2.20 2.38 Comfortable 2.27 2.28 2.35 2.11 2.15 2.16 In control 2.01 1.97 1.93 2.73 2.68 2.61 All (2) 2.14 2.13 2.14 2.42 2.42 2.39 IRF was most effective and useful for more complex search tasks4 and that the differences in all pair-wise comparisons between tasks were significant5 . We analyse interaction logs generated by both interfaces to investigate the amount of RF subjects provided. There are a maximum of 14 representations per document: 4 topranking sentences, 1 title, 1 summary, 4 summary sentences and 4 summary sentences in document context. Explicit RF Implicit RF Measure HC MC LC HC MC LC Proportion Feedback 2.14 2.39 2.65 21.50 19.36 15.32 Documents Viewed 10.63 10.43 10.81 10.84 12.19 14.81 For IRF there is a clear pattern: as complexity increases the subjects viewed fewer documents but viewed more representations for each document. Correlation analysis revealed a strong negative correlation between the number of documents viewed and the amount of feedback searchers provide9 ; as the number of documents viewed increases the proportion of feedback falls (searchers view less representations of each document). 3.1.2 Terms The same RF algorithm was used to select query modification terms in all systems . Explicit RF Implicit RF Differential HC MC LC HC MC LC Relevant 2.50 2.46 2.41 1.94 2.35 2.68 Useful 2.61 2.61 2.59 2.06 2.54 2.70 Kruskal-Wallis Tests were applied within each type of RF. Explicit RF Implicit RFProportion of terms HC MC LC HC MC LC Accepted 65.31 67.32 68.65 67.45 67.24 67.59 The average number of terms accepted from IRF is approximately the same across all search tasks and generally the same as that of ERF14 As Table 2 shows, subjects marked fewer documents relevant for highly complex tasks . 3.1.3 Summary In this section we have presented an investigation on the effect of search task complexity on the utility of IRF. In our experiment subjects were divided into two groups based on their level of search experience, the frequency with which they searched and the types of searches they performed. 3.2.1 Feedback We analyse the results from the attitude statements described at the beginning of Section 3.1.1. 14 all T(16) \u2265 80, all p \u2264 .31, (Wilcoxon Signed-Rank Test) 15 ERF: \u03c72 (2) = 3.67, p = .16; IRF: \u03c72 (2) = 2.55, p = .28 (KruskalWallis Tests) Table 5. The results demonstrate a strong preference in inexperienced subjects for IRF; they found it more easy and effective than experienced subjects. In a similar way to Section 3.1.1 we analysed the proportion of feedback that searchers provided to the experimental systems. Explicit RF Implicit RF Differential Inexp. Relevant 2.58 2.44 2.33 2.21 Useful 2.88 2.63 2.33 2.23 The differences between subject groups were significant21 Experienced subjects generally reacted to the query modification terms chosen by the system more positively than inexperienced 16 easy: U(24) = 391, p = .016; effective: U(24) = 399, p = .011; \u03b1 = (Mann-Whitney Tests) 17 all T(24) \u2265 231, all p \u2264 .001 (Wilcoxon Signed-Rank Test) 18 U(24) = 390, p = .018; \u03b1 = (Mann-Whitney Test) 19 T(24) = 222, p = .020 (Wilcoxon Signed-Rank Test) 20 ERF: all U(24) \u2264 319, p \u2265 .26, IRF: all U(24) \u2264 313, p \u2265 .30 (MannWhitney Tests) 21 ERF: all U(24) \u2265 388, p \u2264 .020, IRF: all U(24) \u2265 384, p \u2264 .024 Explicit RF Implicit RF Differential Inexp. Easy 2.46 2.46 1.84 1.98 Effective 2.75 2.63 2.32 2.43 Useful 2.50 2.46 2.28 2.27 All (1) 2.57 2.52 2.14 2.23 Comfortable 2.46 2.14 2.05 2.24 In control 1.96 1.98 2.73 2.64 All (2) 2.21 2.06 2.39 2.44 subjects. Table 7. Explicit RF Implicit RFProportion of terms Inexp. Accepted 63.76 70.44 64.43 71.35 Our analysis of the data show that differences between subject groups for each type of RF are significant; experienced subjects accepted more expansion terms regardless of type of RF. 3.3 Search Stage From our observations of experimental subjects as they searched we conjectured that RF may be used differently at different times during a search. In this section we use data gathered from interaction logs and subject opinions to investigate the extent to which RF was used and the extent to which it appeared to benefit our experimental subjects at different stages in their search 3.3.1 Feedback The interaction logs for all searches on the Explicit RF and Implicit RF were analysed and each search is divided up into nine equal length time slices. Figure 2 appears to show the existence of a relationship between the stage in the search and the amount of relevance information provided to the different types of feedback algorithm. Figure 2 also shows the proportion of feedback for tasks of different complexity. 24 Although increasing toward the end of the start stage. 25 Although not statistically significant; \u03c72 (2) = 3.54, p = .17 (Friedman Rank Sum Test) 3.3.2 Terms The terms recommended by the system are chosen based on the frequency of their occurrence in the relevant items. Explicit RF Implicit RFProportion of terms start middle end start middle end Accepted 66.87 66.98 67.34 61.85 68.54 73.22 The results show an apparent association between the stage in the search and the number of feedback terms subjects accept. In this section we discuss the implications of the findings presented in the previous section for each research question. 4.1 Search Task The results of our study showed that ERF was preferred for less complex tasks and IRF for more complex tasks. The association between RF method and task complexity has implications for the design of user studies of RF systems and the RF systems themselves. 4.2 Search Experience We analysed the affect of search experience on the utility of IRF. 4.3 Search Stage We also analysed the effects of search stage on the use and usefulness of IRF. The findings suggest that searchers interact differently for IRF and ERF.", "body2": "However, it is also possible to offer support to searchers for different search activities, such as selecting the terms to present to the system or choosing which search strategy to adopt ; both of which can be problematic for searchers. However, in practice RF techniques have been underutilised as they place an increased cognitive burden on searchers to directly indicate relevant results . IRF has been shown to display mixed effectiveness because the factors that are good indicators of user interest are often erratic and the inferences drawn from user interaction are not always valid . In this paper we use data derived from that experiment to study factors affecting the utility of IRF. In this section we describe the user study conducted to address our research questions. We used the Web as the test collection in this study and Google1 as the underlying search engine. Interacting with a representation guides searchers to a different representation from the same document, e.g., moving the mouse over a document title displays a summary of the document. The searcher can also add or remove terms from the query at will. This system was used as a baseline against which the IRF system could be compared. The results of the main experiment indicated that these two systems were comparable in terms of effectiveness. These subjects were not involved in the main experiment. In our pilot tests (and in a posteriori analysis of the main experiment results) we verified that subjects reporting of individual task complexity matched our estimation of the complexity of the task. Unaware of any major differences, you decide to find out how and why petrol prices vary worldwide. Throughout the night they seem to be complaining about everything they can, reducing the credibility of their earlier statements so you decide to research which factors actually are important in determining the price of petrol in the UK. Varying task complexity (Petrol Prices topic). Subjects were not chosen and classified into their groups until they had completed an entry questionnaire that asked them about their search experience and computer use. All were familiar with Web searching, and some with searching in other domains. The order in which systems were used and search tasks attempted was randomised according to a Latin square experimental design. System logging was also used to record subject interaction. the subject was asked to complete an introductory questionnaire. This contained questions about the subject\"s education, general search experience, computer experience and Web search experience. the subject was given a tutorial on the interface, followed by a training topic on a version of the interface with no RF. The subject could terminate a search early if they were unable to find any more information they felt helped them complete the task. after completion of the search, the subject was asked to complete a post-search questionnaire. the remaining tasks were attempted by the subject, following steps v. and vi. the subject completed a post-experiment questionnaire and participated in a post-experiment interview. They were not told which behaviours would be used or how it would be used. We now describe the findings of our analysis. Our analysis uses data from questionnaires, post-experiment interviews and background system logging on the ERF and IRF systems. We present our findings in terms of the RF provided by subjects and the terms recommended by the systems. 2. the process of providing the feedback: How you conveyed relevance to the system made you feel: comfortable/uncomfortable, in control/not in control. Subject perceptions of RF method (lower = better). This correction reduces the number of Type I errors i.e., rejecting null hypotheses that are true. In tasks of lower complexity the subjects felt they were better able to provide feedback on whether or not documents were relevant to the task. This proportion measures the searcher\"s level of interaction with a document, we take it to measure the user\"s interest in the document: the more document representations viewed the more interested we assume a user is in the content of the document. Feedback and documents viewed. This has no effect on the number of RF terms chosen, but may affect the quality of the terms selected. In the next section we investigate the effect of task complexity on the terms chosen as a result of IRF. Subject perceptions of system terms (lower = better). Term Acceptance (percentage of top six terms). Differences may reside in the nature of the terms accepted; future work will investigate this issue. As such, levels of search experience may affect searchers\" use and perceptions of IRF. The data are the same as that analysed in the previous section, but here we focus on search experience rather than the search task. 13 This was the smallest number of query modification terms that were offered in both systems. It appears from these results that experienced subjects found ERF more useful and were more at ease with the ERF process. Exp. Table 7 shows the average number of accepted terms per subject group. Search experience appears to affect how subjects use the terms recommended as a result of the RF process. We do not divide tasks based on this limit as subjects often terminated their search in less than 15 minutes. Distribution of RF provision per search task. This would appear to correspond to stages of initial exploration, detailed analysis of document representations and storage and presentation of findings. 23 IRF: all Z \u2265 1.87, p \u2264 .031, ERF: start vs. end Z = 2.58, p = .005 (Dunn\"s post-hoc tests). 24 Although increasing toward the end of the start stage. Term Acceptance (proportion of top six terms). Further, trends in the data suggest that the complexity of the task affects how subjects provide IRF and the proportion of system terms accepted. In this section we discuss the implications of the findings presented in the previous section for each research question. In these situations subjects appeared to prefer IRF as they do not need to make a relevance decision to obtain the benefits of RF, i.e., term suggestions, whereas in ERF they do. Monitoring such interaction across a number of studies may lead to a set of criteria that could help IR systems automatically detect task complexity and tailor support to suit. These subjects may understand the RF process less, but may be more willing to sacrifice control over feedback in favour of IRF, a process that they perceive more positively. Without an analysis of this type it would not have been possible to establish the existence of such patterns of behaviour. The development of such a system represents part of our ongoing work in this area.", "introduction": "Information Retrieval (IR) systems are designed to help searchers solve problems. In the traditional interaction metaphor employed by Web search systems such as Yahoo! and MSN Search, the system generally only supports the retrieval of potentially relevant documents from the collection. However, it is also possible to offer support to searchers for different search activities, such as selecting the terms to present to the system or choosing which search strategy to adopt ; both of which can be problematic for searchers. As the quality of the query submitted to the system directly affects the quality of search results, the issue of how to improve search queries has been studied extensively in IR research . Techniques such as Relevance Feedback (RF) have been proposed as a way in which the IR system can support the iterative development of a search query by suggesting alternative terms for query modification. However, in practice RF techniques have been underutilised as they place an increased cognitive burden on searchers to directly indicate relevant results . Implicit Relevance Feedback (IRF) has been proposed as a way in which search queries can be improved by passively observing searchers as they interact. IRF has been implemented either through the use of surrogate measures based on interaction with documents (such as reading time, scrolling or document retention) or using interaction with browse-based result interfaces . IRF has been shown to display mixed effectiveness because the factors that are good indicators of user interest are often erratic and the inferences drawn from user interaction are not always valid . In this paper we present a study into the use and effectiveness of IRF in an online search environment. The study aims to investigate the factors that affect IRF, in particular three research questions: (i) is the use of and perceived quality of terms generated by IRF affected by the search task? (ii) is the use of and perceived quality of terms generated by IRF affected by the level of search experience of system users? (iii) is IRF equally used and does it generate terms that are equally useful at all search stages? This study aims to establish when, and under what circumstances, IRF performs well in terms of its use and the query modification terms selected as a result of its use. The main experiment from which the data are taken was designed to test techniques for selecting query modification terms and techniques for displaying retrieval results . In this paper we use data derived from that experiment to study factors affecting the utility of IRF.", "conclusion": "In this paper we have presented an investigation of Implicit Relevance Feedback (IRF).. We aimed to answer three research questions about factors that may affect the provision and usefulness of IRF.. These factors were search task complexity, the subjects\" search experience and the stage in the search.. Our overall conclusion was that all factors 29 This may also be true for experienced subjects, but the data we have is insufficient to draw this conclusion.. appear to have some effect on the use and effectiveness of IRF, although the interaction effects between factors are not statistically significant.. Our conclusions per each research question are: (i) IRF is generally more useful for complex search tasks, where searchers want to focus on the search task and get new ideas for their search from the system, (ii) IRF is preferred to ERF overall and generally preferred by inexperienced subjects wanting to reduce the burden of providing RF, and (iii) within a single search session IRF is affected by temporal location in a search (i.e., it is used in the middle, not the beginning or end) and task complexity.. Studies of this nature are important to establish the circumstances where a promising technique such as IRF are useful and those when it is not.. It is only after such studies have been run and analysed in this way can we develop an understanding of IRF that allow it to be successfully implemented in operational IR systems."}
{"id": "C-33", "keywords": ["context-awar", "context provid", "negoti"], "title": "Rewards-Based Negotiation for Providing Context Information", "abstract": "How to provide appropriate context information is a challenging problem in context-aware computing. Most existing approaches use a centralized selection mechanism to decide which context information is appropriate. In this paper, we propose a novel approach based on negotiation with rewards to solving such problem. Distributed context providers negotiate with each other to decide who can provide context and how they allocate proceeds. In order to support our approach, we have designed a concrete negotiation model with rewards. We also evaluate our approach and show that it indeed can choose an appropriate context provider and allocate the proceeds fairly.", "references": ["Context is key", "Negotiation behavior", "Modelling and using imperfect context information", "Adaptive middleware for context-aware applications in smart-homes", "Providing contextual information to pervasive computing applications", "The design and applications of a context service", "Enhanced reputation mechanism for mobile ad-hoc networks", "The Art and Science of Negotiation", "Persuasive negotiation for autonomous agents: A rhetorical approach", "Inconsistency detection and resolution for context-aware middleware support"], "full_text": "1. INTRODUCTION Context-awareness is a key concept in pervasive computing. Context informs both recognition and mapping by providing a structured, unified view of the world in which the system operates . Context-aware applications exploit context information, such as location, preferences of users and so on, to adapt their behaviors in response to changing requirements of users and pervasive environments. However, one specific kind of context can often be provided by different context providers (sensors or other data sources of context information) with different quality levels. For example, in a smart home, thermometer A\"s measurement precision is 0.1 \u25e6 C, and thermometer B\"s measurement precision is 0.5 \u25e6 C. Thus A could provide more precise context information about temperature than B. Moreover, sometimes different context providers may provide conflictive context information. For example, different sensors report that the same person is in different places at the same time. Because context-aware applications utilize context information to adapt their behaviors, inappropriate context information may lead to inappropriate behavior. Thus we should design a mechanism to provide appropriate context information for current context-aware applications. In pervasive environments, context providers considered as relatively independent entities, have their own interests. They hope to get proceeds when they provide context information. However, most existing approaches consider context providers as entities without any personal interests, and use a centralized arbitrator provided by the middleware to decide who can provide appropriate context. Thus the burden of the middleware is very heavy, and its decision may be unfair and harm some providers\" interests. Moreover, when such arbitrator is broken down, it will cause serious consequences for context-aware applications. In this paper, we let distributed context providers themselves decide who provide context information. Since high reputation could help providers get more opportunities to provide context and get more proceeds in the future, providers try to get the right to provide good context to enhance their reputation. In order to get such right, context providers may agree to share some portion of the proceeds with its opponents. Thus context providers negotiate with each other to reach agreement on the issues who can provide context and how they allocate the proceeds. Our approach has some specific advantages: 1. We do not need an arbitrator provided by the middleware of pervasive computing to decide who provides context. Thus it will reduce the burden of the middleware. 2. It is more reasonable that distributed context providers decide who provide context, because it can avoid the serious consequences caused by a breakdown of a centralized arbitrator. 3. It can guarantee providers\" interests and provide fair proceeds allocation when providers negotiate with each other to reach agreement on their concerned problems. 4. This approach can choose an appropriate provider automatically. It does not need any applications and users\" intervention. The negotiation model we have designed to support our approach is also a novel model in negotiation domain. This model can help negotiators reach agreement in the present negotiation process by providing some guarantees over the outcome of next negotiation process (i.e. rewards). Negotiator may find current offer and reward worth more than counter-offer which will delay the agreement, and accepts current offer and reward. Without the reward, it may find current offer worth less than the counter-offer, and proposes its counter-offer. It will cost more time to reach agreement. It also expands the negotiation space considered in present negotiation process, and therefore provides more possibilities to find better agreement. The remainder of this paper is organized as follows. Section 2 presents some assumptions. Section 3 describes our approach based on negotiation detailedly, including utility functions, negotiation protocol and context providers\" strategies. Section 4 evaluates our approach. In section 5 we introduce some related work and conclude in section 6. 2. SOME ASSUMPTIONS Before introducing our approach, we would like to give some assumptions: 1. All context providers are well-meaning and honest. During the negotiation process, they exchange information honestly. Rewards confirmed in this negotiation process will be fulfilled in the next negotiation process. 2. All providers must guarantee the system\"s interests. They should provide appropriate context information for current applications. After guaranteeing the system\"s interest, they can try to maximize their own personal interests. The assumption is reasonable, because when an inappropriate context provider gets the right to provide bad context, as a punishment, its reputation will decrease, and the proceeds is also very small. 3. As context providers are independent, factors which influence their negotiation stance and behavior are private and not available to their opponents. Their utility functions are also private. 4. Since the negotiation takes place in pervasive environments, time is a critical factors. The current application often hopes to get context information as quickly as possible, so the time cost to reach agreement should be as short as possible. Context providers often have strict deadline by when the negotiation must be completed. After presenting these assumptions, we will propose our approach based on negotiation with rewards in the next section. 3. OUR APPROACH In the beginning, we introduce the concepts of reputation and Quality of Context (QoC) attributes. Both will be used in our approach. Reputation of an agent is a perception regarding its behavior norms, which is held by other agents, based on experiences and observation of its past actions . Here agent means context provider. Each provider\"s reputation indicates its historical ability to provide appropriate context information. Quality of Context (QoC) attributes characterize the quality of context information. When applications require context information, they should specify their QoC requirements which express constraints of QoC attributes. Context providers can specify QoC attributes for the context information they deliver. Although we can decide who provides appropriate context according to QoC requirements and context providers\" QoC information, applications\" QoC requirements might not reflect the actual quality requirements. Thus, in addition to QoC, reputation information of context providers is another factor affecting the decision who can provide context information. Negotiation is a process by which a joint decision is made by two or more parties. The parties first verbalize contradictory demands and then move towards agreement by a process of concession making or search for new alternatives . In pervasive environments, all available context providers negotiate with each other to decide who can provide context information. This process will be repeated because a kind of context is needed more than one time. Negotiation using persuasive arguments (such as threats, promises of future rewards, and appeals) allows negotiation parties to influence each others\" preferences to reach better deals effectively and efficiently . This pervasive negotiation is effective in repeated interaction because arguments can be constructed to directly impact future encounters. In this paper, for simplicity, we let negotiation take place between two providers. We extend Raiffa\"s basic model for bilateral negotiation , and allow negotiators to negotiate with each other by exchanging arguments in the form of promises of future rewards or requests for future rewards. Rewards mean some extra proceeds in the next negotiation process. They can influence outcomes of current and future negotiation. In our approach, as described by Figure 1, the current application requires Context Manager to provide a specific type of context information satisfying QoC requirements. Context Manager finds that provider A and B can provide such kind of context with different quality levels. Then the manager tells A and B to negotiate to reach agreement on who can provide the context information and how they will allocate the proceeds. Both providers get reputation information from the database Reputation of Context Providers and QoC requirements, and then negotiate with each other according to our negotiation model. When negotiation is completed, the chosen provider will provide the context information to Context Manager, and then Context Manager delivers such information to the application and also stores it in Context Knowledge Base where current and historical context information is stored. The current application gives the feedback information about the provided context, and then Context Manager will update the chosen provider\"s reputation information according to the feedback information. Context Manager also provides the proceeds to providers according to the feedback information and the time cost on negotiation. In the following parts of this section, we describe our negotiation model in detail, including context providers\" utility functions to evaluate offers and rewards, negotiation protocol, and strategies to generate offers and rewards. Context Knowledge Base Reputation of Context Providers Context provider A Context Manager Negotiate Application\"s QoC requirements and feedback Provide QoC requirements and proceeds Manage Context Provide Context Getreputation Getreputation Update reputation information according to feedback Context provider B Figure 1: Negotiate to provide appropriate context information. 3.1 Utility function During the negotiation process, one provider proposes an offer and a reward to the other provider. An offer is noted as o = (c, p): c indicates the chosen context provider and its domain is Dc (i.e. the two context providers participating in the negotiation); p means the proposer\"s portion of the proceeds, and its domain is Dp = . Its opponent\"s portion of the proceeds is 1\u2212p. The reward ep\"s domain is Dep = , and |ep| means the extra portion of proceeds the proposer promises to provide or requests in the next negotiation process. ep < 0 means the proposer promises to provide reward, ep > 0 means the proposer requests reward and ep =0 means no reward. The opponent evaluates the offer and reward to decide to accept them or propose a counter-offer and a reward. Thus context providers should have utility functions to evaluate offers and rewards. Time is a critical factor, and only at times in the set T = {0, 1, 2, . . . tdeadline}, context providers can propose their offers. The set O include all available offers. Context provider A\"s utility function of the offer and reward at time t UA : O \u00d7 Dep \u00d7 T \u2192 [\u22121, 1] is defined as: UA(o,ep,t)=(wA 1 \u00b7UA c (c)+wA 2 \u00b7UA p (p)+wA 3 \u00b7UA ep(ep))\u00b7\u03b4A(t) (1) Similarly, the utility function of A\"s opponent (i.e. B) can be defined as: UB(o,ep,t)=(wB 1 \u00b7UB c (c)+wB 2 \u00b7UB p (1\u2212p)+wB 3 \u00b7UB ep(\u2212ep))\u00b7\u03b4B(t) In (1), wA 1 , wA 2 and wA 3 are weights given to c, p and ep respectively, and wA 1 + wA 2 + wA 3 =1. Usually, the context provider pays the most attention to the system\"s interests, pays the least attention to the reward, thus wA 1 > wA 2 > wA 3 . UA c : Dc \u2192 [\u22121, 1] is the utility function of the issue who provides context. This function is determined by two factors: the distance between c\"s QoC and current application\"s QoC requirements, and c\"s reputation. The two negotiators acquire c\"s QoC information from c, and we use the approach proposed in to calculate the distance between c\"s QoC and the application\"s Qoc requirements. The required context has n QoC attributes and let the application\"s wishes for this context be a = (a1, a2 . . . an) (where ai = means the application\"s indifference to the i-th QoC attribute), c\"s QoC attributes cp = (cp1, cp2 . . . cpn) (where cpi = means c\"s inability to provide a quantitative value for the i-th QoC attribute). Because numerical distance values of different properties are combined, e.g. location precision in metres with refresh rate in Hz, thus a standard scale for all dimension is needed. The scaling factors for the QoC attributes are s = (s1, s2 . . . sn). In addition, different QoC attributes may have different weights: w = (w1, w2 . . . wn). Then d = (d1, d2 . . . dn) di = (cpi \u2212 ai) \u00b7 si \u00b7 wi where cpi\u2212ai = 0 for ai = and cpi\u2212ai = o(ai) for cpi = ( o(.) determines the application\"s satisfaction or dissatisfaction when c is unable to provide an estimate of a QoC attribute, given the value wished for by the application). The distance can be linear distance (1-norm), Euclidean distance (2-norm), or the maximum distance (max-norm): |d| = |d1| + |d2| + . . . + |dn| (1 \u2212 norm) ||d||2 = |d1|2 + |d2|2 + . . . + |dn|2 (2 \u2212 norm) ||d||\u221e = max{|d1|, |d2| . . . |dn|} (max \u2212 norm) The detail description of this calculation can be found in . Reputation of c can be acquired from the database Reputation of Context Providers. UA c (c) : R \u00d7 Drep \u2192 [\u22121, 1] can be defined as: UA c (c) = wA c1 \u00b7 UA d (d) + wA c2 \u00b7 UA rep(rep) wA c1 and wA c2 are weights given to the distance and reputation respectively, and wA c1 + wA c2 = 1. Drep is the domain of reputation information. UA d : R \u2192 is a monotonedecreasing function and UA rep : Drep \u2192 [\u22121, 1] is a monotoneincreasing function. UA p : Dp \u2192 is the utility function of the portion of proceeds A will receive and it is also a monotone-increasing function. A\"s utility function of reward ep UA ep : Dep \u2192 [\u22121, 1] is also a monotone-increasing function and UA ep(0) = 0. \u03b4A : T \u2192 is the time discount function. It is also a monotone-decreasing function. When time t cost on negotiation increases, \u03b4A(t) will decrease, and the utility will also decrease. Thus both negotiators want to reach agreement as quickly as possible to avoid loss of utility. 3.2 Negotiation protocol When provider A and B have got QoC requirements and reputation information, they begin to negotiate. They first set their reserved (the lowest acceptable) utility which can guarantee the system\"s interests and their personal interests. When the context provider finds the utility of an offer and a reward is lower than its reserved utility, it will reject this proposal and terminate the negotiation process. The provider who starts the negotiation is chosen randomly. We assume A starts the negotiation, and it proposes offer o and reward ep to B according to its strategy (see subsection 3.3). When B receives the proposal from A, it uses its utility function to evaluate it. If it is lower than its reserved utility, the provider terminates the negotiation. Otherwise, if UB(o, ep, t) \u2265 UB(o , ep , t + 1) i.e. the utility of o and ep proposed by A at time t is greater than the utility of offer o\" and reward ep\" which B will propose to A at time t + 1, B will accept this offer and reward. The negotiation is completed. However, if UB(o, ep, t) < UB(o , ep , t + 1) then B will reject A\"s proposal, and propose its counter-offer and reward to A. When A receives B\"s counter-offer and reward, A evaluates them using its utility function, and compares the utility with the utility of offer and reward it wants to propose to B at time t+2, decides to accept it or give its counter-offer and reward. This negotiation process continues and in each negotiation round, context providers concede in order to reach agreement. The negotiation will be successfully finished when agreement is reached, or be terminated forcibly due to deadline or the utility lower than reserved utility. When negotiation is forced to be terminated, Context manager will ask A and B to calculate UA c (A), UA c (B), UB c (A) and UB c (B) respectively. If UA c (A) + UB c (A) > UA c (B) + UB c (B) Context Manager let A provide context. If UA c (A) + UB c (A) < UA c (B) + UB c (B) then B will get the right to provide context information. When UA c (A) + UB c (A) = UA c (B) + UB c (B) Context Manager will select a provider from A and B randomly. In addition, Context Manager allocates the proceeds between the two providers. Although we can select one provider when negotiation is terminated forcibly, however, this may lead to the unfair allocation of the proceeds. Moreover, more time negotiators cost on negotiation, less proceeds will be given. Thus negotiators will try to reach agreement as soon as possible in order to avoid unnecessary loss. When the negotiation is finished, the chosen provider provides the context information to Context Manager which will deliver the information to current application. According to the application\"s feedback information about this context, Context Manager updates the provider\"s reputation stored in Reputation of Context Providers. The provider\"s reputation may be enhanced or decreased. In addition, according to the feedback and the negotiation time, Context Manager will give proceeds to the provider. Then the provider will share the proceeds with its opponent according to the negotiation outcome and the reward confirmed in the last negotiation process. For example, in the last negotiation process A promised to give reward ep (0 \u2264 ep < 1) to B, and A\"s portion of the proceeds is p in current negotiation. Then A\"s actual portion of the proceeds is p \u00b7 (1 \u2212 ep), and its opponent B\"s portion of the proceeds is 1\u2212p+p\u00b7ep. 3.3 Negotiation strategy The context provider might want to pursue the right to provide context information blindly in order to enhance its reputation. However when it finally provides bad context information, its reputation will be decreased and the proceeds is also very small. Thus the context provider should take action according to its strategy. The aim of provider\"s negotiation strategy is to determine the best course of action which will result in a negotiation outcome maximizing its utility function (i.e how to generate an offer and a reward). In our negotiation model, the context provider generates its offer and reward according to its pervious offer and reward and the last one sent by its opponent. At the beginning of the negotiation, context providers initialize their offers and rewards according to their beliefs and their reserved utility. If context provider A considers that it can provide good context and wants to enhance reputation, then it will propose that A provides the context information, shares some proceeds with its opponent B, and even promises to give reward. However, if A considers that it may provide bad context, A will propose that its opponent B provide the context, and require B to share some proceeds and provide reward. During the negotiation process, we assume that at time t A proposes offer ot and reward ept to B, at time t + 1, B proposes counter-offer ot+1 and reward ept+1 to A. Then at time t + 2, when the utility of B\"s proposal is greater than A\"s reserved utility, A gives its response. Now we calculate the expected utility to be conceded at time t +2, we use Cu to express the conceded utility. Cu = (UA(ot, ept, t) \u2212 UA(ot+1, ept+1, t + 1)) \u00b7 cA(t + 2) (UA(ot, ept, t) > UA(ot+1, ept+1, t + 1), otherwise, A will accept B\"s proposal) where cA : T \u2192 is a monotoneincreasing function. cA(t) indicates A\"s utility concession rate1 . A concedes a little in the beginning before conceding significantly towards the deadline. Then A generates its offer ot+2 = (ct+2, pt+2) and reward ept+2 at time t + 2. The expected utility of A at time t + 2 is: UA(ot+2, ept+2, t + 2) = UA(ot, ept, t + 2) \u2212 Cu If UA(ot+2, ept+2, t + 2) \u2264 UA(ot+1, ept+1, t + 1) then A will accept B\"s proposal (i.e. ot+1 and ept+1). Otherwise, A will propose its counter-offer and reward based on Cu. We assume that Cu is distributed evenly on c, p and ep (i.e. the utility to be conceded on c, p and ep is 1 Cu respectively). If |UA c (ct)\u2212(UA c (ct)\u2212 Cu \u03b4A(t+2) )| \u2264 |UA c (ct+1)\u2212(UA c (ct)\u2212 Cu \u03b4A(t+2) )| i.e. the expected utility of c at time t+2 is UA c (ct)\u2212 Cu \u03b4A(t+2) and it is closer to the utility of A\"s proposal ct at time t, then at time t + 2, ct+2 = ct, else the utility is closer to B\"proposal ct+1 and ct+2 = ct+1. When ct+2 is equal to ct, the actual conceded utility of c is 0, and the total concession of p and ep is Cu. We divide the total concession of p and ep evenly, and get the conceded utility of p and ep respectively. We calculate pt+2 and ept+2 as follows: pt+2 = (UA p )\u22121 (UA p (pt) \u2212 Cu \u03b4A(t + 2) ept+2 = (UA ep)\u22121 (UA ep(ept) \u2212 Cu \u03b4A(t + 2) When ct+2 is equal to ct+1, the actual conceded utility of c is |UA c (ct+2) \u2212 UA c (ct)|, the total concession of p and ep is Cu \u03b4A(t+2) \u2212 |UA c (ct+2) \u2212 UA c (ct)|, then: pt+2 = (UA p )\u22121 (UA p (pt)\u2212 Cu \u03b4A(t + 2) \u2212|UA c (ct+2)\u2212UA c (ct)|)) ept+2 = (UA ep)\u22121 (UA ep(ept)\u22121 ( Cu \u03b4A(t+2) \u2212|UA c (ct+2)\u2212UA c (ct)|)) Now, we have generated the offer and reward A will propose at time t + 2. Similarly, B also can generate its offer and reward. For example, cA(t) = ( t tdeadline \u03b2 (0 < \u03b2 < 1) Utility function and weight of c, p and ep Uc, w1 Up, w2 Uep, w3 A 0.5(1 \u2212 dA 500 ) + 0. , 0.6 0.9p, 0.3 0.9ep, 0.1 B 0.52(1 \u2212 dB 500 ) + 0. , 0.5 0.9p, 0.45 0.8ep, 0.05 Table 1: Utility functions and weights of c, p and ep for each provider 4. EVALUATION In this section, we evaluate the effectiveness of our approach by simulated experiments. Context providers A and B negotiate to reach agreement. They get QoC requirements and calculate the distance between Qoc requirements and their QoC. For simplicity, in our experiments, we assume that the distance has been calculated, and dA represents distance between QoC requirements and A\"s QoC, dB represents distance between QoC requirements and B\"s QoC. The domain of dA and dB is . We assume reputation value is a real number and its domain is , repA represents A\"s reputation value and repB represents B\"s reputation value. We assume that both providers pay the most attention to the system\"s interests, and pay the least attention to the reward, thus w1 > w2 > w3, and the weight of Ud approximates the weight of Urep. A and B\"s utility functions and weights of c, p and ep are defined in Table 1. We set deadline tdeadline = 100, and define time discount function \u03b4(t) and concession rate function c(t) of A and B as follows: \u03b4A(t) = 0.9t \u03b4B(t) = 0.88t cA(t) = ( tdeadline 0.8 cB(t) = ( tdeadline 0.6 Given different values of dA, dB, repA and repB, A and B negotiate to reach agreement. The provider that starts the negotiation is chosen at random. We hope that when dA dB and repA repB, A will get the right to provide context and get a major portion of the proceeds, and when \u2206d = dA \u2212 dB is in a small range (e.g. ) and \u2206rep = repA \u2212 repB is in a small range (e.g. ), A and B will get approximately equal opportunities to provide context, and allocate the proceeds evenly. When dA\u2212dB 500 approximates to dA\u2212 (i.e. the two providers\" abilities to provide context information are approximately equal), we also hope that A and B get equal opportunities to provide context and allocate the proceeds evenly. According to the three situations above, we make three experiments as follows: Experiment 1 : In this experiment, A and B negotiate with each other for 50 times, and at each time, we assign different values to dA, dB, repA, repB (satisfying dA dB and repA repB) and the reserved utilities of A and B. When the experiment is completed, we find 3 negotiation games are terminated due to the utility lower than the reserved utility. A gets the right to provide context for 47 times. The average portion of proceeds A get is about 0.683, and B\"s average portion of proceeds is 0.317. The average time cost to reach agreement is 8.4. We also find that when B asks A to provide context in its first offer, B can require and get more portion of the proceeds because of its goodwill. Experiment 2 : A and B also negotiate with each other for 50 times in this experiment given different values of dA, dB, repA, repB (satisfying \u221250 \u2264 \u2206d = dA \u2212 dB \u2264 50 and \u221250 \u2264 \u2206rep = drep \u2212drep \u2264 50) and the reserved utilities of A and B. After the experiment, we find that there are 8 negotiation games terminated due to the utility lower than the reserved utility. A and B get the right to provide context for 20 times and 22 times respectively. The average portion of proceeds A get is 0.528 and B\"s average portion of the proceeds is 0.472. The average time cost on negotiation is 10.5. Experiment 3 : In this experiment, A and B also negotiate with each other for 50 times given dA, dB, repA, repB (satisfying \u22120.2 \u2264 dA\u2212dB 500 \u2212 dA\u2212 \u2264 0.2) and the reserved utilities of A and B. There are 6 negotiation games terminated forcibly. A and B get the right to provide context for 21 times and 23 times respectively. The average portion of proceeds A get is 0.481 and B\"s average portion of the proceeds is 0.519. The average time cost on negotiation is 9.2. One thing should be mentioned is that except for d, rep, p and ep, other factors (e.g. weights, time discount function \u03b4(t) and concession rate function c(t)) could also affect the negotiation outcome. These factors should be adjusted according to providers\" beliefs at the beginning of each negotiation process. In our experiments, for similarity, we assign values to them without any particularity in advance. These experiments\" results prove that our approach can choose an appropriate context provider and can provide a relatively fair proceeds allocation. When one provider is obviously more appropriate than the other provider, the provider will get the right to provide context and get a major portion of the proceeds. When both providers have the approximately same abilities to provide context, their opportunities to provide context are equal and they can get about a half portion of the proceeds respectively. 5. RELATED WORK In , Huebscher and McCann have proposed an adaptive middleware design for context-aware applications. Their adaptive middleware uses utility functions to choose the best context provider (given the QoC requirements of applications and the QoC of alternative means of context acquisition). In our negotiation model, the calculation of utility function Uc was inspired by this approach. Henricksen and Indulska propose an approach to modelling and using imperfect information in . They characterize various types and sources of imperfect context information and present a set of novel context modelling constructs. They also outline a software infrastructure that supports the management and use of imperfect context information. Judd and Steenkiste in describe a generic interface to query context services allowing clients to specify their quality requirements as bounds on accuracy, confidence, update time and sample interval. In , Lei et al. present a context service which accepts freshness and confidence meta-data from context sources, and passes this along to clients so that they can adjust their level of trust accordingly. presents a framework for realizing dynamic context consistency management. The framework supports inconsistency detection based on a semantic matching and inconsistency triggering model, and inconsistency resolution with proactive actions to context sources. Most approaches to provide appropriate context utilize a centralized arbitrator. In our approach, we let distributed context providers themselves decide who can provide appropriate context information. Our approach can reduce the burden of the middleware, because we do not need the middleware to provide a context selection mechanism. It can avoid the serious consequences caused by a breakdown of the arbitrator. Also, it can guarantee context providers\" interests. 6. CONCLUSION AND FUTURE WORK How to provide the appropriate context information is a challenging problem in pervasive computing. In this paper, we have presented a novel approach based on negotiation with rewards to attempt to solve such problem. Distributed context providers negotiate with each other to reach agreement on the issues who can provide the appropriate context and how they allocate the proceeds. The results of our experiments have showed that our approach can choose an appropriate context provider, and also can guarantee providers\" interests by a relatively fair proceeds allocation. In this paper, we only consider how to choose an appropriate context provider from two providers. In the future work, this negotiation model will be extended, and more than two context providers can negotiate with each other to decide who is the most appropriate context provider. In the extended negotiation model, how to design efficient negotiation strategies will be a challenging problem. We assume that the context provider will fulfill its promise of reward in the next negotiation process. In fact, the context provider might deceive its opponent and provide illusive promise. We should solve this problem in the future. We also should deal with interactions which are interrupted by failing communication links in the future work.", "body1": "Context-awareness is a key concept in pervasive computing. In pervasive environments, context providers considered as relatively independent entities, have their own interests. They hope to get proceeds when they provide context information. 3. 4. The negotiation model we have designed to support our approach is also a novel model in negotiation domain. Negotiator may find current offer and reward worth more than counter-offer which will delay the agreement, and accepts current offer and reward. The remainder of this paper is organized as follows. Section 2 presents some assumptions. Before introducing our approach, we would like to give some assumptions: 1. 2. They should provide appropriate context information for current applications. 3. In the beginning, we introduce the concepts of reputation and Quality of Context (QoC) attributes. Here agent means context provider. In pervasive environments, all available context providers negotiate with each other to decide who can provide context information. Negotiation using persuasive arguments (such as threats, promises of future rewards, and appeals) allows negotiation parties to influence each others\" preferences to reach better deals effectively and efficiently . Context Manager finds that provider A and B can provide such kind of context with different quality levels. Context Knowledge Base Reputation of Context Providers Context provider A Context Manager Negotiate Application\"s QoC requirements and feedback Provide QoC requirements and proceeds Manage Context Provide Context Getreputation Getreputation Update reputation information according to feedback Context provider B Figure 1: Negotiate to provide appropriate context information. 3.1 Utility function During the negotiation process, one provider proposes an offer and a reward to the other provider. Time is a critical factor, and only at times in the set T = {0, 1, 2, . Context provider A\"s utility function of the offer and reward at time t UA : O \u00d7 Dep \u00d7 T \u2192 [\u22121, 1] is defined as: UA(o,ep,t)=(wA 1 \u00b7UA c (c)+wA 2 \u00b7UA p (p)+wA 3 \u00b7UA ep(ep))\u00b7\u03b4A(t) (1) Similarly, the utility function of A\"s opponent (i.e. UA c : Dc \u2192 [\u22121, 1] is the utility function of the issue who provides context. Then d = (d1, d2 . Reputation of c can be acquired from the database Reputation of Context Providers. 3.2 Negotiation protocol When provider A and B have got QoC requirements and reputation information, they begin to negotiate. When B receives the proposal from A, it uses its utility function to evaluate it. The negotiation is completed. When UA c (A) + UB c (A) = UA c (B) + UB c (B) Context Manager will select a provider from A and B randomly. When the negotiation is finished, the chosen provider provides the context information to Context Manager which will deliver the information to current application. According to the application\"s feedback information about this context, Context Manager updates the provider\"s reputation stored in Reputation of Context Providers. 3.3 Negotiation strategy The context provider might want to pursue the right to provide context information blindly in order to enhance its reputation. At the beginning of the negotiation, context providers initialize their offers and rewards according to their beliefs and their reserved utility. During the negotiation process, we assume that at time t A proposes offer ot and reward ept to B, at time t + 1, B proposes counter-offer ot+1 and reward ept+1 to A. Cu = (UA(ot, ept, t) \u2212 UA(ot+1, ept+1, t + 1)) \u00b7 cA(t + 2) (UA(ot, ept, t) > UA(ot+1, ept+1, t + 1), otherwise, A will accept B\"s proposal) where cA : T \u2192 is a monotoneincreasing function. Otherwise, A will propose its counter-offer and reward based on Cu. In this section, we evaluate the effectiveness of our approach by simulated experiments. The domain of dA and dB is . According to the three situations above, we make three experiments as follows: Experiment 1 : In this experiment, A and B negotiate with each other for 50 times, and at each time, we assign different values to dA, dB, repA, repB (satisfying dA dB and repA repB) and the reserved utilities of A and B. The average portion of proceeds A get is about 0.683, and B\"s average portion of proceeds is 0.317. Experiment 2 : A and B also negotiate with each other for 50 times in this experiment given different values of dA, dB, repA, repB (satisfying \u221250 \u2264 \u2206d = dA \u2212 dB \u2264 50 and \u221250 \u2264 \u2206rep = drep \u2212drep \u2264 50) and the reserved utilities of A and B. Experiment 3 : In this experiment, A and B also negotiate with each other for 50 times given dA, dB, repA, repB (satisfying \u22120.2 \u2264 dA\u2212dB 500 \u2212 dA\u2212 \u2264 0.2) and the reserved utilities of A and B. One thing should be mentioned is that except for d, rep, p and ep, other factors (e.g. In , Huebscher and McCann have proposed an adaptive middleware design for context-aware applications. In , Lei et al. Most approaches to provide appropriate context utilize a centralized arbitrator.", "body2": "Thus we should design a mechanism to provide appropriate context information for current context-aware applications. In pervasive environments, context providers considered as relatively independent entities, have their own interests. It is more reasonable that distributed context providers decide who provide context, because it can avoid the serious consequences caused by a breakdown of a centralized arbitrator. It can guarantee providers\" interests and provide fair proceeds allocation when providers negotiate with each other to reach agreement on their concerned problems. It does not need any applications and users\" intervention. rewards). It also expands the negotiation space considered in present negotiation process, and therefore provides more possibilities to find better agreement. The remainder of this paper is organized as follows. In section 5 we introduce some related work and conclude in section 6. Rewards confirmed in this negotiation process will be fulfilled in the next negotiation process. All providers must guarantee the system\"s interests. The assumption is reasonable, because when an inappropriate context provider gets the right to provide bad context, as a punishment, its reputation will decrease, and the proceeds is also very small. Their utility functions are also private. After presenting these assumptions, we will propose our approach based on negotiation with rewards in the next section. Reputation of an agent is a perception regarding its behavior norms, which is held by other agents, based on experiences and observation of its past actions . The parties first verbalize contradictory demands and then move towards agreement by a process of concession making or search for new alternatives . This process will be repeated because a kind of context is needed more than one time. In our approach, as described by Figure 1, the current application requires Context Manager to provide a specific type of context information satisfying QoC requirements. In the following parts of this section, we describe our negotiation model in detail, including context providers\" utility functions to evaluate offers and rewards, negotiation protocol, and strategies to generate offers and rewards. Context Knowledge Base Reputation of Context Providers Context provider A Context Manager Negotiate Application\"s QoC requirements and feedback Provide QoC requirements and proceeds Manage Context Provide Context Getreputation Getreputation Update reputation information according to feedback Context provider B Figure 1: Negotiate to provide appropriate context information. Thus context providers should have utility functions to evaluate offers and rewards. The set O include all available offers. Usually, the context provider pays the most attention to the system\"s interests, pays the least attention to the reward, thus wA 1 > wA 2 > wA 3 . wn). |dn|} (max \u2212 norm) The detail description of this calculation can be found in . Thus both negotiators want to reach agreement as quickly as possible to avoid loss of utility. We assume A starts the negotiation, and it proposes offer o and reward ep to B according to its strategy (see subsection 3.3). the utility of o and ep proposed by A at time t is greater than the utility of offer o\" and reward ep\" which B will propose to A at time t + 1, B will accept this offer and reward. If UA c (A) + UB c (A) < UA c (B) + UB c (B) then B will get the right to provide context information. Thus negotiators will try to reach agreement as soon as possible in order to avoid unnecessary loss. When the negotiation is finished, the chosen provider provides the context information to Context Manager which will deliver the information to current application. Then A\"s actual portion of the proceeds is p \u00b7 (1 \u2212 ep), and its opponent B\"s portion of the proceeds is 1\u2212p+p\u00b7ep. In our negotiation model, the context provider generates its offer and reward according to its pervious offer and reward and the last one sent by its opponent. However, if A considers that it may provide bad context, A will propose that its opponent B provide the context, and require B to share some proceeds and provide reward. Now we calculate the expected utility to be conceded at time t +2, we use Cu to express the conceded utility. ot+1 and ept+1). Similarly, B also can generate its offer and reward. For simplicity, in our experiments, we assume that the distance has been calculated, and dA represents distance between QoC requirements and A\"s QoC, dB represents distance between QoC requirements and B\"s QoC. the two providers\" abilities to provide context information are approximately equal), we also hope that A and B get equal opportunities to provide context and allocate the proceeds evenly. A gets the right to provide context for 47 times. We also find that when B asks A to provide context in its first offer, B can require and get more portion of the proceeds because of its goodwill. The average time cost on negotiation is 10.5. The average time cost on negotiation is 9.2. When both providers have the approximately same abilities to provide context, their opportunities to provide context are equal and they can get about a half portion of the proceeds respectively. Judd and Steenkiste in describe a generic interface to query context services allowing clients to specify their quality requirements as bounds on accuracy, confidence, update time and sample interval. The framework supports inconsistency detection based on a semantic matching and inconsistency triggering model, and inconsistency resolution with proactive actions to context sources. Also, it can guarantee context providers\" interests.", "introduction": "Context-awareness is a key concept in pervasive computing. Context informs both recognition and mapping by providing a structured, unified view of the world in which the system operates . Context-aware applications exploit context information, such as location, preferences of users and so on, to adapt their behaviors in response to changing requirements of users and pervasive environments. However, one specific kind of context can often be provided by different context providers (sensors or other data sources of context information) with different quality levels. For example, in a smart home, thermometer A\"s measurement precision is 0.1 \u25e6 C, and thermometer B\"s measurement precision is 0.5 \u25e6 C. Thus A could provide more precise context information about temperature than B. Moreover, sometimes different context providers may provide conflictive context information. For example, different sensors report that the same person is in different places at the same time. Because context-aware applications utilize context information to adapt their behaviors, inappropriate context information may lead to inappropriate behavior. Thus we should design a mechanism to provide appropriate context information for current context-aware applications. In pervasive environments, context providers considered as relatively independent entities, have their own interests. They hope to get proceeds when they provide context information. However, most existing approaches consider context providers as entities without any personal interests, and use a centralized arbitrator provided by the middleware to decide who can provide appropriate context. Thus the burden of the middleware is very heavy, and its decision may be unfair and harm some providers\" interests. Moreover, when such arbitrator is broken down, it will cause serious consequences for context-aware applications. In this paper, we let distributed context providers themselves decide who provide context information. Since high reputation could help providers get more opportunities to provide context and get more proceeds in the future, providers try to get the right to provide good context to enhance their reputation. In order to get such right, context providers may agree to share some portion of the proceeds with its opponents. Thus context providers negotiate with each other to reach agreement on the issues who can provide context and how they allocate the proceeds. Our approach has some specific advantages: 1. We do not need an arbitrator provided by the middleware of pervasive computing to decide who provides context. Thus it will reduce the burden of the middleware. It is more reasonable that distributed context providers decide who provide context, because it can avoid the serious consequences caused by a breakdown of a centralized arbitrator. It can guarantee providers\" interests and provide fair proceeds allocation when providers negotiate with each other to reach agreement on their concerned problems. This approach can choose an appropriate provider automatically. It does not need any applications and users\" intervention. The negotiation model we have designed to support our approach is also a novel model in negotiation domain. This model can help negotiators reach agreement in the present negotiation process by providing some guarantees over the outcome of next negotiation process (i.e. Negotiator may find current offer and reward worth more than counter-offer which will delay the agreement, and accepts current offer and reward. Without the reward, it may find current offer worth less than the counter-offer, and proposes its counter-offer. It will cost more time to reach agreement. It also expands the negotiation space considered in present negotiation process, and therefore provides more possibilities to find better agreement. The remainder of this paper is organized as follows. Section 3 describes our approach based on negotiation detailedly, including utility functions, negotiation protocol and context providers\" strategies. In section 5 we introduce some related work and conclude in section 6.", "conclusion": "How to provide the appropriate context information is a challenging problem in pervasive computing.. In this paper, we have presented a novel approach based on negotiation with rewards to attempt to solve such problem.. Distributed context providers negotiate with each other to reach agreement on the issues who can provide the appropriate context and how they allocate the proceeds.. The results of our experiments have showed that our approach can choose an appropriate context provider, and also can guarantee providers\" interests by a relatively fair proceeds allocation.. In this paper, we only consider how to choose an appropriate context provider from two providers.. In the future work, this negotiation model will be extended, and more than two context providers can negotiate with each other to decide who is the most appropriate context provider.. In the extended negotiation model, how to design efficient negotiation strategies will be a challenging problem.. We assume that the context provider will fulfill its promise of reward in the next negotiation process.. In fact, the context provider might deceive its opponent and provide illusive promise.. We should solve this problem in the future.. We also should deal with interactions which are interrupted by failing communication links in the future work."}
{"id": "J-4", "keywords": ["sponsor search", "search engin", "keyword auction"], "title": "Revenue Analysis of a Family of Ranking Rules for Keyword Auctions", "abstract": "Keyword auctions lie at the core of the business models of today's leading search engines. Advertisers bid for placement alongside search results, and are charged for clicks on their ads. Advertisers are typically ranked according to a score that takes into account their bids and potential click-through rates. We consider a family of ranking rules that contains those typically used to model Yahoo! and Google's auction designs as special cases. We find that in general neither of these is necessarily revenue-optimal in equilibrium, and that the choice of ranking rule can be guided by considering the correlation between bidders' values and click-through rates. We propose a simple approach to determine a revenue-optimal ranking rule within our family, taking into account effects on advertiser satisfaction and user experience. We illustrate the approach using Monte-Carlo simulations based on distributions fitted to Yahoo! bid and click-through rate data for a high-volume keyword.", "references": ["Truthful auctions for pricing search keywords", "Equilibrium bids in auctions of sponsored links: Theory and evidence", "Internet advertising and the Generalized Second Price auction: Selling billions of dollars worth of keywords", "Implementing sponsored search in Web search engines: Computational evaluation of alternative mechanisms", "Characterizing optimal keyword auctions", "The Art of Computer Programming", "Auction Theory", "An analysis of alternative slot auction designs for sponsored search", "Optimal auction design", "An Introduction to Copulas", "Position auctions"], "full_text": "1. INTRODUCTION Major search engines like Google, Yahoo!, and MSN sell advertisements by auctioning off space on keyword search results pages. For example, when a user searches the web for iPod, the highest paying advertisers (for example, Apple or Best Buy) for that keyword may appear in a separate sponsored section of the page above or to the right of the algorithmic results. The sponsored results are displayed in a format similar to algorithmic results: as a list of items each containing a title, a text description, and a hyperlink to a web page. Generally, advertisements that appear in a higher position on the page garner more attention and more clicks from users. Thus, all else being equal, advertisers prefer higher positions to lower positions. Advertisers bid for placement on the page in an auctionstyle format where the larger their bid the more likely their listing will appear above other ads on the page. By convention, sponsored search advertisers generally bid and pay per click, meaning that they pay only when a user clicks on their ad, and do not pay if their ad is displayed but not clicked. Overture Services, formerly GoTo.com and now owned by Yahoo! Inc., is credited with pioneering sponsored search advertising. Overture\"s success prompted a number of companies to adopt similar business models, most prominently Google, the leading web search engine today. Microsoft\"s MSN, previously an affiliate of Overture, now operates its own keyword auction marketplace. Sponsored search is one of the fastest growing, most effective, and most profitable forms of advertising, generating roughly $ after nearly doubling every year for the previous five years. The search engine evaluates the advertisers\" bids and allocates the positions on the page accordingly. Notice that, although bids are expressed as payments per click, the search engine cannot directly allocate clicks, but rather allocates impressions, or placements on the screen. Clicks relate only stochastically to impressions. Until recently, Yahoo! ranked bidders in decreasing order of advertisers\" stated values per click, while Google ranks in decreasing order of advertisers\" stated values per impression. In Google\"s case, value per impression is computed by multiplying the advertiser\"s (perclick) bid by the advertisement\"s expected click-through rate, where this expectation may consider a number of unspecified factors including historical click-through rate, position on the page, advertiser identity, user identity, and the context of other items on the page. We refer to these rules as rank-by-bid and rank-by-revenue, respectively.1 We analyze a family of ranking rules that contains the Yahoo! and Google models as special cases. We consider rank1 These are industry terms. We will see, however, that rankby-revenue is not necessarily revenue-optimal. 50 ing rules where bidders are ranked in decreasing order of score eq b, where e denotes an advertiser\"s click-through rate (normalized for position) and b his bid. Notice that q = 0 corresponds to Yahoo!\"s rank-by-bid rule and q = 1 corresponds to Google\"s rank-by-revenue rule. Our premise is that bidders are playing a symmetric equilibrium, as defined by Edelman, Ostrovsky, and Schwarz and Varian . We show through simulation that although q = 1 yields the efficient allocation, settings of q considerably less than 1 can yield superior revenue in equilibrium under certain conditions. The key parameter is the correlation between advertiser value and click-through rate. If this correlation is strongly positive, then smaller q are revenue-optimal. Our simulations are based on distributions fitted to data from Yahoo! keyword auctions. We propose that search engines set thresholds of acceptable loss in advertiser satisfaction and user experience, then choose the revenue-optimal q consistent with these constraints. We also compare the potential gains from tuning q with the gains from setting reserve prices, and find that the former may be much more significant. In Section 2 we give a formal model of keyword auctions, and establish its equilibrium properties in Section 3. In Section 4 we note that giving agents bidding credits can have the same effect as tuning the ranking rule explicitly. In Section 5 we give a general formulation of the optimal keyword auction design problem as an optimization problem, in a manner analogous to the single-item auction setting. We then provide some theoretical insight into how tuning q can improve revenue, and why the correlation between bidders\" values and click-through rates is relevant. In Section 6 we consider the effect of q on advertiser satisfaction and user experience. In Section 7 we describe our simulations and interpret their results. Related work. As mentioned the papers of Edelman et al. and Varian lay the groundwork for our study. Both papers independently define an appealing refinement of Nash equilibrium for keyword auctions and analyze its equilibrium properties. They called this refinement locally envy-free equilibrium and symmetric equilibrium, respectively. Varian also provides some empirical analysis. The general model of keyword auctions used here, where bidders are ranked according to a weight times their bid, was introduced by Aggarwal, Goel, and Motwani . That paper also makes a connection between the revenue of keyword auctions in incomplete information settings with the revenue in symmetric equilibrium. Iyengar and Kumar study the optimal keyword auction design problem in a setting of incomplete information, and also make the connection to symmetric equilibrium. We make use of this connection when formulating the optimal auction design problem in our setting. The work most closely related to ours is that of Feng, Bhargava, and Pennock . They were the first to realize that the correlation between bidder values and click-through rates should be a key parameter affecting the revenue performance of various ranking mechanisms. For simplicity, they assume bidders bid their true values, so their model is very different from ours and consequently so are their findings. According to their simulations, rank-by-revenue always (weakly) dominates rank-by-bid in terms of revenue, whereas our results suggest that rank-by-bid may do much better for negative correlations. Lahaie gives an example that suggests rank-by-bid should yield more revenue when values and click-through rates are positively correlated, whereas rank-by-revenue should do better when the correlation is negative. In this work we make a deeper study of this conjecture. 2. MODEL There are K positions to be allocated among N bidders, where N > K. We assume that the (expected) click-through rate of bidder s in position t is of the form esxt, i.e. separable into an advertiser effect es \u2208 and position effect xt \u2208 . We assume that x1 > x2 > . . . > xK > 0 and let xt = 0 for t > K. We also refer to es as the relevance of bidder s. It is useful to interpret xt as the probability that an ad in position t will be noticed, and es as the probability that it will be clicked on if noticed. Bidder s has value vs for each click. Bidders have quasilinear utility, so that the utility to bidder s of obtaining position t at a price of p per click is esxt(vs \u2212 p). A weight ws is associated with agent s, and agents bid for position. If agent s bids bs, his corresponding score is wsbs. Agents are ranked by score, so that the agent with highest score is ranked first, and so on. We assume throughout that agents are numbered such that agent s obtains position s. An agent pays per click the lowest bid necessary to retain his position, so that the agent in slot s pays ws+1 ws bs+1. The auctioneer may introduce a reserve score of r, so that an agent\"s ad appears only if his score is at least r. For agent s, this translates into a reserve price (minimum bid) of r/ws. 3. EQUILIBRIUM We consider the pure-strategy Nash equilibria of the auction game. This is a full-information concept. The motivation for this choice is that in a keyword auction, bidders are allowed to continuously adjust their bids over time, and hence obtain estimates of their profits in various positions. As a result it is reasonable to assume that if bids stabilize, bidders should be playing best-responses to each other\"s bids . Formally, in a Nash equilibrium of this game the following inequalities hold. esxs vs \u2212 ws+1 ws bs+1 \u2265 esxt vs \u2212 wt+1 ws bt+1 \u2200t > s (1) esxs vs \u2212 ws+1 ws bs+1 \u2265 esxt vs \u2212 wt ws bt \u2200t < s (2) Inequalities (1) and (2) state that bidder s does not prefer a lower or higher position to his own, respectively. It can be hard to derive any theoretical insight into the properties of these Nash equilibria-multiple allocations of positions to bidders can potentially arise in equilibrium . Edelman, Ostrovsky, and Schwarz introduced a refinement of Nash equilibrium called locally envy-free equilibrium that is more tractable to analyze; Varian independently proposed this solution concept and called it symmetric equilibrium. In a symmetric equilibrium, inequality (1) holds for all s, t rather than just for t > s. So for all s and all t = s, we have esxs vs \u2212 ws+1 ws bs+1 \u2265 esxt vs \u2212 wt+1 ws bt+1 51 or equivalently xs(wsvs \u2212 ws+1bs+1) \u2265 xt(wsvs \u2212 wt+1bt+1). (3) Edelman et al. note that this equilibrium arises if agents are raising their bids to increase the payments of those above them, a practice which is believed to be common in actual keyword auctions. Varian provides some empirical evidence that Google bid data agrees well with the hypothesis that bidders are playing a symmetric equilibrium. Varian does a thorough analysis of the properties of symmetric equilibrium, assuming ws = es = 1 for all bidders. It is straightforward to adapt his analysis to the case where bidders are assigned arbitrary weights and have separable click-through rates.2 As a result we find that in symmetric equilibrium, bidders are ranked in order of decreasing wsvs. To be clear, although the auctioneer only has access to the bids bs and not the values vs, in symmetric equilibrium the bids are such that ranking according to wsbs is equivalent to ranking according to wsvs. The smallest possible bid profile that can arise in symmetric equilibrium is given by the recursion xsws+1bs+1 = (xs \u2212 xs+1)ws+1vs+1 + xs+1ws+2bs+2. In this work we assume that bidders are playing the smallest symmetric equilibrium. This is an appropriate selection for our purposes: by optimizing revenue in this equilibrium, we are optimizing a lower bound on the revenue in any symmetric equilibrium. Unraveling the recursion yields xsws+1bs+1 = KX t=s (xt \u2212 xt+1)wt+1vt+1. (4) Agent s\"s total expected payment is es/ws times the quantity on the left-hand side of (4). The base case of the recursion occurs for s = K, where we find that the first excluded bidder bids his true value, as in the original analysis. Multiplying each of the inequalities (4) by the corresponding es/ws to obtain total payments, and summing over all positions, we obtain a total equilibrium revenue of KX s=1 KX t=s wt+1 ws es(xt \u2212 xt+1)vt+1. (5) To summarize, the minimum possible revenue in symmetric equilibrium can be computed as follows, given the agents\" relevance-value pairs (es, vs): first rank the agents in decreasing order of wsvs, and then evaluate (5). With a reserve score of r, it follows from inequality (3) that no bidder with wsvs < r would want to participate in the auction. Let K(r) be the number of bidders with wsvs \u2265 r, and assume it is at most K. We can impose a reserve score of r by introducing a bidder with value r and weight 1, and making him the first excluded bidder (who in symmetric equilibrium bids truthfully). In this case the recursion yields xsws+1bs+1 = K(r)\u22121 t=s (xt \u2212 xt+1)wt+1vt+1 + xK(r)r and the revenue formula is adapted similarly. If we redefine wsvs to be vs and wsbs to be bs, we recover Varian\"s setup and his original analysis goes through unchanged. 4. BIDDING CREDITS An indirect way to influence the allocation is to introduce bidding credits.3 Suppose bidder s is only required to pay a fraction cs \u2208 of the price he faces, or equivalently a (1 \u2212 cs) fraction of his clicks are received for free. Then in a symmetric equilibrium, we have esxs vs \u2212 ws+1 ws csbs+1 \u2265 esxt vs \u2212 wt+1 ws csbt+1 or equivalently xs ws cs vs \u2212 ws+1bs+1 \u2265 xt ws cs vs \u2212 wt+1bt+1 If we define ws = ws cs and bs = csbs, we recover inequality (3). Hence the equilibrium revenue will be as if we had used weights w rather than w. The bids will be scaled versions of the bids that arise with weights w (and no credits), where each bid is scaled by the corresponding factor 1/cs. This technique allows one to use credits instead of explicit changes in the weights to affect revenue. For instance, rankby-revenue will yield the same revenue as rank-by-bid if we set credits to cs = es. 5. REVENUE We are interested in setting the weights w to achieve optimal expected revenue. The setup is as follows. The auctioneer chooses a function g so that the weighting scheme is ws \u2261 g(es). We do not consider weights that also depend on the agents\" bids because this would invalidate the equilibrium analysis of the previous section.4 A pool of N bidders is then obtained by i.i.d. draws of value-relevance pairs from a common probability density f(es, vs). We assume the density is continuous and has full support on \u00d7[0, \u221e). The revenue to the auctioneer is then the revenue generated in symmetric equilibrium under weighting scheme w. This assumes the auctioneer is patient enough not to care about revenue until bids have stabilized. The problem of finding an optimal weighting scheme can be formulated as an optimization problem very similar to the one derived by Myerson for the single-item auction case (with incomplete information). Let Qsk(e, v; w) = 1 if agent s obtains slot k in equilibrium under weighting scheme w, where e = (e1, . . . , eN ) and v = (v1, . . . , vN ), and let it be 0 otherwise. Note that the total payment of agent s in equilibrium is esxs ws+1 ws bs+1 = KX t=s es(xt \u2212 xt+1) wt+1 ws vt+1 = esxsvs \u2212 Z vs KX k=1 esxkQsk(es, e\u2212s, y, v\u2212s; w) dy. The derivation then continues just as in the case of a singleitem auction . We take the expectation of this payment, Hal Varian suggested to us that bidding credits could be used to affect revenue in keyword auctions, which prompted us to look into this connection. The analysis does not generalize to weights that depend on bids. It is unclear whether an equilibrium would exist at all with such weights. 52 and sum over all agents to obtain the objective Z \u221e Z \u221e \" NX s=1 KX k=1 esxk\u03c8(es, vs)Qsk(e, v; w) f(e, v) dv de, where \u03c8 is the virtual valuation \u03c8(es, vs) = vs \u2212 1 \u2212 F(vs|es) f(vs|es) According to this analysis, we should rank bidders by virtual score es\u03c8(es, vs) to optimize revenue (and exclude any bidders with negative virtual score). However, unlike in the incomplete information setting, here we are constrained to ranking rules that correspond to a certain weighting scheme ws \u2261 g(es). We remark that the virtual score cannot be reproduced exactly via a weighting scheme. Lemma 1. There is no weighting scheme g such that the virtual score equals the score, for any density f. Proof. Assume there is a g such that e\u03c8(e, v) = g(e)v. (The subscript s is suppressed for clarity.) This is equivalent to dv log(1 \u2212 F(v|e)) = h(e)/v, (6) where h(e) = (g(e)/e\u22121)\u22121 . Let \u00afv be such that F(\u00afv|e) < 1; under the assumption of full support, there is always such a \u00afv. Integrating (6) with respect to v from 0 to \u00afv, we find that the left-hand side converges whereas the right-hand side diverges, a contradiction. Of course, to rank bidders by virtual score, we only need g(es)vs = h(es\u03c8(es, vs)) for some monotonically increasing transformation h. (A necessary condition for this is that \u03c8(es, vs) be increasing in vs for all es.) Absent this regularity condition, the optimization problem seems quite difficult because it is so general: we need to maximize expected revenue over the space of all functions g. To simplify matters, we now restrict our attention to the family of weights ws = eq s for q \u2208 (\u2212\u221e, +\u221e). It should be much simpler to find the optimum within this family, since it is just one-dimensional. Note that it covers rank-by-bid (q = 0) and rank-by-revenue (q = 1) as special cases. To see how tuning q can improve matters, consider again the equilibrium revenue: R(q) = KX s=1 KX t=s et+1 es \u00abq es(xt \u2212 xt+1)vt+1. (7) If the bidders are ranked in decreasing order of relevance, then et es \u2264 1 for t > s and decreasing q slightly without affecting the allocation will increase revenue. Similarly, if bidders are ranked in increasing order of relevance, increasing q slightly will yield an improvement. Now suppose there is perfect positive correlation between value and relevance. In this case, rank-by-bid will always lead to the same allocation as rank-by-revenue, and bidders will always be ranked in decreasing order of relevance. It then follows from (7) that q = 0 will yield more revenue in equilibrium than q = 1.5 It may appear that this contradicts the revenue-equivalence theorem , because mechanisms that always lead to the same allocation in equilibrium should yield the same revenue. Note though that with perfect correlation, there are If a good estimate of f is available, Monte-Carlo simulations can be used to estimate the revenue curve as a function of q, and the optimum can be located. Simulations can also be used to quantify the effect of correlation on the location of the optimum. We do this in Section 7. 6. EFFICIENCY AND RELEVANCE In principle the revenue-optimal parameter q may lie anywhere in (\u2212\u221e, \u221e). However, tuning the ranking rule also has consequences for advertiser satisfaction and user experience, and taking these into account reduces the range of allowable q. The total relevance of the equilibrium allocation is L(q) = KX s=1 esxs, i.e. the aggregate click-through rate. Presumably users find the ad display more interesting and less of a nuisance if they are more inclined to click on the ads, so we adopt total relevance as a measure of user experience. Let ps = ws+1 ws bs+1 be the price per click faced by bidder s. The total value (efficiency) generated by the auction in equilibrium is V (q) = KX s=1 esxsvs KX s=1 esxs(vs \u2212 ps) + KX s=1 esxsps. As we see total value can be reinterpreted as total profits to the bidders and auctioneer combined. Since we only consider deviations from maximum efficiency that increase the auctioneer\"s profits, any decrease in efficiency in our setting corresponds to a decrease in bidder profits. We therefore adopt efficiency as a measure of advertiser satisfaction. We would expect total relevance to increase with q, since more weight is placed on each bidder\"s individual relevance. We would expect efficiency to be maximized at q = 1, since in this case a bidder\"s weight is exactly his relevance. Proposition 1. Total relevance is non-decreasing in q. Proof. Recall that in symmetric equilibrium, bidders are ranked in order of decreasing wsvs. Let > 0. Perform an exchange sort to obtain the ranking that arises with q + starting from the ranking that arises with q (for a description of exchange sort and its properties,10). Assume that is large enough to make the rankings distinct. Agents s and t, where s is initially ranked lower than t, are swapped in the process if and only if the following conditions hold: eq svs \u2264 eq t vt eq+ s vs > eq+ t vt which together imply that es > et and hence es > et as > 0. At some point in the sort, agent s occupies some slot \u03b1, \u03b2 such that vs = \u03b1es + \u03b2. So the assumption of full support is violated, which is necessary for revenue equivalence. Recall that a density has full support over a given domain if every point in the domain has positive density. 53 k while agent t occupies slot k \u2212 1. After the swap, total relevance will have changed by the amount esxk\u22121 + etxk \u2212 etxk\u22121 \u2212 esxk = (es \u2212 et)(xk\u22121 \u2212 xk) > 0 As relevance strictly increases with each swap in the sort, total relevance is strictly greater when using q + rather than q. Proposition 2. Total value is non-decreasing in q for q \u2264 1 and non-increasing in q for q \u2265 1. Proof. Let q \u2265 1 and let > 0. Perform an exchange sort to obtain the second ranking from the first as in the previous proof. If agents s and t are swapped, where s was initially ranked lower than t, then es > et. This follows by the same reasoning as in the previous proof. Now e1\u2212q s \u2264 e1\u2212q t as 1 \u2212 q \u2264 0. This together with eq svs \u2264 eq t vt implies that esvs \u2264 etvt. Hence after swapping agents s and t, total value has not increased. The case for q \u2264 1 is similar. Since the trends described in Propositions 1 and 2 hold pointwise (i.e. for any set of bidders), they also hold in expectation. Proposition 2 confirms that efficiency is indeed maximized at q = 1. These results motivate the following approach. Although tuning q can optimize current revenue, this may come at the price of future revenue because advertisers and users may be lost, seeing as their satisfaction decreases. To guarantee future revenue will not be hurt too much, the auctioneer can impose bounds on the percent efficiency and relevance loss he is willing to tolerate, with q = 1 being a natural baseline. By Proposition 2, a lower bound on efficiency will yield upper and lower bounds on the search space for q. By Proposition 1, a lower bound on relevance will yield another lower bound on q. The revenue curve can then be plotted within the allowable range of q to find the revenue-optimal setting. 7. SIMULATIONS To add a measure of reality to our simulations, we fit distributions for value and relevance to Yahoo! bid and clickthrough rate data for a certain keyword that draws over a million searches per month. (We do not reveal the identity of the keyword to respect the privacy of the advertisers.) We obtained click and impression data for the advertisers bidding on the keyword. From this we estimated advertiser and position effects using a maximum-likelihood criterion. We found that, indeed, position effects are monotonically decreasing with lower rank. We then fit a beta distribution to the advertiser effects resulting in parameters a = 2.71 and b = 25.43. We obtained bids of advertisers for the keyword. Using Varian\"s technique, we derived bounds on the bidders\" actual values given these bids. By this technique, upper and lower bounds are obtained on bidder values given the bids according to inequality (3). If the interval for a given value is empty, i.e. its upper bound lies below its lower bound, then we compute the smallest perturbation to the bids necessary to make the interval non-empty, which involves solving a quadratic program. We found that the mean absolute deviation required to fit bids to symmetric equilibrium was 0 1 2 3 4 5 6 7 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 Value Density 0 0.05 0.1 0.15 0.2 0.25 10 Relevance Density Figure 1: Empirical marginal distributions of value and relevance. always at most 0.08, and usually significantly less, over different days in a period of two weeks.6 We fit a lognormal distribution to the lower bounds on the bidders\" values, resulting in parameters \u03bc = 0.35 and \u03c3 = 0.71. The empirical distributions of value and relevance together with the fitted lognormal and beta curves are given in Figure 1. It appears that mixtures of beta and lognormal distributions might be better fits, but since these distributions are used mainly for illustration purposes, we err on the side of simplicity. We used a Gaussian copula to create dependence between value and relevance.7 Given the marginal distributions for value and relevance together with this copula, we simulated the revenue effect of varying q for different levels of Spearman correlation, with 12 slots and 13 bidders. The results are shown in Figure 2.8 It is apparent from the figure that the optimal choice of q moves to the right as correlation decreases; this agrees with our intuition from Section 5. The choice is very sensitive to the level of correlation. If choosing only between rankby-bid and rank-by-revenue, rank-by-bid is best for positive correlation whereas rank-by-revenue is best for negative correlation. At zero correlation, they give about the same expected revenue in this instance. Figure 2 also shows that in principle, the optimal q may be negative. It may also occur beyond 1 for different distributions, but we do not know if these would be realistic. The trends in efficiency and relevance are as described in the results from Section 6. (Any small deviation from these trends is due to the randomness inherent in the simulations.) The curves level off as q \u2192 +\u221e because eventually agents are ranked purely according to relevance, and similarly as q \u2192 \u2212\u221e. A typical Spearman correlation between value and relevance for the keyword was about 0.4-for different days in a week the correlation lay within [0.36, 0.55]. Simulation results with this correlation are in Figure 3. In this instance rank-by-bid is in fact optimal, yielding 25% more revenue than rank-by-revenue. However, at q = 0 efficiency and relevance are 9% and 17% lower than at q = 1, respectively. Imposing a bound of, say, 5% on efficiency and relevance loss from the baseline at q = 1, the optimal setting is q = 0.6, yielding 11% more revenue than the baseline. See Varian for a definition of mean absolute deviation. A copula is a function that takes marginal distributions and gives a joint distribution with these marginals. It can be designed so that the variables are correlated. See for example Nelsen . The y-axes in Figures 2-4 have been normalized because the simulations are based on proprietary data. Only relative values are meaningful. 54 -2 -1.5 -1 -0.5 0 0.5 1 1.5 2 R(q) Revenue 10 -2 -1.5 -1 -0.5 0 0.5 1 1.5 2 V(q) Efficiency 0.6 0.8 1.2 1.4 1.6 1.8 2.2 2.4 2.6 -2 -1.5 -1 -0.5 0 0.5 1 1.5 2 L(q) Relevance Figure 2: Revenue, efficiency, and relevance for different parameters q under varying Spearman correlation (key at right). Estimated standard errors are less than 1% of the values shown. -1 -0.5 0.5 1.5 2.5 3.5 -2 -1.5 -1 -0.5 0 0.5 1 1.5 2 R(q) Revenue 2.5 3.5 4.5 5.5 6.5 7.5 -2 -1.5 -1 -0.5 0 0.5 1 1.5 2 V(q) Efficiency 0.8 1.2 1.4 1.6 1.8 2.2 2.4 -2 -1.5 -1 -0.5 0 0.5 1 1.5 2 L(q) Relevance Figure 3: Revenue, efficiency, and relevance for different parameters q with Spearman correlation of 0.4. Estimated standard errors are less than 1% of the values shown. We also looked into the effect of introducing a reserve score. Results are shown in Figure 4. Naturally, both efficiency and relevance suffer with an increasing reserve score. The optimal setting is r = 0.2, which gives only an 8% increase in revenue from r = 0. However, it results in a 13% efficiency loss and a 26% relevance loss. Tuning weights seems to be a much more desirable approach than introducing a reserve score in this instance. The reason why efficiency and relevance suffer more with a reserve score is that this approach will often exclude bidders entirely, whereas this never occurs when tuning weights. The two approaches are not mutually exclusive, however, and some combination of the two might prove better than either alone, although we did not investigate this possibility. 8. CONCLUSIONS In this work we looked into the revenue properties of a family of ranking rules that contains the Yahoo! and Google models as special cases. In practice, it should be very simple to move between rules within the family: this simply involves changing the exponent q applied to advertiser effects. We also showed that, in principle, the same effect could be obtained by using bidding credits. Despite the simplicity of the rule change, simulations revealed that properly tuning q can significantly improve revenue. In the simulations, the revenue improvements were greater than what could be obtained using reserve prices. On the other hand, we showed that advertiser satisfaction and user experience could suffer if q is made too small. We proposed that the auctioneer set bounds on the decrease in advertiser and user satisfaction he is willing to tolerate, which would imply bounds on the range of allowable q. With appropriate estimates for the distributions of value and relevance, and knowledge of their correlation, the revenue curve can then be plotted within this range to locate the optimum. There are several ways to push this research further. It would be interesting to do this analysis for a variety of keywords, to see if the optimal setting of q is always so sensitive to the level of correlation. If it is, then simply using rank-bybid where there is positive correlation, and rank-by-revenue where there is negative correlation, could be fine to a first approximation and already improve revenue. It would also be interesting to compare the effects of tuning q versus reserve pricing for keywords that have few bidders. In this instance reserve pricing should be more competitive, but this is still an open question. In principle the minimum revenue in Nash equilibrium can be found by linear programming. However, many allocations can arise in Nash equilibrium, and a linear program needs to be solved for each of these. There is as yet no efficient way to enumerate all possible Nash allocations, so finding the minimum revenue is currently infeasible. If this problem could be solved, we could run simulations for Nash equilibrium instead of symmetric equilibrium, to see if our insights are robust to the choice of solution concept. Larger classes of ranking rules could be relevant. For instance, it is possible to introduce discounts ds and rank according to wsbs \u2212 ds; the equilibrium analysis generalizes to this case as well. With this larger class the virtual score can equal the score, e.g. in the case of a uniform marginal distribution over values. It is unclear, though, whether such extensions help with more realistic distributions. 55 0.5 1.5 2.5 0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 R(r) Revenue 0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 V(r) Efficiency 0.5 1.5 2.5 0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 L(r) Relevance Figure 4: Revenue, efficiency, and relevance for different reserve scores r, with Spearman correlation of 0.4 and q = 1. samples.", "body1": "Major search engines like Google, Yahoo!, and MSN sell advertisements by auctioning off space on keyword search results pages. Overture Services, formerly GoTo.com and now owned by Yahoo! The search engine evaluates the advertisers\" bids and allocates the positions on the page accordingly. 50 ing rules where bidders are ranked in decreasing order of score eq b, where e denotes an advertiser\"s click-through rate (normalized for position) and b his bid. We show through simulation that although q = 1 yields the efficient allocation, settings of q considerably less than 1 can yield superior revenue in equilibrium under certain conditions. In Section 2 we give a formal model of keyword auctions, and establish its equilibrium properties in Section 3. Related work. Both papers independently define an appealing refinement of Nash equilibrium for keyword auctions and analyze its equilibrium properties. The general model of keyword auctions used here, where bidders are ranked according to a weight times their bid, was introduced by Aggarwal, Goel, and Motwani . The work most closely related to ours is that of Feng, Bhargava, and Pennock . There are K positions to be allocated among N bidders, where N > K. We assume that the (expected) click-through rate of bidder s in position t is of the form esxt, i.e. A weight ws is associated with agent s, and agents bid for position. Agents are ranked by score, so that the agent with highest score is ranked first, and so on. An agent pays per click the lowest bid necessary to retain his position, so that the agent in slot s pays ws+1 ws bs+1. We consider the pure-strategy Nash equilibria of the auction game. esxs vs \u2212 ws+1 ws bs+1 \u2265 esxt vs \u2212 wt+1 ws bt+1 \u2200t > s (1) esxs vs \u2212 ws+1 ws bs+1 \u2265 esxt vs \u2212 wt ws bt \u2200t < s (2) Inequalities (1) and (2) state that bidder s does not prefer a lower or higher position to his own, respectively. Edelman, Ostrovsky, and Schwarz introduced a refinement of Nash equilibrium called locally envy-free equilibrium that is more tractable to analyze; Varian independently proposed this solution concept and called it symmetric equilibrium. It is straightforward to adapt his analysis to the case where bidders are assigned arbitrary weights and have separable click-through rates.2 As a result we find that in symmetric equilibrium, bidders are ranked in order of decreasing wsvs. To be clear, although the auctioneer only has access to the bids bs and not the values vs, in symmetric equilibrium the bids are such that ranking according to wsbs is equivalent to ranking according to wsvs. The smallest possible bid profile that can arise in symmetric equilibrium is given by the recursion xsws+1bs+1 = (xs \u2212 xs+1)ws+1vs+1 + xs+1ws+2bs+2. In this work we assume that bidders are playing the smallest symmetric equilibrium. With a reserve score of r, it follows from inequality (3) that no bidder with wsvs < r would want to participate in the auction. An indirect way to influence the allocation is to introduce bidding credits.3 Suppose bidder s is only required to pay a fraction cs \u2208 of the price he faces, or equivalently a (1 \u2212 cs) fraction of his clicks are received for free. We are interested in setting the weights w to achieve optimal expected revenue. The problem of finding an optimal weighting scheme can be formulated as an optimization problem very similar to the one derived by Myerson for the single-item auction case (with incomplete information). The derivation then continues just as in the case of a singleitem auction . The analysis does not generalize to weights that depend on bids. 52 and sum over all agents to obtain the objective Z \u221e Z \u221e \" NX s=1 KX k=1 esxk\u03c8(es, vs)Qsk(e, v; w) f(e, v) dv de, where \u03c8 is the virtual valuation \u03c8(es, vs) = vs \u2212 1 \u2212 F(vs|es) f(vs|es) According to this analysis, we should rank bidders by virtual score es\u03c8(es, vs) to optimize revenue (and exclude any bidders with negative virtual score). Proof. (The subscript s is suppressed for clarity.) To simplify matters, we now restrict our attention to the family of weights ws = eq s for q \u2208 (\u2212\u221e, +\u221e). To see how tuning q can improve matters, consider again the equilibrium revenue: R(q) = KX s=1 KX t=s et+1 es \u00abq es(xt \u2212 xt+1)vt+1. In this case, rank-by-bid will always lead to the same allocation as rank-by-revenue, and bidders will always be ranked in decreasing order of relevance. In principle the revenue-optimal parameter q may lie anywhere in (\u2212\u221e, \u221e). The total relevance of the equilibrium allocation is L(q) = KX s=1 esxs, i.e. As we see total value can be reinterpreted as total profits to the bidders and auctioneer combined. We would expect efficiency to be maximized at q = 1, since in this case a bidder\"s weight is exactly his relevance. Proposition 1.10). 53 k while agent t occupies slot k \u2212 1. Proposition 2. Since the trends described in Propositions 1 and 2 hold pointwise (i.e. These results motivate the following approach. To add a measure of reality to our simulations, we fit distributions for value and relevance to Yahoo! We obtained bids of advertisers for the keyword. The empirical distributions of value and relevance together with the fitted lognormal and beta curves are given in Figure 1. We used a Gaussian copula to create dependence between value and relevance.7 Given the marginal distributions for value and relevance together with this copula, we simulated the revenue effect of varying q for different levels of Spearman correlation, with 12 slots and 13 bidders. A typical Spearman correlation between value and relevance for the keyword was about 0.4-for different days in a week the correlation lay within [0.36, 0.55]. See Varian for a definition of mean absolute deviation. A copula is a function that takes marginal distributions and gives a joint distribution with these marginals. 54 -2 -1.5 -1 -0.5 0 0.5 1 1.5 2 R(q) Revenue 10 -2 -1.5 -1 -0.5 0 0.5 1 1.5 2 V(q) Efficiency 0.6 0.8 1.2 1.4 1.6 1.8 2.2 2.4 2.6 -2 -1.5 -1 -0.5 0 0.5 1 1.5 2 L(q) Relevance Figure 2: Revenue, efficiency, and relevance for different parameters q under varying Spearman correlation (key at right). -1 -0.5 0.5 1.5 2.5 3.5 -2 -1.5 -1 -0.5 0 0.5 1 1.5 2 R(q) Revenue 2.5 3.5 4.5 5.5 6.5 7.5 -2 -1.5 -1 -0.5 0 0.5 1 1.5 2 V(q) Efficiency 0.8 1.2 1.4 1.6 1.8 2.2 2.4 -2 -1.5 -1 -0.5 0 0.5 1 1.5 2 L(q) Relevance Figure 3: Revenue, efficiency, and relevance for different parameters q with Spearman correlation of 0.4. We also looked into the effect of introducing a reserve score. The optimal setting is r = 0.2, which gives only an 8% increase in revenue from r = 0. The two approaches are not mutually exclusive, however, and some combination of the two might prove better than either alone, although we did not investigate this possibility.", "body2": "By convention, sponsored search advertisers generally bid and pay per click, meaning that they pay only when a user clicks on their ad, and do not pay if their ad is displayed but not clicked. Sponsored search is one of the fastest growing, most effective, and most profitable forms of advertising, generating roughly $ after nearly doubling every year for the previous five years. We will see, however, that rankby-revenue is not necessarily revenue-optimal. Our premise is that bidders are playing a symmetric equilibrium, as defined by Edelman, Ostrovsky, and Schwarz and Varian . We also compare the potential gains from tuning q with the gains from setting reserve prices, and find that the former may be much more significant. In Section 7 we describe our simulations and interpret their results. and Varian lay the groundwork for our study. Varian also provides some empirical analysis. We make use of this connection when formulating the optimal auction design problem in our setting. In this work we make a deeper study of this conjecture. Bidders have quasilinear utility, so that the utility to bidder s of obtaining position t at a price of p per click is esxt(vs \u2212 p). If agent s bids bs, his corresponding score is wsbs. We assume throughout that agents are numbered such that agent s obtains position s. The auctioneer may introduce a reserve score of r, so that an agent\"s ad appears only if his score is at least r. For agent s, this translates into a reserve price (minimum bid) of r/ws. Formally, in a Nash equilibrium of this game the following inequalities hold. It can be hard to derive any theoretical insight into the properties of these Nash equilibria-multiple allocations of positions to bidders can potentially arise in equilibrium . Varian does a thorough analysis of the properties of symmetric equilibrium, assuming ws = es = 1 for all bidders. It is straightforward to adapt his analysis to the case where bidders are assigned arbitrary weights and have separable click-through rates.2 As a result we find that in symmetric equilibrium, bidders are ranked in order of decreasing wsvs. To be clear, although the auctioneer only has access to the bids bs and not the values vs, in symmetric equilibrium the bids are such that ranking according to wsbs is equivalent to ranking according to wsvs. The smallest possible bid profile that can arise in symmetric equilibrium is given by the recursion xsws+1bs+1 = (xs \u2212 xs+1)ws+1vs+1 + xs+1ws+2bs+2. (5) To summarize, the minimum possible revenue in symmetric equilibrium can be computed as follows, given the agents\" relevance-value pairs (es, vs): first rank the agents in decreasing order of wsvs, and then evaluate (5). If we redefine wsvs to be vs and wsbs to be bs, we recover Varian\"s setup and his original analysis goes through unchanged. For instance, rankby-revenue will yield the same revenue as rank-by-bid if we set credits to cs = es. The revenue to the auctioneer is then the revenue generated in symmetric equilibrium under weighting scheme w. This assumes the auctioneer is patient enough not to care about revenue until bids have stabilized. Note that the total payment of agent s in equilibrium is esxs ws+1 ws bs+1 = KX t=s es(xt \u2212 xt+1) wt+1 ws vt+1 = esxsvs \u2212 Z vs KX k=1 esxkQsk(es, e\u2212s, y, v\u2212s; w) dy. We take the expectation of this payment, Hal Varian suggested to us that bidding credits could be used to affect revenue in keyword auctions, which prompted us to look into this connection. It is unclear whether an equilibrium would exist at all with such weights. There is no weighting scheme g such that the virtual score equals the score, for any density f. Assume there is a g such that e\u03c8(e, v) = g(e)v. Absent this regularity condition, the optimization problem seems quite difficult because it is so general: we need to maximize expected revenue over the space of all functions g. Note that it covers rank-by-bid (q = 0) and rank-by-revenue (q = 1) as special cases. Now suppose there is perfect positive correlation between value and relevance. We do this in Section 7. However, tuning the ranking rule also has consequences for advertiser satisfaction and user experience, and taking these into account reduces the range of allowable q. Let ps = ws+1 ws bs+1 be the price per click faced by bidder s. The total value (efficiency) generated by the auction in equilibrium is V (q) = KX s=1 esxsvs KX s=1 esxs(vs \u2212 ps) + KX s=1 esxsps. We would expect total relevance to increase with q, since more weight is placed on each bidder\"s individual relevance. We would expect efficiency to be maximized at q = 1, since in this case a bidder\"s weight is exactly his relevance. Total relevance is non-decreasing in q. Perform an exchange sort to obtain the ranking that arises with q + starting from the ranking that arises with q (for a description of exchange sort and its properties, see Knuth pp. Recall that a density has full support over a given domain if every point in the domain has positive density. After the swap, total relevance will have changed by the amount esxk\u22121 + etxk \u2212 etxk\u22121 \u2212 esxk = (es \u2212 et)(xk\u22121 \u2212 xk) > 0 As relevance strictly increases with each swap in the sort, total relevance is strictly greater when using q + rather than q. Total value is non-decreasing in q for q \u2264 1 and non-increasing in q for q \u2265 1. The case for q \u2264 1 is similar. Proposition 2 confirms that efficiency is indeed maximized at q = 1. The revenue curve can then be plotted within the allowable range of q to find the revenue-optimal setting. We then fit a beta distribution to the advertiser effects resulting in parameters a = 2.71 and b = 25.43. always at most 0.08, and usually significantly less, over different days in a period of two weeks.6 We fit a lognormal distribution to the lower bounds on the bidders\" values, resulting in parameters \u03bc = 0.35 and \u03c3 = 0.71. It appears that mixtures of beta and lognormal distributions might be better fits, but since these distributions are used mainly for illustration purposes, we err on the side of simplicity. The curves level off as q \u2192 +\u221e because eventually agents are ranked purely according to relevance, and similarly as q \u2192 \u2212\u221e. Imposing a bound of, say, 5% on efficiency and relevance loss from the baseline at q = 1, the optimal setting is q = 0.6, yielding 11% more revenue than the baseline. See Varian for a definition of mean absolute deviation. Only relative values are meaningful. Estimated standard errors are less than 1% of the values shown. Naturally, both efficiency and relevance suffer with an increasing reserve score. The reason why efficiency and relevance suffer more with a reserve score is that this approach will often exclude bidders entirely, whereas this never occurs when tuning weights. The two approaches are not mutually exclusive, however, and some combination of the two might prove better than either alone, although we did not investigate this possibility.", "introduction": "Major search engines like Google, Yahoo!, and MSN sell advertisements by auctioning off space on keyword search results pages. For example, when a user searches the web for iPod, the highest paying advertisers (for example, Apple or Best Buy) for that keyword may appear in a separate sponsored section of the page above or to the right of the algorithmic results. The sponsored results are displayed in a format similar to algorithmic results: as a list of items each containing a title, a text description, and a hyperlink to a web page. Generally, advertisements that appear in a higher position on the page garner more attention and more clicks from users. Thus, all else being equal, advertisers prefer higher positions to lower positions. Advertisers bid for placement on the page in an auctionstyle format where the larger their bid the more likely their listing will appear above other ads on the page. By convention, sponsored search advertisers generally bid and pay per click, meaning that they pay only when a user clicks on their ad, and do not pay if their ad is displayed but not clicked. Overture Services, formerly GoTo.com and now owned by Yahoo! Inc., is credited with pioneering sponsored search advertising. Overture\"s success prompted a number of companies to adopt similar business models, most prominently Google, the leading web search engine today. Microsoft\"s MSN, previously an affiliate of Overture, now operates its own keyword auction marketplace. Sponsored search is one of the fastest growing, most effective, and most profitable forms of advertising, generating roughly $ after nearly doubling every year for the previous five years. The search engine evaluates the advertisers\" bids and allocates the positions on the page accordingly. Notice that, although bids are expressed as payments per click, the search engine cannot directly allocate clicks, but rather allocates impressions, or placements on the screen. Clicks relate only stochastically to impressions. ranked bidders in decreasing order of advertisers\" stated values per click, while Google ranks in decreasing order of advertisers\" stated values per impression. In Google\"s case, value per impression is computed by multiplying the advertiser\"s (perclick) bid by the advertisement\"s expected click-through rate, where this expectation may consider a number of unspecified factors including historical click-through rate, position on the page, advertiser identity, user identity, and the context of other items on the page. We refer to these rules as rank-by-bid and rank-by-revenue, respectively.1 We analyze a family of ranking rules that contains the Yahoo! and Google models as special cases. We consider rank1 These are industry terms. We will see, however, that rankby-revenue is not necessarily revenue-optimal. 50 ing rules where bidders are ranked in decreasing order of score eq b, where e denotes an advertiser\"s click-through rate (normalized for position) and b his bid. Notice that q = 0 corresponds to Yahoo! \"s rank-by-bid rule and q = 1 corresponds to Google\"s rank-by-revenue rule. Our premise is that bidders are playing a symmetric equilibrium, as defined by Edelman, Ostrovsky, and Schwarz and Varian . We show through simulation that although q = 1 yields the efficient allocation, settings of q considerably less than 1 can yield superior revenue in equilibrium under certain conditions. The key parameter is the correlation between advertiser value and click-through rate. If this correlation is strongly positive, then smaller q are revenue-optimal. Our simulations are based on distributions fitted to data from Yahoo! We propose that search engines set thresholds of acceptable loss in advertiser satisfaction and user experience, then choose the revenue-optimal q consistent with these constraints. We also compare the potential gains from tuning q with the gains from setting reserve prices, and find that the former may be much more significant. In Section 2 we give a formal model of keyword auctions, and establish its equilibrium properties in Section 3. In Section 4 we note that giving agents bidding credits can have the same effect as tuning the ranking rule explicitly. In Section 5 we give a general formulation of the optimal keyword auction design problem as an optimization problem, in a manner analogous to the single-item auction setting. We then provide some theoretical insight into how tuning q can improve revenue, and why the correlation between bidders\" values and click-through rates is relevant. In Section 6 we consider the effect of q on advertiser satisfaction and user experience. In Section 7 we describe our simulations and interpret their results. As mentioned the papers of Edelman et al. and Varian lay the groundwork for our study. Both papers independently define an appealing refinement of Nash equilibrium for keyword auctions and analyze its equilibrium properties. They called this refinement locally envy-free equilibrium and symmetric equilibrium, respectively. Varian also provides some empirical analysis. The general model of keyword auctions used here, where bidders are ranked according to a weight times their bid, was introduced by Aggarwal, Goel, and Motwani . That paper also makes a connection between the revenue of keyword auctions in incomplete information settings with the revenue in symmetric equilibrium. Iyengar and Kumar study the optimal keyword auction design problem in a setting of incomplete information, and also make the connection to symmetric equilibrium. We make use of this connection when formulating the optimal auction design problem in our setting. The work most closely related to ours is that of Feng, Bhargava, and Pennock . They were the first to realize that the correlation between bidder values and click-through rates should be a key parameter affecting the revenue performance of various ranking mechanisms. For simplicity, they assume bidders bid their true values, so their model is very different from ours and consequently so are their findings. According to their simulations, rank-by-revenue always (weakly) dominates rank-by-bid in terms of revenue, whereas our results suggest that rank-by-bid may do much better for negative correlations. Lahaie gives an example that suggests rank-by-bid should yield more revenue when values and click-through rates are positively correlated, whereas rank-by-revenue should do better when the correlation is negative. In this work we make a deeper study of this conjecture.", "conclusion": "In this work we looked into the revenue properties of a family of ranking rules that contains the Yahoo!. and Google models as special cases.. In practice, it should be very simple to move between rules within the family: this simply involves changing the exponent q applied to advertiser effects.. We also showed that, in principle, the same effect could be obtained by using bidding credits.. Despite the simplicity of the rule change, simulations revealed that properly tuning q can significantly improve revenue.. In the simulations, the revenue improvements were greater than what could be obtained using reserve prices.. On the other hand, we showed that advertiser satisfaction and user experience could suffer if q is made too small.. We proposed that the auctioneer set bounds on the decrease in advertiser and user satisfaction he is willing to tolerate, which would imply bounds on the range of allowable q.. With appropriate estimates for the distributions of value and relevance, and knowledge of their correlation, the revenue curve can then be plotted within this range to locate the optimum.. There are several ways to push this research further.. It would be interesting to do this analysis for a variety of keywords, to see if the optimal setting of q is always so sensitive to the level of correlation.. If it is, then simply using rank-bybid where there is positive correlation, and rank-by-revenue where there is negative correlation, could be fine to a first approximation and already improve revenue.. It would also be interesting to compare the effects of tuning q versus reserve pricing for keywords that have few bidders.. In this instance reserve pricing should be more competitive, but this is still an open question.. In principle the minimum revenue in Nash equilibrium can be found by linear programming.. However, many allocations can arise in Nash equilibrium, and a linear program needs to be solved for each of these.. There is as yet no efficient way to enumerate all possible Nash allocations, so finding the minimum revenue is currently infeasible.. If this problem could be solved, we could run simulations for Nash equilibrium instead of symmetric equilibrium, to see if our insights are robust to the choice of solution concept.. Larger classes of ranking rules could be relevant.. For instance, it is possible to introduce discounts ds and rank according to wsbs \u2212 ds; the equilibrium analysis generalizes to this case as well.. With this larger class the virtual score can equal the score, e.g.. in the case of a uniform marginal distribution over values.. It is unclear, though, whether such extensions help with more realistic distributions.. 55 0.5 1.5 2.5 0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 R(r) Revenue 0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 V(r) Efficiency 0.5 1.5 2.5 0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 L(r) Relevance Figure 4: Revenue, efficiency, and relevance for different reserve scores r, with Spearman correlation of 0.4 and q = 1.. samples."}
{"id": "H-46", "keywords": ["expertis search", "expert find", "intranet search", "languag model"], "title": "Broad Expertise Retrieval in Sparse Data Environments", "abstract": "Expertise retrieval has been largely unexplored on data other than the W3C collection. At the same time, many intranets of universities and other knowledge-intensive organisations offer examples of relatively small but clean multilingual expertise data, covering broad ranges of expertise areas. We first present two main expertise retrieval tasks, along with a set of baseline approaches based on generative language modeling, aimed at finding expertise relations between topics and people. For our experimental evaluation, we introduce (and release) a new test set based on a crawl of a university site. Using this test set, we conduct two series of experiments. The first is aimed at determining the effectiveness of baseline expertise retrieval methods applied to the new test set. The second is aimed at assessing refined models that exploit characteristic features of the new test set, such as the organizational structure of the university, and the hierarchical structure of the topics in the test set. Expertise retrieval models are shown to be robust with respect to environments smaller than the W3C collection, and current techniques appear to be generalizable to other settings.", "references": ["Incorporating Context in the Language Modeling Framework for ad hoc Information Retrieval", "Finding similar experts", "Determining expert profiles (with an application to expert finding)", "Formal models for expert finding in enterprise corpora", "The role of artificial intelligence technologies in the implementation of people-finder knowledge management systems", "Expertise identification using email communications", "Integrating word relationships into language models", "Elements of Information Theory", "P@noptic expert: Searching for experts not just for documents", "Overview of the TREC-2005 Enterprise Track", "Working Knowledge: How Organizations Manage What They Know", "Accurate methods for the statistics of surprise and coincidence", "Tell me what you do and I'll tell you what you are: Learning occupation-related activities for biographies", "Relevance based language models", "Cross-lingual relevance models", "Voting for candidates: adapting data fusion techniques for an expert search task", "Foundations of Statistical Natural Language Processing", "Expertise browser: a quantitative approach to identifying expertise", "Hierarchical language models for expert finding in enterprise corpora", "Overview of the TREC 2006 Enterprise Track", "Language model information retrieval with document expansion"], "full_text": "1. INTRODUCTION An organization\"s intranet provides a means for exchanging information between employees and for facilitating employee collaborations. To efficiently and effectively achieve this, it is necessary to provide search facilities that enable employees not only to access documents, but also to identify expert colleagues. At the TREC Enterprise Track the need to study and understand expertise retrieval has been recognized through the introduction of Expert Finding tasks. The goal of expert finding is to identify a list of people who are knowledgeable about a given topic. This task is usually addressed by uncovering associations between people and topics ; commonly, a co-occurrence of the name of a person with topics in the same context is assumed to be evidence of expertise. An alternative task, which using the same idea of people-topic associations, is expert profiling, where the task is to return a list of topics that a person is knowledgeable about . The launch of the Expert Finding task at TREC has generated a lot of interest in expertise retrieval, with rapid progress being made in terms of modeling, algorithms, and evaluation aspects. However, nearly all of the expert finding or profiling work performed has been validated experimentally using the W3C collection from the Enterprise Track. While this collection is currently the only publicly available test collection for expertise retrieval tasks, it only represents one type of intranet. With only one test collection it is not possible to generalize conclusions to other realistic settings. In this paper we focus on expertise retrieval in a realistic setting that differs from the W3C setting-one in which relatively small amounts of clean, multilingual data are available, that cover a broad range of expertise areas, as can be found on the intranets of universities and other knowledge-intensive organizations. Typically, this setting features several additional types of structure: topical structure (e.g., topic hierarchies as employed by the organization), organizational structure (faculty, department, ...), as well as multiple types of documents (research and course descriptions, publications, and academic homepages). This setting is quite different from the W3C setting in ways that might impact upon the performance of expertise retrieval tasks. We focus on a number of research questions in this paper: Does the relatively small amount of data available on an intranet affect the quality of the topic-person associations that lie at the heart of expertise retrieval algorithms? How do state-of-the-art algorithms developed on the W3C data set perform in the alternative scenario of the type described above? More generally, do the lessons from the Expert Finding task at TREC carry over to this setting? How does the inclusion or exclusion of different documents affect expertise retrieval tasks? In addition to, how can the topical and organizational structure be used for retrieval purposes? To answer our research questions, we first present a set of baseline approaches, based on generative language modeling, aimed at finding associations between topics and people. This allows us to formulate the expert finding and expert profiling tasks in a uniform way, and has the added benefit of allowing us to understand the relations between the two tasks. For our experimental evaluation, we introduce a new data set (the UvT Expert Collection) which is representative of the type of intranet that we described above. Our collection is based on publicly available data, crawled from the website of Tilburg University (UvT). This type of data is particularly interesting, since (1) it is clean, heterogeneous, structured, and focused, but comprises a limited number of documents; (2) contains information on the organizational hierarchy; (3) it is bilingual (English and Dutch); and (4) the list of expertise areas of an individual are provided by the employees themselves. Using the UvT Expert collection, we conduct two sets of experiments. The first is aimed at determining the effectiveness of baseline expertise finding and profiling methods in this new setting. A second group of experiments is aimed at extensions of the baseline methods that exploit characteristic features of the UvT Expert Collection; specifically, we propose and evaluate refined expert finding and profiling methods that incorporate topicality and organizational structure. Apart from the research questions and data set that we contribute, our main contributions are as follows. The baseline models developed for expertise finding perform well on the new data set. While on the W3C setting the expert finding task appears to be more difficult than profiling, for the UvT data the opposite is the case. We find that profiling on the UvT data set is considerably more difficult than on the W3C set, which we believe is due to the large (but realistic) number of topical areas that we used for profiling: about 1,500 for the UvT set, versus 50 in the W3C case. Taking the similarity between topics into account can significantly improve retrieval performance. The best performing similarity measures are content-based, therefore they can be applied on the W3C (and other) settings as well. Finally, we demonstrate that the organizational structure can be exploited in the form of a context model, improving MAP scores for certain models by up to 70%. The remainder of this paper is organized as follows. In the next section we review related work. Then, in Section 3 we provide detailed descriptions of the expertise retrieval tasks that we address in this paper: expert finding and expert profiling. In Section 4 we present our baseline models, of which the performance is then assessed in Section 6 using the UvT data set that we introduce in Section 5. Advanced models exploiting specific features of our data are presented in Section 7 and evaluated in Section 8. We formulate our conclusions in Section 9. 2. RELATED WORK Initial approaches to expertise finding often employed databases containing information on the skills and knowledge of each individual in the organization . Most of these tools (usually called yellow pages or people-finding systems) rely on people to self-assess their skills against a predefined set of keywords. For updating profiles in these systems in an automatic fashion there is a need for intelligent technologies . More recent approaches use specific document sets (such as email or software ) to find expertise. In contrast with focusing on particular document types, there is also an increased interest in the development of systems that index and mine published intranet documents as sources of evidence for expertise. One such published approach is the P@noptic system , which builds a representation of each person by concatenating all documents associated with that person-this is similar to Model 1 of Balog et al. , who formalize and compare two methods. Balog et al.\"s Model 1 directly models the knowledge of an expert from associated documents, while their Model 2 first locates documents on the topic and then finds the associated experts. In the reported experiments the second method performs significantly better when there are sufficiently many associated documents per candidate. editions of the Expert Finding task at TREC implemented (variations on) one of these two models; see . Macdonald and Ounis propose a different approach for ranking candidate expertise with respect to a topic based on data fusion techniques, without using collectionspecific heuristics; they find that applying field-based weighting models improves the ranking of candidates. Petkova and Croft propose yet another approach, based on a combination of the above Model 1 and 2, explicitly modeling topics. Turning to other expert retrieval tasks that can also be addressed using topic-people associations, Balog and de Rijke addressed the task of determining topical expert profiles. While their methods proved to be efficient on the W3C corpus, they require an amount of data that may not be available in the typical knowledge-intensive organization. Balog and de Rijke study the related task of finding experts that are similar to a small set of experts given as input. As an aside, creating a textual summary of a person shows some similarities to biography finding, which has received a considerable amount of attention recently; see e.g., . We use generative language modeling to find associations between topics and people. In our modeling of expert finding and profiling we collect evidence for expertise from multiple sources, in a heterogeneous collection, and integrate it with the co-occurrence of candidates\" names and query terms-the language modeling setting allows us to do this in a transparent manner. Our modeling proceeds in two steps. In the first step, we consider three baseline models, two taken from (the Models 1 and 2 mentioned above), and one a refined version of a model introduced in (which we refer to as Model 3 below); this third model is also similar to the model described by Petkova and Croft . The models we consider in our second round of experiments are mixture models similar to contextual language models and to the expanded documents of Tao et al. ; however, the features that we use for definining our expansions-including topical structure and organizational structure-have not been used in this way before. 3. TASKS In the expertise retrieval scenario that we envisage, users seeking expertise within an organization have access to an interface that combines a search box (where they can search for experts or topics) with navigational structures (of experts and of topics) that allows them to click their way to an expert page (providing the profile of a person) or a topic page (providing a list of experts on the topic). To feed the above interface, we face two expertise retrieval tasks, expert finding and expert profiling, that we first define and then formalize using generative language models. In order to model either task, the probability of the query topic being associated to a candidate expert plays a key role in the final estimates for searching and profiling. By using language models, both the candidates and the query are characterized by distributions of terms in the vocabulary (used in the documents made available by the organization whose expertise retrieval needs we are addressing). 3.1 Expert finding Expert finding involves the task of finding the right person with the appropriate skills and knowledge: Who are the experts on topic X?. E.g., an employee wants to ascertain who worked on a particular project to find out why particular decisions were made without having to trawl through documentation (if there is any). Or, they may be in need a trained specialist for consultancy on a specific problem. Within an organization there are usually many possible candidates who could be experts for given topic. We can state this problem as follows: What is the probability of a candidate ca being an expert given the query topic q? That is, we determine p(ca|q), and rank candidates ca according to this probability. The candidates with the highest probability given the query are deemed the most likely experts for that topic. The challenge is how to estimate this probability accurately. Since the query is likely to consist of only a few terms to describe the expertise required, we should be able to obtain a more accurate estimate by invoking Bayes\" Theorem, and estimating: p(ca|q) = p(q|ca)p(ca) p(q) , (1) where p(ca) is the probability of a candidate and p(q) is the probability of a query. Since p(q) is a constant, it can be ignored for ranking purposes. Thus, the probability of a candidate ca being an expert given the query q is proportional to the probability of a query given the candidate p(q|ca), weighted by the a priori belief p(ca) that candidate ca is an expert. p(ca|q) \u221d p(q|ca)p(ca) (2) In this paper our main focus is on estimating the probability of a query given the candidate p(q|ca), because this probability captures the extent to which the candidate knows about the query topic. Whereas the candidate priors are generally assumed to be uniformand thus will not influence the ranking-it has been demonstrated that a sensible choice of priors may improve the performance . 3.2 Expert profiling While the task of expert searching was concerned with finding experts given a particular topic, the task of expert profiling seeks to answer a related question: What topics does a candidate know about? Essentially, this turns the questions of expert finding around. The profiling of an individual candidate involves the identification of areas of skills and knowledge that they have expertise about and an evaluation of the level of proficiency in each of these areas. This is the candidate\"s topical profile. Generally, topical profiles within organizations consist of tabular structures which explicitly catalogue the skills and knowledge of each individual in the organization. However, such practice is limited by the resources available for defining, creating, maintaining, and updating these profiles over time. By focusing on automatic methods which draw upon the available evidence within the document repositories of an organization, our aim is to reduce the human effort associated with the maintenance of topical profiles1 A topical profile of a candidate, then, is defined as a vector where each element i of the vector corresponds to the candidate ca\"s expertise on a given topic ki, (i.e., s(ca, ki)). Each topic ki defines a particular knowledge area or skill that the organization uses to define the candidate\"s topical profile. Thus, it is assumed that a list of topics, {k1, . . . , kn}, where n is the number of pre-defined topics, is given: profile(ca) = s(ca, k1), s(ca, k2), . . . , s(ca, kn) . (3) Context and evidence are needed to help users of expertise finding systems to decide whom to contact when seeking expertise in a particular area. Examples of such context are: Who does she work with? What are her contact details? Is she well-connected, just in case she is not able to help us herself? What is her role in the organization? Who is her superior? Collaborators, and affiliations, etc. are all part of the candidate\"s social profile, and can serve as a background against which the system\"s recommendations should be interpreted. In this paper we only address the problem of determining topical profiles, and leave social profiling to further work. We state the problem of quantifying the competence of a person on a certain knowledge area as follows: What is the probability of a knowledge area (ki) being part of the candidate\"s (expertise) profile? where s(ca, ki) is defined by p(ki|ca). Our task, then, is to estimate p(ki|ca), which is equivalent to the problem of obtaining p(q|ca), where the topic ki is represented as a query topic q, i.e., a sequence of keywords representing the expertise required. Both the expert finding and profiling tasks rely on the accurate estimation of p(q|ca). The only difference derives from the prior probability that a person is an expert (p(ca)), which can be incorporated into the expert finding task. This prior does not apply to the profiling task since the candidate (individual) is fixed. 4. BASELINE MODELS In this section we describe our baseline models for estimating p(q|ca), i.e., associations between topics and people. Both expert finding and expert profiling boil down to this estimation. We employ three models for calculating this probability. 4.1 From topics to candidates Using Candidate Models: Model 1 Model 1 defines the probability of a query given a candidate (p(q|ca)) using standard language modeling techniques, based on a multinomial unigram language model. For each candidate ca, a candidate language model \u03b8ca is inferred such that the probability of a term given \u03b8ca is nonzero for all terms, i.e., p(t|\u03b8ca) > 0. From the candidate model the query is generated with the following probability: p(q|\u03b8ca) = t\u2208q p(t|\u03b8ca)n(t,q) where each term t in the query q is sampled identically and independently, and n(t, q) is the number of times t occurs in q. The candidate language model is inferred as follows: (1) an empirical model p(t|ca) is computed; (2) it is smoothed with background probabilities. Using the associations between a candidate and a document, the probability p(t|ca) can be approximated by: p(t|ca) = p(t|d)p(d|ca), where p(d|ca) is the probability that candidate ca generates a supporting document d, and p(t|d) is the probability of a term t occurring in the document d. We use the maximum-likelihood estimate of a term, that is, the normalised frequency of the term t in document d. The strength of the association between document d and candidate ca expressed by p(d|ca) reflects the degree to which the candidates expertise is described using this document. The estimation of this probability is presented later, in Section 4.2. The candidate model is then constructed as a linear interpolation of p(t|ca) and the background model p(t) to ensure there are no zero probabilities, which results in the final estimation: p(q|\u03b8ca) = (4) t\u2208q (1 \u2212 \u03bb) p(t|d)p(d|ca) + \u03bbp(t) )n(t,q) Model 1 amasses all the term information from all the documents associated with the candidate, and uses this to represent that candidate. This model is used to predict how likely a candidate would produce a query q. This can can be intuitively interpreted as the probability of this candidate talking about the query topic, where we assume that this is indicative of their expertise. Using Document Models: Model 2 Model 2 takes a different approach. Here, the process is broken into two parts. Given a candidate ca, (1) a document that is associated with a candidate is selected with probability p(d|ca), and (2) from this document a query q is generated with probability p(q|d). Then the sum over all documents is taken to obtain p(q|ca), such that: p(q|ca) = p(q|d)p(d|ca). (5) The probability of a query given a document is estimated by inferring a document language model \u03b8d for each document d in a similar manner as the candidate model was inferred: p(t|\u03b8d) = (1 \u2212 \u03bb)p(t|d) + \u03bbp(t), (6) where p(t|d) is the probability of the term in the document. The probability of a query given the document model is: p(q|\u03b8d) = t\u2208q p(t|\u03b8d)n(t,q) The final estimate of p(q|ca) is obtained by substituting p(q|d) for p(q|\u03b8d) into Eq. 5 (see for full details). Conceptually, Model 2 differs from Model 1 because the candidate is not directly modeled. Instead, the document acts like a hidden variable in the process which separates the query from the candidate. This process is akin to how a user may search for candidates with a standard search engine: initially by finding the documents which are relevant, and then seeing who is associated with that document. By examining a number of documents the user can obtain an idea of which candidates are more likely to discuss the topic q. Using Topic Models: Model 3 We introduce a third model, Model 3. Instead of attempting to model the query generation process via candidate or document models, we represent the query as a topic language model and directly estimate the probability of the candidate p(ca|q). This approach is similar to the model presented in . As with the previous models, a language model is inferred, but this time for the query. We adapt the work of Lavrenko and Croft to estimate a topic model from the query. The procedure is as follows. Given a collection of documents and a query topic q, it is assumed that there exists an unknown topic model \u03b8k that assigns probabilities p(t|\u03b8k) to the term occurrences in the topic documents. Both the query and the documents are samples from \u03b8k (as opposed to the previous approaches, where a query is assumed to be sampled from a specific document or candidate model). The main task is to estimate p(t|\u03b8k), the probability of a term given the topic model. Since the query q is very sparse, and as there are no examples of documents on the topic, this distribution needs to be approximated. Lavrenko and Croft suggest a reasonable way of obtaining such an approximation, by assuming that p(t|\u03b8k) can be approximated by the probability of term t given the query q. We can then estimate p(t|q) using the joint probability of observing the term t together with the query terms, q1, . . . , qm, and dividing by the joint probability of the query terms: p(t|\u03b8k) \u2248 p(t|q) = p(t, q1, . . . , qm) p(q1, . . . , qm) p(t, q1, . . . , qm) t \u2208T p(t , q1, . . . , qm) where p(q1, . . . , qm) = t \u2208T p(t , q1, . . . , qm), and T is the entire vocabulary of terms. In order to estimate the joint probability p(t, q1, . . . , qm), we follow and assume t and q1, . . . , qm are mutually independent, once we pick a source distribution from the set of underlying source distributions U. If we choose U to be a set of document models. then to construct this set, the query q would be issued against the collection, and the top n returned are assumed to be relevant to the topic, and thus treated as samples from the topic model. (Note that candidate models could be used instead.) With the document models forming U, the joint probability of term and query becomes: p(t, q1, . . . , qm) = d\u2208U p(d) p(t|\u03b8d) mY i=1 p(qi|\u03b8d) . (7) Here, p(d) denotes the prior distribution over the set U, which reflects the relevance of the document to the topic. We assume that p(d) is uniform across U. In order to rank candidates according to the topic model defined, we use the Kullback-Leibler divergence metric (KL, ) to measure the difference between the candidate models and the topic model: KL(\u03b8k||\u03b8ca) = p(t|\u03b8k) log p(t|\u03b8k) p(t|\u03b8ca) . (8) Candidates with a smaller divergence from the topic model are considered to be more likely experts on that topic. The candidate model \u03b8ca is defined in Eq. 4. By using KL divergence instead of the probability of a candidate given the topic model p(ca|\u03b8k), we avoid normalization problems. 4.2 Document-candidate associations For our models we need to be able to estimate the probability p(d|ca), which expresses the extent to which a document d characterizes the candidate ca. In , two methods are presented for estimating this probability, based on the number of person names recognized in a document. However, in our (intranet) setting it is reasonable to assume that authors of documents can unambiguously be identified (e.g., as the author of an article, the teacher assigned to a course, the owner of a web page, etc.) Hence, we set p(d|ca) to be 1 if candidate ca is author of document d, otherwise the probability is 0. In Section 6 we describe how authorship can be determined on different types of documents within the collection. 5. THE UVT EXPERT COLLECTION The UvT Expert collection used in the experiments in this paper fits the scenario outlined in Section 3. The collection is based on the Webwijs (Webwise) system developed at Tilburg University (UvT) in the Netherlands. Webwijs ( webwijs/) is a publicly accessible database of UvT employees who are involved in research or teaching; currently, experts, each of whom has a page with contact information and, if made available by the expert, a research description and publications list. In addition, topics and is encouraged to suggest new topics that need to be approved by the Webwijs editor. Each topic has a separate page that shows all experts associated with that topic and, if available, a list of related topics. Webwijs is available in Dutch and English, and this bilinguality has been preserved in the collection. Every Dutch Webwijs page has an English translation. Not all Dutch topics have an English translation, but the reverse is true: the 981 English topics all have a Dutch equivalent. About 42% of the experts teach courses at Tilburg University; these courses were also crawled and included in the profile. In addition, about 27% of the experts link to their academic homepage from their Webwijs page. These home pages were crawled and added to the collection. (This means that if experts put the full-text versions of their publications on their academic homepage, these were also available for indexing.) full-text versions of publications from the UvT institutional repository and Dutch English no. no. of experts with \u2265 1 topic 743 727 no. 981 no. of expert- avg. no. of topics/expert 5.8 5.9 max. no. of topics/expert (no. of experts) 60 (1) 35 (1) min. no. of topics/expert (no. of experts) 1 (74) 1 (106) avg. no. of experts/topic 2.9 3.3 max. no. of experts/topic (no. of topics) 30 (1) 30 (1) min. no. of experts/topic (no. of topics) 1 (615) 1 (346) no. of experts with HP 318 318 no. of experts with CD 318 318 avg. no. of CDs per teaching expert 3.5 3.5 no. of experts with RD 329 313 no. of experts with PUB 734 734 avg. no. of PUBs per expert 27.0 27.0 avg. no. of PUB citations per expert 25.2 25.2 avg. no. of full-text PUBs per expert 1.8 1.8 Table 2: Descriptive statistics of the Dutch and English versions of the UvT Expert collection. converted them to plain text. We ran the TextCat language identifier to classify the language of the home pages and the fulltext publications. We restricted ourselves to pages where the classifier was confident about the language used on the page. This resulted in four document types: research descriptions (RD), course descriptions (CD), publications (PUB; full-text and citationonly versions), and academic homepages (HP). Everything was bundled into the UvT Expert collection which is available at http: //ilk.uvt.nl/uvt-expert-collection/. The UvT Expert collection was extracted from a different organizational setting than the W3C collection and differs from it in a number of ways. The UvT setting is one with relatively small amounts of multilingual data. Document-author associations are clear and the data is structured and clean. The collection covers a broad range of expertise areas, as one can typically find on intranets of universities and other knowledge-intensive institutes. Additionally, our university setting features several types of structure (topical and organizational), as well as multiple document types. Another important difference between the two data sets is that the expertise areas in the UvT Expert collection are self-selected instead of being based on group membership or assignments by others. Size is another dimension along which the W3C and UvT Expert collections differ: the latter is the smaller of the two. Also realistic are the large differences in the amount of information available for each expert. Utilizing Webwijs is voluntary; 425 Dutch experts did not select any topics at all. This leaves us with 743 Dutch and 727 English usable expert profiles. Table 2 provides descriptive statistics for the UvT Expert collection. Universities tend to have a hierarchical structure that goes from the faculty level, to departments, research groups, down to the individual researchers. In the UvT Expert collection we have information about the affiliations of researchers with faculties and institutes, providing us with a two-level organizational hierarchy. Tilburg University has 22 organizational units at the faculty level (including the university office and several research institutes) and 71 departments, which amounts to 3.2 departments per faculty. As to the topical hierarchy used by Webwijs, topics are top nodes in the hierarchy. This hierarchy has an average topic chain length of 2.65 and a maximum length of 7 topics. 6. EVALUATION Below, we evaluate Section 4\"s models for expert finding and profiling onthe UvT Expert collection. We detail our research questions and experimental setup, and then present our results. 6.1 Research Questions We address the following research questions. Both expert finding and profiling rely on the estimations of p(q|ca). The question is how the models compare on the different tasks, and in the setting of the UvT Expert collection. In , Model 2 outperformed Model 1 on the W3C collection. How do they compare on our data set? And how does Model 3 compare to Model 1? What about performance differences between the two languages in our test collection? 6.2 Experimental Setup The output of our models was evaluated against the self-assigned topic labels, which were treated as relevance judgements. Results were evaluated separately for English and Dutch. For English we only used topics for which the Dutch translation was available; for Dutch all topics were considered. The results were averaged for the queries in the intersection of relevance judgements and results; missing queries do not contribute a value of 0 to the scores. We use standard information retrieval measures, such as Mean Average Precision (MAP) and Mean Reciprocal Rank (MRR). We also report the percentage of topics (%q) and candidates (%ca) covered, for the expert finding and profiling tasks, respectively. 6.3 Results Table 1 shows the performance of Model 1, 2, and 3 on the expert finding and profiling tasks. The rows of the table correspond to the various document types (RD, CD, PUB, and HP) and to their combinations. RD+CD+PUB+HP is equivalent to the full collection and will be referred as the BASELINE of our experiments. Looking at Table 1 we see that Model 2 performs the best across the board. However, when the data is clean and very focused (RD), Model 3 outperforms it in a number of cases. Model 1 has the best coverage of candidates (%ca) and topics (%q). The various document types differ in their characteristics and how they improve the finding and profiling tasks. Expert profiling benefits much from the clean data present in the RD and CD document types, while the publications contribute the most to the expert finding task. Adding the homepages does not prove to be particularly useful. When we compare the results across languages, we find that the coverage of English topics (%q) is higher than of the Dutch ones for expert finding. Apart from that, the scores fall in the same range for both languages. For the profiling task the coverage of the candidates (%ca) is very similar for both languages. However, the performance is substantially better for the English topics. While it is hard to compare scores across collections, we conclude with a brief comparison of the absolute scores in Table 1 to those reported in on the W3C test set ( edition). For expert finding the MAP scores for Model 2 reported here are about 50% higher than the corresponding figures in , while our MRR scores are slightly below those in . For expert profiling, the differences are far more dramatic: the MAP scores for Model 2 reported here are around 50% below the scores in , while the (best) MRR scores are about the same as those in . The cause for the latter differences seems to reside in the number of knowledge areas considered here-approx. 30 times more than in the W3C setting. 7. ADVANCED MODELS Now that we have developed and assessed basic language modeling techniques for expertise retrieval, we turn to refined models that exploit special features of our test collection. 7.1 Exploiting knowledge area similarity One way to improve the scoring of a query given a candidate is to consider what other requests the candidate would satisfy and use them as further evidence to support the original query, proportional Expert finding Expert profiling Document types Model 1 Model 2 Model 3 Model 1 Model 2 Model 3 %q MAP MRR %q MAP MRR %q MAP MRR %ca MAP MRR %ca MAP MRR %ca MAP MRR English RD 97.8 0.126 0.269 83.5 0.144 0.311 83.3 0.129 0.271 100 0.089 0.189 39.3 0.232 0.465 41.1 0.166 0.337 CD 97.8 0.118 0.227 91.7 0.123 0.248 91.7 0.118 0.226 32.8 0.188 0.381 32.4 0.195 0.385 32.7 0.203 0.370 PUB 97.8 0.200 0.330 98.0 0.216 0.372 98.0 0.145 0.257 78.9 0.167 0.364 74.5 0.212 0.442 78.9 0.135 0.299 HP 97.8 0.081 0.186 97.4 0.071 0.168 97.2 0.062 0.149 31.2 0.150 0.299 28.8 0.185 0.335 30.1 0.136 0.287 RD+CD 97.8 0.188 0.352 92.9 0.193 0.360 92.9 0.150 0.273 100 0.145 0.286 61.3 0.251 0.477 63.2 0.217 0.416 RD+CD+PUB 97.8 0.235 0.373 98.1 0.277 0.439 98.1 0.178 0.305 100 0.196 0.380 87.2 0.280 0.533 89.5 0.170 0.344 RD+CD+PUB+HP 97.8 0.237 0.372 98.6 0.280 0.441 98.5 0.166 0.293 100 0.199 0.387 88.7 0.281 0.525 90.9 0.169 0.329 Dutch RD 61.3 0.094 0.229 38.4 0.137 0.336 38.3 0.127 0.295 38.0 0.127 0.386 34.1 0.138 0.420 38.0 0.105 0.327 CD 61.3 0.107 0.212 49.7 0.128 0.256 49.7 0.136 0.261 32.5 0.151 0.389 31.8 0.158 0.396 32.5 0.170 0.380 PUB 61.3 0.193 0.319 59.5 0.218 0.368 59.4 0.173 0.291 78.8 0.126 0.364 76.0 0.150 0.424 78.8 0.103 0.294 HP 61.3 0.063 0.169 56.6 0.064 0.175 56.4 0.062 0.163 29.8 0.108 0.308 27.8 0.125 0.338 29.8 0.098 0.255 RD+CD 61.3 0.159 0.314 51.9 0.184 0.360 51.9 0.169 0.324 60.5 0.151 0.410 57.2 0.166 0.431 60.4 0.159 0.384 RD+CD+PUB 61.3 0.244 0.398 61.5 0.260 0.424 61.4 0.210 0.350 90.3 0.165 0.445 88.2 0.189 0.479 90.3 0.126 0.339 RD+CD+PUB+HP 61.3 0.249 0.401 62.6 0.265 0.436 62.6 0.195 0.344 91.9 0.164 0.426 90.1 0.195 0.488 91.9 0.125 0.328 Table 1: Performance of the models on the expert finding and profiling tasks, using different document types and their combinations. %q is the number of topics covered (applies to the expert finding task), %ca is the number of candidates covered (applies to the expert profiling task). The top and bottom blocks correspond to English and Dutch respectively. The best scores are in boldface. to how related the other requests are to the original query. This can be modeled by interpolating between the p(q|ca) and the further supporting evidence from all similar requests q , as follows: p (q|ca) = \u03bbp(q|ca) + (1 \u2212 \u03bb) p(q|q )p(q |ca), (9) where p(q|q ) represents the similarity between the two topics q and q . To be able to work with similarity methods that are not necessarily probabilities, we set p(q|q ) = w(q,q ) , where \u03b3 is a normalizing constant, such that \u03b3 = q w(q , q ). We consider four methods for calculating the similarity score between two topics. Three approaches are strictly content-based, and establish similarity by examining co-occurrence patterns of topics within the collection, while the last approach exploits the hierarchical structure of topical areas that may be present within an organization (see for further examples of integrating word relationships into language models). The Kullback-Leibler (KL) divergence metric defined in Eq. 8 provides a measure of how different or similar two probability distributions are. A topic model is inferred for q and q using the method presented in Section 4.1 to describe the query across the entire vocabulary. Since a lower KL score means the queries are more similar, we let w(q, q ) = max(KL(\u03b8q||\u00b7) \u2212 KL(\u03b8q||\u03b8q )). Pointwise Mutual Information (PMI, ) is a measure of association used in information theory to determine the extent of independence between variables. The dependence between two queries is reflected by the SI(q, q ) score, where scores greater than zero indicate that it is likely that there is a dependence, which we take to mean that the queries are likely to be similar: SI(q, q ) = log p(q, q ) p(q)p(q ) (10) We estimate the probability of a topic p(q) using the number of documents relevant to query q within the collection. The joint probability p(q, q ) is estimated similarly, by using the concatenation of q and q as a query. To obtain p(q|q ), we then set w(q, q ) = SI(q, q ) when SI(q, q ) > 0 otherwise w(q, q ) = 0, because we are only interested in including queries that are similar. The log-likelihood statistic provides another measure of dependence, which is more reliable than the pointwise mutual information measure . Let k1 be the number of co-occurrences of q and q , k2 the number of occurrences of q not co-occurring with q , n1 the total number of occurrences of q , and n2 the total number of topic tokens minus the number of occurrences of q . Then, let p1 = k1/n1, p2 = k2/n2, and p = (k1 + k2)/(n1 + n2), (q, q ) = 2( (p1, k1, n1) + (p2, k2, n2) \u2212 (p, k1, n1) \u2212 (p, k2, n2)), where (p, n, k) = k log p + (n \u2212 k) log(1 \u2212 p). The higher score indicate that queries are also likely to be similar, thus we set w(q, q ) = (q, q ). Finally, we also estimate the similarity of two topics based on their distance within the topic hierarchy. The topic hierarchy is viewed as a directed graph, and for all topic-pairs the shortest path SP(q, q ) is calculated. We set the similarity score to be the reciprocal of the shortest path: w(q, q ) = 1/SP(q, q ). 7.2 Contextual information Given the hierarchy of an organization, the units to which a person belong are regarded as a context so as to compensate for data sparseness. We model it as follows: p (q|ca) = 1 \u2212 ou\u2208OU(ca) \u03bbou \u00b7 p(q|ca) ou\u2208OU(ca) \u03bbou \u00b7 p(q|ou), where OU(ca) is the set of organizational units of which candidate ca is a member of, and p(q|o) expresses the strength of the association between query q and the unit ou. The latter probability can be estimated using either of the three basic models, by simply replacing ca with ou in the corresponding equations. An organizational unit is associated with all the documents that its members have authored. That is, p(d|ou) = maxca\u2208ou p(d|ca). 7.3 A simple multilingual model For knowledge institutes in Europe, academic or otherwise, a multilingual (or at least bilingual) setting is typical. The following model builds on a kind of independence assumption: there is no spill-over of expertise/profiles across language boundaries. While a simplification, this is a sensible first approach. That is: p (q|ca) =P l\u2208L \u03bbl \u00b7 p(ql|ca), where L is the set of languages used in the collection, ql is the translation of the query q to language l, and \u03bbl is a language specific smoothing parameter, such that l\u2208L \u03bbl = 1. 8. ADVANCED MODELS: EVALUATION In this section we present an experimental evaluation of our advanced models. Expert finding Expert profiling Language Model 1 Model 2 Model 3 Model 1 Model 2 Model 3 %q MAP MRR %q MAP MRR %q MAP MRR %ca MAP MRR %ca MAP MRR %ca MAP MRR English only 97.8 0.237 0.372 98.6 0.280 0.441 98.5 0.166 0.293 100 0.199 0.387 88.7 0.281 0.525 90.9 0.169 0.329 Dutch only 61.3 0.249 0.401 62.6 0.265 0.436 62.6 0.195 0.344 91.9 0.164 0.426 90.1 0.195 0.488 91.9 0.125 0.328 Combination 99.4 0.297 0.444 99.7 0.324 0.491 99.7 0.223 0.388 100 0.241 0.445 92.1 0.313 0.564 93.2 0.224 0.411 Table 3: Performance of the combination of languages on the expert finding and profiling tasks (on candidates). Best scores for each model are in italic, absolute best scores for the expert finding and profiling tasks are in boldface. Method Model 1 Model 2 Model 3 MAP MRR MAP MRR MAP MRR English BASELINE 0.296 0.454 0.339 0.509 0.221 0.333 KLDIV 0.291 0.453 0.327 0.503 0.219 0.330 PMI 0.291 0.453 0.337 0.509 0.219 0.331 LL 0.319 0.490 0.360 0.524 0.233 0.368 HDIST 0.299 0.465 0.346 0.537 0.219 0.332 Dutch BASELINE 0.240 0.350 0.271 0.403 0.227 0.389 KLDIV 0.239 0.347 0.253 0.386 0.224 0.385 PMI 0.239 0.350 0.260 0.392 0.227 0.389 LL 0.255 0.372 0.281 0.425 0.231 0.389 HDIST 0.253 0.365 0.271 0.407 0.236 0.402 Method Model 1 Model 2 Model 3 MAP MRR MAP MRR MAP MRR English BASELINE 0.485 0.546 0.499 0.548 0.381 0.416 KLDIV 0.510 0.564 0.513 0.558 0.381 0.416 PMI 0.486 0.546 0.495 0.542 0.407 0.451 LL 0.558 0.589 0.586 0.617 0.408 0.453 HDIST 0.507 0.567 0.512 0.563 0.386 0.420 Dutch BASELINE 0.263 0.313 0.294 0.358 0.262 0.315 KLDIV 0.284 0.336 0.271 0.321 0.261 0.314 PMI 0.265 0.317 0.265 0.316 0.273 0.330 LL 0.312 0.351 0.330 0.377 0.284 0.331 HDIST 0.280 0.327 0.288 0.341 0.266 0.321 Table 4: Performance on the expert finding (top) and profiling (bottom) tasks, using knowledge area similarities. Runs were evaluated on the main topics set. Best scores are in boldface. 8.1 Research Questions Our questions follow the refinements presented in the preceding section: Does exploiting the knowledge area similarity improve effectiveness? Which of the various methods for capturing word relationships is most effective? Furthermore, is our way of bringing in contextual information useful? For which tasks? And finally, is our simple way of combining the monolingual scores sufficient for obtaining significant improvements? 8.2 Experimental setup Given that the self-assessments are also sparse in our collection, in order to be able to measure differences between the various models, we selected a subset of topics, and evaluated (some of the) runs only on this subset. This set is referred as main topics, and consists of topics that are located at the top level of the topical hierarchy. (A main topic has subtopics, but is not a subtopic of any other topic.) This main set consists of 132 Dutch and 119 English topics. The relevance judgements were restricted to the main topic set, but were not expanded with subtopics. 8.3 Exploiting knowledge area similarity Table 4 presents the results. The four methods used for estimating knowledge-area similarity are KL divergence (KLDIV), PointLang. Topics Model 1 Model 2 Model 3 MAP MRR MAP MRR MAP MRR Expert finding UK ALL 0.423 0.545 0.654 0.799 0.494 0.629 UK MAIN 0.500 0.621 0.704 0.834 0.587 0.699 NL ALL 0.439 0.560 0.672 0.826 0.480 0.630 NL MAIN 0.440 0.584 0.645 0.816 0.515 0.655 Expert profiling UK ALL 0.240 0.640 0.306 0.778 0.223 0.616 UK MAIN 0.523 0.677 0.519 0.648 0.461 0.587 NL ALL 0.203 0.716 0.254 0.770 0.183 0.627 NL MAIN 0.332 0.576 0.380 0.624 0.332 0.549 Table 5: Evaluating the context models on organizational units. wise mutual information (PMI), log-likelihood (LL), and distance within topic hierarchy (HDIST). We managed to improve upon the baseline in all cases, but the improvement is more noticeable for the profiling task. For both tasks, the LL method performed best. The content-based approaches performed consistently better than HDIST. 8.4 Contextual information A two level hierarchy of organizational units (faculties and institutes) is available in the UvT Expert collection. The unit a person belongs to is used as a context for that person. First, we evaluated the models of the organizational units, using all topics (ALL) and only the main topics (MAIN). An organizational unit is considered to be relevant for a given topic (or vice versa) if at least one member of the unit selected the given topic as an expertise area. Table 5 reports on the results. As far as expert finding goes, given a topic, the corresponding organizational unit can be identified with high precision. However, the expert profiling task shows a different picture: the scores are low, and the task seems hard. The explanation may be that general concepts (i.e., our main topics) may belong to several organizational units. Second, we performed another evaluation, where we combined the contextual models with the candidate models (to score candidates again). Table 6 reports on the results. We find a positive impact of the context models only for expert finding. Noticably, for expert finding (and Model 1), it improves over 50% (for English) and over 70% (for Dutch) on MAP. The poor performance on expert profiling may be due to the fact that context models alone did not perform very well on the profiling task to begin with. 8.5 Multilingual models In this subsection we evaluate the method for combining results across multiple languages that we described in Section 7.3. In our setting the set of languages consists of English and Dutch: L = {UK, NL}. The weights on these languages were set to be identical (\u03bbUK = \u03bbNL = 0.5). We performed experiments with various \u03bb settings, but did not observe significant differences in performance. Table 3 reports on the multilingual results, where performance is evaluated on the full topic set. All three models significantly imLang. Method Model 1 Model 2 Model 3 MAP MRR MAP MRR MAP MRR Expert finding UK BL 0.296 0.454 0.339 0.509 0.221 0.333 UK CT 0.330 0.491 0.342 0.500 0.228 0.342 NL BL 0.240 0.350 0.271 0.403 0.227 0.389 NL CT 0.251 0.382 0.267 0.410 0.246 0.404 Expert profiling UK BL 0.485 0.546 0.499 0.548 0.381 0.416 UK CT 0.562 0.620 0.508 0.558 0.440 0.486 NL BL 0.263 0.313 0.294 0.358 0.262 0.315 NL CT 0.330 0.384 0.317 0.387 0.294 0.345 Table 6: Performance of the context models (CT) compared to the baseline (BL). Best scores are in boldface. proved over all measures for both tasks. The coverage of topics and candidates for the expert finding and profiling tasks, respectively, is close to 100% in all cases. The relative improvement of the precision scores ranges from 10% to 80%. These scores demonstrate that despite its simplicity, our method for combining results over multiple languages achieves substantial improvements over the baseline. 9. CONCLUSIONS In this paper we focused on expertise retrieval (expert finding and profiling) in a new setting of a typical knowledge-intensive organization in which the available data is of high quality, multilingual, and covering a broad range of expertise area. Typically, the amount of available data in such an organization (e.g., a university, a research institute, or a research lab) is limited when compared to the W3C collection that has mostly been used for the experimental evaluation of expertise retrieval so far. To examine expertise retrieval in this setting, we introduced (and released) the UvT Expert collection as a representative case of such knowledge intensive organizations. The new collection reflects the typical properties of knowledge-intensive institutes noted above and also includes several features which may are potentially useful for expertise retrieval, such as topical and organizational structure. We evaluated how current state-of-the-art models for expert finding and profiling performed in this new setting and then refined these models in order to try and exploit the different characteristics within the data environment (language, topicality, and organizational structure). We found that current models of expertise retrieval generalize well to this new environment; in addition we found that refining the models to account for the differences results in significant improvements, thus making up for problems caused by data sparseness issues. Future work includes setting up manual assessments of automatically generated profiles by the employees themselves, especially in cases where the employees have not provided a profile themselves. 10. ACKNOWLEDGMENTS Krisztian Balog was supported by the Netherlands Organisation for Scientific Research (NWO) under project number 220-80-001. Maarten de Rijke was also supported by NWO under project numbers 017.001.190, 220-80-001, 264-70-050, 354-20-005, 600.065.120, 612-13-001, 612.000.106, 612.066.302, 612.069.006, 640.001.501, 640.002.501, and by the E.U. IST programme of the 6th FP for RTD under project MultiMATCH contract IST-. The work of Toine Bogers and Antal van den Bosch was funded by the IOP-MMI-program of SenterNovem / The Dutch Ministry of Economic Affairs, as part of the `A Propos project.", "body1": "An organization\"s intranet provides a means for exchanging information between employees and for facilitating employee collaborations. This task is usually addressed by uncovering associations between people and topics ; commonly, a co-occurrence of the name of a person with topics in the same context is assumed to be evidence of expertise. The launch of the Expert Finding task at TREC has generated a lot of interest in expertise retrieval, with rapid progress being made in terms of modeling, algorithms, and evaluation aspects. In this paper we focus on expertise retrieval in a realistic setting that differs from the W3C setting-one in which relatively small amounts of clean, multilingual data are available, that cover a broad range of expertise areas, as can be found on the intranets of universities and other knowledge-intensive organizations. We focus on a number of research questions in this paper: Does the relatively small amount of data available on an intranet affect the quality of the topic-person associations that lie at the heart of expertise retrieval algorithms? Apart from the research questions and data set that we contribute, our main contributions are as follows. Taking the similarity between topics into account can significantly improve retrieval performance. The remainder of this paper is organized as follows. Initial approaches to expertise finding often employed databases containing information on the skills and knowledge of each individual in the organization . In contrast with focusing on particular document types, there is also an increased interest in the development of systems that index and mine published intranet documents as sources of evidence for expertise. editions of the Expert Finding task at TREC implemented (variations on) one of these two models; see . Turning to other expert retrieval tasks that can also be addressed using topic-people associations, Balog and de Rijke addressed the task of determining topical expert profiles. We use generative language modeling to find associations between topics and people. In the expertise retrieval scenario that we envisage, users seeking expertise within an organization have access to an interface that combines a search box (where they can search for experts or topics) with navigational structures (of experts and of topics) that allows them to click their way to an expert page (providing the profile of a person) or a topic page (providing a list of experts on the topic). To feed the above interface, we face two expertise retrieval tasks, expert finding and expert profiling, that we first define and then formalize using generative language models. 3.1 Expert finding Expert finding involves the task of finding the right person with the appropriate skills and knowledge: Who are the experts on topic X?. Within an organization there are usually many possible candidates who could be experts for given topic. Whereas the candidate priors are generally assumed to be uniformand thus will not influence the ranking-it has been demonstrated that a sensible choice of priors may improve the performance . 3.2 Expert profiling While the task of expert searching was concerned with finding experts given a particular topic, the task of expert profiling seeks to answer a related question: What topics does a candidate know about? Generally, topical profiles within organizations consist of tabular structures which explicitly catalogue the skills and knowledge of each individual in the organization. We state the problem of quantifying the competence of a person on a certain knowledge area as follows: What is the probability of a knowledge area (ki) being part of the candidate\"s (expertise) profile? Both the expert finding and profiling tasks rely on the accurate estimation of p(q|ca). In this section we describe our baseline models for estimating p(q|ca), i.e., associations between topics and people. 4.1 From topics to candidates Using Candidate Models: Model 1 Model 1 defines the probability of a query given a candidate (p(q|ca)) using standard language modeling techniques, based on a multinomial unigram language model. The candidate model is then constructed as a linear interpolation of p(t|ca) and the background model p(t) to ensure there are no zero probabilities, which results in the final estimation: p(q|\u03b8ca) = (4) t\u2208q (1 \u2212 \u03bb) p(t|d)p(d|ca) + \u03bbp(t) )n(t,q) Model 1 amasses all the term information from all the documents associated with the candidate, and uses this to represent that candidate. Using Document Models: Model 2 Model 2 takes a different approach. Instead, the document acts like a hidden variable in the process which separates the query from the candidate. Instead of attempting to model the query generation process via candidate or document models, we represent the query as a topic language model and directly estimate the probability of the candidate p(ca|q). The procedure is as follows. 4.2 Document-candidate associations For our models we need to be able to estimate the probability p(d|ca), which expresses the extent to which a document d characterizes the candidate ca. The UvT Expert collection used in the experiments in this paper fits the scenario outlined in Section 3. Webwijs is available in Dutch and English, and this bilinguality has been preserved in the collection. About 42% of the experts teach courses at Tilburg University; these courses were also crawled and included in the profile. converted them to plain text. The UvT Expert collection was extracted from a different organizational setting than the W3C collection and differs from it in a number of ways. Another important difference between the two data sets is that the expertise areas in the UvT Expert collection are self-selected instead of being based on group membership or assignments by others. Size is another dimension along which the W3C and UvT Expert collections differ: the latter is the smaller of the two. Tilburg University has 22 organizational units at the faculty level (including the university office and several research institutes) and 71 departments, which amounts to 3.2 departments per faculty. As to the topical hierarchy used by Webwijs, topics are top nodes in the hierarchy. Below, we evaluate Section 4\"s models for expert finding and profiling onthe UvT Expert collection. 6.1 Research Questions We address the following research questions. 6.3 Results Table 1 shows the performance of Model 1, 2, and 3 on the expert finding and profiling tasks. Looking at Table 1 we see that Model 2 performs the best across the board. When we compare the results across languages, we find that the coverage of English topics (%q) is higher than of the Dutch ones for expert finding. While it is hard to compare scores across collections, we conclude with a brief comparison of the absolute scores in Table 1 to those reported in on the W3C test set ( edition). Now that we have developed and assessed basic language modeling techniques for expertise retrieval, we turn to refined models that exploit special features of our test collection. %q is the number of topics covered (applies to the expert finding task), %ca is the number of candidates covered (applies to the expert profiling task). to how related the other requests are to the original query. The Kullback-Leibler (KL) divergence metric defined in Eq. Pointwise Mutual Information (PMI, ) is a measure of association used in information theory to determine the extent of independence between variables. The log-likelihood statistic provides another measure of dependence, which is more reliable than the pointwise mutual information measure . Finally, we also estimate the similarity of two topics based on their distance within the topic hierarchy. 7.2 Contextual information Given the hierarchy of an organization, the units to which a person belong are regarded as a context so as to compensate for data sparseness. 7.3 A simple multilingual model For knowledge institutes in Europe, academic or otherwise, a multilingual (or at least bilingual) setting is typical.", "body2": "The goal of expert finding is to identify a list of people who are knowledgeable about a given topic. An alternative task, which using the same idea of people-topic associations, is expert profiling, where the task is to return a list of topics that a person is knowledgeable about . With only one test collection it is not possible to generalize conclusions to other realistic settings. This setting is quite different from the W3C setting in ways that might impact upon the performance of expertise retrieval tasks. A second group of experiments is aimed at extensions of the baseline methods that exploit characteristic features of the UvT Expert Collection; specifically, we propose and evaluate refined expert finding and profiling methods that incorporate topicality and organizational structure. We find that profiling on the UvT data set is considerably more difficult than on the W3C set, which we believe is due to the large (but realistic) number of topical areas that we used for profiling: about 1,500 for the UvT set, versus 50 in the W3C case. Finally, we demonstrate that the organizational structure can be exploited in the form of a context model, improving MAP scores for certain models by up to 70%. We formulate our conclusions in Section 9. More recent approaches use specific document sets (such as email or software ) to find expertise. In the reported experiments the second method performs significantly better when there are sufficiently many associated documents per candidate. Petkova and Croft propose yet another approach, based on a combination of the above Model 1 and 2, explicitly modeling topics. As an aside, creating a textual summary of a person shows some similarities to biography finding, which has received a considerable amount of attention recently; see e.g., . ; however, the features that we use for definining our expansions-including topical structure and organizational structure-have not been used in this way before. In the expertise retrieval scenario that we envisage, users seeking expertise within an organization have access to an interface that combines a search box (where they can search for experts or topics) with navigational structures (of experts and of topics) that allows them to click their way to an expert page (providing the profile of a person) or a topic page (providing a list of experts on the topic). By using language models, both the candidates and the query are characterized by distributions of terms in the vocabulary (used in the documents made available by the organization whose expertise retrieval needs we are addressing). Or, they may be in need a trained specialist for consultancy on a specific problem. p(ca|q) \u221d p(q|ca)p(ca) (2) In this paper our main focus is on estimating the probability of a query given the candidate p(q|ca), because this probability captures the extent to which the candidate knows about the query topic. Whereas the candidate priors are generally assumed to be uniformand thus will not influence the ranking-it has been demonstrated that a sensible choice of priors may improve the performance . This is the candidate\"s topical profile. In this paper we only address the problem of determining topical profiles, and leave social profiling to further work. Our task, then, is to estimate p(ki|ca), which is equivalent to the problem of obtaining p(q|ca), where the topic ki is represented as a query topic q, i.e., a sequence of keywords representing the expertise required. This prior does not apply to the profiling task since the candidate (individual) is fixed. We employ three models for calculating this probability. The estimation of this probability is presented later, in Section 4.2. This can can be intuitively interpreted as the probability of this candidate talking about the query topic, where we assume that this is indicative of their expertise. Conceptually, Model 2 differs from Model 1 because the candidate is not directly modeled. Using Topic Models: Model 3 We introduce a third model, Model 3. We adapt the work of Lavrenko and Croft to estimate a topic model from the query. By using KL divergence instead of the probability of a candidate given the topic model p(ca|\u03b8k), we avoid normalization problems. In Section 6 we describe how authorship can be determined on different types of documents within the collection. Each topic has a separate page that shows all experts associated with that topic and, if available, a list of related topics. Not all Dutch topics have an English translation, but the reverse is true: the 981 English topics all have a Dutch equivalent. of full-text PUBs per expert 1.8 1.8 Table 2: Descriptive statistics of the Dutch and English versions of the UvT Expert collection. Everything was bundled into the UvT Expert collection which is available at http: //ilk.uvt.nl/uvt-expert-collection/. Additionally, our university setting features several types of structure (topical and organizational), as well as multiple document types. Another important difference between the two data sets is that the expertise areas in the UvT Expert collection are self-selected instead of being based on group membership or assignments by others. In the UvT Expert collection we have information about the affiliations of researchers with faculties and institutes, providing us with a two-level organizational hierarchy. Tilburg University has 22 organizational units at the faculty level (including the university office and several research institutes) and 71 departments, which amounts to 3.2 departments per faculty. This hierarchy has an average topic chain length of 2.65 and a maximum length of 7 topics. We detail our research questions and experimental setup, and then present our results. We also report the percentage of topics (%q) and candidates (%ca) covered, for the expert finding and profiling tasks, respectively. RD+CD+PUB+HP is equivalent to the full collection and will be referred as the BASELINE of our experiments. Adding the homepages does not prove to be particularly useful. However, the performance is substantially better for the English topics. 30 times more than in the W3C setting. 7.1 Exploiting knowledge area similarity One way to improve the scoring of a query given a candidate is to consider what other requests the candidate would satisfy and use them as further evidence to support the original query, proportional Expert finding Expert profiling Document types Model 1 Model 2 Model 3 Model 1 Model 2 Model 3 %q MAP MRR %q MAP MRR %q MAP MRR %ca MAP MRR %ca MAP MRR %ca MAP MRR English RD 97.8 0.126 0.269 83.5 0.144 0.311 83.3 0.129 0.271 100 0.089 0.189 39.3 0.232 0.465 41.1 0.166 0.337 CD 97.8 0.118 0.227 91.7 0.123 0.248 91.7 0.118 0.226 32.8 0.188 0.381 32.4 0.195 0.385 32.7 0.203 0.370 PUB 97.8 0.200 0.330 98.0 0.216 0.372 98.0 0.145 0.257 78.9 0.167 0.364 74.5 0.212 0.442 78.9 0.135 0.299 HP 97.8 0.081 0.186 97.4 0.071 0.168 97.2 0.062 0.149 31.2 0.150 0.299 28.8 0.185 0.335 30.1 0.136 0.287 RD+CD 97.8 0.188 0.352 92.9 0.193 0.360 92.9 0.150 0.273 100 0.145 0.286 61.3 0.251 0.477 63.2 0.217 0.416 RD+CD+PUB 97.8 0.235 0.373 98.1 0.277 0.439 98.1 0.178 0.305 100 0.196 0.380 87.2 0.280 0.533 89.5 0.170 0.344 RD+CD+PUB+HP 97.8 0.237 0.372 98.6 0.280 0.441 98.5 0.166 0.293 100 0.199 0.387 88.7 0.281 0.525 90.9 0.169 0.329 Dutch RD 61.3 0.094 0.229 38.4 0.137 0.336 38.3 0.127 0.295 38.0 0.127 0.386 34.1 0.138 0.420 38.0 0.105 0.327 CD 61.3 0.107 0.212 49.7 0.128 0.256 49.7 0.136 0.261 32.5 0.151 0.389 31.8 0.158 0.396 32.5 0.170 0.380 PUB 61.3 0.193 0.319 59.5 0.218 0.368 59.4 0.173 0.291 78.8 0.126 0.364 76.0 0.150 0.424 78.8 0.103 0.294 HP 61.3 0.063 0.169 56.6 0.064 0.175 56.4 0.062 0.163 29.8 0.108 0.308 27.8 0.125 0.338 29.8 0.098 0.255 RD+CD 61.3 0.159 0.314 51.9 0.184 0.360 51.9 0.169 0.324 60.5 0.151 0.410 57.2 0.166 0.431 60.4 0.159 0.384 RD+CD+PUB 61.3 0.244 0.398 61.5 0.260 0.424 61.4 0.210 0.350 90.3 0.165 0.445 88.2 0.189 0.479 90.3 0.126 0.339 RD+CD+PUB+HP 61.3 0.249 0.401 62.6 0.265 0.436 62.6 0.195 0.344 91.9 0.164 0.426 90.1 0.195 0.488 91.9 0.125 0.328 Table 1: Performance of the models on the expert finding and profiling tasks, using different document types and their combinations. The best scores are in boldface. Three approaches are strictly content-based, and establish similarity by examining co-occurrence patterns of topics within the collection, while the last approach exploits the hierarchical structure of topical areas that may be present within an organization (see for further examples of integrating word relationships into language models). Since a lower KL score means the queries are more similar, we let w(q, q ) = max(KL(\u03b8q||\u00b7) \u2212 KL(\u03b8q||\u03b8q )). To obtain p(q|q ), we then set w(q, q ) = SI(q, q ) when SI(q, q ) > 0 otherwise w(q, q ) = 0, because we are only interested in including queries that are similar. The higher score indicate that queries are also likely to be similar, thus we set w(q, q ) = (q, q ). We set the similarity score to be the reciprocal of the shortest path: w(q, q ) = 1/SP(q, q ). That is, p(d|ou) = maxca\u2208ou p(d|ca). That is: p (q|ca) =P l\u2208L \u03bbl \u00b7 p(ql|ca), where L is the set of languages used in the collection, ql is the translation of the query q to language l, and \u03bbl is a language specific smoothing parameter, such that l\u2208L \u03bbl = 1.", "introduction": "An organization\"s intranet provides a means for exchanging information between employees and for facilitating employee collaborations. To efficiently and effectively achieve this, it is necessary to provide search facilities that enable employees not only to access documents, but also to identify expert colleagues. At the TREC Enterprise Track the need to study and understand expertise retrieval has been recognized through the introduction of Expert Finding tasks. The goal of expert finding is to identify a list of people who are knowledgeable about a given topic. This task is usually addressed by uncovering associations between people and topics ; commonly, a co-occurrence of the name of a person with topics in the same context is assumed to be evidence of expertise. An alternative task, which using the same idea of people-topic associations, is expert profiling, where the task is to return a list of topics that a person is knowledgeable about . The launch of the Expert Finding task at TREC has generated a lot of interest in expertise retrieval, with rapid progress being made in terms of modeling, algorithms, and evaluation aspects. However, nearly all of the expert finding or profiling work performed has been validated experimentally using the W3C collection from the Enterprise Track. While this collection is currently the only publicly available test collection for expertise retrieval tasks, it only represents one type of intranet. With only one test collection it is not possible to generalize conclusions to other realistic settings. In this paper we focus on expertise retrieval in a realistic setting that differs from the W3C setting-one in which relatively small amounts of clean, multilingual data are available, that cover a broad range of expertise areas, as can be found on the intranets of universities and other knowledge-intensive organizations. Typically, this setting features several additional types of structure: topical structure (e.g., topic hierarchies as employed by the organization), organizational structure (faculty, department, ...), as well as multiple types of documents (research and course descriptions, publications, and academic homepages). This setting is quite different from the W3C setting in ways that might impact upon the performance of expertise retrieval tasks. We focus on a number of research questions in this paper: Does the relatively small amount of data available on an intranet affect the quality of the topic-person associations that lie at the heart of expertise retrieval algorithms? How do state-of-the-art algorithms developed on the W3C data set perform in the alternative scenario of the type described above? More generally, do the lessons from the Expert Finding task at TREC carry over to this setting? How does the inclusion or exclusion of different documents affect expertise retrieval tasks? In addition to, how can the topical and organizational structure be used for retrieval purposes? To answer our research questions, we first present a set of baseline approaches, based on generative language modeling, aimed at finding associations between topics and people. This allows us to formulate the expert finding and expert profiling tasks in a uniform way, and has the added benefit of allowing us to understand the relations between the two tasks. For our experimental evaluation, we introduce a new data set (the UvT Expert Collection) which is representative of the type of intranet that we described above. Our collection is based on publicly available data, crawled from the website of Tilburg University (UvT). This type of data is particularly interesting, since (1) it is clean, heterogeneous, structured, and focused, but comprises a limited number of documents; (2) contains information on the organizational hierarchy; (3) it is bilingual (English and Dutch); and (4) the list of expertise areas of an individual are provided by the employees themselves. Using the UvT Expert collection, we conduct two sets of experiments. The first is aimed at determining the effectiveness of baseline expertise finding and profiling methods in this new setting. A second group of experiments is aimed at extensions of the baseline methods that exploit characteristic features of the UvT Expert Collection; specifically, we propose and evaluate refined expert finding and profiling methods that incorporate topicality and organizational structure. Apart from the research questions and data set that we contribute, our main contributions are as follows. The baseline models developed for expertise finding perform well on the new data set. While on the W3C setting the expert finding task appears to be more difficult than profiling, for the UvT data the opposite is the case. We find that profiling on the UvT data set is considerably more difficult than on the W3C set, which we believe is due to the large (but realistic) number of topical areas that we used for profiling: about 1,500 for the UvT set, versus 50 in the W3C case. Taking the similarity between topics into account can significantly improve retrieval performance. The best performing similarity measures are content-based, therefore they can be applied on the W3C (and other) settings as well. Finally, we demonstrate that the organizational structure can be exploited in the form of a context model, improving MAP scores for certain models by up to 70%. The remainder of this paper is organized as follows. In the next section we review related work. Then, in Section 3 we provide detailed descriptions of the expertise retrieval tasks that we address in this paper: expert finding and expert profiling. In Section 4 we present our baseline models, of which the performance is then assessed in Section 6 using the UvT data set that we introduce in Section 5. Advanced models exploiting specific features of our data are presented in Section 7 and evaluated in Section 8. We formulate our conclusions in Section 9.", "conclusion": "In this section we present an experimental evaluation of our advanced models.. Expert finding Expert profiling Language Model 1 Model 2 Model 3 Model 1 Model 2 Model 3 %q MAP MRR %q MAP MRR %q MAP MRR %ca MAP MRR %ca MAP MRR %ca MAP MRR English only 97.8 0.237 0.372 98.6 0.280 0.441 98.5 0.166 0.293 100 0.199 0.387 88.7 0.281 0.525 90.9 0.169 0.329 Dutch only 61.3 0.249 0.401 62.6 0.265 0.436 62.6 0.195 0.344 91.9 0.164 0.426 90.1 0.195 0.488 91.9 0.125 0.328 Combination 99.4 0.297 0.444 99.7 0.324 0.491 99.7 0.223 0.388 100 0.241 0.445 92.1 0.313 0.564 93.2 0.224 0.411 Table 3: Performance of the combination of languages on the expert finding and profiling tasks (on candidates).. Best scores for each model are in italic, absolute best scores for the expert finding and profiling tasks are in boldface.. Method Model 1 Model 2 Model 3 MAP MRR MAP MRR MAP MRR English BASELINE 0.296 0.454 0.339 0.509 0.221 0.333 KLDIV 0.291 0.453 0.327 0.503 0.219 0.330 PMI 0.291 0.453 0.337 0.509 0.219 0.331 LL 0.319 0.490 0.360 0.524 0.233 0.368 HDIST 0.299 0.465 0.346 0.537 0.219 0.332 Dutch BASELINE 0.240 0.350 0.271 0.403 0.227 0.389 KLDIV 0.239 0.347 0.253 0.386 0.224 0.385 PMI 0.239 0.350 0.260 0.392 0.227 0.389 LL 0.255 0.372 0.281 0.425 0.231 0.389 HDIST 0.253 0.365 0.271 0.407 0.236 0.402 Method Model 1 Model 2 Model 3 MAP MRR MAP MRR MAP MRR English BASELINE 0.485 0.546 0.499 0.548 0.381 0.416 KLDIV 0.510 0.564 0.513 0.558 0.381 0.416 PMI 0.486 0.546 0.495 0.542 0.407 0.451 LL 0.558 0.589 0.586 0.617 0.408 0.453 HDIST 0.507 0.567 0.512 0.563 0.386 0.420 Dutch BASELINE 0.263 0.313 0.294 0.358 0.262 0.315 KLDIV 0.284 0.336 0.271 0.321 0.261 0.314 PMI 0.265 0.317 0.265 0.316 0.273 0.330 LL 0.312 0.351 0.330 0.377 0.284 0.331 HDIST 0.280 0.327 0.288 0.341 0.266 0.321 Table 4: Performance on the expert finding (top) and profiling (bottom) tasks, using knowledge area similarities.. Runs were evaluated on the main topics set.. 8.1 Research Questions Our questions follow the refinements presented in the preceding section: Does exploiting the knowledge area similarity improve effectiveness?. Which of the various methods for capturing word relationships is most effective?. Furthermore, is our way of bringing in contextual information useful?. And finally, is our simple way of combining the monolingual scores sufficient for obtaining significant improvements?. 8.2 Experimental setup Given that the self-assessments are also sparse in our collection, in order to be able to measure differences between the various models, we selected a subset of topics, and evaluated (some of the) runs only on this subset.. This set is referred as main topics, and consists of topics that are located at the top level of the topical hierarchy.. (A main topic has subtopics, but is not a subtopic of any other topic.). This main set consists of 132 Dutch and 119 English topics.. The relevance judgements were restricted to the main topic set, but were not expanded with subtopics.. 8.3 Exploiting knowledge area similarity Table 4 presents the results.. The four methods used for estimating knowledge-area similarity are KL divergence (KLDIV), PointLang.. Topics Model 1 Model 2 Model 3 MAP MRR MAP MRR MAP MRR Expert finding UK ALL 0.423 0.545 0.654 0.799 0.494 0.629 UK MAIN 0.500 0.621 0.704 0.834 0.587 0.699 NL ALL 0.439 0.560 0.672 0.826 0.480 0.630 NL MAIN 0.440 0.584 0.645 0.816 0.515 0.655 Expert profiling UK ALL 0.240 0.640 0.306 0.778 0.223 0.616 UK MAIN 0.523 0.677 0.519 0.648 0.461 0.587 NL ALL 0.203 0.716 0.254 0.770 0.183 0.627 NL MAIN 0.332 0.576 0.380 0.624 0.332 0.549 Table 5: Evaluating the context models on organizational units.. wise mutual information (PMI), log-likelihood (LL), and distance within topic hierarchy (HDIST).. We managed to improve upon the baseline in all cases, but the improvement is more noticeable for the profiling task.. For both tasks, the LL method performed best.. The content-based approaches performed consistently better than HDIST.. 8.4 Contextual information A two level hierarchy of organizational units (faculties and institutes) is available in the UvT Expert collection.. The unit a person belongs to is used as a context for that person.. First, we evaluated the models of the organizational units, using all topics (ALL) and only the main topics (MAIN).. An organizational unit is considered to be relevant for a given topic (or vice versa) if at least one member of the unit selected the given topic as an expertise area.. Table 5 reports on the results.. As far as expert finding goes, given a topic, the corresponding organizational unit can be identified with high precision.. However, the expert profiling task shows a different picture: the scores are low, and the task seems hard.. The explanation may be that general concepts (i.e., our main topics) may belong to several organizational units.. Second, we performed another evaluation, where we combined the contextual models with the candidate models (to score candidates again).. Table 6 reports on the results.. We find a positive impact of the context models only for expert finding.. Noticably, for expert finding (and Model 1), it improves over 50% (for English) and over 70% (for Dutch) on MAP.. The poor performance on expert profiling may be due to the fact that context models alone did not perform very well on the profiling task to begin with.. 8.5 Multilingual models In this subsection we evaluate the method for combining results across multiple languages that we described in Section 7.3.. In our setting the set of languages consists of English and Dutch: L = {UK, NL}.. The weights on these languages were set to be identical (\u03bbUK = \u03bbNL = 0.5).. We performed experiments with various \u03bb settings, but did not observe significant differences in performance.. Table 3 reports on the multilingual results, where performance is evaluated on the full topic set.. Method Model 1 Model 2 Model 3 MAP MRR MAP MRR MAP MRR Expert finding UK BL 0.296 0.454 0.339 0.509 0.221 0.333 UK CT 0.330 0.491 0.342 0.500 0.228 0.342 NL BL 0.240 0.350 0.271 0.403 0.227 0.389 NL CT 0.251 0.382 0.267 0.410 0.246 0.404 Expert profiling UK BL 0.485 0.546 0.499 0.548 0.381 0.416 UK CT 0.562 0.620 0.508 0.558 0.440 0.486 NL BL 0.263 0.313 0.294 0.358 0.262 0.315 NL CT 0.330 0.384 0.317 0.387 0.294 0.345 Table 6: Performance of the context models (CT) compared to the baseline (BL).. proved over all measures for both tasks.. The coverage of topics and candidates for the expert finding and profiling tasks, respectively, is close to 100% in all cases.. The relative improvement of the precision scores ranges from 10% to 80%.. These scores demonstrate that despite its simplicity, our method for combining results over multiple languages achieves substantial improvements over the baseline.. CONCLUSIONS In this paper we focused on expertise retrieval (expert finding and profiling) in a new setting of a typical knowledge-intensive organization in which the available data is of high quality, multilingual, and covering a broad range of expertise area.. Typically, the amount of available data in such an organization (e.g., a university, a research institute, or a research lab) is limited when compared to the W3C collection that has mostly been used for the experimental evaluation of expertise retrieval so far.. To examine expertise retrieval in this setting, we introduced (and released) the UvT Expert collection as a representative case of such knowledge intensive organizations.. The new collection reflects the typical properties of knowledge-intensive institutes noted above and also includes several features which may are potentially useful for expertise retrieval, such as topical and organizational structure.. We evaluated how current state-of-the-art models for expert finding and profiling performed in this new setting and then refined these models in order to try and exploit the different characteristics within the data environment (language, topicality, and organizational structure).. We found that current models of expertise retrieval generalize well to this new environment; in addition we found that refining the models to account for the differences results in significant improvements, thus making up for problems caused by data sparseness issues.. Future work includes setting up manual assessments of automatically generated profiles by the employees themselves, especially in cases where the employees have not provided a profile themselves.. ACKNOWLEDGMENTS Krisztian Balog was supported by the Netherlands Organisation for Scientific Research (NWO) under project number 220-80-001.. Maarten de Rijke was also supported by NWO under project numbers 017.001.190, 220-80-001, 264-70-050, 354-20-005, 600.065.120, 612-13-001, 612.000.106, 612.066.302, 612.069.006, 640.001.501, 640.002.501, and by the E.U.. IST programme of the 6th FP for RTD under project MultiMATCH contract IST-.. The work of Toine Bogers and Antal van den Bosch was funded by the IOP-MMI-program of SenterNovem / The Dutch Ministry of Economic Affairs, as part of the `A Propos project."}
{"id": "H-87", "keywords": ["adapt filter", "topic track", "cross-benchmark evalu", "logist regress", "rocchio"], "title": "Robustness of Adaptive Filtering Methods In a Cross-benchmark Evaluation", "abstract": "This paper reports a cross-benchmark evaluation of regularized logistic regression (LR) and incremental Rocchio for adaptive filtering. Using four corpora from the Topic Detection and Tracking (TDT) forum and the Text Retrieval Conferences (TREC) we evaluated these methods with non-stationary topics at various granularity levels, and measured performance with different utility settings. We found that LR performs strongly and robustly in optimizing T11SU (a TREC utility function) while Rocchio is better for optimizing Ctrk (the TDT tracking cost), a high-recall oriented objective function. Using systematic cross-corpus parameter optimization with both methods, we obtained the best results ever reported on TDT5, TREC10 and TREC11. Relevance feedback on a small portion (0.05~0.2%) of the TDT5 test documents yielded significant performance improvements, measuring up to a 54% reduction in Ctrk and a 20.9% increase in T11SU (with b=0.1), compared to the results of the top-performing system in TDT2004 without relevance feedback information.", "references": ["Incremental relevance feedback for information filtering", "Learning while filtering documents", "Topic detection and tracking overview", "Overview of the TDT 2004 Evaluation and Results", "Elements of Statistical Learning", "The TREC-9 filtering track final report", "The TREC-10 filtering track final report", "The TREC 2002 filtering track report", "Microsoft Cambridge at TREC-9", "Boosting and Rocchio applied to text filtering", "Margin-based local regression for adaptive filtering", "Maximum likelihood estimation for filtering thresholds", "Using Bayesian priors to combine classifiers for adaptive filtering", "Robustness of regularized linear classification methods in text categorization", "Text Categorization Based on Regularized Linear Classification Methods"], "full_text": "1. INTRODUCTION Adaptive filtering (AF) has been a challenging research topic in information retrieval. The task is for the system to make an online topic membership decision (yes or no) for every document, as soon as it arrives, with respect to each pre-defined topic of interest. in the Topic Detection and Tracking (TDT) in the Text Retrieval Conferences (TREC), benchmark evaluations have been conducted by NIST under the following conditions: \u2022 A very small number (1 to 4) of positive training examples was provided for each topic at the starting point. \u2022 Relevance feedback was available but only for the systemaccepted documents (with a yes decision) in the TREC evaluations for AF. \u2022 Relevance feedback (RF) was not allowed in the TDT evaluations for AF (or topic tracking in the TDT terminology). \u2022 was the first time that TREC and TDT metrics were jointly used in evaluating AF methods on the same benchmark (the TDT5 corpus) where non-stationary topics dominate. The above conditions attempt to mimic realistic situations where an AF system would be used. That is, the user would be willing to provide a few positive examples for each topic of interest at the start, and might or might not be able to provide additional labeling on a small portion of incoming documents through relevance feedback. Furthermore, topics of interest might change over time, with new topics appearing and growing, and old topics shrinking and diminishing. These conditions make adaptive filtering a difficult task in statistical learning (online classification), for the following reasons: 1) it is difficult to learn accurate models for prediction based on extremely sparse training data; 2) it is not obvious how to correct the sampling bias (i.e., relevance feedback on system-accepted documents only) during the adaptation process; 3) it is not well understood how to effectively tune parameters in AF methods using cross-corpus validation where the validation and evaluation topics do not overlap, and the documents may be from different sources or different epochs. None of these problems is addressed in the literature of statistical learning for batch classification where all the training data are given at once. The first two problems have been studied in the adaptive filtering literature, including topic profile adaptation using incremental Rocchio, Gaussian-Exponential density models, logistic regression in a Bayesian framework, etc., and threshold optimization strategies using probabilistic calibration or local fitting techniques . Although these works provide valuable insights for understanding the problems and possible solutions, it is difficult to draw conclusions regarding the effectiveness and robustness of current methods because the third problem has not been thoroughly investigated. Addressing the third issue is the main focus in this paper. We argue that robustness is an important measure for evaluating and comparing AF methods. By robust we mean consistent and strong performance across benchmark corpora with a systematic method for parameter tuning across multiple corpora. Most AF methods have pre-specified parameters that may influence the performance significantly and that must be determined before the test process starts. Available training examples, on the other hand, are often insufficient for tuning the parameters. In TDT5, for example, there is only one labeled training example per topic at the start; parameter optimization on such training data is doomed to be ineffective. This leaves only one option (assuming tuning on the test set is not an alternative), that is, choosing an external corpus as the validation set. Notice that the validation-set topics often do not overlap with the test-set topics, thus the parameter optimization is performed under the tough condition that the validation data and the test data may be quite different from each other. Now the important question is: which methods (if any) are robust under the condition of using cross-corpus validation to tune parameters? Current literature does not offer an answer because no thorough investigation on the robustness of AF methods has been reported. In this paper we address the above question by conducting a cross-benchmark evaluation with two effective approaches in AF: incremental Rocchio and regularized logistic regression (LR). Rocchio-style classifiers have been popular in AF, with good performance in benchmark evaluations (TREC and TDT) if appropriate parameters were used and if combined with an effective threshold calibration strategy . Logistic regression is a classical method in statistical learning, and one of the best in batch-mode text categorization . It was recently evaluated in adaptive filtering and was found to have relatively strong performance (Section 5.1). Furthermore, a recent paper reported that the joint use of Rocchio and LR in a Bayesian framework outperformed the results of using each method alone on the TREC11 corpus. Stimulated by those findings, we decided to include Rocchio and LR in our crossbenchmark evaluation for robustness testing. Specifically, we focus on how much the performance of these methods depends on parameter tuning, what the most influential parameters are in these methods, how difficult (or how easy) to optimize these influential parameters using cross-corpus validation, how strong these methods perform on multiple benchmarks with the systematic tuning of parameters on other corpora, and how efficient these methods are in running AF on large benchmark corpora. The organization of the paper is as follows: Section 2 introduces the four benchmark corpora (TREC10 and TREC11, TDT3 and TDT5) used in this study. Section 3 analyzes the differences among the TREC and TDT metrics (utilities and tracking cost) and the potential implications of those differences. Section 4 outlines the Rocchio and LR approaches to AF, respectively. Section 5 reports the experiments and results. Section 6 concludes the main findings in this study. 2. BENCHMARK CORPORA We used four benchmark corpora in our study. Table 1 shows the statistics about these data sets. consisting of roughly 806, with 84 topic labels (subject categories). The first two weeks of documents is the training set, and the remaining 11 & \u00bd months is the test set. consisting of the same set of documents as those in TREC10 but with a slightly different splitting point for the training and test sets. The TREC11 topics (50) are quite different from those in TREC10; they are queries for retrieval with relevance judgments by NIST assessors . dry run1 The tracking part of the corpus consists of 71,388 news stories from multiple sources in English and Mandarin (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America and PRI the World). Machine-translated versions of the non-English stories (Xinhua, Zaobao and VOA Mandarin) are provided as well. The splitting point for training-test sets is different for each topic in TDT. . The tracking part of the corpus consists of 407, from 15 news agents or broadcast sources in English, Arabic and Mandarin, with machine-translated versions of the non-English stories. We only used the English versions of those documents in our experiments for this paper. The TDT topics differ from TREC topics both conceptually and statistically. Instead of generic, ever-lasting subject categories (as those in TREC), TDT topics are defined at a finer level of granularity, for events that happen at certain times and locations, and that are born and die, typically associated with a bursty distribution over chronologically ordered news stories. The average size of TDT topics (events) is two orders of magnitude smaller than that of the TREC10 topics. Figure 1 compares the document densities of a TREC topic (Civil Wars) and two TDT topics (Gunshot and APEC Summit Meeting, respectively) over a 3-month time period, where the area under each curve is normalized to one. The granularity differences among topics and the corresponding non-stationary distributions make the cross-benchmark evaluation interesting. For example, algorithms favoring large and stable topics may not work well for short-lasting and nonstationary topics, and vice versa. Cross-benchmark evaluations allow us to test this hypothesis and possibly identify the weaknesses in current approaches to adaptive filtering in tracking the drifting trends of topics. Table 1: Statistics of benchmark corpora for adaptive filtering evaluations N(tr) is the number of the initial training documents; N(ts) is the number of the test documents; n+ is the number of positive examples of a predefined topic; * is an average over all the topics. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Week P(topic|week) Gunshot (TDT5) APEC Summit Meeting (TDT3) Civil War(TREC10) Figure 1: The temporal nature of topics 3. METRICS To make our results comparable to the literature, we decided to use both TREC-conventional and TDT-conventional metrics in our evaluation. 3.1 TREC11 metrics Let A, B, C and D be, respectively, the numbers of true positives, false alarms, misses and true negatives for a specific topic, and DCBAN +++= be the total number of test documents. The TREC-conventional metrics are defined as: Precision )/( BAA += , Recall )/( CAA += )(2 )21( CABA +++ ( ) \u03b7\u03b7\u03b2 \u03b7\u03b2 \u2212+\u2212 ),/(max 11 , CABA SUT where parameters \u03b2 and \u03b7 were set to 0.5 and -0.5 respectively in TREC10 and TREC11 . For evaluating the performance of a system, the performance scores are computed for individual topics first and then averaged over topics (macroaveraging). 3.2 TDT metrics The TDT-conventional metric for topic tracking is defined as: famisstrk PTPwPTPwTC ))(1( 21 \u2212+= where P(T) is the percentage of documents on topic T, missP is the miss rate by the system on that topic, faP is the false alarm rate, and 1w and 2w are the costs (pre-specified constants) for a miss and a false alarm, respectively. The TDT benchmark evaluations have used the settings of 11 =w , 1.02 =w and 02.0)( =TP for all topics. For evaluating the performance of a system, Ctrk is computed for each topic first and then the resulting scores are averaged for a single measure (the topic-weighted Ctrk). To make the intuition behind this measure transparent, we substitute the terms in the definition of Ctrk as follows: CA TP =)( , DB TP =\u2212 )(1 , CA Pmiss = , DB Pfa = , )( )( 21 21 BwCw DB DB CA CA wTCtrk +\u22c5= \u22c5+ \u22c5= Clearly, trkC is the average cost per error on topic T, with 1w and 2w controlling the penalty ratio for misses vs. false alarms. In addition to trkC , also employed 1.011 =\u03b2SUT as a utility metric. To distinguish this from the 5.011 =\u03b2SUT in TREC11, we call former TDT5SU in the rest of this paper. Corpus #Topics N(tr) N(ts) Avg n+ (tr) Avg n+ (ts) Max n+ (ts) Min n+ (ts) #Topics per doc (ts) TREC10 84 20,307 783,.3 39,448 38 1.57 TREC11 50 80.664 726,419 3 378.0 597 198 1.12 TDT3 53 18,738* 37,770* 4 79.3 520 1 1.06 TDT5 111 199,419* 207,991* 1 71.3 710 1 1.01 3.3 The correlations and the differences From an optimization point of view, TDT5SU and T11SU are both utility functions while Ctrk is a cost function. Our objective is to maximize the former or to minimize the latter on test documents. The differences and correlations among these objective functions can be analyzed through the shared counts of A, B, C and D in their definitions. For example, both TDT5SU and T11SU are positively correlated to the values of A and D, and negatively correlated to the values of B and C; the only difference between them is in their penalty ratios for misses vs. false alarms, i.e., 10:1 in TDT5SU and 2:1 in T11SU. The Ctrk function, on the other hand, is positively correlated to the values of C and B, and negatively correlated to the values of A and D; hence, it is negatively correlated to T11SU and TDT5SU. More importantly, there is a subtle and major difference between Ctrk and the utility functions: T11SU and TDT5SU. That is, Ctrk has a very different penalty ratio for misses vs. false alarms: it favors recall-oriented systems to an extreme. At first glance, one would think that the penalty ratio in Ctrk is 10:1 since 11 =w and 1.02 =w . However, this is not true if 02.0)( =TP is an inaccurate estimate of the on-topic documents on average for the test corpus. Using TDT3 as an example, the true percentage is: 002. 3.79 )( \u2248= TP where N is the average size of the test sets in TDT3, and n+ is the average number of positive examples per topic in the test sets. Using 02.0)(\u02c6 =TP as an (inaccurate) estimate of 0.002 enlarges the intended penalty ratio of 10:1 to 100:1, roughly speaking. To wit: )1.010( 1.010 )770( 3. ))(1(2)(1 )02.01(202.01)( BC NN DB CA faPTPwmissPTPw faPwmissPwTtrkC \u00d7+\u00d7=\u00d7+\u00d7\u2248 \u00d7\u2212\u00d7+\u00d7\u00d7= \u00d7\u2212\u00d7+\u00d7\u00d7= \u00d7\u2212\u22c5+\u00d7\u00d7= \u2212\u00d7+\u00d7\u00d7= \u03c1\u03c1 where 10 002.0 02.0 )( )(\u02c6 === TP TP \u03c1 is the factor of enlargement in the estimation of P(T) compared to the truth. Comparing the above result to formula 2, we can see the actual penalty ratio for misses vs. false alarms was 100:1 in the evaluations on TDT3 using Ctrk. Similarly, we can compute the enlargement factor for TDT5 using the statistics in Table 1 as follows: 3.58 991,207/3.71 02.0 )( )(\u02c6 === TP TP which means the actual penalty ratio for misses vs. false alarms in the evaluation on TDT5 using Ctrk was approximately 583:1. The implications of the above analysis are rather significant: \u2022 Ctrk defined in the same formula does not necessarily mean the same objective function in evaluation; instead, the optimization criterion depends on the test corpus. \u2022 Systems optimized for Ctrk would not optimize TDT5SU (and T11SU) because the former favors high-recall oriented to an extreme while the latter does not. \u2022 Parameters tuned on one corpus (e.g., TDT3) might not work for an evaluation on another corpus (say, TDT5) unless we account for the previously-unknown subtle dependency of Ctrk on data. \u2022 Results in Ctrk in the past years of TDT evaluations may not be directly comparable to each other because the evaluation collections changed most years and hence the penalty ratio in Ctrk varied. Although these problems with Ctrk were not originally anticipated, it offered an opportunity to examine the ability of systems in trading off precision for extreme recall. evaluation for AF. Comparing the metrics in TDT and TREC from a utility or cost optimization point of view is important for understanding the evaluation results of adaptive filtering methods. This is the first time this issue is explicitly analyzed, to our knowledge. 4. METHODS 4.1 Incremental Rocchio for AF We employed a common version of Rocchio-style classifiers which computes a prototype vector per topic (T) as follows: |)(| |)(| )( )(')( TD TD TqTp TDdTDd \u2208 \u2211\u2211 \u2212+ \u2212+= rr rr rr \u03b3\u03b2\u03b1 The first term on the RHS is the weighted vector representation of topic description whose elements are terms weights. The second term is the weighted centroid of the set )(TD+ of positive training examples, each of which is a vector of withindocument term weights. The third term is the weighted centroid of the set )(TD\u2212 of negative training examples which are the nearest neighbors of the positive centroid. The three terms are given pre-specified weights of \u03b2\u03b1, and \u03b3 , controlling the relative influence of these components in the prototype. The prototype of a topic is updated each time the system makes a yes decision on a new document for that topic. If relevance feedback is available (as is the case in TREC adaptive filtering), the new document is added to the pool of either )(TD+ or )(TD\u2212 , and the prototype is recomputed accordingly; if relevance feedback is not available (as is the case in TDT event tracking), the system\"s prediction (yes) is treated as the truth, and the new document is added to )(TD+ for updating the prototype. Both cases are part of our experiments in this paper ( evaluations for AF). To distinguish the two, we call the first case simply Rocchio and the second case PRF Rocchio where PRF stands for pseudorelevance feedback. The predictions on a new document are made by computing the cosine similarity between each topic prototype and the document vector, and then comparing the resulting scores against a threshold: =\u2212 )( )( ))),((cos( no yes dTpsign new \u03b8 rr Threshold calibration in incremental Rocchio is a challenging research topic. Multiple approaches have been developed. The simplest is to use a universal threshold for all topics, tuned on a validation set and fixed during the testing phase. More elaborate methods include probabilistic threshold calibration which converts the non-probabilistic similarity scores to probabilities (i.e., )|( dTP ) for utility optimization , and margin-based local regression for risk reduction . It is beyond the scope of this paper to compare all the different ways to adapt Rocchio-style methods for AF. Instead, our focus here is to investigate the robustness of Rocchio-style methods in terms of how much their performance depends on elaborate system tuning, and how difficult (or how easy) it is to get good performance through cross-corpus parameter optimization. Hence, we decided to use a relatively simple version of Rocchio as the baseline, i.e., with a universal threshold tuned on a validation corpus and fixed for all topics in the testing phase. This simple version of Rocchio has been commonly used in the past TDT benchmark evaluations for topic tracking, evaluations for adaptive filtering with and without relevance feedback (Section 5.1). Results of more complex variants of Rocchio are also discussed when relevant. 4.2 Logistic Regression for AF Logistic regression (LR) estimates the posterior probability of a topic given a document using a sigmoid function )1/(1),|1( xw ewxyP rrrr \u22c5\u2212 +== where x is the document vector whose elements are term weights, w is the vector of regression coefficients, and }1,1{ \u2212+\u2208y is the output variable corresponding to yes or no with respect to a particular topic. Given a training set of labeled documents { }),(,),,( 11 nn yxyxD = , the standard regression problem is defined as to find the maximum likelihood estimates of the regression coefficients (the model parameters): { } { } { }))exp(1(1logminarg )|(logmaxarg)|(maxarg ii xwyn wDP wDP mlw rr \u22c5\u2212+\u2211 == == This is a convex optimization problem which can be solved using a standard conjugate gradient algorithm in O(INF) time for training per topic, where I is the average number of iterations needed for convergence, and N and F are the number of training documents and number of features respectively . Once the regression coefficients are optimized on the training data, the filtering prediction on each incoming document is made as: ( ) =\u2212 )( )( ),|( no yes wxyPsign optnew \u03b8 rr Note that w is constantly updated whenever a new relevance judgment is available in the testing phase of AF, while the optimal threshold opt\u03b8 is constant, depending only on the predefined utility (or cost) function for evaluation. If T11SU is the metric, for example, with the penalty ratio of 2:1 for misses and false alarms (Section 3.1), the optimal threshold for LR is 33.0)12/(1 =+ for all topics. We modified the standard (above) version of LR to allow more flexible optimization criteria as follows: \u2212++= \u2211= \u22c5\u2212 2 )1log(minarg \u03bc\u03bb rrr rr weysw xwy map ii where )( iys is taken to be \u03b1 , \u03b2 and \u03b3 for query, positive and negative documents respectively, which are similar to those in Rocchio, giving different weights to the three kinds of training examples: topic descriptions (queries), on-topic documents and off-topic documents. The second term in the objective function is for regularization, equivalent to adding a Gaussian prior to the regression coefficients with mean \u03bc and covariance variance matrix \u0399\u22c5\u03bb2/1 , where \u0399 is the identity matrix. Tuning \u03bb (\u22650) is theoretically justified for reducing model complexity (the effective degree of freedom) and avoiding over-fitting on training data . How to find an effective \u03bc is an open issue for research, depending on the user\"s belief about the parameter space and the optimal range. The solution of the modified objective function is called the Maximum A Posteriori (MAP) estimate, which reduces to the maximum likelihood solution for standard LR if 0=\u03bb . 5. EVALUATIONS We report our empirical findings in four parts: official evaluation results, the cross-corpus parameter optimization results, and the results corresponding to the amounts of relevance feedback. 5.. Multiple research teams participated and multiple runs from each team were allowed. Ctrk and TDT5SU were used as the metrics. Figure 2 and Figure 3 show the results; the best run from each team was selected with respect to Ctrk or TDT5SU, respectively. Our Rocchio (with adaptive profiles but fixed universal threshold for all topics) had the best result in Ctrk, and our logistic regression had the best result in TDT5SU. All the parameters of our runs were tuned on the TDT3 corpus. Results for other sites are also listed anonymously for comparison. Metric = Ctrk (the lower the better) 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 Ours Site2 Site3 Site4 Figure 2: results in Ctrk of systems using true relevance feedback. (Ours is the Rocchio method.) We also put the 1st and 3rd quartiles as sticks for each site. Site4 0.382 Metric = TDT5SU (the higher the better) 0.382 0.2 0.4 0.6 0.8 Ours Site3 Site2 Site4 Figure 3: results in TDT5SU of systems using true relevance feedback. (Ours is LR with 0=\u03bc and 005.0=\u03bb ). 0.2 0.4 0.6 0.8 1.2 Ours Site2 Site5 Site4 Site6 Ctrk Figure 4: results in Ctrk of systems without using true relevance feedback. (Ours is PRF Rocchio.) Adaptive filtering without using true relevance feedback was also a part of the evaluations. In this case, systems had only one labeled training example per topic during the entire training and testing processes, although unlabeled test documents could be used as soon as predictions on them were made.. Figure 4 shows the summarized official submissions from each team. Our PRF Rocchio (with a fixed threshold for all the topics) had the best performance. We use quartiles rather than standard deviations since the former is more resistant to outliers. 5.2 Cross-corpus parameter optimization How much the strong performance of our systems depends on parameter tuning is an important question. Both Rocchio and LR have parameters that must be prespecified before the AF process. The shared parameters include the sample weights\u03b1 , \u03b2 and \u03b3 , the sample size of the negative training documents (i.e., )(TD\u2212 ), the term-weighting scheme, and the maximal number of non-zero elements in each document vector. The method-specific parameters include the decision threshold in Rocchio, and \u03bc , \u03bb and MI (the maximum number of iterations in training) in LR. Given that we only have one labeled example per topic in the TDT5 training sets, it is impossible to effectively optimize these parameters on the training data, and we had to choose an external corpus for validation. Among the choices of TREC10, TREC11 and TDT3, we chose TDT3 (c.f. Section 2) because it is most similar to TDT5 in terms of the nature of the topics (Section 2). We optimized the parameters of our systems on TDT3,. We also tested our methods on TREC10 and TREC11 for further analysis. Since exhaustive testing of all possible parameter settings is computationally intractable, we followed a step-wise forward chaining procedure instead: we pre-specified an order of the parameters in a method (Rocchio or LR), and then tuned one parameter at the time while fixing the settings of the remaining parameters. We repeated this procedure for several passes as time allowed. 0.05 0.26 0.67 0.69 0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 0.02 0.03 0.04 0.06 0.08 0.1 0.15 0.2 0.25 0.3 Threshold TDT5SU TDT3 TDT5 TREC10 TREC11 Figure 5: Performance curves of adaptive Rocchio Figure 5 compares the performance curves in TDT5SU for Rocchio on TDT3, TDT5, TREC10 and TREC11 when the decision threshold varied. These curves peak at different locations: the TDT3-optimal is closest to the TDT5-optimal while the TREC10-optimal and TREC1-optimal are quite far away from the TDT5-optimal. If we were using TREC10 or TREC11 instead of TDT3 as the validation corpus for TDT5, or if the TDT3 corpus were not available,. The difficulty comes from the ad-hoc (non-probabilistic) scores generated by the Rocchio method: the distribution of the scores depends on the corpus, making cross-corpus threshold optimization a tricky problem. Logistic regression has less difficulty with respect to threshold tuning because it produces probabilistic scores of )|1Pr( xy = upon which the optimal threshold can be directly computed if probability estimation is accurate. Given the penalty ratio for misses vs. false alarms as 2:1 in T11SU, 10:1 in TDT5SU and 583:1 in Ctrk (Section 3.3), the corresponding optimal thresholds (t) are 0.33, 0. respectively. Although the theoretical threshold could be inaccurate, it still suggests the range of near-optimal settings. With these threshold settings in our experiments for LR, we focused on the crosscorpus validation of the Bayesian prior parameters, that is, \u03bc and \u03bb. Table 2 summarizes the results 3 . We measured the performance of the runs on TREC10 and TREC11 using T11SU, and the performance of the runs on TDT3 and TDT5 using TDT5SU. For comparison we also include the best results of Rocchio-based methods on these corpora, which are our own results of Rocchio on TDT3 and TDT5, and the best results reported by NIST for TREC10 and TREC11. From this set of results, we see that LR significantly outperformed Rocchio on all the corpora, even in the runs of standard LR without any tuning, i.e. \u03bb=0. This empirical finding is consistent with a previous report for LR on TREC11 although our results of LR (0.585~0.608 in T11SU) are stronger than the results (0.49 for standard LR and 0.54 for LR using Rocchio prototype as the prior) in that report. More importantly, our cross-benchmark evaluation gives strong evidence for the robustness of LR. The robustness, we believe, comes from the probabilistic nature of the system-generated scores. That is, compared to the ad-hoc scores in Rocchio, the normalized posterior probabilities make the threshold optimization in LR a much easier problem. Moreover, logistic regression is known to converge towards the Bayes classifier asymptotically while Rocchio classifiers\" parameters do not. Another interesting observation in these results is that the performance of LR did not improve when using a Rocchio prototype as the mean in the prior; instead, the performance decreased in some cases. This observation does not support the previous report by , but we are not surprised because we are not convinced that Rocchio prototypes are more accurate than LR models for topics in the early stage of the AF process, and we believe that using a Rocchio prototype as the mean in the Gaussian prior would introduce undesirable bias to LR. We also believe that variance reduction (in the testing phase) should be controlled by the choice of \u03bb (but not \u03bc ), for which we conducted the experiments as shown in Figure 6. Table 2: Results of LR with different Bayesian priors Corpus TDT3 TDT5 TREC10 TREC11 LR(\u03bc=0,\u03bb=0) 0. LR(\u03bc=0,\u03bb=0.01) LR(\u03bc=roc*,\u03bb=0.01) 0.475 The LR results (0.77~0.78) official result (0.73) because parameter optimization has been improved afterwards. The TREC10-best result (0.496 by Oracle) is only available in T10U which is not directly comparable to the scores in T11SU, just indicative. *: \u03bc was set to the Rocchio prototype 0.2 0.4 0.6 0.8 0.000 0.001 0.005 0.050 0.500 Lambda Performance Ctrk on TDT3 TDT5SU on TDT3 TDT5SU on TDT5 T11SU on TREC11 Figure 6: LR with varying lambda. The performance of LR is summarized with respect to \u03bb tuning on the corpora of TREC10, TREC11 and TDT3. The performance on each corpus was measured using the corresponding metrics, that is, T11SU for the runs on TREC10 and TREC11, and TDT5SU and Ctrk for the runs on TDT3,. In the case of maximizing the utilities, the safe interval for \u03bb is between 0 and 0.01, meaning that the performance of regularized LR is stable, the same as or improved slightly over the performance of standard LR. In the case of minimizing Ctrk, the safe range for \u03bb is between 0 and 0.1, and setting \u03bb between 0.005 and 0.05 yielded relatively large improvements over the performance of standard LR because training a model for extremely high recall is statistically more tricky, and hence more regularization is needed. In either case, tuning \u03bb is relatively safe, and easy to do successfully by cross-corpus tuning. Another influential choice in our experiment settings is term weighting: we examined the choices of binary, TF and TF-IDF (the ltc version) schemes. We found TF-IDF most effective for both Rocchio and LR, and used this setting in all our experiments. 5.3 Percentages of labeled data How much relevance feedback (RF) would be needed during the AF process is a meaningful question in real-world applications. To answer it, we evaluated Rocchio and LR on TDT with the following settings: \u2022 Basic Rocchio, no adaptation at all \u2022 PRF Rocchio, updating topic profiles without using true relevance feedback; \u2022 Adaptive Rocchio, updating topic profiles using relevance feedback on system-accepted documents plus 10 documents randomly sampled from the pool of systemrejected documents; \u2022 LR with 0 rr =\u03bc , 01.0=\u03bb and threshold = 0.004; \u2022 All the parameters in Rocchio tuned on TDT3. Table 3 summarizes the results in Ctrk: Adaptive Rocchio with relevance feedback on 0.6% of the test documents reduced the tracking cost by 54% over the result of the PRF Rocchio, evaluation for topic tracking without relevance feedback information. Incremental LR, on the other hand, was weaker but still impressive. Recall that Ctrk is an extremely high-recall oriented metric, causing frequent updating of profiles and hence an efficiency problem in LR. For this reason we set a higher threshold (0.004) instead of the theoretically optimal threshold in LR to avoid an untolerable computation cost. The computation time in machine-hours was 0.33 for the run of adaptive Rocchio and 14 for the run of LR on TDT5 when optimizing Ctrk. Table 4 summarizes the results in TDT5SU; adaptive LR was the winner in this case, with relevance feedback on 0.05% of the test documents improving the utility by 20.9% over the results of PRF Rocchio. Table 3: AF methods on TDT5 (Performance in Ctrk) Base Roc PRF Roc Adp Roc LR % of RF 0% 0% 0.6% 0.2% Ctrk 0. \u00b1% +7% (baseline) -54% -46% Table 4: AF methods on TDT5 (Performance in TDT5SU) Base Roc PRF Roc Adp Roc LR(\u03bb=.01) % of RF 0% 0% 0.04% 0.05% TDT5SU 0. 0.69 0.78 \u00b1% -11.7% (baseline) +6.9% +20.9% Evidently, both Rocchio and LR are highly effective in adaptive filtering, in terms of using of a small amount of labeled data to significantly improve the model accuracy in statistical learning, which is the main goal of AF. 5.4 Summary of Adaptation Process After we decided the parameter settings using validation, we perform the adaptive filtering in the following steps for each topic: 1) Train the LR/Rocchio model using the provided positive training examples and 30 randomly sampled negative examples; 2) For each document in the test corpus: we first make a prediction about relevance, and then get relevance feedback for those (predicted) positive documents. 3) Model and IDF statistics will be incrementally updated if we obtain its true relevance feedback. 6. CONCLUDING REMARKS We presented a cross-benchmark evaluation of incremental Rocchio and incremental LR in adaptive filtering, focusing on their robustness in terms of performance consistency with respect to cross-corpus parameter optimization. Our main conclusions from this study are the following: \u2022 Parameter optimization in AF is an open challenge but has not been thoroughly studied in the past. \u2022 Robustness in cross-corpus parameter tuning is important for evaluation and method comparison. \u2022 We found LR more robust than Rocchio; it had the best results (in T11SU) ever reported on TDT5, TREC10 and TREC11 without extensive tuning. \u2022 We found Rocchio performs strongly when a good validation corpus is available, and a preferred choice when optimizing Ctrk is the objective, favoring recall over precision to an extreme. For future research we want to study explicit modeling of the temporal trends in topic distributions and content drifting. Acknowledgments This material is based upon work supported in parts by the National Science Foundation (NSF) under grant IIS-- and by the Defense Advanced Research Project Agency (DARPA) under Contract No.. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the sponsors.", "body1": "Adaptive filtering (AF) has been a challenging research topic in information retrieval. \u2022 Relevance feedback (RF) was not allowed in the TDT evaluations for AF (or topic tracking in the TDT terminology). \u2022 was the first time that TREC and TDT metrics were jointly used in evaluating AF methods on the same benchmark (the TDT5 corpus) where non-stationary topics dominate. The above conditions attempt to mimic realistic situations where an AF system would be used. Although these works provide valuable insights for understanding the problems and possible solutions, it is difficult to draw conclusions regarding the effectiveness and robustness of current methods because the third problem has not been thoroughly investigated. We argue that robustness is an important measure for evaluating and comparing AF methods. Most AF methods have pre-specified parameters that may influence the performance significantly and that must be determined before the test process starts. This leaves only one option (assuming tuning on the test set is not an alternative), that is, choosing an external corpus as the validation set. Logistic regression is a classical method in statistical learning, and one of the best in batch-mode text categorization . The organization of the paper is as follows: Section 2 introduces the four benchmark corpora (TREC10 and TREC11, TDT3 and TDT5) used in this study. We used four benchmark corpora in our study. consisting of the same set of documents as those in TREC10 but with a slightly different splitting point for the training and test sets. dry run1 The tracking part of the corpus consists of 71,388 news stories from multiple sources in English and Mandarin (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America and PRI the World). Machine-translated versions of the non-English stories (Xinhua, Zaobao and VOA Mandarin) are provided as well. . The TDT topics differ from TREC topics both conceptually and statistically. The granularity differences among topics and the corresponding non-stationary distributions make the cross-benchmark evaluation interesting. To make our results comparable to the literature, we decided to use both TREC-conventional and TDT-conventional metrics in our evaluation. 3.1 TREC11 metrics Let A, B, C and D be, respectively, the numbers of true positives, false alarms, misses and true negatives for a specific topic, and DCBAN +++= be the total number of test documents. 3.2 TDT metrics The TDT-conventional metric for topic tracking is defined as: famisstrk PTPwPTPwTC ))(1( 21 \u2212+= where P(T) is the percentage of documents on topic T, missP is the miss rate by the system on that topic, faP is the false alarm rate, and 1w and 2w are the costs (pre-specified constants) for a miss and a false alarm, respectively. In addition to trkC , also employed 1.011 =\u03b2SUT as a utility metric. Corpus #Topics N(tr) N(ts) Avg n+ (tr) Avg n+ (ts) Max n+ (ts) Min n+ (ts) #Topics per doc (ts) TREC10 84 20,307 783,.3 39,448 38 1.57 TREC11 50 80.664 726,419 3 378.0 597 198 1.12 TDT3 53 18,738* 37,770* 4 79.3 520 1 1.06 TDT5 111 199,419* 207,991* 1 71.3 710 1 1.01 3.3 The correlations and the differences From an optimization point of view, TDT5SU and T11SU are both utility functions while Ctrk is a cost function. More importantly, there is a subtle and major difference between Ctrk and the utility functions: T11SU and TDT5SU. That is, Ctrk has a very different penalty ratio for misses vs. false alarms: it favors recall-oriented systems to an extreme. \u2022 Systems optimized for Ctrk would not optimize TDT5SU (and T11SU) because the former favors high-recall oriented to an extreme while the latter does not. \u2022 Parameters tuned on one corpus (e.g., TDT3) might not work for an evaluation on another corpus (say, TDT5) unless we account for the previously-unknown subtle dependency of Ctrk on data. \u2022 Results in Ctrk in the past years of TDT evaluations may not be directly comparable to each other because the evaluation collections changed most years and hence the penalty ratio in Ctrk varied. Although these problems with Ctrk were not originally anticipated, it offered an opportunity to examine the ability of systems in trading off precision for extreme recall. Comparing the metrics in TDT and TREC from a utility or cost optimization point of view is important for understanding the evaluation results of adaptive filtering methods. 4.1 Incremental Rocchio for AF We employed a common version of Rocchio-style classifiers which computes a prototype vector per topic (T) as follows: |)(| |)(| )( )(')( TD TD TqTp TDdTDd \u2208 \u2211\u2211 \u2212+ \u2212+= rr rr rr \u03b3\u03b2\u03b1 The first term on the RHS is the weighted vector representation of topic description whose elements are terms weights. The prototype of a topic is updated each time the system makes a yes decision on a new document for that topic. The predictions on a new document are made by computing the cosine similarity between each topic prototype and the document vector, and then comparing the resulting scores against a threshold: =\u2212 )( )( ))),((cos( no yes dTpsign new \u03b8 rr Threshold calibration in incremental Rocchio is a challenging research topic. Hence, we decided to use a relatively simple version of Rocchio as the baseline, i.e., with a universal threshold tuned on a validation corpus and fixed for all topics in the testing phase. This simple version of Rocchio has been commonly used in the past TDT benchmark evaluations for topic tracking, evaluations for adaptive filtering with and without relevance feedback (Section 5.1). Results of more complex variants of Rocchio are also discussed when relevant. 4.2 Logistic Regression for AF Logistic regression (LR) estimates the posterior probability of a topic given a document using a sigmoid function )1/(1),|1( xw ewxyP rrrr \u22c5\u2212 +== where x is the document vector whose elements are term weights, w is the vector of regression coefficients, and }1,1{ \u2212+\u2208y is the output variable corresponding to yes or no with respect to a particular topic. Once the regression coefficients are optimized on the training data, the filtering prediction on each incoming document is made as: ( ) =\u2212 )( )( ),|( no yes wxyPsign optnew \u03b8 rr Note that w is constantly updated whenever a new relevance judgment is available in the testing phase of AF, while the optimal threshold opt\u03b8 is constant, depending only on the predefined utility (or cost) function for evaluation. We modified the standard (above) version of LR to allow more flexible optimization criteria as follows: \u2212++= \u2211= \u22c5\u2212 2 )1log(minarg \u03bc\u03bb rrr rr weysw xwy map ii where )( iys is taken to be \u03b1 , \u03b2 and \u03b3 for query, positive and negative documents respectively, which are similar to those in Rocchio, giving different weights to the three kinds of training examples: topic descriptions (queries), on-topic documents and off-topic documents. We report our empirical findings in four parts: official evaluation results, the cross-corpus parameter optimization results, and the results corresponding to the amounts of relevance feedback. Ctrk and TDT5SU were used as the metrics. Metric = Ctrk (the lower the better) 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 Ours Site2 Site3 Site4 Figure 2: results in Ctrk of systems using true relevance feedback. 0.2 0.4 0.6 0.8 1.2 Ours Site2 Site5 Site4 Site6 Ctrk Figure 4: results in Ctrk of systems without using true relevance feedback. 5.2 Cross-corpus parameter optimization How much the strong performance of our systems depends on parameter tuning is an important question. Both Rocchio and LR have parameters that must be prespecified before the AF process. 0.05 0.26 0.67 0.69 0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 0.02 0.03 0.04 0.06 0.08 0.1 0.15 0.2 0.25 0.3 Threshold TDT5SU TDT3 TDT5 TREC10 TREC11 Figure 5: Performance curves of adaptive Rocchio Figure 5 compares the performance curves in TDT5SU for Rocchio on TDT3, TDT5, TREC10 and TREC11 when the decision threshold varied. Although the theoretical threshold could be inaccurate, it still suggests the range of near-optimal settings. Another interesting observation in these results is that the performance of LR did not improve when using a Rocchio prototype as the mean in the prior; instead, the performance decreased in some cases. The TREC10-best result (0.496 by Oracle) is only available in T10U which is not directly comparable to the scores in T11SU, just indicative. *: \u03bc was set to the Rocchio prototype 0.2 0.4 0.6 0.8 0.000 0.001 0.005 0.050 0.500 Lambda Performance Ctrk on TDT3 TDT5SU on TDT3 TDT5SU on TDT5 T11SU on TREC11 Figure 6: LR with varying lambda. The performance of LR is summarized with respect to \u03bb tuning on the corpora of TREC10, TREC11 and TDT3. 5.3 Percentages of labeled data How much relevance feedback (RF) would be needed during the AF process is a meaningful question in real-world applications. To answer it, we evaluated Rocchio and LR on TDT with the following settings: \u2022 Basic Rocchio, no adaptation at all \u2022 PRF Rocchio, updating topic profiles without using true relevance feedback; \u2022 Adaptive Rocchio, updating topic profiles using relevance feedback on system-accepted documents plus 10 documents randomly sampled from the pool of systemrejected documents; \u2022 LR with 0 rr =\u03bc , 01.0=\u03bb and threshold = 0.004; \u2022 All the parameters in Rocchio tuned on TDT3. Table 3 summarizes the results in Ctrk: Adaptive Rocchio with relevance feedback on 0.6% of the test documents reduced the tracking cost by 54% over the result of the PRF Rocchio, evaluation for topic tracking without relevance feedback information. 5.4 Summary of Adaptation Process After we decided the parameter settings using validation, we perform the adaptive filtering in the following steps for each topic: 1) Train the LR/Rocchio model using the provided positive training examples and 30 randomly sampled negative examples; 2) For each document in the test corpus: we first make a prediction about relevance, and then get relevance feedback for those (predicted) positive documents.", "body2": "\u2022 Relevance feedback was available but only for the systemaccepted documents (with a yes decision) in the TREC evaluations for AF. \u2022 Relevance feedback (RF) was not allowed in the TDT evaluations for AF (or topic tracking in the TDT terminology). \u2022 was the first time that TREC and TDT metrics were jointly used in evaluating AF methods on the same benchmark (the TDT5 corpus) where non-stationary topics dominate. The first two problems have been studied in the adaptive filtering literature, including topic profile adaptation using incremental Rocchio, Gaussian-Exponential density models, logistic regression in a Bayesian framework, etc., and threshold optimization strategies using probabilistic calibration or local fitting techniques . Addressing the third issue is the main focus in this paper. By robust we mean consistent and strong performance across benchmark corpora with a systematic method for parameter tuning across multiple corpora. In TDT5, for example, there is only one labeled training example per topic at the start; parameter optimization on such training data is doomed to be ineffective. Rocchio-style classifiers have been popular in AF, with good performance in benchmark evaluations (TREC and TDT) if appropriate parameters were used and if combined with an effective threshold calibration strategy . Specifically, we focus on how much the performance of these methods depends on parameter tuning, what the most influential parameters are in these methods, how difficult (or how easy) to optimize these influential parameters using cross-corpus validation, how strong these methods perform on multiple benchmarks with the systematic tuning of parameters on other corpora, and how efficient these methods are in running AF on large benchmark corpora. Section 6 concludes the main findings in this study. The first two weeks of documents is the training set, and the remaining 11 & \u00bd months is the test set. The TREC11 topics (50) are quite different from those in TREC10; they are queries for retrieval with relevance judgments by NIST assessors . dry run1 The tracking part of the corpus consists of 71,388 news stories from multiple sources in English and Mandarin (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America and PRI the World). The splitting point for training-test sets is different for each topic in TDT. We only used the English versions of those documents in our experiments for this paper. Figure 1 compares the document densities of a TREC topic (Civil Wars) and two TDT topics (Gunshot and APEC Summit Meeting, respectively) over a 3-month time period, where the area under each curve is normalized to one. Table 1: Statistics of benchmark corpora for adaptive filtering evaluations N(tr) is the number of the initial training documents; N(ts) is the number of the test documents; n+ is the number of positive examples of a predefined topic; * is an average over all the topics. To make our results comparable to the literature, we decided to use both TREC-conventional and TDT-conventional metrics in our evaluation. For evaluating the performance of a system, the performance scores are computed for individual topics first and then averaged over topics (macroaveraging). To make the intuition behind this measure transparent, we substitute the terms in the definition of Ctrk as follows: CA TP =)( , DB TP =\u2212 )(1 , CA Pmiss = , DB Pfa = , )( )( 21 21 BwCw DB DB CA CA wTCtrk +\u22c5= \u22c5+ \u22c5= Clearly, trkC is the average cost per error on topic T, with 1w and 2w controlling the penalty ratio for misses vs. false alarms. To distinguish this from the 5.011 =\u03b2SUT in TREC11, we call former TDT5SU in the rest of this paper. The Ctrk function, on the other hand, is positively correlated to the values of C and B, and negatively correlated to the values of A and D; hence, it is negatively correlated to T11SU and TDT5SU. More importantly, there is a subtle and major difference between Ctrk and the utility functions: T11SU and TDT5SU. That is, Ctrk has a very different penalty ratio for misses vs. The implications of the above analysis are rather significant: \u2022 Ctrk defined in the same formula does not necessarily mean the same objective function in evaluation; instead, the optimization criterion depends on the test corpus. \u2022 Systems optimized for Ctrk would not optimize TDT5SU (and T11SU) because the former favors high-recall oriented to an extreme while the latter does not. \u2022 Parameters tuned on one corpus (e.g., TDT3) might not work for an evaluation on another corpus (say, TDT5) unless we account for the previously-unknown subtle dependency of Ctrk on data. \u2022 Results in Ctrk in the past years of TDT evaluations may not be directly comparable to each other because the evaluation collections changed most years and hence the penalty ratio in Ctrk varied. evaluation for AF. This is the first time this issue is explicitly analyzed, to our knowledge. The three terms are given pre-specified weights of \u03b2\u03b1, and \u03b3 , controlling the relative influence of these components in the prototype. To distinguish the two, we call the first case simply Rocchio and the second case PRF Rocchio where PRF stands for pseudorelevance feedback. Instead, our focus here is to investigate the robustness of Rocchio-style methods in terms of how much their performance depends on elaborate system tuning, and how difficult (or how easy) it is to get good performance through cross-corpus parameter optimization. Hence, we decided to use a relatively simple version of Rocchio as the baseline, i.e., with a universal threshold tuned on a validation corpus and fixed for all topics in the testing phase. This simple version of Rocchio has been commonly used in the past TDT benchmark evaluations for topic tracking, evaluations for adaptive filtering with and without relevance feedback (Section 5.1). Results of more complex variants of Rocchio are also discussed when relevant. Given a training set of labeled documents { }),(,),,( 11 nn yxyxD = , the standard regression problem is defined as to find the maximum likelihood estimates of the regression coefficients (the model parameters): { } { } { }))exp(1(1logminarg )|(logmaxarg)|(maxarg ii xwyn wDP wDP mlw rr \u22c5\u2212+\u2211 == == This is a convex optimization problem which can be solved using a standard conjugate gradient algorithm in O(INF) time for training per topic, where I is the average number of iterations needed for convergence, and N and F are the number of training documents and number of features respectively . If T11SU is the metric, for example, with the penalty ratio of 2:1 for misses and false alarms (Section 3.1), the optimal threshold for LR is 33.0)12/(1 =+ for all topics. The solution of the modified objective function is called the Maximum A Posteriori (MAP) estimate, which reduces to the maximum likelihood solution for standard LR if 0=\u03bb . Multiple research teams participated and multiple runs from each team were allowed. Results for other sites are also listed anonymously for comparison. (Ours is LR with 0=\u03bc and 005.0=\u03bb ). We use quartiles rather than standard deviations since the former is more resistant to outliers. 5.2 Cross-corpus parameter optimization How much the strong performance of our systems depends on parameter tuning is an important question. We repeated this procedure for several passes as time allowed. Given the penalty ratio for misses vs. false alarms as 2:1 in T11SU, 10:1 in TDT5SU and 583:1 in Ctrk (Section 3.3), the corresponding optimal thresholds (t) are 0.33, 0. respectively. Moreover, logistic regression is known to converge towards the Bayes classifier asymptotically while Rocchio classifiers\" parameters do not. Table 2: Results of LR with different Bayesian priors Corpus TDT3 TDT5 TREC10 TREC11 LR(\u03bc=0,\u03bb=0) 0. LR(\u03bc=0,\u03bb=0.01) LR(\u03bc=roc*,\u03bb=0.01) 0.475 The LR results (0.77~0.78) official result (0.73) because parameter optimization has been improved afterwards. The TREC10-best result (0.496 by Oracle) is only available in T10U which is not directly comparable to the scores in T11SU, just indicative. *: \u03bc was set to the Rocchio prototype 0.2 0.4 0.6 0.8 0.000 0.001 0.005 0.050 0.500 Lambda Performance Ctrk on TDT3 TDT5SU on TDT3 TDT5SU on TDT5 T11SU on TREC11 Figure 6: LR with varying lambda. We found TF-IDF most effective for both Rocchio and LR, and used this setting in all our experiments. 5.3 Percentages of labeled data How much relevance feedback (RF) would be needed during the AF process is a meaningful question in real-world applications. To answer it, we evaluated Rocchio and LR on TDT with the following settings: \u2022 Basic Rocchio, no adaptation at all \u2022 PRF Rocchio, updating topic profiles without using true relevance feedback; \u2022 Adaptive Rocchio, updating topic profiles using relevance feedback on system-accepted documents plus 10 documents randomly sampled from the pool of systemrejected documents; \u2022 LR with 0 rr =\u03bc , 01.0=\u03bb and threshold = 0.004; \u2022 All the parameters in Rocchio tuned on TDT3. Table 3: AF methods on TDT5 (Performance in Ctrk) Base Roc PRF Roc Adp Roc LR % of RF 0% 0% 0.6% 0.2% Ctrk 0. \u00b1% +7% (baseline) -54% -46% Table 4: AF methods on TDT5 (Performance in TDT5SU) Base Roc PRF Roc Adp Roc LR(\u03bb=.01) % of RF 0% 0% 0.04% 0.05% TDT5SU 0. 0.69 0.78 \u00b1% -11.7% (baseline) +6.9% +20.9% Evidently, both Rocchio and LR are highly effective in adaptive filtering, in terms of using of a small amount of labeled data to significantly improve the model accuracy in statistical learning, which is the main goal of AF. 3) Model and IDF statistics will be incrementally updated if we obtain its true relevance feedback.", "introduction": "Adaptive filtering (AF) has been a challenging research topic in information retrieval. The task is for the system to make an online topic membership decision (yes or no) for every document, as soon as it arrives, with respect to each pre-defined topic of interest. in the Topic Detection and Tracking (TDT) in the Text Retrieval Conferences (TREC), benchmark evaluations have been conducted by NIST under the following conditions: \u2022 A very small number (1 to 4) of positive training examples was provided for each topic at the starting point. \u2022 Relevance feedback was available but only for the systemaccepted documents (with a yes decision) in the TREC evaluations for AF. \u2022 Relevance feedback (RF) was not allowed in the TDT evaluations for AF (or topic tracking in the TDT terminology). \u2022 was the first time that TREC and TDT metrics were jointly used in evaluating AF methods on the same benchmark (the TDT5 corpus) where non-stationary topics dominate. The above conditions attempt to mimic realistic situations where an AF system would be used. That is, the user would be willing to provide a few positive examples for each topic of interest at the start, and might or might not be able to provide additional labeling on a small portion of incoming documents through relevance feedback. Furthermore, topics of interest might change over time, with new topics appearing and growing, and old topics shrinking and diminishing. These conditions make adaptive filtering a difficult task in statistical learning (online classification), for the following reasons: 1) it is difficult to learn accurate models for prediction based on extremely sparse training data; 2) it is not obvious how to correct the sampling bias (i.e., relevance feedback on system-accepted documents only) during the adaptation process; 3) it is not well understood how to effectively tune parameters in AF methods using cross-corpus validation where the validation and evaluation topics do not overlap, and the documents may be from different sources or different epochs. None of these problems is addressed in the literature of statistical learning for batch classification where all the training data are given at once. The first two problems have been studied in the adaptive filtering literature, including topic profile adaptation using incremental Rocchio, Gaussian-Exponential density models, logistic regression in a Bayesian framework, etc., and threshold optimization strategies using probabilistic calibration or local fitting techniques . Although these works provide valuable insights for understanding the problems and possible solutions, it is difficult to draw conclusions regarding the effectiveness and robustness of current methods because the third problem has not been thoroughly investigated. Addressing the third issue is the main focus in this paper. We argue that robustness is an important measure for evaluating and comparing AF methods. By robust we mean consistent and strong performance across benchmark corpora with a systematic method for parameter tuning across multiple corpora. Most AF methods have pre-specified parameters that may influence the performance significantly and that must be determined before the test process starts. Available training examples, on the other hand, are often insufficient for tuning the parameters. In TDT5, for example, there is only one labeled training example per topic at the start; parameter optimization on such training data is doomed to be ineffective. This leaves only one option (assuming tuning on the test set is not an alternative), that is, choosing an external corpus as the validation set. Notice that the validation-set topics often do not overlap with the test-set topics, thus the parameter optimization is performed under the tough condition that the validation data and the test data may be quite different from each other. Now the important question is: which methods (if any) are robust under the condition of using cross-corpus validation to tune parameters? Current literature does not offer an answer because no thorough investigation on the robustness of AF methods has been reported. In this paper we address the above question by conducting a cross-benchmark evaluation with two effective approaches in AF: incremental Rocchio and regularized logistic regression (LR). Rocchio-style classifiers have been popular in AF, with good performance in benchmark evaluations (TREC and TDT) if appropriate parameters were used and if combined with an effective threshold calibration strategy . Logistic regression is a classical method in statistical learning, and one of the best in batch-mode text categorization . It was recently evaluated in adaptive filtering and was found to have relatively strong performance (Section 5.1). Furthermore, a recent paper reported that the joint use of Rocchio and LR in a Bayesian framework outperformed the results of using each method alone on the TREC11 corpus. Stimulated by those findings, we decided to include Rocchio and LR in our crossbenchmark evaluation for robustness testing. Specifically, we focus on how much the performance of these methods depends on parameter tuning, what the most influential parameters are in these methods, how difficult (or how easy) to optimize these influential parameters using cross-corpus validation, how strong these methods perform on multiple benchmarks with the systematic tuning of parameters on other corpora, and how efficient these methods are in running AF on large benchmark corpora. The organization of the paper is as follows: Section 2 introduces the four benchmark corpora (TREC10 and TREC11, TDT3 and TDT5) used in this study. Section 3 analyzes the differences among the TREC and TDT metrics (utilities and tracking cost) and the potential implications of those differences. Section 4 outlines the Rocchio and LR approaches to AF, respectively. Section 5 reports the experiments and results. Section 6 concludes the main findings in this study.", "conclusion": "We presented a cross-benchmark evaluation of incremental Rocchio and incremental LR in adaptive filtering, focusing on their robustness in terms of performance consistency with respect to cross-corpus parameter optimization.. Our main conclusions from this study are the following: \u2022 Parameter optimization in AF is an open challenge but has not been thoroughly studied in the past.. \u2022 Robustness in cross-corpus parameter tuning is important for evaluation and method comparison.. \u2022 We found LR more robust than Rocchio; it had the best results (in T11SU) ever reported on TDT5, TREC10 and TREC11 without extensive tuning.. \u2022 We found Rocchio performs strongly when a good validation corpus is available, and a preferred choice when optimizing Ctrk is the objective, favoring recall over precision to an extreme.. For future research we want to study explicit modeling of the temporal trends in topic distributions and content drifting.. Acknowledgments This material is based upon work supported in parts by the National Science Foundation (NSF) under grant IIS-- and by the Defense Advanced Research Project Agency (DARPA) under Contract No.. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the sponsors."}
{"id": "I-45", "keywords": ["commit machin", "agent interact", "agent orient program languag", "belief desir intent", "bdi"], "title": "Implementing Commitment-Based Interactions", "abstract": "Although agent interaction plays a vital role in MAS, and message-centric approaches to agent interaction have their drawbacks, present agent-oriented programming languages do not provide support for implementing agent interaction that is flexible and robust. Instead, messages are provided as a primitive building block. In this paper we consider one approach for modelling agent interactions: the commitment machines framework. This framework supports modelling interactions at a higher level (using social commitments), resulting in more flexible interactions. We investigate how commitment-based interactions can be implemented in conventional agent-oriented programming languages. The contributions of this paper are: a mapping from a commitment machine to a collection of BDI-style plans; extensions to the semantics of BDI programming languages; and an examination of two issues that arise when distributing commitment machines (turn management and race conditions) and solutions to these problems.", "references": ["Multiagent Programming: Languages", "Hermes: Designing goal-oriented agent interactions", "Hermes: Implementing goal-oriented agent interactions", "Hermes versus prometheus: A comparative evaluation of two agent interaction design approaches", "Teamwork", "Communication for goal directed agents", "Towards a testbed for multi-party dialogues", "Using a performative subsumption lattice to support commitment-based conversations", "STAPLE: An agent programming language based on the joint intention theory", "Representing and executing protocols as joint actions", "Towards flexible teamwork in persistent teams: Extended report", "An AgentSpeak meta-interpreter and its applications", "Designing commitment-based agent interactions", "Implementing flexible and robust agent interactions using distributed commitment machines", "Enhancing commitment machines", "Declarative & procedural goals in intelligent agent systems", "Towards design tools for protocol development", "Flexible protocol specification and execution: Applying event calculus planning using commitments", "Reasoning about commitments in the event calculus: An approach for specifying and executing protocols"], "full_text": "1. INTRODUCTION Agents are social, and agent interaction plays a vital role in multiagent systems. Consequently, design and implementation of agent interaction is an important research topic. The standard approach for designing agent interactions is messagecentric: interactions are defined by interaction protocols that give the permissible sequences of messages, specified using notations such as finite state machines, Petri nets, or Agent UML. It has been argued that this message-centric approach to interaction design is not a good match for intelligent agents. Intelligent agents should exhibit the ability to persist in achieving their goals in the face of failure (robustness) by trying different approaches (flexibility). On the other hand, when following an interaction protocol, an agent has limited flexibility and robustness: the ability to persistently try alternative means to achieving the interaction\"s aim is limited to those options that the protocol\"s designer provided, and in practice, message-centric design processes do not tend to lead to protocols that are flexible or robust. Recognising these limitations of the traditional approach to designing agent interactions, a number of approaches have been proposed in recent years that move away from message-centric interaction protocols, and instead consider designing agent interactions using higher-level concepts such as social commitments or interaction goals . There has also been work on richer forms of interaction in specific settings, such as teams of cooperative agents . However, although there has been work on designing flexible and robust agent interactions, there has been virtually no work on providing programming language support for implementing such interactions. Current Agent Oriented Programming Languages (AOPLs) do not provide support for implementing flexible and robust agent interactions using higher-level concepts than messages. Indeed, modern AOPLs , with virtually no exceptions, provide only simple message sending as the basis for implementing agent interaction. This paper presents what, to the best of our knowledge, is the second AOPL to support high-level, flexible, and robust agent interaction implementation. The first such language, STAPLE, was proposed a few years ago , but is not described in detail, and is arguably impractical for use by non-specialists, due to its logical basis and heavy reliance on temporal and modal logic. This paper presents a scheme for extending BDI-like AOPLs to support direct implementation of agent interactions that are designed using Yolum & Singh\"s commitment machine (CM) framework . In the remainder of this paper we briefly review commitment machines and present a simple abstraction of BDI AOPLs which lies in the common subset of languages such as Jason, 3APL, and CAN. We then present a scheme for translating commitment machines to this language, and indicate how the language needs to be extended to support this. We then extend our scheme to address a range of issues concerned with distribution, including turn tracking , and race conditions. 2. BACKGROUND 2.1 Commitment Machines The aim of the commitment machine framework is to allow for the definition of interactions that are more flexible than traditional message-centric approaches. A Commitment Machine (CM) specifies an interaction between entities (e.g. agents, services, processes) in terms of actions that change the interaction state. This interact state consists of fluents (predicates that change value over time), but also social commitments, both base-level and conditional. A base-level social commitment is an undertaking by debtor A to creditor B to bring about condition p, denoted C(A, B, p). This is sometimes abbreviated to C(p), where it is not important to specify the identities of the entities in question. For example, a commitment by customer C to merchant M to make the fluent paid true would be written as C(C, M, paid). A conditional social commitment is an undertaking by debtor A to creditor B that should condition q become true, A will then commit to bringing about condition p. This is denoted by CC(A, B, q, p), and, where the identity of the entities involved is unimportant (or obvious), is abbreviated to CC(q p) where the arrow is a reminder of the causal link between q becoming true and the creation of a commitment to make p true. For example, a commitment to make the fluent paid true once goods have been received would be written CC(goods paid). The semantics of commitments (both base-level and conditional) is defined with rules that specify how commitments change over time. For example, the commitment C(p) (or CC(q p)) is discharged when p becomes true; and the commitment CC(q p) is replaced by C(p) when q becomes true. In this paper we use the more symmetric semantics proposed by and subsequently reformalised by . In brief, these semantics deal with a number of more complex cases, such as where commitments are created when conditions already hold: if p holds when CC(p q) is meant to be created, then C(q) is created instead of CC(p q). An interaction is defined by specifying the entities involved, the possible contents of the interaction state (both fluents and commitments), and (most importantly) the actions that each entity can perform along with the preconditions and effects of each action, specified as add and delete lists. A commitment machine (CM) defines a range of possible interactions that each start in some state1 , and perform actions until reaching a final state. A final state is one that has no base-level commitments. One way of visualising the interactions that are possible with a given commitment machine is to generate the finite state machine corresponding to the CM. For example, figure 1 gives the FSM2 corresponding to the NetBill commitment machine: a simple CM where a customer (C) and merchant (M) attempt to trade using the following actions3 Unlike standard interaction protocols, or finite state machines, there is no designated initial state for the interaction. The finite state machine is software-generated: the nodes and connections were computed by an implementation of the axioms (available from ) and were then laid out by graphviz . We use the notation A(X) : P \u21d2 E to indicate that action A is performed by entity X, has precondition P (with : P omitted if empty) and effect E. \u2022 sendRequest(C) \u21d2 request \u2022 sendQuote(M) \u21d2 offer where offer \u2261 promiseGoods \u2227 promiseReceipt and promiseGoods \u2261 CC(M, C, accept, goods) and promiseReceipt \u2261 CC(M, C, pay, receipt) \u2022 sendAccept(C) \u21d2 accept where accept \u2261 CC(C, M, goods, pay) \u2022 sendGoods(M) \u21d2 promiseReceipt \u2227 goods where promiseReceipt \u2261 CC(M, C, pay, receipt) \u2022 sendEPO(C) : goods \u21d2 pay \u2022 sendReceipt(M) : pay \u21d2 receipt. The commitment accept is the customer\"s promise to pay once goods have been sent, promiseGoods is the merchant\"s promise to send the goods once the customer accepts, and promiseReceipt is the merchant\"s promise to send a receipt once payment has been made. As seen in figure 1, commitment machines can support a range of interaction sequences. 2.2 An Abstract Agent ProgrammingLanguage Agent programming languages in the BDI tradition (e.g. dMARS, JAM, PRS, UM-PRS, JACK, AgentSpeak(L), Jason, 3APL, CAN, Jadex) define agent behaviour in terms of event-triggered plans, where each plan specifies what it is triggered by, under what situations it can be considered to be applicable (defined using a so-called context condition), and a plan body: a sequence of steps that can include posting events which in turn triggers further plans. Given a collection of plans and an event e that has been posted the agent first collects all plans types that are triggered by that event (the relevant plans), then evaluates the context conditions of these plans to obtain a set of applicable plan instances. One of these is chosen and is executed. We now briefly define the formal syntax and semantics of a Simple Abstract (BDI) Agent Programming Language (SAAPL). This language is intended to be an abstraction that is in the common subset of such languages as Jason [1, Chapter 1], 3APL [1, Chapter 2], and CAN . Thus, it is intentionally incomplete in some areas, for instance it doesn\"t commit to a particular mechanism for dealing with plan failure, since different mechanisms are used by different AOPLs. An agent program (denoted by \u03a0) consists of a collection of plan clauses of the form e : C \u2190 P where e is an event, C is a context condition (a logical formula over the agent\"s beliefs), and P is the plan body. The plan body is built up from the following constructs. We have the empty step which always succeeds and does nothing, operations to add (+b) and delete (\u2212b) beliefs, sending a message m to agent N (\u2191N m), and posting an event4 (e). These can be sequenced (P; P). C ::= b | C \u2227 C | C \u2228 C | \u00acC | \u2203x.C P ::= | +b | \u2212b | e | \u2191N m | P; P Formal semantics for this language is given in figure 2. This semantics is based on the semantics for AgentSpeak given by , which in turn is based on the semantics for CAN . The semantics is in the style of Plotkin\"s Structural Operational Semantics, and assumes that operations exist that check whether a condition We use \u2193N m as short hand for the event corresponding to receiving message m from agent N. 874 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 1: Finite State Machine for NetBill (shaded = final states) follows from a belief set, that add a belief to a belief set, and that delete a belief from a belief set. In the case of beliefs being a set of ground atoms these operations are respectively consequence checking (B |= C), and set addition (B \u222a {b}) and deletion (B \\ {b}). More sophisticated belief management methods may be used, but are not considered here. We define a basic configuration S = Q, N, B, P where Q is a (global) message queue (modelled as a sequence5 where messages are added at one end and removed from the other end), N is the name of the agent, B is the beliefs of the agent and P is the plan body being executed (i.e. the intention). We also define an agent configuration, where instead of a single plan body P there is a set of plan instances, \u0393. Finally, a complete MAS is a pair Q, As of a global message queue Q and a set of agent configurations (without the queue, Q). The global message queue is a sequence of triplets of the form sender:recipient:message. A transition S0 \u2212\u2192 S1 specifies that executing S0 a single step yields S1. We annotate the arrow with an indication of whether the configuration in question is basic, an agent configuration, or a MAS configuration. The transition relation is defined using rules of the form S \u2212\u2192 S or of the form S \u2212\u2192 Sr S \u2212\u2192 Sr ; the latter are conditional with the top (numerator) being the premise and the bottom (denominator) being the conclusion. Note that there is non-determinism in SAAPL, e.g. the choice of plan to execute from a set of applicable plans. This is resolved by using selection functions: SO selects one of the applicable plan instances to handle a given event, SI selects which of the plan instances that can be executed should be executed next, and SA selects which agent should execute (a step) next. 3. IMPLEMENTING COMMITMENT-BASED INTERACTIONS In this section we present a mapping from a commitment machine to a collection of SAAPL programs (one for each role). We begin by considering the simple case of two interacting agents, and The + operator is used to denote sequence concatenation. assume that the agents take turns to act. In section 4 we relax these assumptions. Each action A(X) : P \u21d2 E is mapped to a number of plans: there is a plan (for agent X) with context condition P that performs the action (i.e. applies the effects E to the agent\"s beliefs) and sends a message to the other agent, and a plan (for the other agent) that updates its state when a message is received from X. For example, given the action sendAccept(C) \u21d2 accept we have the following plans, where each plan is preceded by M: or C: to indicate which agent that plan belongs to. Note that where the identify of the sender (respectively recipient) is obvious, i.e. the other agent, we abbreviate \u2191N m to \u2191m (resp. \u2193N m to \u2193m). Turn taking is captured through the event \u0131 (short for interact): the agent that is active has an \u0131 event that is being handled. Handling the event involves sending a message to the other agent, and then doing nothing until a response is received. C: \u0131 : true \u2190 +accept; \u2191sendAccept. M: \u2193sendAccept : true \u2190 +accept; \u0131. If the action has a non-trivial precondition then there are two plans in the recipient: one to perform the action (if possible), and another to report an error if the action\"s precondition doesn\"t hold (we return to this in section 4). For example, the action sendReceipt(M) : pay \u21d2 receipt generates the following plans: M: \u0131 : pay \u2190 +receipt; \u2191sendReceipt. C: \u2193sendReceipt : pay \u2190 +receipt; \u0131. C: \u2193sendReceipt : \u00acpay \u2190 . . . report error . . . . In addition to these plans, we also need plans to start and finish the interaction. An interaction can be completed whenever there are no base-level commitments, so both agents have the following plans: \u0131 : \u00ac\u2203p.C(p) \u2190 \u2191done. \u2193done : \u00ac\u2203p.C(p) \u2190 . \u2193done : \u2203p.C(p) \u2190 . . . report error . . . . An interaction is started by setting up an agent\"s initial beliefs, and then having it begin to interact. Exactly how to do this depends on the agent platform: e.g. the agent platform in question may offer a simple way to load beliefs from a file. A generic approach that is a little cumbersome, but is portable, is to send each of the agents involved in the interaction a sequence of init messages, each The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 875 Q, N, B, +b Basic \u2212\u2192 Q, N, B \u222a {b}, Q, N, B, \u2212b Basic \u2212\u2192 Q, N, B \\ {b}, \u0394 = {Pi\u03b8|(ti : ci \u2190 Pi) \u2208 \u03a0 \u2227 ti\u03b8 = e \u2227 B |= ci\u03b8} Q, N, B, e Basic \u2212\u2192 Q, N, B, SO(\u0394) Q, N, B, P1 Basic \u2212\u2192 Q , N, B , P Q, N, B, P1; P2 Basic \u2212\u2192 Q , N, B , P ; P2 Q, N, B, ; P Basic \u2212\u2192 Q, N, B, P Q, N, B, \u2191NB m Basic \u2212\u2192 Q + N:NB:m, N, B, Q = NA:N:m + Q Q, N, B, \u0393 Agent \u2212\u2192 Q , N, B, \u0393 \u222a {\u2193NA m} P = SI(\u0393) Q, N, B, P Basic \u2212\u2192 Q , N, B , P Q, N, B, \u0393 Agent \u2212\u2192 Q , N, B , (\u0393 \\ {P}) \u222a {P } P = SI(\u0393) P = Q, N, B, \u0393 Agent \u2212\u2192 Q, N, B, (\u0393 \\ {P}) N, B, \u0393 = SA(As) Q, N, B, \u0393 Agent \u2212\u2192 Q , N, B , \u0393 Q, As MAS \u2212\u2192 Q , (As \u222a { N, B , \u0393 }) \\ { N, B, \u0393 } Figure 2: Operational Semantics for SAAPL containing a belief to be added; and then send one of the agents a start message which begins the interaction. Both agents thus have the following two plans: \u2193init(B) : true \u2190 +B. \u2193start : true \u2190 \u0131. Figure 3 gives the SAAPL programs for both merchant and customer that implement the NetBill protocol. For conciseness the error reporting plans are omitted. We now turn to refining the context conditions. There are three refinements that we consider. Firstly, we need to prevent performing actions that have no effect on the interaction state. Secondly, an agent may want to specify that certain actions that it is able to perform should not be performed unless additional conditions hold. For example, the customer may not want to agree to the merchant\"s offer unless the goods have a certain price or property. Thirdly, the context conditions of the plans that terminate the interaction need to be refined in order to avoid terminating the interaction prematurely. For each plan of the form \u0131 : P \u2190 +E; \u2191m we replace the context condition P with the enhanced condition P \u2227 P \u2227 \u00acE where P is any additional conditions that the agent wishes to impose, and \u00acE is the negation of the effects of the action. For example, the customer\"s payment plan becomes (assuming no additional conditions, i.e. no P ): \u0131 : goods \u2227 \u00acpay \u2190 +pay; \u2191sendEPO. For each plan of the form \u2193m : P \u2190 +E; \u0131 we could add \u00acE to the precondition, but this is redundant, since it is already checked by the performer of the action, and if the action has no effect then Customer\"s plans: \u0131 : true \u2190 +request; \u2191sendRequest. \u0131 : true \u2190 +accept; \u2191sendAccept. \u0131 : goods \u2190 +pay; \u2191sendEPO. \u2193sendQuote : true \u2190 +promiseGoods; +promiseReceipt; \u0131. \u2193sendGoods : true \u2190 +promiseReceipt; +goods; \u0131. \u2193sendReceipt : pay \u2190 +receipt; \u0131. Merchant\"s plans: \u0131 : true \u2190 +promiseGoods; +promiseReceipt; \u2191sendQuote. \u0131 : true \u2190 +promiseReceipt; +goods; \u2191sendGoods. \u0131 : pay \u2190 +receipt; \u2191sendReceipt. \u2193sendRequest : true \u2190 +request; \u0131. \u2193sendAccept : true \u2190 +accept; \u0131. \u2193sendEPO : goods \u2190 +pay; \u0131. Shared plans (i.e. plans of both agents): \u0131 : \u00ac\u2203p.C(p) \u2190 \u2191done. \u2193done : \u00ac\u2203p.C(p) \u2190 . \u2193init(B) : true \u2190 +B. \u2193start : true \u2190 \u0131. Where accept \u2261 CC(goods pay) promiseGoods \u2261 CC(accept goods) promiseReceipt \u2261 CC(pay receipt) offer \u2261 promiseGoods \u2227 promiseReceipt Figure 3: SAAPL Implementation of NetBill the sender won\"t perform it and send the message (see also the discussion in section 4). When specifying additional conditions (P ), some care needs to be taken to avoid situations where progress cannot be made because the only action(s) possible are prevented by additional conditions. One way of indicating preference between actions (in many agent platforms) is to reorder the agent\"s plans. This is clearly safe, since actions are not prevented, just considered in a different order. The third refinement of context conditions concerns the plans that terminate the interaction. In the Commitment Machine framework any state that has no base-level commitment is final, in that the interaction may end there (or it may continue). However, only some of these final states are desirable final states. Which final states are considered to be desirable depends on the domain and the desired interaction outcome. In the NetBill example, the desirable final state is one where the goods have been sent and paid for, and a receipt issued (i.e. goods \u2227 pay \u2227 receipt). In order to prevent an agent from terminating the interaction too early we add this as a precondition to the termination plan: \u0131 : goods \u2227 pay \u2227 receipt \u2227 \u00ac\u2203p.C(p) \u2190 \u2191done. Figure 4 shows the plans that are changed from figure 3. In order to support the realisation of CMs, we need to change SAAPL in a number of ways. These changes, which are discussed below, can be applied to existing BDI languages to make them commitment machine supportive. We present the three changes, explain what they involve, and for each change explain how the change was implemented using the 3APL agent oriented programming language. The three changes are: 1. extending the beliefs of the agent so that they can contain commitments; 876 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Customer\"s plans: \u0131 : \u00acrequest \u2190 +request; \u2191sendRequest. \u0131 : \u00acaccept \u2190 +accept; \u2191sendAccept. \u0131 : goods \u2227 \u00acpay \u2190 +pay; \u2191sendEPO. Merchant\"s plans: \u0131 : \u00acoffer \u2190 +promiseGoods; +promiseReceipt; \u2191sendQuote. \u0131 : \u00ac(promiseReceipt \u2227 goods) \u2190 +promiseReceipt; +goods; \u2191sendGoods. \u0131 : pay \u2227 \u00acreceipt \u2190 +receipt; \u2191sendReceipt. Where accept \u2261 CC(goods pay) promiseGoods \u2261 CC(accept goods) promiseReceipt \u2261 CC(pay receipt) offer \u2261 promiseGoods \u2227 promiseReceipt Figure 4: SAAPL Implementation of NetBill with refined context conditions (changed plans only) 2. changing the definition of |= to encompass implied commitments; and 3. whenever a belief is added, updating existing commitments, according to the rules of commitment dynamics. Extending the notion of beliefs to encompass commitments in fact requires no change in agent platforms that are prolog-like and support terms as beliefs (e.g. Jason, 3APL, CAN). However, other agent platforms do require an extension. For example, JACK, which is an extension of Java, would require changes to support commitments that can be nested. In the case of 3APL no change is needed to support this. Whenever a context condition contains commitments, determining whether the context condition is implied by the agent\"s beliefs (B |= C) needs to take into account the notion of implied commitments . In brief, a commitment can be considered to follow from a belief set B if the commitment is in the belief set (C \u2208 B), but also under other conditions. For example, a commitment to pay C(pay) can be considered to be implied by a belief set containing pay because the commitment may have held and been discharged when pay was made true. Similar rules apply for conditional commitments. These rules, which were introduced in were subsequently re-formalised in a simpler form by resulting in the four inference rules in the bottom part of figure 5. The change that needs to be made to SAAPL to support commitment machine implementations is to extend the definition of |= to include these four rules. For 3APL this was realised by having each agent include the following Prolog clauses: holds(X) :- clause(X,true). holds(c(P)) :- holds(P). holds(c(P)) :- clause(cc(Q,P),true), holds(Q). holds(cc(_,Q)) :- holds(Q). holds(cc(_,Q)) :- holds(c(Q)). The first clause simply says that anything holds if it is in agent\"s beliefs (clause(X,true) is true if X is a fact). The remaining four clauses correspond respectively to the inference rules C1, C2, CC1 and CC2. To use these rules we then modify context conditions in our program so that instead of writing, for example, cc(m,c, pay, receipt) we write holds(cc(m,c, pay, receipt)). B = norm(B \u222a {b}) Q, N, B, +b \u2212\u2192 Q, N, B , function norm(B) B \u2190 B for each b \u2208 B do if b = C(p) \u2227 B |= p then B \u2190 B \\ {b} elseif b = CC(p q) then if B |= q then B \u2190 B \\ {b} elseif B |= p then B \u2190 (B \\ {b}) \u222a {C(q)} elseif B |= C(q) then B \u2190 B \\ {b} endif endif endfor return B end function B |= P B |= C(P) C1 CC(Q P) \u2208 B B |= Q B |= P C2 B |= CC(P Q) B |= Q CC1 B |= C(Q) B |= CC(P Q) CC2 Figure 5: New Operational Semantics The final change is to update commitments when a belief is added. Formally, this is done by modifying the semantic rule for belief addition so that it applies an algorithm to update commitments. The modified rule and algorithm (which mirrors the definition of norm in ) can be found in the top part of figure 5. For 3APL this final change was achieved by manually inserting update after updating beliefs, and defining the following rules for update: update <- c(P) AND holds(P) | {Deletec(P) ; update}, update <- cc(P,Q) AND holds(Q) | {Deletecc(P,Q) ; update}, update <- cc(P,Q) AND holds(P) | {Deletecc(P,Q) ; Addc(Q) ; update}, update <- cc(P,Q) AND holds(c(Q)) | {Deletecc(P,Q) ; update}, update <- true | Skip where Deletec and Deletecc delete respectively a base-level and conditional commitment, and Addc adds a base-level commitment. One aspect that doesn\"t require a change is linking commitments and actions. This is because commitments don\"t trigger actions directly: they may trigger actions indirectly, but in general their effect is to prevent completion of an interaction while there are outstanding (base level) commitments. Figure 6 shows the message sequences from a number of runs of a 3APL implementation of the NetBill commitment machine6 . In order to illustrate the different possible interactions the code was modified so that each agent selected randomly from the actions that it could perform, and a number of runs were made with the customer as the initiator, and then with the merchant as the initiator. There are other possible sequences of messages, not shown, Source code is available from The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 877 Figure 6: Sample runs from 3APL implementation (alternating turns) including the obvious one: request, quote, accept, goods, payment, receipt, and then done. One minor difference between the 3APL implementation and SAAPL concerns the semantics of messages. In the semantics of SAAPL (and of most AOPLs), receiving a message is treated as an event. However, in 3APL, receiving a message is modelled as the addition to the agent\"s beliefs of a fact indicating that the message was received . Thus in the 3APL implementation we have PG rules that are triggered by these beliefs, rather than by any event. One issue with this approach is that the belief remains there, so we need to ensure that the belief in question is either deleted once handled, or that we modify preconditions of plans to avoid handling it more than once. In our implementation we delete these received beliefs when they are handled, to avoid duplicate handling of messages. 4. BEYOND TWO PARTICIPANTS Generalising to more than two interaction participants requires revisiting how turn management is done, since it is no longer possible to assume alternating turns . In fact, perhaps surprisingly, even in the two participant setting, an alternating turn setup is an unreasonable assumption! For example, consider the path (in figure 1) from state 1 to 15 (sendGoods) then to state 12 (sendAccept). The result, in an alternating turn setup, is a dead-end: there is only a single possible action in state 12, namely sendEPO, but this action is done by the customer, and it is the merchant\"s turn to act! Figure 7 shows the FSM for NetBill with alternating initiative. A solution to this problem that works in this example, but doesn\"t generalise7 , is to weaken the alternating turn taking regime by allowing an agent to act twice in a row if its second action is driven by a commitment. A general solution is to track whose turn it is to act. This can be done by working out which agents have actions that are able to be performed in the current state. If there is only a single active agent, then it is clearly that agent\"s turn to act. However, if more than one agent is active then somehow the agents need to work out who should act next. Working this out by negotiation is not a particularly good solution for two reasons. Firstly, this negotiation has to be done at every step of the interaction where more than one agent is active (in the NetBill, this applies to seven out of sixteen states), so it is highly desirable to have a light-weight mechanism for doing this. Secondly, it is not clear how the negotiation can avoid an infinite regress situation (you go first, no, you go first, . ..) without imposing some arbitrary rule. It is also possible to resolve who should act by imposing an arbitrary rule, for example, that the customer always acts in preference to the merchant, or that each agent has a numerical priority (perhaps determined by the order in which they joined the interaction?) that determines who acts. An alternative solution, which exploits the symmetrical properties of commitment machines, is to not try and manage turn taking. Consider actions A1(C) \u21d2 p, A2(C) \u21d2 q, and A3(M) : p \u2227 q \u21d2 r. Figure 7: NetBill with alternating initiative Instead of tracking and controlling whose turn it is, we simply allow the agents to act freely, and rely on the properties of the interaction space to ensure that things work out, a notion that we shall make precise, and prove, in the remainder of this section. The issue with having multiple agents be active simultaneously is that instead of all agents agreeing on the current interaction state, agents can be in different states. This can be visualised as each agent having its own copy of the FSM that it navigates through where it is possible for agents to follow different paths through the FSM. The two specific issues that need to be addressed are: 1. Can agents end up in different final states? 2. Can an agent be in a position where an error occurs because it cannot perform an action corresponding to a received message? We will show that, because actions commute under certain assumptions, agents cannot end up in different final states, and furthermore, that errors cannot occur (again, under certain assumptions). By actions commute we mean that the state resulting from performing a sequence of actions A1 . . . An is the same, regardless of the order in which the actions are performed. This means that even if agents take different paths through the FSM, they still end up in the same resulting state, because once all messages have been processed, all agents will have performed the same set of actions. This addresses the issue of ending up in different final states. We return to the possibility of errors occurring shortly. Definition 1 (Monotonicity) An action is monotonic if it does not delete8 any fluents or commitments. A Commitment Machine is That is directly deletes, it is fine to discharge commitments by adding fluents/commitments. 878 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) monotonic if all of its actions are monotonic. (Adapted from [14, Definition 6]) Theorem 1 If A1 and A2 are monotonic actions, then performing A1 followed by A2 has the same effect on the agent\"s beliefs as performing A2 followed by A1. (Adapted from [14, Theorem 2]). This assumes that both actions can be performed. However, it is possible for the performance of A1 to disable A2 from being done. For example, if A1 has the effect +p, and A2 has precondition \u00acp, then although both actions may be enabled in the initial state, they cannot be performed in either order. We can prevent this by ensuring that actions\" preconditions do not contain negation (or implication), since a monotonic action cannot result in a precondition that is negation-free becoming false. Note that this restriction only applies to the original action precondition, P, not to any additional preconditions imposed by the agent (P ). This is because only P is used to determine whether another agent is able to perform the action. Thus monotonic CMs with preconditions that do not contain negations have actions that commute. However, in fact, the restriction to monotonic CMs is unnecessarily strong: all that is needed is that whenever there is a choice of agent that can act, then the possible actions are monotonic. If there is only a single agent that can act, then no restriction is needed on the actions: they may or may not be monotonic. Definition 2 (Locally Monotonic) A commitment machine is locally monotonic if for any state S either (a) only a single agent has actions that can be performed; or (b) all actions that can be performed in S are monotonic. Theorem 2 In a locally monotonic CM, once all messages have been processed, all agents will be in the same state. Furthermore, no errors can occur. Proof: Once all messages have been processed we have that all agents will have performed the same action set, perhaps in a different order. The essence of the proof is to argue that as long as agents haven\"t yet converged to the same state, all actions must be monotonic, and hence that these actions commute, and cannot disable any other actions. Consider the first point of divergence, where an agent performs action A and at the same time another agent (call it XB) performs action B. Clearly, this state has actions of more than one agent enabled, so, since the CM is locally monotonic, the relevant actions must be monotonic. Therefore, after doing A, the action B must still be enabled, and so the message to do B can be processed by updating the recipient agent\"s beliefs with the effects of B. Furthermore, because monotonic actions commute, the result of doing A before B is the same as doing B before A: \u2212\u2212\u2212\u2212\u2212\u2192 SA yB B SB \u2212\u2212\u2212\u2212\u2212\u2192 SAB However, what happens if the next action after A is not B, but C? Because B is enabled, and C is not done by agent XB (see below), we must have that C is also monotonic, and hence (a) the result of doing A and B and C is the same regardless of the order in which the three actions are done; and (b) C doesn\"t disable B, so B can still be done after C. \u2212\u2212\u2212\u2212\u2212\u2192 SA \u2212\u2212\u2212\u2212\u2212\u2192 SAC yB B y B SB \u2212\u2212\u2212\u2212\u2212\u2192 SAB \u2212\u2212\u2212\u2212\u2212\u2192 SABC The reason why C cannot be done by XB is that messages are processed in the order of their arrival9 . From the perspective of XB the action B was done before C, and therefore from any other agent\"s perspective the message saying that B was done must be received (and processed) before a message saying that C is done. This argument can be extended to show that once agents start taking different paths through the FSM all actions taken until the point where they converge on a single state must be monotonic, and hence it is always possible to converge (because actions aren\"t disabled), so the interaction is error free; and the resulting state once convergence occurs is the same (because monotonic actions commute). This theorem gives a strong theoretical guarantee that not doing turn management will not lead to disaster. This is analogous to proving that disabling all traffic lights would not lead to any accidents, and is only possible because the refined CM axioms are symmetrical. Based on this theorem the generic transformation from CM to code should allow agents to act freely, which is achieved by simply changing \u0131 : P \u2227 P \u2227 \u00acE \u2190 +E; \u2191A to \u0131 : P \u2227 P \u2227 \u00acE \u2190 +E; \u2191A; \u0131 For example, instead of \u0131 : \u00acrequest \u2190 +request; \u2191sendRequest we have \u0131 : \u00acrequest \u2190 +request; \u2191sendRequest; \u0131. One consequence of the theorem is that it is not necessary to ensure that agents process messages before continuing to interact. However, in order to avoid unnecessary parallelism, which can make debugging harder, it may still be desirable to process messages before performing actions. Figure 8 shows a number of runs from the 3APL implementation that has been modified to allow free, non-alternating, interaction. 5. DISCUSSION We have presented a scheme for mapping commitment machines to BDI platforms (using SAAPL as an exemplar), identified three changes that needed to be made to SAAPL to support CM-based interaction, and shown that turn management can be avoided in CMbased interaction, provided the CM is locally monotonic. The three changes to SAAPL, and the translation scheme from commitment machine to BDI plans are both applicable to any BDI language. As we have mentioned in section 1, there has been some work on designing flexible and robust agent interaction, but virtually no work on implementing flexible and robust interactions. We have already discussed STAPLE . Another piece of work that is relevant is the work by Cheong and Winikoff on their Hermes methodology . Although the main focus of their work is a pragmatic design methodology, they also provide guidelines for implementing Hermes designs using BDI platforms (specifically Jadex) . However, since Hermes does not yield a design that is formal, it is only possible to generate skeleton code that then needs to be completed. Also, they do not address the turn taking issue: how to decide which agent acts when more than one agent is able to act. We also assume that the communication medium does not deliver messages out of order, which is the case for (e.g.) TCP. The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 879 Figure 8: Sample runs from 3APL implementation (non-alternating turns) The work of Kremer and Flores (e.g. ) also uses commitments, and deals with implementation. However, they provide infrastructure support (CASA) rather than a programming language, and do not appear to provide assistance to a programmer seeking to implement agents. Although we have implemented the NetBill interaction using 3APL, the changes to the semantics were done by modifying our NetBill 3APL program, rather than by modifying the 3APL implementation itself. Clearly, it would be desirable to modify the semantics of 3APL (or of another language) directly, by changing the implementation. Also, although we have not done so, it should be clear that the translation from a CM to its implementation could easily be automated. Another area for further work is to look at how the assumptions required to ensure that actions commute can be relaxed. Finally, there is a need to perform empirical evaluation. There has already been some work on comparing Hermes with a conventional message-centric approach to designing interaction, and this has shown that using Hermes results in designs that are significantly more flexible and robust . It would be interesting to compare commitment machines with Hermes, but, since commitment machines are a framework, not a design methodology, we need to compare Hermes with a methodology for designing interactions that results in commitment machines .", "body1": "Agents are social, and agent interaction plays a vital role in multiagent systems. It has been argued that this message-centric approach to interaction design is not a good match for intelligent agents. However, although there has been work on designing flexible and robust agent interactions, there has been virtually no work on providing programming language support for implementing such interactions. Indeed, modern AOPLs , with virtually no exceptions, provide only simple message sending as the basis for implementing agent interaction. This paper presents what, to the best of our knowledge, is the second AOPL to support high-level, flexible, and robust agent interaction implementation. This paper presents a scheme for extending BDI-like AOPLs to support direct implementation of agent interactions that are designed using Yolum & Singh\"s commitment machine (CM) framework . 2.1 Commitment Machines The aim of the commitment machine framework is to allow for the definition of interactions that are more flexible than traditional message-centric approaches. A base-level social commitment is an undertaking by debtor A to creditor B to bring about condition p, denoted C(A, B, p). The semantics of commitments (both base-level and conditional) is defined with rules that specify how commitments change over time. A commitment machine (CM) defines a range of possible interactions that each start in some state1 , and perform actions until reaching a final state. We use the notation A(X) : P \u21d2 E to indicate that action A is performed by entity X, has precondition P (with : P omitted if empty) and effect E. \u2022 sendRequest(C) \u21d2 request \u2022 sendQuote(M) \u21d2 offer where offer \u2261 promiseGoods \u2227 promiseReceipt and promiseGoods \u2261 CC(M, C, accept, goods) and promiseReceipt \u2261 CC(M, C, pay, receipt) \u2022 sendAccept(C) \u21d2 accept where accept \u2261 CC(C, M, goods, pay) \u2022 sendGoods(M) \u21d2 promiseReceipt \u2227 goods where promiseReceipt \u2261 CC(M, C, pay, receipt) \u2022 sendEPO(C) : goods \u21d2 pay \u2022 sendReceipt(M) : pay \u21d2 receipt. The commitment accept is the customer\"s promise to pay once goods have been sent, promiseGoods is the merchant\"s promise to send the goods once the customer accepts, and promiseReceipt is the merchant\"s promise to send a receipt once payment has been made. As seen in figure 1, commitment machines can support a range of interaction sequences. 2.2 An Abstract Agent ProgrammingLanguage Agent programming languages in the BDI tradition (e.g. We now briefly define the formal syntax and semantics of a Simple Abstract (BDI) Agent Programming Language (SAAPL). We have the empty step which always succeeds and does nothing, operations to add (+b) and delete (\u2212b) beliefs, sending a message m to agent N (\u2191N m), and posting an event4 (e). C ::= b | C \u2227 C | C \u2228 C | \u00acC | \u2203x.C P ::= | +b | \u2212b | e | \u2191N m | P; P Formal semantics for this language is given in figure 2. 874 The Sixth Intl. We define a basic configuration S = Q, N, B, P where Q is a (global) message queue (modelled as a sequence5 where messages are added at one end and removed from the other end), N is the name of the agent, B is the beliefs of the agent and P is the plan body being executed (i.e. A transition S0 \u2212\u2192 S1 specifies that executing S0 a single step yields S1. Note that there is non-determinism in SAAPL, e.g. INTERACTIONS In this section we present a mapping from a commitment machine to a collection of SAAPL programs (one for each role). Each action A(X) : P \u21d2 E is mapped to a number of plans: there is a plan (for agent X) with context condition P that performs the action (i.e. For example, given the action sendAccept(C) \u21d2 accept we have the following plans, where each plan is preceded by M: or C: to indicate which agent that plan belongs to. M: \u2193sendAccept : true \u2190 +accept; \u0131. If the action has a non-trivial precondition then there are two plans in the recipient: one to perform the action (if possible), and another to report an error if the action\"s precondition doesn\"t hold (we return to this in section 4). C: \u2193sendReceipt : pay \u2190 +receipt; \u0131. C: \u2193sendReceipt : \u00acpay \u2190 . \u2193done : \u00ac\u2203p.C(p) \u2190 . \u2193done : \u2203p.C(p) \u2190 . An interaction is started by setting up an agent\"s initial beliefs, and then having it begin to interact. Figure 3 gives the SAAPL programs for both merchant and customer that implement the NetBill protocol. We now turn to refining the context conditions. For each plan of the form \u0131 : P \u2190 +E; \u2191m we replace the context condition P with the enhanced condition P \u2227 P \u2227 \u00acE where P is any additional conditions that the agent wishes to impose, and \u00acE is the negation of the effects of the action. \u0131 : true \u2190 +accept; \u2191sendAccept. \u0131 : goods \u2190 +pay; \u2191sendEPO. \u2193sendQuote : true \u2190 +promiseGoods; +promiseReceipt; \u0131. \u2193sendGoods : true \u2190 +promiseReceipt; +goods; \u0131. \u2193sendReceipt : pay \u2190 +receipt; \u0131. Merchant\"s plans: \u0131 : true \u2190 +promiseGoods; +promiseReceipt; \u2191sendQuote. \u0131 : true \u2190 +promiseReceipt; +goods; \u2191sendGoods. \u0131 : pay \u2190 +receipt; \u2191sendReceipt. \u2193sendRequest : true \u2190 +request; \u0131. \u2193sendAccept : true \u2190 +accept; \u0131. \u2193sendEPO : goods \u2190 +pay; \u0131. Shared plans (i.e. \u2193init(B) : true \u2190 +B. \u2193start : true \u2190 \u0131. Where accept \u2261 CC(goods pay) promiseGoods \u2261 CC(accept goods) promiseReceipt \u2261 CC(pay receipt) offer \u2261 promiseGoods \u2227 promiseReceipt Figure 3: SAAPL Implementation of NetBill the sender won\"t perform it and send the message (see also the discussion in section 4). When specifying additional conditions (P ), some care needs to be taken to avoid situations where progress cannot be made because the only action(s) possible are prevented by additional conditions. One way of indicating preference between actions (in many agent platforms) is to reorder the agent\"s plans. The third refinement of context conditions concerns the plans that terminate the interaction. In order to support the realisation of CMs, we need to change SAAPL in a number of ways. \u0131 : goods \u2227 \u00acpay \u2190 +pay; \u2191sendEPO. Merchant\"s plans: \u0131 : \u00acoffer \u2190 +promiseGoods; +promiseReceipt; \u2191sendQuote. \u0131 : \u00ac(promiseReceipt \u2227 goods) \u2190 +promiseReceipt; +goods; \u2191sendGoods. \u0131 : pay \u2227 \u00acreceipt \u2190 +receipt; \u2191sendReceipt. Where accept \u2261 CC(goods pay) promiseGoods \u2261 CC(accept goods) promiseReceipt \u2261 CC(pay receipt) offer \u2261 promiseGoods \u2227 promiseReceipt Figure 4: SAAPL Implementation of NetBill with refined context conditions (changed plans only) 2. changing the definition of |= to encompass implied commitments; and 3. whenever a belief is added, updating existing commitments, according to the rules of commitment dynamics. Extending the notion of beliefs to encompass commitments in fact requires no change in agent platforms that are prolog-like and support terms as beliefs (e.g. Whenever a context condition contains commitments, determining whether the context condition is implied by the agent\"s beliefs (B |= C) needs to take into account the notion of implied commitments . holds(c(P)) :- holds(P). holds(c(P)) :- clause(cc(Q,P),true), holds(Q). holds(cc(_,Q)) :- holds(Q). holds(cc(_,Q)) :- holds(c(Q)). The first clause simply says that anything holds if it is in agent\"s beliefs (clause(X,true) is true if X is a fact). B = norm(B \u222a {b}) Q, N, B, +b \u2212\u2192 Q, N, B , function norm(B) B \u2190 B for each b \u2208 B do if b = C(p) \u2227 B |= p then B \u2190 B \\ {b} elseif b = CC(p q) then if B |= q then B \u2190 B \\ {b} elseif B |= p then B \u2190 (B \\ {b}) \u222a {C(q)} elseif B |= C(q) then B \u2190 B \\ {b} endif endif endfor return B end function B |= P B |= C(P) C1 CC(Q P) \u2208 B B |= Q B |= P C2 B |= CC(P Q) B |= Q CC1 B |= C(Q) B |= CC(P Q) CC2 Figure 5: New Operational Semantics The final change is to update commitments when a belief is added. One aspect that doesn\"t require a change is linking commitments and actions. Figure 6 shows the message sequences from a number of runs of a 3APL implementation of the NetBill commitment machine6 . One minor difference between the 3APL implementation and SAAPL concerns the semantics of messages. Generalising to more than two interaction participants requires revisiting how turn management is done, since it is no longer possible to assume alternating turns . In fact, perhaps surprisingly, even in the two participant setting, an alternating turn setup is an unreasonable assumption! A general solution is to track whose turn it is to act. Consider actions A1(C) \u21d2 p, A2(C) \u21d2 q, and A3(M) : p \u2227 q \u21d2 r. Figure 7: NetBill with alternating initiative Instead of tracking and controlling whose turn it is, we simply allow the agents to act freely, and rely on the properties of the interaction space to ensure that things work out, a notion that we shall make precise, and prove, in the remainder of this section. The issue with having multiple agents be active simultaneously is that instead of all agents agreeing on the current interaction state, agents can be in different states. By actions commute we mean that the state resulting from performing a sequence of actions A1 . 878 The Sixth Intl. For example, if A1 has the effect +p, and A2 has precondition \u00acp, then although both actions may be enabled in the initial state, they cannot be performed in either order. Thus monotonic CMs with preconditions that do not contain negations have actions that commute. Theorem 2 In a locally monotonic CM, once all messages have been processed, all agents will be in the same state. Proof: Once all messages have been processed we have that all agents will have performed the same action set, perhaps in a different order. Consider the first point of divergence, where an agent performs action A and at the same time another agent (call it XB) performs action B. \u2212\u2212\u2212\u2212\u2212\u2192 SA \u2212\u2212\u2212\u2212\u2212\u2192 SAC yB B y B SB \u2212\u2212\u2212\u2212\u2212\u2192 SAB \u2212\u2212\u2212\u2212\u2212\u2192 SABC The reason why C cannot be done by XB is that messages are processed in the order of their arrival9 . This argument can be extended to show that once agents start taking different paths through the FSM all actions taken until the point where they converge on a single state must be monotonic, and hence it is always possible to converge (because actions aren\"t disabled), so the interaction is error free; and the resulting state once convergence occurs is the same (because monotonic actions commute). This theorem gives a strong theoretical guarantee that not doing turn management will not lead to disaster. Based on this theorem the generic transformation from CM to code should allow agents to act freely, which is achieved by simply changing \u0131 : P \u2227 P \u2227 \u00acE \u2190 +E; \u2191A to \u0131 : P \u2227 P \u2227 \u00acE \u2190 +E; \u2191A; \u0131 For example, instead of \u0131 : \u00acrequest \u2190 +request; \u2191sendRequest we have \u0131 : \u00acrequest \u2190 +request; \u2191sendRequest; \u0131. One consequence of the theorem is that it is not necessary to ensure that agents process messages before continuing to interact. Figure 8 shows a number of runs from the 3APL implementation that has been modified to allow free, non-alternating, interaction.", "body2": "The standard approach for designing agent interactions is messagecentric: interactions are defined by interaction protocols that give the permissible sequences of messages, specified using notations such as finite state machines, Petri nets, or Agent UML. There has also been work on richer forms of interaction in specific settings, such as teams of cooperative agents . Current Agent Oriented Programming Languages (AOPLs) do not provide support for implementing flexible and robust agent interactions using higher-level concepts than messages. Indeed, modern AOPLs , with virtually no exceptions, provide only simple message sending as the basis for implementing agent interaction. The first such language, STAPLE, was proposed a few years ago , but is not described in detail, and is arguably impractical for use by non-specialists, due to its logical basis and heavy reliance on temporal and modal logic. We then extend our scheme to address a range of issues concerned with distribution, including turn tracking , and race conditions. This interact state consists of fluents (predicates that change value over time), but also social commitments, both base-level and conditional. For example, a commitment to make the fluent paid true once goods have been received would be written CC(goods paid). An interaction is defined by specifying the entities involved, the possible contents of the interaction state (both fluents and commitments), and (most importantly) the actions that each entity can perform along with the preconditions and effects of each action, specified as add and delete lists. The finite state machine is software-generated: the nodes and connections were computed by an implementation of the axioms (available from ) and were then laid out by graphviz . We use the notation A(X) : P \u21d2 E to indicate that action A is performed by entity X, has precondition P (with : P omitted if empty) and effect E. \u2022 sendRequest(C) \u21d2 request \u2022 sendQuote(M) \u21d2 offer where offer \u2261 promiseGoods \u2227 promiseReceipt and promiseGoods \u2261 CC(M, C, accept, goods) and promiseReceipt \u2261 CC(M, C, pay, receipt) \u2022 sendAccept(C) \u21d2 accept where accept \u2261 CC(C, M, goods, pay) \u2022 sendGoods(M) \u21d2 promiseReceipt \u2227 goods where promiseReceipt \u2261 CC(M, C, pay, receipt) \u2022 sendEPO(C) : goods \u21d2 pay \u2022 sendReceipt(M) : pay \u21d2 receipt. The commitment accept is the customer\"s promise to pay once goods have been sent, promiseGoods is the merchant\"s promise to send the goods once the customer accepts, and promiseReceipt is the merchant\"s promise to send a receipt once payment has been made. As seen in figure 1, commitment machines can support a range of interaction sequences. One of these is chosen and is executed. The plan body is built up from the following constructs. These can be sequenced (P; P). The semantics is in the style of Plotkin\"s Structural Operational Semantics, and assumes that operations exist that check whether a condition We use \u2193N m as short hand for the event corresponding to receiving message m from agent N. More sophisticated belief management methods may be used, but are not considered here. The global message queue is a sequence of triplets of the form sender:recipient:message. The transition relation is defined using rules of the form S \u2212\u2192 S or of the form S \u2212\u2192 Sr S \u2212\u2192 Sr ; the latter are conditional with the top (numerator) being the premise and the bottom (denominator) being the conclusion. This is resolved by using selection functions: SO selects one of the applicable plan instances to handle a given event, SI selects which of the plan instances that can be executed should be executed next, and SA selects which agent should execute (a step) next. In section 4 we relax these assumptions. applies the effects E to the agent\"s beliefs) and sends a message to the other agent, and a plan (for the other agent) that updates its state when a message is received from X. C: \u0131 : true \u2190 +accept; \u2191sendAccept. M: \u2193sendAccept : true \u2190 +accept; \u0131. For example, the action sendReceipt(M) : pay \u21d2 receipt generates the following plans: M: \u0131 : pay \u2190 +receipt; \u2191sendReceipt. C: \u2193sendReceipt : pay \u2190 +receipt; \u0131. An interaction can be completed whenever there are no base-level commitments, so both agents have the following plans: \u0131 : \u00ac\u2203p.C(p) \u2190 \u2191done. \u2193done : \u00ac\u2203p.C(p) \u2190 . . \u2193start : true \u2190 \u0131. For conciseness the error reporting plans are omitted. Thirdly, the context conditions of the plans that terminate the interaction need to be refined in order to avoid terminating the interaction prematurely. For each plan of the form \u2193m : P \u2190 +E; \u0131 we could add \u00acE to the precondition, but this is redundant, since it is already checked by the performer of the action, and if the action has no effect then Customer\"s plans: \u0131 : true \u2190 +request; \u2191sendRequest. \u0131 : true \u2190 +accept; \u2191sendAccept. \u0131 : goods \u2190 +pay; \u2191sendEPO. \u2193sendQuote : true \u2190 +promiseGoods; +promiseReceipt; \u0131. \u2193sendGoods : true \u2190 +promiseReceipt; +goods; \u0131. \u2193sendReceipt : pay \u2190 +receipt; \u0131. Merchant\"s plans: \u0131 : true \u2190 +promiseGoods; +promiseReceipt; \u2191sendQuote. \u0131 : true \u2190 +promiseReceipt; +goods; \u2191sendGoods. \u0131 : pay \u2190 +receipt; \u2191sendReceipt. \u2193sendRequest : true \u2190 +request; \u0131. \u2193sendAccept : true \u2190 +accept; \u0131. \u2193sendEPO : goods \u2190 +pay; \u0131. plans of both agents): \u0131 : \u00ac\u2203p.C(p) \u2190 \u2191done. \u2193init(B) : true \u2190 +B. \u2193start : true \u2190 \u0131. Where accept \u2261 CC(goods pay) promiseGoods \u2261 CC(accept goods) promiseReceipt \u2261 CC(pay receipt) offer \u2261 promiseGoods \u2227 promiseReceipt Figure 3: SAAPL Implementation of NetBill the sender won\"t perform it and send the message (see also the discussion in section 4). When specifying additional conditions (P ), some care needs to be taken to avoid situations where progress cannot be made because the only action(s) possible are prevented by additional conditions. This is clearly safe, since actions are not prevented, just considered in a different order. Figure 4 shows the plans that are changed from figure 3. \u0131 : \u00acaccept \u2190 +accept; \u2191sendAccept. \u0131 : goods \u2227 \u00acpay \u2190 +pay; \u2191sendEPO. Merchant\"s plans: \u0131 : \u00acoffer \u2190 +promiseGoods; +promiseReceipt; \u2191sendQuote. \u0131 : \u00ac(promiseReceipt \u2227 goods) \u2190 +promiseReceipt; +goods; \u2191sendGoods. \u0131 : pay \u2227 \u00acreceipt \u2190 +receipt; \u2191sendReceipt. Where accept \u2261 CC(goods pay) promiseGoods \u2261 CC(accept goods) promiseReceipt \u2261 CC(pay receipt) offer \u2261 promiseGoods \u2227 promiseReceipt Figure 4: SAAPL Implementation of NetBill with refined context conditions (changed plans only) 2. changing the definition of |= to encompass implied commitments; and 3. whenever a belief is added, updating existing commitments, according to the rules of commitment dynamics. In the case of 3APL no change is needed to support this. For 3APL this was realised by having each agent include the following Prolog clauses: holds(X) :- clause(X,true). holds(c(P)) :- holds(P). holds(c(P)) :- clause(cc(Q,P),true), holds(Q). holds(cc(_,Q)) :- holds(Q). holds(cc(_,Q)) :- holds(c(Q)). To use these rules we then modify context conditions in our program so that instead of writing, for example, cc(m,c, pay, receipt) we write holds(cc(m,c, pay, receipt)). For 3APL this final change was achieved by manually inserting update after updating beliefs, and defining the following rules for update: update <- c(P) AND holds(P) | {Deletec(P) ; update}, update <- cc(P,Q) AND holds(Q) | {Deletecc(P,Q) ; update}, update <- cc(P,Q) AND holds(P) | {Deletecc(P,Q) ; Addc(Q) ; update}, update <- cc(P,Q) AND holds(c(Q)) | {Deletecc(P,Q) ; update}, update <- true | Skip where Deletec and Deletecc delete respectively a base-level and conditional commitment, and Addc adds a base-level commitment. This is because commitments don\"t trigger actions directly: they may trigger actions indirectly, but in general their effect is to prevent completion of an interaction while there are outstanding (base level) commitments. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 877 Figure 6: Sample runs from 3APL implementation (alternating turns) including the obvious one: request, quote, accept, goods, payment, receipt, and then done. In our implementation we delete these received beliefs when they are handled, to avoid duplicate handling of messages. Generalising to more than two interaction participants requires revisiting how turn management is done, since it is no longer possible to assume alternating turns . A solution to this problem that works in this example, but doesn\"t generalise7 , is to weaken the alternating turn taking regime by allowing an agent to act twice in a row if its second action is driven by a commitment. An alternative solution, which exploits the symmetrical properties of commitment machines, is to not try and manage turn taking. Consider actions A1(C) \u21d2 p, A2(C) \u21d2 q, and A3(M) : p \u2227 q \u21d2 r. Figure 7: NetBill with alternating initiative Instead of tracking and controlling whose turn it is, we simply allow the agents to act freely, and rely on the properties of the interaction space to ensure that things work out, a notion that we shall make precise, and prove, in the remainder of this section. We will show that, because actions commute under certain assumptions, agents cannot end up in different final states, and furthermore, that errors cannot occur (again, under certain assumptions). A Commitment Machine is That is directly deletes, it is fine to discharge commitments by adding fluents/commitments. However, it is possible for the performance of A1 to disable A2 from being done. This is because only P is used to determine whether another agent is able to perform the action. Definition 2 (Locally Monotonic) A commitment machine is locally monotonic if for any state S either (a) only a single agent has actions that can be performed; or (b) all actions that can be performed in S are monotonic. Furthermore, no errors can occur. The essence of the proof is to argue that as long as agents haven\"t yet converged to the same state, all actions must be monotonic, and hence that these actions commute, and cannot disable any other actions. Because B is enabled, and C is not done by agent XB (see below), we must have that C is also monotonic, and hence (a) the result of doing A and B and C is the same regardless of the order in which the three actions are done; and (b) C doesn\"t disable B, so B can still be done after C. From the perspective of XB the action B was done before C, and therefore from any other agent\"s perspective the message saying that B was done must be received (and processed) before a message saying that C is done. This argument can be extended to show that once agents start taking different paths through the FSM all actions taken until the point where they converge on a single state must be monotonic, and hence it is always possible to converge (because actions aren\"t disabled), so the interaction is error free; and the resulting state once convergence occurs is the same (because monotonic actions commute). This is analogous to proving that disabling all traffic lights would not lead to any accidents, and is only possible because the refined CM axioms are symmetrical. Based on this theorem the generic transformation from CM to code should allow agents to act freely, which is achieved by simply changing \u0131 : P \u2227 P \u2227 \u00acE \u2190 +E; \u2191A to \u0131 : P \u2227 P \u2227 \u00acE \u2190 +E; \u2191A; \u0131 For example, instead of \u0131 : \u00acrequest \u2190 +request; \u2191sendRequest we have \u0131 : \u00acrequest \u2190 +request; \u2191sendRequest; \u0131. However, in order to avoid unnecessary parallelism, which can make debugging harder, it may still be desirable to process messages before performing actions. Figure 8 shows a number of runs from the 3APL implementation that has been modified to allow free, non-alternating, interaction.", "introduction": "Agents are social, and agent interaction plays a vital role in multiagent systems. Consequently, design and implementation of agent interaction is an important research topic. The standard approach for designing agent interactions is messagecentric: interactions are defined by interaction protocols that give the permissible sequences of messages, specified using notations such as finite state machines, Petri nets, or Agent UML. It has been argued that this message-centric approach to interaction design is not a good match for intelligent agents. Intelligent agents should exhibit the ability to persist in achieving their goals in the face of failure (robustness) by trying different approaches (flexibility). On the other hand, when following an interaction protocol, an agent has limited flexibility and robustness: the ability to persistently try alternative means to achieving the interaction\"s aim is limited to those options that the protocol\"s designer provided, and in practice, message-centric design processes do not tend to lead to protocols that are flexible or robust. Recognising these limitations of the traditional approach to designing agent interactions, a number of approaches have been proposed in recent years that move away from message-centric interaction protocols, and instead consider designing agent interactions using higher-level concepts such as social commitments or interaction goals . There has also been work on richer forms of interaction in specific settings, such as teams of cooperative agents . However, although there has been work on designing flexible and robust agent interactions, there has been virtually no work on providing programming language support for implementing such interactions. Current Agent Oriented Programming Languages (AOPLs) do not provide support for implementing flexible and robust agent interactions using higher-level concepts than messages. Indeed, modern AOPLs , with virtually no exceptions, provide only simple message sending as the basis for implementing agent interaction. This paper presents what, to the best of our knowledge, is the second AOPL to support high-level, flexible, and robust agent interaction implementation. The first such language, STAPLE, was proposed a few years ago , but is not described in detail, and is arguably impractical for use by non-specialists, due to its logical basis and heavy reliance on temporal and modal logic. This paper presents a scheme for extending BDI-like AOPLs to support direct implementation of agent interactions that are designed using Yolum & Singh\"s commitment machine (CM) framework . In the remainder of this paper we briefly review commitment machines and present a simple abstraction of BDI AOPLs which lies in the common subset of languages such as Jason, 3APL, and CAN. We then present a scheme for translating commitment machines to this language, and indicate how the language needs to be extended to support this. We then extend our scheme to address a range of issues concerned with distribution, including turn tracking , and race conditions.", "conclusion": "We have presented a scheme for mapping commitment machines to BDI platforms (using SAAPL as an exemplar), identified three changes that needed to be made to SAAPL to support CM-based interaction, and shown that turn management can be avoided in CMbased interaction, provided the CM is locally monotonic.. The three changes to SAAPL, and the translation scheme from commitment machine to BDI plans are both applicable to any BDI language.. As we have mentioned in section 1, there has been some work on designing flexible and robust agent interaction, but virtually no work on implementing flexible and robust interactions.. We have already discussed STAPLE .. Another piece of work that is relevant is the work by Cheong and Winikoff on their Hermes methodology .. Although the main focus of their work is a pragmatic design methodology, they also provide guidelines for implementing Hermes designs using BDI platforms (specifically Jadex) .. However, since Hermes does not yield a design that is formal, it is only possible to generate skeleton code that then needs to be completed.. Also, they do not address the turn taking issue: how to decide which agent acts when more than one agent is able to act.. We also assume that the communication medium does not deliver messages out of order, which is the case for (e.g.). on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 879 Figure 8: Sample runs from 3APL implementation (non-alternating turns) The work of Kremer and Flores (e.g.. ) also uses commitments, and deals with implementation.. However, they provide infrastructure support (CASA) rather than a programming language, and do not appear to provide assistance to a programmer seeking to implement agents.. Although we have implemented the NetBill interaction using 3APL, the changes to the semantics were done by modifying our NetBill 3APL program, rather than by modifying the 3APL implementation itself.. Clearly, it would be desirable to modify the semantics of 3APL (or of another language) directly, by changing the implementation.. Also, although we have not done so, it should be clear that the translation from a CM to its implementation could easily be automated.. Another area for further work is to look at how the assumptions required to ensure that actions commute can be relaxed.. Finally, there is a need to perform empirical evaluation.. There has already been some work on comparing Hermes with a conventional message-centric approach to designing interaction, and this has shown that using Hermes results in designs that are significantly more flexible and robust .. It would be interesting to compare commitment machines with Hermes, but, since commitment machines are a framework, not a design methodology, we need to compare Hermes with a methodology for designing interactions that results in commitment machines ."}
{"id": "J-36", "keywords": ["game theori", "inform acquisit", "socrat game", "nash equilibrium", "correl equilibrium", "algorithm"], "title": "Playing games in many possible worlds", "abstract": "In traditional game theory, players are typically endowed with exogenously given knowledge of the structure of the game--either full omniscient knowledge or partial but fixed information. In real life, however, people are often unaware of the utility of taking a particular action until they perform research into its consequences. In this paper, we model this phenomenon. We imagine a player engaged in a question and- answer session, asking questions both about his or her own preferences and about the state of reality; thus we call this setting \"Socratic\" game theory. In a Socratic game, players begin with an a priori probability distribution over many possible worlds, with a different utility function for each world. Players can make queries, at some cost, to learn partial information about which of the possible worlds is the actual world, before choosing an action. We consider two query models: (1) an unobservable-query model, in which players learn only the response to their own queries, and (2) an observable-query model, in which players also learn which queries their opponents made.The results in this paper consider cases in which the underlying worlds of a two-player Socratic game are either constant-sum games or strategically zero-sum games, a class that generalizes constant-sum games to include all games in which the sum of payoffs depends linearly on the interaction between the players. When the underlying worlds are constant sum, we give polynomial-time algorithms to find Nash equilibria in both the observable- and unobservable-query models. When the worlds are strategically zero sum, we give efficient algorithms to find Nash equilibria in unobservablequery Socratic games and correlated equilibria in observablequery Socratic games.", "references": ["Faster approximation algorithms for the minimum latency problem", "Subjectivity and correlation in randomized strategies", "Correlated equilibrium as an expression of Bayesian rationality", "Information acquisition and efficient mechanism design", "Information in mechanism design", "The complexity of decentralized control of Markov Decision Processes", "The minimum latency problem", "Optimal plans for aggregation", "Query strategies for priced information", "3-NASH is PPAD-complete", "Settling the complexity of 2-player Nash-equilibrium", "Auctions and information acquisition: Sealed-bid or dynamic formats?", "Complexity results about Nash equilibria", "Location of bank accounts to optimize float: An analytic study of exact and approximate algorithms", "Gathering information before signing a contract", "The complexity of computing a Nash equilbrium", "Three-player games are hard", "Approximation algorithms for the test cover problem", "On making the right choice: The deliberation-without-attention effect", "Approximate solutions for partially observable stochastic games with common payoffs", "The complexity of pure Nash equilibria", "Multi-stage Information Acquisition in Auction Design", "Optimality and domination in repeated games with bounded players", "Efficient algorithms for learning to play repeated games against computationally bounded adversaries", "Drew Fudenberg and Jean Tirole", "An improved approximation ratio for the minimum latency problem", "Reducibility among equilibrium problems", "The ellipsoid method and its consequences in combinatorial optimization", " Sorting and selection with structured costs", "Dynamic programming for partially observable stochastic games", "Games with incomplete information played by Bayesian players", "Existence of correlated equilibria", "Time-dependent utility and action under uncertainty", "A New Introduction to Modal Logic", "When choice is demotivating: Can one desire too much of a good thing?", "Bounded rationality and strategic complexity in repeated games", "Selection with monotone comparison costs", "A polynomial algorithm in linear programming", "The complexity of two-person zero-sum games in extensive form", "Efficient computation of equilibria for extensive two-person games", "Mechanism Design for Computationally Limited Agents", "Bargaining with limited computation: Deliberation equilibrium", "Costly valuation computation in auctions", "Strategic deliberation and truthful revelation: An impossibility result", "Equilibrium points of bimatrix games", "Playing large games using simple strategies", "An efficient exact algorithm for singly connected graphical games", "Information acquisition and the excess refund puzzle", "Computation of equilibria in finite games", "On minimizing a set of tests", "Strategically zero-sum games: The class of games whose completely mixed equilibria cannot be improved upon", "Equilibrium points in n-person games", "Finitely repeated games with finite automata", "On the complexity of the parity argument and other inefficient proofs of existence", "Algorithms, games, and the internet", "Computing correlated equilibria in multi-player games", "Computing equilibria in multiplayer games", "On bounded rationality and computational complexity", "Auction design with costly preference elicitation", "Information acquisition in auctions", "The LP formulation of finite zero-sum games with incomplete information", "Strategic implications of uncertainty over one's own private value in auctions", "Mid-auction information acquisition", "Modeling Bounded Rationality", "The Paradox of Choice: Why More is Less", "Models of Bounded Rationality", "Choice in context: Tradeoff contrast and extremeness aversion", "Dynamic models of deliberation and the theory of games", "Reinforcement Learning: An Introduction", "Theory of Games and Economic Behavior", "Computing equilibria for two-person games", "Approximate reasoning using anytime algorithms"], "full_text": "1.. A smoky room. Democratic Party strategists huddle around a map. How should the Kennedy campaign allocate its remaining advertising budget? Should it focus on, say, California or New York? The Nixon campaign faces the same dilemma. Of course, neither campaign knows the effectiveness of its advertising in each state. Perhaps Californians are susceptible to Nixon\"s advertising, but are unresponsive to Kennedy\"s. In light of this uncertainty, the Kennedy campaign may conduct a survey, at some cost, to estimate the effectiveness of its advertising. Moreover, the larger-and more expensive-the survey, the more accurate it will be. Is the cost of a survey worth the information that it provides? How should one balance the cost of acquiring more information against the risk of playing a game with higher uncertainty? In this paper, we model situations of this type as Socratic games. As in traditional game theory, the players in a Socratic game choose actions to maximize their payoffs, but we model players with incomplete information who can make costly queries to reduce their uncertainty about the state of the world before they choose their actions. This approach contrasts with traditional game theory, in which players are usually modeled as having fixed, exogenously given information about the structure of the game and its payoffs. (In traditional games of incomplete and imperfect information, there is information that the players do not have; in Socratic games, unlike in these games, the players have a chance to acquire the missing information, at some cost.) A number of related models have been explored by economists and computer scientists motivated by similar situations, often with a focus on mechanism design and auctions; a sampling of this research includes the work of Larson and Sandholm , Parkes , Fong , Compte and Jehiel , Rezende , Persico and Matthews , Cr\u00b4emer and Khalil , Rasmusen , and Bergemann and V\u00a8alim\u00a8aki . The model of Bergemann and V\u00a8alim\u00a8aki is similar in many regards to the one that we explore here; see Section 7 for some discussion. A Socratic game proceeds as follows. A real world is cho150 sen randomly from a set of possible worlds according to a common prior distribution. Each player then selects an arbitrary query from a set of available costly queries and receives a corresponding piece of information about the real world. Finally each player selects an action and receives a payoff-a function of the players\" selected actions and the identity of the real world-less the cost of the query that he or she made. Compared to traditional game theory, the distinguishing feature of our model is the introduction of explicit costs to the players for learning arbitrary partial information about which of the many possible worlds is the real world. Our research was initially inspired by recent results in psychology on decision making, but it soon became clear that Socratic game theory is also a general tool for understanding the exploitation versus exploration tradeoff, well studied in machine learning, in a strategic multiplayer environment. This tension between the risk arising from uncertainty and the cost of acquiring information is ubiquitous in economics, political science, and beyond. Our results. We consider Socratic games under two models: an unobservable-query model where players learn only the response to their own queries and an observable-query model where players also learn which queries their opponents made. We give efficient algorithms to find Nash equilibriai.e., tuples of strategies from which no player has unilateral incentive to deviate-in broad classes of two-player Socratic games in both models. Our first result is an efficient algorithm to find Nash equilibria in unobservable-query Socratic games with constant-sum worlds, in which the sum of the players\" payoffs is independent of their actions. Our techniques also yield Nash equilibria in unobservable-query Socratic games with strategically zero-sum worlds. Strategically zero-sum games generalize constant-sum games by allowing the sum of the players\" payoffs to depend on individual players\" choices of strategy, but not on any interaction of their choices. Our second result is an efficient algorithm to find Nash equilibria in observable-query Socratic games with constant-sum worlds. Finally, we give an efficient algorithm to find correlated equilibria-a weaker but increasingly well-studied solution concept for games -in observable-query Socratic games with strategically zero-sum worlds. Like all games, Socratic games can be viewed as a special case of extensive-form games, which represent games by trees in which internal nodes represent choices made by chance or by the players, and the leaves represent outcomes that correspond to a vector of payoffs to the players. Algorithmically, the generality of extensive-form games makes them difficult to solve efficiently, and the special cases that are known to be efficiently solvable do not include even simple Socratic games. Every (complete-information) classical game is a trivial Socratic game (with a single possible world and a single trivial query), and efficiently finding Nash equilibria in classical games has been shown to be hard . Therefore we would not expect to find a straightforward polynomial-time algorithm to compute Nash equilibria in general Socratic games. However, it is well known that Nash equilibria can be found efficiently via an LP for two-player constant-sum games (and strategically zero-sum games ). A Socratic game is itself a classical game, so one might hope that these results can be applied to Socratic games with constant-sum (or strategically zero-sum) worlds. We face two major obstacles in extending these classical results to Socratic games. First, a Socratic game with constant-sum worlds is not itself a constant-sum classical game-rather, the resulting classical game is only strategically zero sum. Worse yet, a Socratic game with strategically zero-sum worlds is not itself classically strategically zero sum-indeed, there are no known efficient algorithmic techniques to compute Nash equilibria in the resulting class of classical games. (Exponential-time algorithms like Lemke/Howson, of course, can be used .) Thus even when it is easy to find Nash equilibria in each of the worlds of a Socratic game, we require new techniques to solve the Socratic game itself. Second, even when the Socratic game itself is strategically zero sum, the number of possible strategies available to each player is exponential in the natural representation of the game. As a result, the standard linear programs for computing equilibria have an exponential number of variables and an exponential number of constraints. For unobservable-query Socratic games with strategically zero-sum worlds, we address these obstacles by formulating a new LP that uses only polynomially many variables (though still an exponential number of constraints) and then use ellipsoid-based techniques to solve it. For observablequery Socratic games, we handle the exponentiality by decomposing the game into stages, solving the stages separately, and showing how to reassemble the solutions efficiently. To solve the stages, it is necessary to find Nash equilibria in Bayesian strategically zero-sum games, and we give an explicit polynomial-time algorithm to do so. 2. GAMES AND SOCRATIC GAMES In this section, we review background on game theory and formally introduce Socratic games. We present these models in the context of two-player games, but the multiplayer case is a natural extension. Throughout the paper, boldface variables will be used to denote a pair of variables (e.g., a = ai, aii ). Let Pr[x \u2190 \u03c0] denote the probability that a particular value x is drawn from the distribution \u03c0, and let Ex\u223c\u03c0[g(x)] denote the expectation of g(x) when x is drawn from \u03c0. 2.1 Background on Game Theory Consider two players, Player I and Player II, each of whom is attempting to maximize his or her utility (or payoff). A (two-player) game is a pair A, u , where, for i \u2208 {i,ii}, \u2022 Ai is the set of pure strategies for Player i, and A = Ai, Aii ; and \u2022 ui : A \u2192 R is the utility function for Player i, and u = ui, uii . We require that A and u be common knowledge. If each Player i chooses strategy ai \u2208 Ai, then the payoffs to Players I and II are ui(a) and uii(a), respectively. A game is constant sum if, for all a \u2208 A, we have that ui(a) + uii(a) = c for some fixed c independent of a. Player i can also play a mixed strategy \u03b1i \u2208 Ai, where Ai denotes the space of probability measures over the set Ai. Payoff functions are generalized as ui (\u03b1) = ui (\u03b1i, \u03b1ii) := Ea\u223c\u03b1[ui (a)] = a\u2208A \u03b1(a)ui (a), where the quantity \u03b1(a) = 151 \u03b1i(ai) \u00b7 \u03b1ii(aii) denotes the joint probability of the independent events that each Player i chooses action ai from the distribution \u03b1i. This generalization to mixed strategies is known as von Neumann/Morgenstern utility , in which players are indifferent between a guaranteed payoff x and an expected payoff of x. A Nash equilibrium is a pair \u03b1 of mixed strategies so that neither player has an incentive to change his or her strategy unilaterally. Formally, the strategy pair \u03b1 is a Nash equilibrium if and only if both ui(\u03b1i, \u03b1ii) = max\u03b1i\u2208Ai ui(\u03b1i, \u03b1ii) and uii(\u03b1i, \u03b1ii) = max\u03b1ii\u2208Aii uii(\u03b1i, \u03b1ii); that is, the strategies \u03b1i and \u03b1ii are mutual best responses. A correlated equilibrium is a distribution \u03c8 over A that obeys the following: if a \u2208 A is drawn randomly according to \u03c8 and Player i learns ai, then no Player i has incentive to deviate unilaterally from playing ai. (A Nash equilibrium is a correlated equilibrium in which \u03c8(a) = \u03b1i(ai) \u00b7 \u03b1ii(aii) is a product distribution.) Formally, in a correlated equilibrium, for every a \u2208 A we must have that ai is a best response to a randomly chosen \u02c6aii \u2208 Aii drawn according to \u03c8(ai, \u02c6aii), and the analogous condition must hold for Player II. 2.2 Socratic Games In this section, we formally define Socratic games. A Socratic game is a 7-tuple A, W, u, S, Q, p, \u03b4 , where, for i \u2208 {i,ii}: \u2022 Ai is, as before, the set of pure strategies for Player i. \u2022 W is a set of possible worlds, one of which is the real world wreal. \u2022 ui = {uw i : A \u2192 R | w \u2208 W} is a set of payoff functions for Player i, one for each possible world. \u2022 S is a set of signals. \u2022 Qi is a set of available queries for Player i. When Player i makes query qi : W \u2192 S, he or she receives the signal qi(wreal). When Player i receives signal qi(wreal) in response to query qi, he or she can infer that wreal \u2208 {w : qi(w) = qi(wreal)}, i.e., the set of possible worlds from which query qi cannot distinguish wreal. \u2022 p : W \u2192 is a probability distribution over the possible worlds. \u2022 \u03b4i : Qi \u2192 R\u22650 gives the query cost for each available query for Player i. Initially, the world wreal is chosen according to the probability distribution p, but the identity of wreal remains unknown to the players. That is, it is as if the players are playing the game A, uwreal but do not know wreal. The players make queries q \u2208 Q, and Player i receives the signal qi(wreal). We consider both observable queries and unobservable queries. When queries are observable, each player learns which query was made by the other player, and the results of his or her own query-that is, each Player i learns qi, qii, and qi(wreal). For unobservable queries, Player i learns only qi and qi(wreal). After learning the results of the queries, the players select strategies a \u2208 A and receive as payoffs wreal i (a) \u2212 \u03b4i(qi). In the Socratic game, a pure strategy for Player i consists of a query qi \u2208 Qi and a response function mapping any result of the query qi to a strategy ai \u2208 Ai to play. A player\"s state of knowledge after a query is a point in R := Q \u00d7 S or Ri := Qi \u00d7 S for observable or unobservable queries, respectively. Thus Player i\"s response function maps R or Ri to Ai. Note that the number of pure strategies is exponential, as there are exponentially many response functions. A mixed strategy involves both randomly choosing a query qi \u2208 Qi and randomly choosing an action ai \u2208 Ai in response to the results of the query. Formally, we will consider a mixed-strategy-function profile f = fquery , fresp to have two parts: \u2022 a function fquery i : Qi \u2192 , where fquery i (qi) is the probability that Player i makes query qi. \u2022 a function fresp i that maps R or Ri to a probability distribution over actions. Player i chooses an action ai \u2208 Ai according to the probability distribution fresp i (q, qi(w)) for observable queries, and according to fresp i (qi, qi(w)) for unobservable queries. (With unobservable queries, for example, the probability that Player I plays action ai conditioned on making query qi in world w is given by Pr[ai \u2190 fresp i (qi, qi(w))].) Mixed strategies are typically defined as probability distributions over the pure strategies, but here we represent a mixed strategy by a pair fquery , fresp , which is commonly referred to as a behavioral strategy in the game-theory literature. As in any game with perfect recall, one can easily map a mixture of pure strategies to a behavioral strategy f = fquery , fresp that induces the same probability of making a particular query qi or playing a particular action after making a query qi in a particular world. Thus it suffices to consider only this representation of mixed strategies. For a strategy-function profile f for observable queries, the (expected) payoff to Player i is given by q\u2208Q,w\u2208W,a\u2208A fquery i (qi) \u00b7 fquery ii (qii) \u00b7 p(w) \u00b7 Pr[ai \u2190 fresp i (q, qi(w))] \u00b7 Pr[aii \u2190 fresp ii (q, qii(w))] \u00b7 (uw i (a) \u2212 \u03b4i(qi)) 5 . The payoffs for unobservable queries are analogous, with fresp j (qj, qj(w)) in place of fresp j (q, qj(w)). 3. STRATEGICALLY ZERO-SUM GAMES We can view a Socratic game G with constant-sum worlds as an exponentially large classical game, with pure strategies make query qi and respond according to fi. However, this classical game is not constant sum. The sum of the players\" payoffs varies depending upon their strategies, because different queries incur different costs. However, this game still has significant structure: the sum of payoffs varies only because of varying query costs. Thus the sum of payoffs does depend on players\" choice of strategies, but not on the interaction of their choices-i.e., for fixed functions gi and gii, we have ui(q, f) + uii(q, f) = gi(qi, fi) + gii(qii, fii) for all strategies q, f . Such games are called strategically zero sum and were introduced by Moulin and Vial , who describe a notion of strategic equivalence and define strategically zero-sum games as those strategically equivalent to zero-sum games. It is interesting to note that two Socratic games with the same queries and strategically equivalent worlds are not necessarily strategically equivalent. A game A, u is strategically zero sum if there exist labels (i, ai) for every Player i and every pure strategy ai \u2208 Ai 152 such that, for all mixed-strategy profiles \u03b1, we have that the sum of the utilities satisfies ui(\u03b1)+uii(\u03b1) = ai\u2208Ai \u03b1i(ai)\u00b7 (i, ai)+ aii\u2208Aii \u03b1ii(aii)\u00b7 (ii, aii). Note that any constant-sum game is strategically zero sum as well. It is not immediately obvious that one can efficiently decide if a given game is strategically zero sum. For completeness, we give a characterization of classical strategically zero-sum games in terms of the rank of a simple matrix derived from the game\"s payoffs, allowing us to efficiently decide if a given game is strategically zero sum and, if it is, to compute the labels (i, ai). Theorem 3.1. Consider a game G = A, u with Ai = {a1 i , . . . , ani i }. Let MG be the ni-by-nii matrix whose i, j th entry MG (i,j) satisfies log2 MG (i,j) = ui(ai i , aj ii) + uii(ai i , aj ii). Then the following are equivalent: (i) G is strategically zero sum; (ii) there exist labels (i, ai) for every player i \u2208 {i,ii} and every pure strategy ai \u2208 Ai such that, for all pure strategies a \u2208 A, we have ui(a) + uii(a) = (i, ai) + (ii, aii); and (iii) rank(MG ) = 1. Proof Sketch. (i \u21d2 ii) is immediate; every pure strategy is a trivially mixed strategy. For (ii \u21d2 iii), let ci be the n-element column vector with jth component 2 (i,a i ) ; then ci \u00b7 cii = MG . For (iii \u21d2 i), if rank(MG ) = 1, then MG u \u00b7 vT . We can prove that G is strategically zero sum by choosing labels (i, aj i ) := log2 uj and (ii, aj ii) := log2 vj. 4. SOCRATIC GAMES WITH UNOBSERVABLE QUERIES We begin with Socratic games with unobservable queries, where a player\"s choice of query is not revealed to her opponent. We give an efficient algorithm to solve unobservablequery Socratic games with strategically zero-sum worlds. Our algorithm is based upon the LP shown in Figure 1, whose feasible points are Nash equilibria for the game. The LP has polynomially many variables but exponentially many constraints. We give an efficient separation oracle for the LP, implying that the ellipsoid method yields an efficient algorithm. This approach extends the techniques of Koller and Megiddo (see also ) to solve constant-sum games represented in extensive form. (Recall that their result does not directly apply in our case; even a Socratic game with constant-sum worlds is not a constant-sum classical game.) Lemma 4.1. Let G = A, W, u, S, Q, p, \u03b4 be an arbitrary unobservable-query Socratic game with strategically zero-sum worlds. Any feasible point for the LP in Figure 1 can be efficiently mapped to a Nash equilibrium for G, and any Nash equilibrium for G can be mapped to a feasible point for the program. Proof Sketch. We begin with a description of the correspondence between feasible points for the LP and Nash equilibria for G. First, suppose that strategy profile f = fquery , fresp forms a Nash equilibrium for G. Then the following setting for the LP variables is feasible: yi qi = fquery i (qi) xi ai,qi,w = Pr[ai \u2190 fresp i (qi, qi(w))] \u00b7 yi qi \u03c1i = w,q\u2208Q,a\u2208A p(w) \u00b7 xi ai,qi,w \u00b7 xii aii,qii,w \u00b7 [uw i (a) \u2212 \u03b4i(qi)]. (We omit the straightforward calculations that verify feasibility.) Next, suppose xi ai,qi,w, yi qi , \u03c1i is feasible for the LP. Let f be the strategy-function profile defined as fquery i : qi \u2192 yi qi fresp i (qi, qi(w)) : ai \u2192 xi ai,qi,w/yi qi Verifying that this strategy profile is a Nash equilibrium requires checking that fresp i (qi, qi(w)) is a well-defined function (from constraint VI), that fquery i and fresp i (qi, qi(w)) are probability distributions (from constraints III and IV), and that each player is playing a best response to his or her opponent\"s strategy (from constraints I and II). Finally, from constraints I and II, the expected payoff to Player i is at most \u03c1i. Because the right-hand side of constraint VII is equal to the expected sum of the payoffs from f and is at most \u03c1i + \u03c1ii, the payoffs are correct and imply the lemma. We now give an efficient separation oracle for the LP in Figure 1, thus allowing the ellipsoid method to solve the LP in polynomial time. Recall that a separation oracle is a function that, given a setting for the variables in the LP, either returns feasible or returns a particular constraint of the LP that is violated by that setting of the variables. An efficient, correct separation oracle allows us to solve the LP efficiently via the ellipsoid method. Lemma 4.2. There exists a separation oracle for the LP in Figure 1 that is correct and runs in polynomial time. Proof. Here is a description of the separation oracle SP. On input xi ai,qi,w, yi qi , \u03c1i : 1. Check each of the constraints (III), (IV), (V), (VI), and (VII). If any one of these constraints is violated, then return it. 2. Define the strategy profile f as follows: fquery i : qi \u2192 yi qi fresp i (qi, qi(w)) : ai \u2192 xi ai,qi,w/yi qi For each query qi, we will compute a pure best-response function \u02c6f qi i for Player I to strategy fii after making query qi. More specifically, given fii and the result qi(wreal) of the query qi, it is straightforward to compute the probability that, conditioned on the fact that the result of query qi is qi(w), the world is w and Player II will play action aii \u2208 Aii. Therefore, for each query qi and response qi(w), Player I can compute the expected utility of each pure response ai to the induced mixed strategy over Aii for Player II. Player I can then select the ai maximizing this expected payoff. Let \u02c6fi be the response function such that \u02c6fi(qi, qi(w)) = \u02c6f qi i (qi(w)) for every qi \u2208 Qi. Similarly, compute \u02c6fii. 153 Player i does not prefer \u2018make query qi, then play according to the function fi\" : \u2200qi \u2208 Qi, fi : Ri \u2192 Ai : \u03c1i \u2265 w\u2208W,aii\u2208Aii,qii\u2208Qii,ai=fi(qi,qi(w)) p(w) \u00b7 xii aii,qii,w \u00b7 [uw i (a) \u2212 \u03b4i(qi)] (I) \u2200qii \u2208 Qii, fii : Rii \u2192 Aii : \u03c1ii \u2265 w\u2208W,ai\u2208Ai,qi\u2208Qi,aii=fii(qii,qii(w)) p(w) \u00b7 xi ai,qi,w \u00b7 [uw ii (a) \u2212 \u03b4ii(qii)] (II) Every player\"s choices form a probability distribution in every world: \u2200i \u2208 {i,ii}, w \u2208 W : 1 = ai\u2208Ai,qi\u2208Qi xi ai,qi,w (III) \u2200i \u2208 {i,ii}, w \u2208 W : 0 \u2264 xi ai,qi,w (IV) Queries are independent of the world, and actions depend only on query output: \u2200i \u2208 {i,ii}, qi \u2208 Qi, w \u2208 W, w \u2208 W such that qi(w) = qi(w ) : yi qi ai\u2208Ai xi ai,qi,w (V) xi ai,qi,w = xi ai,qi,w (VI) The payoffs are consistent with the labels (i, ai, w): \u03c1i + \u03c1ii = i\u2208{i,ii} w\u2208W,qi\u2208Qi,ai\u2208Ai p(w) \u00b7 xi ai,qi,w \u00b7 [ (i, ai, w) \u2212 \u03b4i(qi)] (VII) Figure 1: An LP to find Nash equilibria in unobservable-query Socratic games with strategically zero-sum worlds. The input is a Socratic game A, W, u, S, Q, p, \u03b4 so that world w is strategically zero sum with labels (i, ai, w). Player i makes query qi \u2208 Qi with probability yi qi and, when the actual world is w \u2208 W, makes query qi and plays action ai with probability xi ai,qi,w. The expected payoff to Player i is given by \u03c1i. 3. Let \u02c6\u03c1 qi i be the expected payoff to Player I using the strategy make query qi and play response function \u02c6fi if Player II plays according to fii. Let \u02c6\u03c1i = maxqi\u2208Qq \u02c6\u03c1 qi i and let \u02c6qi = arg maxqi\u2208Qq \u02c6\u03c1 qi i . Similarly, define \u02c6\u03c1 qii ii , \u02c6\u03c1ii, and \u02c6qii. 4. For the \u02c6fi and \u02c6qi defined in Step 3, return constraint (I-\u02c6qi- \u02c6fi) or (II-\u02c6qii- \u02c6fii) if either is violated. If both are satisfied, then return feasible. We first note that the separation oracle runs in polynomial time and then prove its correctness. Steps 1 and 4 are clearly polynomial. For Step 2, we have described how to compute the relevant response functions by examining every action of Player I, every world, every query, and every action of Player II. There are only polynomially many queries, worlds, query results, and pure actions, so the running time of Steps 2 and 3 is thus polynomial. We now sketch the proof that the separation oracle works correctly. The main challenge is to show that if any constraint (I-qi-fi ) is violated then (I-\u02c6qi- \u02c6fi) is violated in Step 4. First, we observe that, by construction, the function \u02c6fi computed in Step 3 must be a best response to Player II playing fii, no matter what query Player I makes. Therefore the strategy make query \u02c6qi, then play response function \u02c6fi must be a best response to Player II playing fii, by definition of \u02c6qi. The right-hand side of each constraint (I-qi-fi ) is equal to the expected payoff that Player I receives when playing the pure strategy make query qi and then play response function fi against Player II\"s strategy of fii. Therefore, because the pure strategy make query \u02c6qi and then play response function \u02c6fi is a best response to Player II playing fii, the right-hand side of constraint (I-\u02c6qi- \u02c6fi) is at least as large as the right hand side of any constraint (I-\u02c6qi-fi ). Therefore, if any constraint (I-qi-fi ) is violated, constraint (I-\u02c6qi- \u02c6fi) is also violated. An analogous argument holds for Player II. These lemmas and the well-known fact that Nash equilibria always exist imply the following theorem: Theorem 4.3. Nash equilibria can be found in polynomial time for any two-player unobservable-query Socratic game with strategically zero-sum worlds. 5. SOCRATIC GAMES WITH OBSERVABLE QUERIES In this section, we give efficient algorithms to find (1) a Nash equilibrium for observable-query Socratic games with constant-sum worlds and (2) a correlated equilibrium in the broader class of Socratic games with strategically zero-sum worlds. Recall that a Socratic game G = A, W, u, S, Q, p, \u03b4 with observable queries proceeds in two stages: Stage 1: The players simultaneously choose queries q \u2208 Q. Player i receives as output qi, qii, and qi(wreal). Stage 2: The players simultaneously choose strategies a \u2208 A. The payoff to Player i is u wreal i (a) \u2212 \u03b4i(qi). Using backward induction, we first solve Stage 2 and then proceed to the Stage-1 game. For a query q \u2208 Q, we would like to analyze the Stage-2 game \u02c6Gq resulting from the players making queries q in Stage 1. Technically, however, \u02c6Gq is not actually a game, because at the beginning of Stage 2 the players have different information about the world: Player I knows qi(wreal), and 154 Player II knows qii(wreal). Fortunately, the situation in which players have asymmetric private knowledge has been well studied in the game-theory literature. A Bayesian game is a quadruple A, T, r, u , where: \u2022 Ai is the set of pure strategies for Player i. \u2022 Ti is the set of types for Player i. \u2022 r is a probability distribution over T; r(t) denotes the probability that Player i has type ti for all i. \u2022 ui : A \u00d7 T \u2192 R is the payoff function for Player i. If the players have types t and play pure strategies a, then ui(a, t) denotes the payoff for Player i. Initially, a type t is drawn randomly from T according to the distribution r. Player i learns his type ti, but does not learn any other player\"s type. Player i then plays a mixed strategy \u03b1i \u2208 Ai-that is, a probability distribution over Ai-and receives payoff ui(\u03b1, t). A strategy function is a function hi : Ti \u2192 Ai; Player i plays the mixed strategy hi(ti) \u2208 Ai when her type is ti. A strategy-function profile h is a Bayesian Nash equilibrium if and only if no Player i has unilateral incentive to deviate from hi if the other players play according to h. For a two-player Bayesian game, if \u03b1 = h(t), then the profile h is a Bayesian Nash equilibrium exactly when the following condition and its analogue for Player II hold: Et\u223cr[ui(\u03b1, t)] = maxhi Et\u223cr[ui( hi(ti), \u03b1ii , t)]. These conditions hold if and only if, for all ti \u2208 Ti occurring with positive probability, Player i\"s expected utility conditioned on his type being ti is maximized by hi(ti). A Bayesian game is constant sum if for all a \u2208 A and all t \u2208 T, we have ui(a, t) + uii(a, t) = ct, for some constant ct independent of a. A Bayesian game is strategically zero sum if the classical game A, u(\u00b7, t) is strategically zero sum for every t \u2208 T. Whether a Bayesian game is strategically zero sum can be determined as in Theorem 3.1. (For further discussion of Bayesian games, see .) We now formally define the Stage-2 game as a Bayesian game. Given a Socratic game G = A, W, u, S, Q, p, \u03b4 and a query profile q \u2208 Q, we define the Stage-2 Bayesian game Gstage2(q) := A, Tq , pstage2(q) , ustage2(q) , where: \u2022 Ai, the set of pure strategies for Player i, is the same as in the original Socratic game; \u2022 Tq i = {qi(w) : w \u2208 W}, the set of types for Player i, is the set of signals that can result from query qi; \u2022 pstage2(q) (t) = Pr[q(w) = t | w \u2190 p]; and \u2022 u stage2(q) i (a, t) = w\u2208W Pr[w \u2190 p | q(w) = t] \u00b7 uw i (a). We now define the Stage-1 game in terms of the payoffs for the Stage-2 games. Fix any algorithm alg that finds a Bayesian Nash equilibrium hq,alg := alg(Gstage2(q)) for each Stage-2 game. Define valuealg i (Gstage2(q)) to be the expected payoff received by Player i in the Bayesian game Gstage2(q) if each player plays according to hq,alg , that is, valuealg i (Gstage2(q)) := w\u2208W p(w) \u00b7 u stage2(q) i (hq,alg (q(w)), q(w)). Define the game Galg stage1 := Astage1 , ustage1(alg) , where: \u2022 Astage1 := Q, the set of available queries in the Socratic game; and \u2022 u stage1(alg) i (q) := valuealg i (Gstage2(q)) \u2212 \u03b4i(qi). I.e., players choose queries q and receive payoffs corresponding to valuealg (Gstage2(q)), less query costs. Lemma 5.1. Consider an observable-query Socratic game G = A, W, u, S, Q, p, \u03b4 . Let Gstage2(q) be the Stage-2 games for all q \u2208 Q, let alg be an algorithm finding a Bayesian Nash equilibrium in each Gstage2(q), and let Galg stage1 be the Stage-1 game. Let \u03b1 be a Nash equilibrium for Galg stage1, and let hq,alg := alg(Gstage2(q)) be a Bayesian Nash equilibrium for each Gstage2(q). Then the following strategy profile is a Nash equilibrium for G: \u2022 In Stage 1, Player i makes query qi with probability \u03b1i(qi). (That is, set fquery (q) := \u03b1(q).) \u2022 In Stage 2, if q is the query in Stage 1 and qi(wreal) denotes the response to Player i\"s query, then Player i chooses action ai with probability hq,alg i (qi(wreal)). (In other words, set fresp i (q, qi(w)) := hq,alg i (qi(w)).) We now find equilibria in the stage games for Socratic games with constant- or strategically zero-sum worlds. We first show that the stage games are well structured in this setting: Lemma 5.2. Consider an observable-query Socratic game G = A, W, u, S, Q, p, \u03b4 with constant-sum worlds. Then the Stage-1 game Galg stage1 is strategically zero sum for every algorithm alg, and every Stage-2 game Gstage2(q) is Bayesian constant sum. If the worlds of G are strategically zero sum, then every Gstage2(q) is Bayesian strategically zero sum. We now show that we can efficiently compute equilibria for these well-structured stage games. Theorem 5.3. There exists a polynomial-time algorithm BNE finding Bayesian Nash equilibria in strategically zerosum Bayesian (and thus classical strategically zero-sum or Bayesian constant-sum) two-player games. Proof Sketch. Let G = A, T, r, u be a strategically zero-sum Bayesian game. Define an unobservable-query Socratic game G\u2217 with one possible world for each t \u2208 T, one available zero-cost query qi for each Player i so that qi reveals ti, and all else as in G. Bayesian Nash equilibria in G correspond directly to Nash equilibria in G\u2217 , and the worlds of G\u2217 are strategically zero sum. Thus by Theorem 4.3 we can compute Nash equilibria for G\u2217 , and thus we can compute Bayesian Nash equilibria for G. (LP\"s for zero-sum two-player Bayesian games have been previously developed and studied .) Theorem 5.4. We can compute a Nash equilibrium for an arbitrary two-player observable-query Socratic game G = A, W, u, S, Q, p, \u03b4 with constant-sum worlds in polynomial time. Proof. Because each world of G is constant sum, Lemma 5.2 implies that the induced Stage-2 games Gstage2(q) are all Bayesian constant sum. Thus we can use algorithm BNE to compute a Bayesian Nash equilibrium hq,BNE := BNE(Gstage2(q)) for each q \u2208 Q, by Theorem 5.3. Furthermore, again by Lemma 5.2, the induced Stage-1 game GBNE stage1 is classical strategically zero sum. Therefore we can again use algorithm BNE to compute a Nash equilibrium \u03b1 := BNE(GBNE stage1), again by Theorem 5.3. Therefore, by Lemma 5.1, we can assemble \u03b1 and the hq,BNE \"s into a Nash equilibrium for the Socratic game G. 155 We would like to extend our results on observable-query Socratic games to Socratic games with strategically zerosum worlds. While we can still find Nash equilibria in the Stage-2 games, the resulting Stage-1 game is not in general strategically zero sum. Thus, finding Nash equilibria in observable-query Socratic games with strategically zerosum worlds seems to require substantially new techniques. However, our techniques for decomposing observable-query Socratic games do allow us to find correlated equilibria in this case. Lemma 5.5. Consider an observable-query Socratic game G = A, W, u, S, Q, p, \u03b4 . Let alg be an arbitrary algorithm that finds a Bayesian Nash equilibrium in each of the derived Stage-2 games Gstage2(q), and let Galg stage1 be the derived Stage1 game. Let \u03c6 be a correlated equilibrium for Galg stage1, and let hq,alg := alg(Gstage2(q)) be a Bayesian Nash equilibrium for each Gstage2(q). Then the following distribution over pure strategies is a correlated equilibrium for G: \u03c8(q, f) := \u03c6(q) i\u2208{i,ii} s\u2208S Pr fi(q, s) \u2190 hq,alg i (s) Thus to find a correlated equilibrium in an observable-query Socratic game with strategically zero-sum worlds, we need only algorithm BNE from Theorem 5.3 along with an efficient algorithm for finding a correlated equilibrium in a general game. Such an algorithm exists (the definition of correlated equilibria can be directly translated into an LP ), and therefore we have the following theorem: Theorem 5.6. We can provide both efficient oracle access and efficient sampling access to a correlated equilibrium for any observable-query two-player Socratic game with strategically zero-sum worlds. Because the support of the correlated equilibrium may be exponentially large, providing oracle and sampling access is the natural way to represent the correlated equilibrium. By Lemma 5.5, we can also compute correlated equilibria in any observable-query Socratic game for which Nash equilibria are computable in the induced Gstage2(q) games (e.g., when Gstage2(q) is of constant size). Another potentially interesting model of queries in Socratic games is what one might call public queries, in which both the choice and outcome of a player\"s query is observable by all players in the game. (This model might be most appropriate in the presence of corporate espionage or media leaks, or in a setting in which the queries-and thus their results-are done in plain view.) The techniques that we have developed in this section also yield exactly the same results as for observable queries. The proof is actually simpler: with public queries, the players\" payoffs are common knowledge when Stage 2 begins, and thus Stage 2 really is a complete-information game. (There may still be uncertainty about the real world, but all players use the observed signals to infer exactly the same set of possible worlds in which wreal may lie; thus they are playing a complete-information game against each other.) Thus we have the same results as in Theorems 5.4 and 5.6 more simply, by solving Stage 2 using a (non-Bayesian) Nash-equilibrium finder and solving Stage 1 as before. Our results for observable queries are weaker than for unobservable: in Socratic games with worlds that are strategically zero sum but not constant sum, we find only a correlated equilibrium in the observable case, whereas we find a Nash equilibrium in the unobservable case. We might hope to extend our unobservable-query techniques to observable queries, but there is no obvious way to do so. The fundamental obstacle is that the LP\"s payoff constraint becomes nonlinear if there is any dependence on the probability that the other player made a particular query. This dependence arises with observable queries, suggesting that observable Socratic games with strategically zero-sum worlds may be harder to solve. 6. RELATED WORK Our work was initially motivated by research in the social sciences indicating that real people seem (irrationally) paralyzed when they are presented with additional options. In this section, we briefly review some of these social-science experiments and then discuss technical approaches related to Socratic game theory. Prima facie, a rational agent\"s happiness given an added option can only increase. However, recent research has found that more choices tend to decrease happiness: for example, students choosing among extra-credit options are more likely to do extra credit if given a small subset of the choices and, moreover, produce higher-quality work . (See also .) The psychology literature explores a number of explanations: people may miscalculate their opportunity cost by comparing their choice to a component-wise maximum of all other options instead of the single best alternative , a new option may draw undue attention to aspects of the other options , and so on. The present work explores an economic explanation of this phenomenon: information is not free. When there are more options, a decision-maker must spend more time to achieve a satisfactory outcome. See, e.g., the work of Skyrms for a philosophical perspective on the role of deliberation in strategic situations. Finally, we note the connection between Socratic games and modal logic , a formalism for the logic of possibility and necessity. The observation that human players typically do not play rational strategies has inspired some attempts to model partially rational players. The typical model of this socalled bounded rationality is to postulate bounds on computational power in computing the consequences of a strategy. The work on bounded rationality differs from the models that we consider here in that instead of putting hard limitations on the computational power of the agents, we instead restrict their a priori knowledge of the state of the world, requiring them to spend time (and therefore money/utility) to learn about it. Partially observable stochastic games (POSGs) are a general framework used in AI to model situations of multi-agent planning in an evolving, unknown environment, but the generality of POSGs seems to make them very difficult . Recent work has been done in developing algorithms for restricted classes of POSGs, most notably classes of cooperative POSGs-e.g., -which are very different from the competitive strategically zero-sum games we address in this paper. The fundamental question in Socratic games is deciding on the comparative value of making a more costly but more informative query, or concluding the data-gathering phase and picking the best option, given current information. This tradeoff has been explored in a variety of other contexts; a sampling of these contexts includes aggregating results 156 from delay-prone information sources , doing approximate reasoning in intelligent systems , deciding when to take the current best guess of disease diagnosis from a beliefpropagation network and when to let it continue inference , among many others. This issue can also be viewed as another perspective on the general question of exploration versus exploitation that arises often in AI: when is it better to actively seek additional information instead of exploiting the knowledge one already has? (See, e.g., .) Most of this work differs significantly from our own in that it considers single-agent planning as opposed to the game-theoretic setting. A notable exception is the work of Larson and Sandholm on mechanism design for interacting agents whose computation is costly and limited. They present a model in which players must solve a computationally intractable valuation problem, using costly computation to learn some hidden parameters, and results for auctions and bargaining games in this model. 7. FUTURE DIRECTIONS Efficiently finding Nash equilibria in Socratic games with non-strategically zero-sum worlds is probably difficult because the existence of such an algorithm for classical games has been shown to be unlikely . There has, however, been some algorithmic success in finding Nash equilibria in restricted classical settings (e.g., ); we might hope to extend our results to analogous Socratic games. An efficient algorithm to find correlated equilibria in general Socratic games seems more attainable. Suppose the players receive recommended queries and responses. The difficulty is that when a player considers a deviation from his recommended query, he already knows his recommended response in each of the Stage-2 games. In a correlated equilibrium, a player\"s expected payoff generally depends on his recommended strategy, and thus a player may deviate in Stage 1 so as to land in a Stage-2 game where he has been given a better than average recommended response. (Socratic games are succinct games of superpolynomial type, so Papadimitriou\"s results do not imply correlated equilibria for them.) Socratic games can be extended to allow players to make adaptive queries, choosing subsequent queries based on previous results. Our techniques carry over to O(1) rounds of unobservable queries, but it would be interesting to compute equilibria in Socratic games with adaptive observable queries or with \u03c9(1) rounds of unobservable queries. Special cases of adaptive Socratic games are closely related to single-agent problems like minimum latency , determining strategies for using priced information , and an online version of minimum test cover . Although there are important technical distinctions between adaptive Socratic games and these problems, approximation techniques from this literature may apply to Socratic games. The question of approximation raises interesting questions even in non-adaptive Socratic games. An -approximate Nash equilibrium is a strategy profile \u03b1 so that no player can increase her payoff by an additive by deviating from \u03b1. Finding approximate Nash equilibria in both adaptive and non-adaptive Socratic games is an interesting direction to pursue. Another natural extension is the model where query results are stochastic. In this paper, we model a query as deterministically partitioning the possible worlds into subsets that the query cannot distinguish. However, one could instead model a query as probabilistically mapping the set of possible worlds into the set of signals. With this modification, our unobservable-query model becomes equivalent to the model of Bergemann and V\u00a8alim\u00a8aki , in which the result of a query is a posterior distribution over the worlds. Our techniques allow us to compute equilibria in such a stochastic-query model provided that each query is represented as a table that, for each world/signal pair, lists the probability that the query outputs that signal in that world. It is also interesting to consider settings in which the game\"s queries are specified by a compact representation of the relevant probability distributions. (For example, one might consider a setting in which the algorithm has only a sampling oracle for the posterior distributions envisioned by Bergemann and V\u00a8alim\u00a8aki.) Efficiently finding equilibria in such settings remains an open problem. Another interesting setting for Socratic games is when the set Q of available queries is given by Q = P(\u0393)-i.e., each player chooses to make a set q \u2208 P(\u0393) of queries from a specified groundset \u0393 of queries. Here we take the query cost to be a linear function, so that \u03b4(q) = \u03b3\u2208q \u03b4({\u03b3}). Natural groundsets include comparison queries (if my opponent is playing strategy aii, would I prefer to play ai or \u02c6ai?), strategy queries (what is my vector of payoffs if I play strategy ai?), and world-identity queries (is the world w \u2208 W the real world?). When one can infer a polynomial bound on the number of queries made by a rational player, then our results yield efficient solutions. (For example, we can efficiently solve games in which every groundset element \u03b3 \u2208 \u0393 has \u03b4({\u03b3}) = \u03a9(M \u2212 M), where M and M denote the maximum and minimum payoffs to any player in any world.) Conversely, it is NP-hard to compute a Nash equilibrium for such a game when every \u03b4({\u03b3}) \u2264 1/|W|2 , even when the worlds are constant sum and Player II has only a single available strategy. Thus even computing a best response for Player I is hard. (This proof proceeds by reduction from set cover; intuitively, for sufficiently low query costs, Player I must fully identify the actual world through his queries. Selecting a minimum-sized set of these queries is hard.) Computing Player I\"s best response can be viewed as maximizing a submodular function, and thus a best response can be (1 \u2212 1/e) \u2248 0.63 approximated greedily . An interesting open question is whether this approximate best-response calculation can be leveraged to find an approximate Nash equilibrium.", "body1": ". Perhaps Californians are susceptible to Nixon\"s advertising, but are unresponsive to Kennedy\"s. In light of this uncertainty, the Kennedy campaign may conduct a survey, at some cost, to estimate the effectiveness of its advertising. A Socratic game proceeds as follows. This tension between the risk arising from uncertainty and the cost of acquiring information is ubiquitous in economics, political science, and beyond. Our results. Strategically zero-sum games generalize constant-sum games by allowing the sum of the players\" payoffs to depend on individual players\" choices of strategy, but not on any interaction of their choices. Algorithmically, the generality of extensive-form games makes them difficult to solve efficiently, and the special cases that are known to be efficiently solvable do not include even simple Socratic games. We face two major obstacles in extending these classical results to Socratic games. For unobservable-query Socratic games with strategically zero-sum worlds, we address these obstacles by formulating a new LP that uses only polynomially many variables (though still an exponential number of constraints) and then use ellipsoid-based techniques to solve it. In this section, we review background on game theory and formally introduce Socratic games. We require that A and u be common knowledge. Payoff functions are generalized as ui (\u03b1) = ui (\u03b1i, \u03b1ii) := Ea\u223c\u03b1[ui (a)] = a\u2208A \u03b1(a)ui (a), where the quantity \u03b1(a) = 151 \u03b1i(ai) \u00b7 \u03b1ii(aii) denotes the joint probability of the independent events that each Player i chooses action ai from the distribution \u03b1i. A Nash equilibrium is a pair \u03b1 of mixed strategies so that neither player has an incentive to change his or her strategy unilaterally. A correlated equilibrium is a distribution \u03c8 over A that obeys the following: if a \u2208 A is drawn randomly according to \u03c8 and Player i learns ai, then no Player i has incentive to deviate unilaterally from playing ai. \u2022 W is a set of possible worlds, one of which is the real world wreal. \u2022 ui = {uw i : A \u2192 R | w \u2208 W} is a set of payoff functions for Player i, one for each possible world. \u2022 S is a set of signals. \u2022 Qi is a set of available queries for Player i. \u2022 \u03b4i : Qi \u2192 R\u22650 gives the query cost for each available query for Player i. Initially, the world wreal is chosen according to the probability distribution p, but the identity of wreal remains unknown to the players. In the Socratic game, a pure strategy for Player i consists of a query qi \u2208 Qi and a response function mapping any result of the query qi to a strategy ai \u2208 Ai to play. \u2022 a function fresp i that maps R or Ri to a probability distribution over actions. The payoffs for unobservable queries are analogous, with fresp j (qj, qj(w)) in place of fresp j (q, qj(w)). We can view a Socratic game G with constant-sum worlds as an exponentially large classical game, with pure strategies make query qi and respond according to fi. However, this classical game is not constant sum. Note that any constant-sum game is strategically zero sum as well. It is not immediately obvious that one can efficiently decide if a given game is strategically zero sum. Theorem 3.1. Proof Sketch. We begin with Socratic games with unobservable queries, where a player\"s choice of query is not revealed to her opponent. Our algorithm is based upon the LP shown in Figure 1, whose feasible points are Nash equilibria for the game. (We omit the straightforward calculations that verify feasibility.) Let f be the strategy-function profile defined as fquery i : qi \u2192 yi qi fresp i (qi, qi(w)) : ai \u2192 xi ai,qi,w/yi qi Verifying that this strategy profile is a Nash equilibrium requires checking that fresp i (qi, qi(w)) is a well-defined function (from constraint VI), that fquery i and fresp i (qi, qi(w)) are probability distributions (from constraints III and IV), and that each player is playing a best response to his or her opponent\"s strategy (from constraints I and II). An efficient, correct separation oracle allows us to solve the LP efficiently via the ellipsoid method. Lemma 4.2. Proof. On input xi ai,qi,w, yi qi , \u03c1i : 1. More specifically, given fii and the result qi(wreal) of the query qi, it is straightforward to compute the probability that, conditioned on the fact that the result of query qi is qi(w), the world is w and Player II will play action aii \u2208 Aii. 153 Player i does not prefer \u2018make query qi, then play according to the function fi\" : \u2200qi \u2208 Qi, fi : Ri \u2192 Ai : \u03c1i \u2265 w\u2208W,aii\u2208Aii,qii\u2208Qii,ai=fi(qi,qi(w)) p(w) \u00b7 xii aii,qii,w \u00b7 [uw i (a) \u2212 \u03b4i(qi)] (I) \u2200qii \u2208 Qii, fii : Rii \u2192 Aii : \u03c1ii \u2265 w\u2208W,ai\u2208Ai,qi\u2208Qi,aii=fii(qii,qii(w)) p(w) \u00b7 xi ai,qi,w \u00b7 [uw ii (a) \u2212 \u03b4ii(qii)] (II) Every player\"s choices form a probability distribution in every world: \u2200i \u2208 {i,ii}, w \u2208 W : 1 = ai\u2208Ai,qi\u2208Qi xi ai,qi,w (III) \u2200i \u2208 {i,ii}, w \u2208 W : 0 \u2264 xi ai,qi,w (IV) Queries are independent of the world, and actions depend only on query output: \u2200i \u2208 {i,ii}, qi \u2208 Qi, w \u2208 W, w \u2208 W such that qi(w) = qi(w ) : yi qi ai\u2208Ai xi ai,qi,w (V) xi ai,qi,w = xi ai,qi,w (VI) The payoffs are consistent with the labels (i, ai, w): \u03c1i + \u03c1ii = i\u2208{i,ii} w\u2208W,qi\u2208Qi,ai\u2208Ai p(w) \u00b7 xi ai,qi,w \u00b7 [ (i, ai, w) \u2212 \u03b4i(qi)] (VII) Figure 1: An LP to find Nash equilibria in unobservable-query Socratic games with strategically zero-sum worlds. Let \u02c6\u03c1i = maxqi\u2208Qq \u02c6\u03c1 qi i and let \u02c6qi = arg maxqi\u2208Qq \u02c6\u03c1 qi i . Similarly, define \u02c6\u03c1 qii ii , \u02c6\u03c1ii, and \u02c6qii. 4. We first note that the separation oracle runs in polynomial time and then prove its correctness. First, we observe that, by construction, the function \u02c6fi computed in Step 3 must be a best response to Player II playing fii, no matter what query Player I makes. These lemmas and the well-known fact that Nash equilibria always exist imply the following theorem: Theorem 4.3. In this section, we give efficient algorithms to find (1) a Nash equilibrium for observable-query Socratic games with constant-sum worlds and (2) a correlated equilibrium in the broader class of Socratic games with strategically zero-sum worlds. Stage 2: The players simultaneously choose strategies a \u2208 A. Using backward induction, we first solve Stage 2 and then proceed to the Stage-1 game. For a query q \u2208 Q, we would like to analyze the Stage-2 game \u02c6Gq resulting from the players making queries q in Stage 1. \u2022 r is a probability distribution over T; r(t) denotes the probability that Player i has type ti for all i. \u2022 ui : A \u00d7 T \u2192 R is the payoff function for Player i. If the players have types t and play pure strategies a, then ui(a, t) denotes the payoff for Player i. Initially, a type t is drawn randomly from T according to the distribution r. Player i learns his type ti, but does not learn any other player\"s type. We now define the Stage-1 game in terms of the payoffs for the Stage-2 games. I.e., players choose queries q and receive payoffs corresponding to valuealg (Gstage2(q)), less query costs. Lemma 5.1. Theorem 5.3. (LP\"s for zero-sum two-player Bayesian games have been previously developed and studied .) Proof. Furthermore, again by Lemma 5.2, the induced Stage-1 game GBNE stage1 is classical strategically zero sum. 155 We would like to extend our results on observable-query Socratic games to Socratic games with strategically zerosum worlds. Lemma 5.5. By Lemma 5.5, we can also compute correlated equilibria in any observable-query Socratic game for which Nash equilibria are computable in the induced Gstage2(q) games (e.g., when Gstage2(q) is of constant size). Another potentially interesting model of queries in Socratic games is what one might call public queries, in which both the choice and outcome of a player\"s query is observable by all players in the game. Our results for observable queries are weaker than for unobservable: in Socratic games with worlds that are strategically zero sum but not constant sum, we find only a correlated equilibrium in the observable case, whereas we find a Nash equilibrium in the unobservable case. Our work was initially motivated by research in the social sciences indicating that real people seem (irrationally) paralyzed when they are presented with additional options. Prima facie, a rational agent\"s happiness given an added option can only increase. Finally, we note the connection between Socratic games and modal logic , a formalism for the logic of possibility and necessity. The observation that human players typically do not play rational strategies has inspired some attempts to model partially rational players. Recent work has been done in developing algorithms for restricted classes of POSGs, most notably classes of cooperative POSGs-e.g., -which are very different from the competitive strategically zero-sum games we address in this paper. The fundamental question in Socratic games is deciding on the comparative value of making a more costly but more informative query, or concluding the data-gathering phase and picking the best option, given current information. This issue can also be viewed as another perspective on the general question of exploration versus exploitation that arises often in AI: when is it better to actively seek additional information instead of exploiting the knowledge one already has?", "body2": "Of course, neither campaign knows the effectiveness of its advertising in each state. The model of Bergemann and V\u00a8alim\u00a8aki is similar in many regards to the one that we explore here; see Section 7 for some discussion. Our research was initially inspired by recent results in psychology on decision making, but it soon became clear that Socratic game theory is also a general tool for understanding the exploitation versus exploration tradeoff, well studied in machine learning, in a strategic multiplayer environment. This tension between the risk arising from uncertainty and the cost of acquiring information is ubiquitous in economics, political science, and beyond. Our techniques also yield Nash equilibria in unobservable-query Socratic games with strategically zero-sum worlds. Like all games, Socratic games can be viewed as a special case of extensive-form games, which represent games by trees in which internal nodes represent choices made by chance or by the players, and the leaves represent outcomes that correspond to a vector of payoffs to the players. A Socratic game is itself a classical game, so one might hope that these results can be applied to Socratic games with constant-sum (or strategically zero-sum) worlds. As a result, the standard linear programs for computing equilibria have an exponential number of variables and an exponential number of constraints. To solve the stages, it is necessary to find Nash equilibria in Bayesian strategically zero-sum games, and we give an explicit polynomial-time algorithm to do so. A (two-player) game is a pair A, u , where, for i \u2208 {i,ii}, \u2022 Ai is the set of pure strategies for Player i, and A = Ai, Aii ; and \u2022 ui : A \u2192 R is the utility function for Player i, and u = ui, uii . Player i can also play a mixed strategy \u03b1i \u2208 Ai, where Ai denotes the space of probability measures over the set Ai. This generalization to mixed strategies is known as von Neumann/Morgenstern utility , in which players are indifferent between a guaranteed payoff x and an expected payoff of x. Formally, the strategy pair \u03b1 is a Nash equilibrium if and only if both ui(\u03b1i, \u03b1ii) = max\u03b1i\u2208Ai ui(\u03b1i, \u03b1ii) and uii(\u03b1i, \u03b1ii) = max\u03b1ii\u2208Aii uii(\u03b1i, \u03b1ii); that is, the strategies \u03b1i and \u03b1ii are mutual best responses. A Socratic game is a 7-tuple A, W, u, S, Q, p, \u03b4 , where, for i \u2208 {i,ii}: \u2022 Ai is, as before, the set of pure strategies for Player i. \u2022 W is a set of possible worlds, one of which is the real world wreal. \u2022 ui = {uw i : A \u2192 R | w \u2208 W} is a set of payoff functions for Player i, one for each possible world. \u2022 S is a set of signals. \u2022 p : W \u2192 is a probability distribution over the possible worlds. \u2022 \u03b4i : Qi \u2192 R\u22650 gives the query cost for each available query for Player i. After learning the results of the queries, the players select strategies a \u2208 A and receive as payoffs wreal i (a) \u2212 \u03b4i(qi). Formally, we will consider a mixed-strategy-function profile f = fquery , fresp to have two parts: \u2022 a function fquery i : Qi \u2192 , where fquery i (qi) is the probability that Player i makes query qi. For a strategy-function profile f for observable queries, the (expected) payoff to Player i is given by q\u2208Q,w\u2208W,a\u2208A fquery i (qi) \u00b7 fquery ii (qii) \u00b7 p(w) \u00b7 Pr[ai \u2190 fresp i (q, qi(w))] \u00b7 Pr[aii \u2190 fresp ii (q, qii(w))] \u00b7 (uw i (a) \u2212 \u03b4i(qi)) 5 . The payoffs for unobservable queries are analogous, with fresp j (qj, qj(w)) in place of fresp j (q, qj(w)). We can view a Socratic game G with constant-sum worlds as an exponentially large classical game, with pure strategies make query qi and respond according to fi. A game A, u is strategically zero sum if there exist labels (i, ai) for every Player i and every pure strategy ai \u2208 Ai 152 such that, for all mixed-strategy profiles \u03b1, we have that the sum of the utilities satisfies ui(\u03b1)+uii(\u03b1) = ai\u2208Ai \u03b1i(ai)\u00b7 (i, ai)+ aii\u2208Aii \u03b1ii(aii)\u00b7 (ii, aii). Note that any constant-sum game is strategically zero sum as well. For completeness, we give a characterization of classical strategically zero-sum games in terms of the rank of a simple matrix derived from the game\"s payoffs, allowing us to efficiently decide if a given game is strategically zero sum and, if it is, to compute the labels (i, ai). Then the following are equivalent: (i) G is strategically zero sum; (ii) there exist labels (i, ai) for every player i \u2208 {i,ii} and every pure strategy ai \u2208 Ai such that, for all pure strategies a \u2208 A, we have ui(a) + uii(a) = (i, ai) + (ii, aii); and (iii) rank(MG ) = 1. We can prove that G is strategically zero sum by choosing labels (i, aj i ) := log2 uj and (ii, aj ii) := log2 vj. We give an efficient algorithm to solve unobservablequery Socratic games with strategically zero-sum worlds. We begin with a description of the correspondence between feasible points for the LP and Nash equilibria for G. First, suppose that strategy profile f = fquery , fresp forms a Nash equilibrium for G. Then the following setting for the LP variables is feasible: yi qi = fquery i (qi) xi ai,qi,w = Pr[ai \u2190 fresp i (qi, qi(w))] \u00b7 yi qi \u03c1i = w,q\u2208Q,a\u2208A p(w) \u00b7 xi ai,qi,w \u00b7 xii aii,qii,w \u00b7 [uw i (a) \u2212 \u03b4i(qi)]. Next, suppose xi ai,qi,w, yi qi , \u03c1i is feasible for the LP. Recall that a separation oracle is a function that, given a setting for the variables in the LP, either returns feasible or returns a particular constraint of the LP that is violated by that setting of the variables. An efficient, correct separation oracle allows us to solve the LP efficiently via the ellipsoid method. There exists a separation oracle for the LP in Figure 1 that is correct and runs in polynomial time. Here is a description of the separation oracle SP. Define the strategy profile f as follows: fquery i : qi \u2192 yi qi fresp i (qi, qi(w)) : ai \u2192 xi ai,qi,w/yi qi For each query qi, we will compute a pure best-response function \u02c6f qi i for Player I to strategy fii after making query qi. Similarly, compute \u02c6fii. Let \u02c6\u03c1 qi i be the expected payoff to Player I using the strategy make query qi and play response function \u02c6fi if Player II plays according to fii. Let \u02c6\u03c1i = maxqi\u2208Qq \u02c6\u03c1 qi i and let \u02c6qi = arg maxqi\u2208Qq \u02c6\u03c1 qi i . Similarly, define \u02c6\u03c1 qii ii , \u02c6\u03c1ii, and \u02c6qii. If both are satisfied, then return feasible. The main challenge is to show that if any constraint (I-qi-fi ) is violated then (I-\u02c6qi- \u02c6fi) is violated in Step 4. An analogous argument holds for Player II. Nash equilibria can be found in polynomial time for any two-player unobservable-query Socratic game with strategically zero-sum worlds. Player i receives as output qi, qii, and qi(wreal). The payoff to Player i is u wreal i (a) \u2212 \u03b4i(qi). Using backward induction, we first solve Stage 2 and then proceed to the Stage-1 game. \u2022 Ti is the set of types for Player i. \u2022 r is a probability distribution over T; r(t) denotes the probability that Player i has type ti for all i. \u2022 ui : A \u00d7 T \u2192 R is the payoff function for Player i. If the players have types t and play pure strategies a, then ui(a, t) denotes the payoff for Player i. Given a Socratic game G = A, W, u, S, Q, p, \u03b4 and a query profile q \u2208 Q, we define the Stage-2 Bayesian game Gstage2(q) := A, Tq , pstage2(q) , ustage2(q) , where: \u2022 Ai, the set of pure strategies for Player i, is the same as in the original Socratic game; \u2022 Tq i = {qi(w) : w \u2208 W}, the set of types for Player i, is the set of signals that can result from query qi; \u2022 pstage2(q) (t) = Pr[q(w) = t | w \u2190 p]; and \u2022 u stage2(q) i (a, t) = w\u2208W Pr[w \u2190 p | q(w) = t] \u00b7 uw i (a). Define the game Galg stage1 := Astage1 , ustage1(alg) , where: \u2022 Astage1 := Q, the set of available queries in the Socratic game; and \u2022 u stage1(alg) i (q) := valuealg i (Gstage2(q)) \u2212 \u03b4i(qi). I.e., players choose queries q and receive payoffs corresponding to valuealg (Gstage2(q)), less query costs. We now show that we can efficiently compute equilibria for these well-structured stage games. There exists a polynomial-time algorithm BNE finding Bayesian Nash equilibria in strategically zerosum Bayesian (and thus classical strategically zero-sum or Bayesian constant-sum) two-player games. Thus by Theorem 4.3 we can compute Nash equilibria for G\u2217 , and thus we can compute Bayesian Nash equilibria for G. We can compute a Nash equilibrium for an arbitrary two-player observable-query Socratic game G = A, W, u, S, Q, p, \u03b4 with constant-sum worlds in polynomial time. Thus we can use algorithm BNE to compute a Bayesian Nash equilibrium hq,BNE := BNE(Gstage2(q)) for each q \u2208 Q, by Theorem 5.3. Therefore, by Lemma 5.1, we can assemble \u03b1 and the hq,BNE \"s into a Nash equilibrium for the Socratic game G. However, our techniques for decomposing observable-query Socratic games do allow us to find correlated equilibria in this case. Because the support of the correlated equilibrium may be exponentially large, providing oracle and sampling access is the natural way to represent the correlated equilibrium. By Lemma 5.5, we can also compute correlated equilibria in any observable-query Socratic game for which Nash equilibria are computable in the induced Gstage2(q) games (e.g., when Gstage2(q) is of constant size). Thus we have the same results as in Theorems 5.4 and 5.6 more simply, by solving Stage 2 using a (non-Bayesian) Nash-equilibrium finder and solving Stage 1 as before. This dependence arises with observable queries, suggesting that observable Socratic games with strategically zero-sum worlds may be harder to solve. In this section, we briefly review some of these social-science experiments and then discuss technical approaches related to Socratic game theory. See, e.g., the work of Skyrms for a philosophical perspective on the role of deliberation in strategic situations. Finally, we note the connection between Socratic games and modal logic , a formalism for the logic of possibility and necessity. Partially observable stochastic games (POSGs) are a general framework used in AI to model situations of multi-agent planning in an evolving, unknown environment, but the generality of POSGs seems to make them very difficult . Recent work has been done in developing algorithms for restricted classes of POSGs, most notably classes of cooperative POSGs-e.g., -which are very different from the competitive strategically zero-sum games we address in this paper. This tradeoff has been explored in a variety of other contexts; a sampling of these contexts includes aggregating results 156 from delay-prone information sources , doing approximate reasoning in intelligent systems , deciding when to take the current best guess of disease diagnosis from a beliefpropagation network and when to let it continue inference , among many others. They present a model in which players must solve a computationally intractable valuation problem, using costly computation to learn some hidden parameters, and results for auctions and bargaining games in this model.", "introduction": "Democratic Party strategists huddle around a map. How should the Kennedy campaign allocate its remaining advertising budget? Should it focus on, say, California or New York? The Nixon campaign faces the same dilemma. Of course, neither campaign knows the effectiveness of its advertising in each state. Perhaps Californians are susceptible to Nixon\"s advertising, but are unresponsive to Kennedy\"s. In light of this uncertainty, the Kennedy campaign may conduct a survey, at some cost, to estimate the effectiveness of its advertising. Moreover, the larger-and more expensive-the survey, the more accurate it will be. Is the cost of a survey worth the information that it provides? How should one balance the cost of acquiring more information against the risk of playing a game with higher uncertainty? In this paper, we model situations of this type as Socratic games. As in traditional game theory, the players in a Socratic game choose actions to maximize their payoffs, but we model players with incomplete information who can make costly queries to reduce their uncertainty about the state of the world before they choose their actions. This approach contrasts with traditional game theory, in which players are usually modeled as having fixed, exogenously given information about the structure of the game and its payoffs. (In traditional games of incomplete and imperfect information, there is information that the players do not have; in Socratic games, unlike in these games, the players have a chance to acquire the missing information, at some cost.) A number of related models have been explored by economists and computer scientists motivated by similar situations, often with a focus on mechanism design and auctions; a sampling of this research includes the work of Larson and Sandholm , Parkes , Fong , Compte and Jehiel , Rezende , Persico and Matthews , Cr\u00b4emer and Khalil , Rasmusen , and Bergemann and V\u00a8alim\u00a8aki . The model of Bergemann and V\u00a8alim\u00a8aki is similar in many regards to the one that we explore here; see Section 7 for some discussion. A Socratic game proceeds as follows. A real world is cho150 sen randomly from a set of possible worlds according to a common prior distribution. Each player then selects an arbitrary query from a set of available costly queries and receives a corresponding piece of information about the real world. Finally each player selects an action and receives a payoff-a function of the players\" selected actions and the identity of the real world-less the cost of the query that he or she made. Compared to traditional game theory, the distinguishing feature of our model is the introduction of explicit costs to the players for learning arbitrary partial information about which of the many possible worlds is the real world. Our research was initially inspired by recent results in psychology on decision making, but it soon became clear that Socratic game theory is also a general tool for understanding the exploitation versus exploration tradeoff, well studied in machine learning, in a strategic multiplayer environment. This tension between the risk arising from uncertainty and the cost of acquiring information is ubiquitous in economics, political science, and beyond. We consider Socratic games under two models: an unobservable-query model where players learn only the response to their own queries and an observable-query model where players also learn which queries their opponents made. We give efficient algorithms to find Nash equilibriai.e., tuples of strategies from which no player has unilateral incentive to deviate-in broad classes of two-player Socratic games in both models. Our first result is an efficient algorithm to find Nash equilibria in unobservable-query Socratic games with constant-sum worlds, in which the sum of the players\" payoffs is independent of their actions. Our techniques also yield Nash equilibria in unobservable-query Socratic games with strategically zero-sum worlds. Strategically zero-sum games generalize constant-sum games by allowing the sum of the players\" payoffs to depend on individual players\" choices of strategy, but not on any interaction of their choices. Our second result is an efficient algorithm to find Nash equilibria in observable-query Socratic games with constant-sum worlds. Finally, we give an efficient algorithm to find correlated equilibria-a weaker but increasingly well-studied solution concept for games -in observable-query Socratic games with strategically zero-sum worlds. Like all games, Socratic games can be viewed as a special case of extensive-form games, which represent games by trees in which internal nodes represent choices made by chance or by the players, and the leaves represent outcomes that correspond to a vector of payoffs to the players. Algorithmically, the generality of extensive-form games makes them difficult to solve efficiently, and the special cases that are known to be efficiently solvable do not include even simple Socratic games. Every (complete-information) classical game is a trivial Socratic game (with a single possible world and a single trivial query), and efficiently finding Nash equilibria in classical games has been shown to be hard . Therefore we would not expect to find a straightforward polynomial-time algorithm to compute Nash equilibria in general Socratic games. However, it is well known that Nash equilibria can be found efficiently via an LP for two-player constant-sum games (and strategically zero-sum games ). A Socratic game is itself a classical game, so one might hope that these results can be applied to Socratic games with constant-sum (or strategically zero-sum) worlds. We face two major obstacles in extending these classical results to Socratic games. First, a Socratic game with constant-sum worlds is not itself a constant-sum classical game-rather, the resulting classical game is only strategically zero sum. Worse yet, a Socratic game with strategically zero-sum worlds is not itself classically strategically zero sum-indeed, there are no known efficient algorithmic techniques to compute Nash equilibria in the resulting class of classical games. (Exponential-time algorithms like Lemke/Howson, of course, can be used .) Thus even when it is easy to find Nash equilibria in each of the worlds of a Socratic game, we require new techniques to solve the Socratic game itself. Second, even when the Socratic game itself is strategically zero sum, the number of possible strategies available to each player is exponential in the natural representation of the game. As a result, the standard linear programs for computing equilibria have an exponential number of variables and an exponential number of constraints. For unobservable-query Socratic games with strategically zero-sum worlds, we address these obstacles by formulating a new LP that uses only polynomially many variables (though still an exponential number of constraints) and then use ellipsoid-based techniques to solve it. For observablequery Socratic games, we handle the exponentiality by decomposing the game into stages, solving the stages separately, and showing how to reassemble the solutions efficiently. To solve the stages, it is necessary to find Nash equilibria in Bayesian strategically zero-sum games, and we give an explicit polynomial-time algorithm to do so.", "conclusion": "Efficiently finding Nash equilibria in Socratic games with non-strategically zero-sum worlds is probably difficult because the existence of such an algorithm for classical games has been shown to be unlikely .. There has, however, been some algorithmic success in finding Nash equilibria in restricted classical settings (e.g., ); we might hope to extend our results to analogous Socratic games.. An efficient algorithm to find correlated equilibria in general Socratic games seems more attainable.. Suppose the players receive recommended queries and responses.. The difficulty is that when a player considers a deviation from his recommended query, he already knows his recommended response in each of the Stage-2 games.. In a correlated equilibrium, a player\"s expected payoff generally depends on his recommended strategy, and thus a player may deviate in Stage 1 so as to land in a Stage-2 game where he has been given a better than average recommended response.. (Socratic games are succinct games of superpolynomial type, so Papadimitriou\"s results do not imply correlated equilibria for them.). Socratic games can be extended to allow players to make adaptive queries, choosing subsequent queries based on previous results.. Our techniques carry over to O(1) rounds of unobservable queries, but it would be interesting to compute equilibria in Socratic games with adaptive observable queries or with \u03c9(1) rounds of unobservable queries.. Special cases of adaptive Socratic games are closely related to single-agent problems like minimum latency , determining strategies for using priced information , and an online version of minimum test cover .. Although there are important technical distinctions between adaptive Socratic games and these problems, approximation techniques from this literature may apply to Socratic games.. The question of approximation raises interesting questions even in non-adaptive Socratic games.. An -approximate Nash equilibrium is a strategy profile \u03b1 so that no player can increase her payoff by an additive by deviating from \u03b1.. Finding approximate Nash equilibria in both adaptive and non-adaptive Socratic games is an interesting direction to pursue.. Another natural extension is the model where query results are stochastic.. In this paper, we model a query as deterministically partitioning the possible worlds into subsets that the query cannot distinguish.. However, one could instead model a query as probabilistically mapping the set of possible worlds into the set of signals.. With this modification, our unobservable-query model becomes equivalent to the model of Bergemann and V\u00a8alim\u00a8aki , in which the result of a query is a posterior distribution over the worlds.. Our techniques allow us to compute equilibria in such a stochastic-query model provided that each query is represented as a table that, for each world/signal pair, lists the probability that the query outputs that signal in that world.. It is also interesting to consider settings in which the game\"s queries are specified by a compact representation of the relevant probability distributions.. (For example, one might consider a setting in which the algorithm has only a sampling oracle for the posterior distributions envisioned by Bergemann and V\u00a8alim\u00a8aki.). Efficiently finding equilibria in such settings remains an open problem.. Another interesting setting for Socratic games is when the set Q of available queries is given by Q = P(\u0393)-i.e., each player chooses to make a set q \u2208 P(\u0393) of queries from a specified groundset \u0393 of queries.. Here we take the query cost to be a linear function, so that \u03b4(q) = \u03b3\u2208q \u03b4({\u03b3}).. Natural groundsets include comparison queries (if my opponent is playing strategy aii, would I prefer to play ai or \u02c6ai?. ), strategy queries (what is my vector of payoffs if I play strategy ai?. ), and world-identity queries (is the world w \u2208 W the real world?).. When one can infer a polynomial bound on the number of queries made by a rational player, then our results yield efficient solutions.. (For example, we can efficiently solve games in which every groundset element \u03b3 \u2208 \u0393 has \u03b4({\u03b3}) = \u03a9(M \u2212 M), where M and M denote the maximum and minimum payoffs to any player in any world.). Conversely, it is NP-hard to compute a Nash equilibrium for such a game when every \u03b4({\u03b3}) \u2264 1/|W|2 , even when the worlds are constant sum and Player II has only a single available strategy.. Thus even computing a best response for Player I is hard.. (This proof proceeds by reduction from set cover; intuitively, for sufficiently low query costs, Player I must fully identify the actual world through his queries.. Selecting a minimum-sized set of these queries is hard.). Computing Player I\"s best response can be viewed as maximizing a submodular function, and thus a best response can be (1 \u2212 1/e) \u2248 0.63 approximated greedily .. An interesting open question is whether this approximate best-response calculation can be leveraged to find an approximate Nash equilibrium."}
{"id": "H-26", "keywords": ["machin learn for inform retriev", "support vector machin", "rank"], "title": "A Support Vector Method for Optimizing Average Precision", "abstract": "Machine learning is commonly used to improve ranked re- trieval systems. Due to computational diculties, few learn- ing techniques have been developed to directly optimize for mean average precision (MAP), despite its widespread use in evaluating such systems. Existing approaches optimiz- ing MAP either do not find a globally optimal solution, or are computationally expensive. In contrast, we present a general SVM learning algorithm that eciently finds a globally optimal solution to a straightforward relaxation of MAP. We evaluate our approach using the TREC 9 and TREC 10 Web Track corpora (WT10g), comparing against SVMs optimized for accuracy and ROCArea. In most cases we show our method to produce statistically significant im- provements in MAP scores.", "references": ["Automatic combination of multiple ranked retrieval systems", "Learning to rank using gradient descent", "Learning to rank with non-smooth cost functions", "Adapting ranking SVM to document retrieval", "Learning a ranking from pairwise preferences", "Ensemble selection from libraries of models", "The relationship between precision-recall and ROC curves", "Overview of the TREC-9 web track", "Overview of the TREC-2001 web track", "Large margin rank boundaries for ordinal regression", "Optimising area under the ROC curve using gradient descent", "Ir evaluation methods for retrieving highly relevant documents", "A support vector method for multivariate performance measures", "Document language models, query models, and risk minimization for information retrieval", "Support vector machines for classification in nonstandard situations", "A markov random field model for term dependencies", "Combining statistical learning with a knowledge-based approach", "The probability ranking principle in ir. journal of documentation", "Large margin methods for structured and interdependent output variables", "Statistical Learning Theory", "Optimizing classifier performance via approximation to the Wilcoxon-Mann-Witney statistic"], "full_text": "1. INTRODUCTION State of the art information retrieval systems commonly use machine learning techniques to learn ranking functions. However, most current approaches do not optimize for the evaluation measure most often used, namely Mean Average Precision (MAP). Instead, current algorithms tend to take one of two general approaches. The first approach is to learn a model that estimates the probability of a document being relevant given a query (e.g., ). If solved effectively, the ranking with best MAP performance can easily be derived from the probabilities of relevance. However, achieving high MAP only requires finding a good ordering of the documents. As a result, finding good probabilities requires solving a more difficult problem than necessary, likely requiring more training data to achieve the same MAP performance. The second common approach is to learn a function that maximizes a surrogate measure. Performance measures optimized include accuracy , ROCArea or modifications of ROCArea , and NDCG . Learning a model to optimize for such measures might result in suboptimal MAP performance. In fact, although some previous systems have obtained good MAP performance, it is known that neither achieving optimal accuracy nor ROCArea can guarantee optimal MAP performance. In this paper, we present a general approach for learning ranking functions that maximize MAP performance. Specifically, we present an SVM algorithm that globally optimizes a hinge-loss relaxation of MAP. This approach simplifies the process of obtaining ranking functions with high MAP performance by avoiding additional intermediate steps and heuristics. The new algorithm also makes it conceptually just as easy to optimize SVMs for MAP as was previously possible only for accuracy and ROCArea. In contrast to recent work directly optimizing for MAP performance by Metzler & Croft and Caruana et al. , our technique is computationally efficient while finding a globally optimal solution. Like , our method learns a linear model, but is much more efficient in practice and, unlike , can handle many thousands of features. We now describe the algorithm in detail and provide proof of correctness. Following this, we provide an analysis of running time. We finish with empirical results from experiments on the TREC 9 and TREC 10 Web Track corpus. We have also developed a software package implementing our algorithm that is available for public use1 2. THE LEARNING PROBLEM Following the standard machine learning setup, our goal is to learn a function h : X \u2192 Y between an input space X (all possible queries) and output space Y (rankings over a corpus). In order to quantify the quality of a prediction, \u02c6y = h(x), we will consider a loss function \u2206 : Y \u00d7 Y \u2192 . \u2206(y, \u02c6y) quantifies the penalty for making prediction \u02c6y if the correct output is y. The loss function allows us to incorporate specific performance measures, which we will exploit for optimizing MAP. We restrict ourselves to the supervised learning scenario, where input/output pairs (x, y) are available for training and are assumed to come from some fixed distribution P(x, y). The goal is to find a function h such that the risk (i.e., expected loss), R\u2206 P (h) = X\u00d7Y \u2206(y, h(x))dP(x, y), is minimized. Of course, P(x, y) is unknown. But given a finite set of training pairs, S = {(xi, yi) \u2208 X \u00d7 Y : i = 1, . . . , n}, the performance of h on S can be measured by the empirical risk, R\u2206 S (h) = nX i=1 \u2206(yi, h(xi)). In the case of learning a ranked retrieval function, X denotes a space of queries, and Y the space of (possibly weak) rankings over some corpus of documents C = {d1, . . . ,d|C|}. We can define average precision loss as \u2206map(y, \u02c6y) = 1 \u2212 MAP(rank(y), rank(\u02c6y)), where rank(y) is a vector of the rank values of each document in C. For example, for a corpus of two documents, {d1, d2}, with d1 having higher rank than d2, rank(y ) = (1, 0). We assume true rankings have two rank values, where relevant documents have rank value 1 and non-relevant documents rank value 0. We further assume that all predicted rankings are complete rankings (no ties). Let p = rank(y) and \u02c6p = rank(\u02c6y). The average precision score is defined as MAP(p, \u02c6p) = rel j:pj =1 Prec@j, where rel = |{i : pi = 1}| is the number of relevant documents, and Prec@j is the percentage of relevant documents in the top j documents in predicted ranking \u02c6y. MAP is the mean of the average precision scores of a group of queries. 2.1 MAP vs ROCArea Most learning algorithms optimize for accuracy or ROCArea. While optimizing for these measures might achieve good MAP performance, we use two simple examples to show it can also be suboptimal in terms of MAP. ROCArea assigns equal penalty to each misordering of a relevant/non-relevant pair. In contrast, MAP assigns greater penalties to misorderings higher up in the predicted ranking. Using our notation, ROCArea can be defined as ROC(p, \u02c6p) = rel \u00b7 (|C| \u2212 rel) i:pi=1 j:pj =0 1[\u02c6pi>\u02c6pj ], where p is the true (weak) ranking, \u02c6p is the predicted ranking, and 1[b] is the indicator function conditioned on b. Doc ID 1 2 3 4 5 6 7 8 p 1 0 0 0 0 1 1 0 rank(h1(x)) 8 7 6 5 4 3 2 1 rank(h2(x)) 1 2 3 4 5 6 7 8 Table 1: Toy Example and Models Suppose we have a hypothesis space with only two hypothesis functions, h1 and h2, as shown in Table 1. These two hypotheses predict a ranking for query x over a corpus of eight documents. Hypothesis MAP ROCArea h1(x) 0.59 0.47 h2(x) 0.51 0.53 Table 2: Performance of Toy Models Table 2 shows the MAP and ROCArea scores of h1 and h2. Here, a learning method which optimizes for ROCArea would choose h2 since that results in a higher ROCArea score, but this yields a suboptimal MAP score. 2.2 MAP vs Accuracy Using a very similar example, we now demonstrate how optimizing for accuracy might result in suboptimal MAP. Models which optimize for accuracy are not directly concerned with the ranking. Instead, they learn a threshold such that documents scoring higher than the threshold can be classified as relevant and documents scoring lower as nonrelevant. Doc ID 1 2 3 4 5 6 7 8 9 10 11 p 1 0 0 0 0 1 1 1 1 0 0 rank(h1(x)) 11 10 9 8 7 6 5 4 3 2 1 rank(h2(x)) 1 2 3 4 5 6 7 8 9 10 11 Table 3: Toy Example and Models We consider again a hypothesis space with two hypotheses. Table 3 shows the predictions of the two hypotheses on a single query x. Hypothesis MAP Best Acc. h1(q) 0.70 0.64 h2(q) 0.64 0.73 Table 4: Performance of Toy Models Table 4 shows the MAP and best accuracy scores of h1(q) and h2(q). The best accuracy refers to the highest achievable accuracy on that ranking when considering all possible thresholds. For instance, with h1(q), a threshold between documents 1 and 2 gives 4 errors (documents 6-9 incorrectly classified as non-relevant), yielding an accuracy of 0.64. Similarly, with h2(q), a threshold between documents 5 and 6 gives 3 errors (documents 10-11 incorrectly classified as relevant, and document 1 as non-relevant), yielding an accuracy of 0.73. A learning method which optimizes for accuracy would choose h2 since that results in a higher accuracy score, but this yields a suboptimal MAP score. 3. OPTIMIZING AVERAGE PRECISION We build upon the approach used by for optimizing ROCArea. Unlike ROCArea, however, MAP does not decompose linearly in the examples and requires a substantially extended algorithm, which we describe in this section. Recall that the true ranking is a weak ranking with two rank values (relevant and non-relevant). Let Cx and C\u00afx denote the set of relevant and non-relevant documents of C for query x, respectively. We focus on functions which are parametrized by a weight vector w, and thus wish to find w to minimize the empirical risk, R\u2206 S (w) \u2261 R\u2206 S (h(\u00b7; w)). Our approach is to learn a discriminant function F : X \u00d7 Y \u2192 over input-output pairs. Given query x, we can derive a prediction by finding the ranking y that maximizes the discriminant function: h(x; w) = argmax y\u2208Y F(x, y; w). (1) We assume F to be linear in some combined feature representation of inputs and outputs \u03a8(x, y) \u2208 RN , i.e., F(x, y; w) = wT \u03a8(x, y). (2) The combined feature function we use is \u03a8(x, y) = |Cx| \u00b7 |C\u00afx| i:di\u2208Cx j:dj \u2208C\u00afx [yij (\u03c6(x, di) \u2212 \u03c6(x, dj))] , where \u03c6 : X \u00d7 C \u2192 N is a feature mapping function from a query/document pair to a point in N dimensional space2 We represent rankings as a matrix of pairwise orderings, Y \u2282 {\u22121, 0, +1}|C|\u00d7|C| . For any y \u2208 Y, yij = +1 if di is ranked ahead of dj, and yij = \u22121 if dj is ranked ahead of di, and yij = 0 if di and dj have equal rank. We consider only matrices which correspond to valid rankings (i.e, obeying antisymmetry and transitivity). Intuitively, \u03a8 is a summation over the vector differences of all relevant/non-relevant document pairings. Since we assume predicted rankings to be complete rankings, yij is either +1 or \u22121 (never 0). Given a learned weight vector w, predicting a ranking (i.e. solving equation (1)) given query x reduces to picking each yij to maximize wT \u03a8(x, y). As is also discussed in , this is attained by sorting the documents by wT \u03c6(x, d) in descending order. We will discuss later the choices of \u03c6 we used for our experiments. 3.1 Structural SVMs The above formulation is very similar to learning a straightforward linear model while training on the pairwise difference of relevant/non-relevant document pairings. Many SVM-based approaches optimize over these pairwise differences (e.g., ), although these methods do not optimize for MAP during training. Previously, it was not clear how to incorporate non-linear multivariate loss functions such as MAP loss directly into global optimization problems such as SVM training. We now present a method based on structural SVMs to address this problem. We use the structural SVM formulation, presented in Optimization Problem 1, to learn a w \u2208 RN Optimization Problem 1. (Structural SVM) min w,\u03be\u22650 w 2 nX i=1 \u03bei (3) s.t. \u2200i, \u2200y \u2208 Y \\ yi : wT \u03a8(xi, yi) \u2265 wT \u03a8(xi, y) + \u2206(yi, y) \u2212 \u03bei (4) The objective function to be minimized (3) is a tradeoff between model complexity, w 2 , and a hinge loss relaxation of MAP loss, \u03bei. As is usual in SVM training, C is a For example, one dimension might be the number of times the query words appear in the document. Algorithm 1 Cutting plane algorithm for solving OP 1 within tolerance . 1: Input: (x1, y1), . . . , (xn, yn), C, 2: Wi \u2190 \u2205 for all i = 1, . . . , n 3: repeat 4: for i = 1, . . . , n do 5: H(y; w) \u2261 \u2206(yi, y) + wT \u03a8(xi, y) \u2212 wT \u03a8(xi, yi) 6: compute \u02c6y = argmaxy\u2208Y H(y; w) 7: compute \u03bei = max{0, maxy\u2208Wi H(y; w)} 8: if H(\u02c6y; w) > \u03bei + then 9: Wi \u2190 Wi \u222a {\u02c6y} 10: w \u2190 optimize (3) over W = i Wi 11: end if 12: end for 13: until no Wi has changed during iteration parameter that controls this tradeoff and can be tuned to achieve good performance in different training tasks. For each (xi, yi) in the training set, a set of constraints of the form in equation (4) is added to the optimization problem. Note that wT \u03a8(x, y) is exactly our discriminant function F(x, y; w) (see equation (2)). During prediction, our model chooses the ranking which maximizes the discriminant (1). If the discriminant value for an incorrect ranking y is greater than for the true ranking yi (e.g., F(xi, y; w) > F(xi, yi; w)), then corresponding slack variable, \u03bei, must be at least \u2206(yi, y) for that constraint to be satisfied. Therefore, the sum of slacks, \u03bei, upper bounds the MAP loss. This is stated formally in Proposition 1. Proposition 1. Let \u03be\u2217 (w) be the optimal solution of the slack variables for OP 1 for a given weight vector w. Then Pn i=1 \u03bei is an upper bound on the empirical risk R\u2206 S (w). (see for proof) Proposition 1 shows that OP 1 learns a ranking function that optimizes an upper bound on MAP error on the training set. Unfortunately there is a problem: a constraint is required for every possible wrong output y, and the number of possible wrong outputs is exponential in the size of C. Fortunately, we may employ Algorithm 1 to solve OP 1. Algorithm 1 is a cutting plane algorithm, iteratively introducing constraints until we have solved the original problem within a desired tolerance . The algorithm starts with no constraints, and iteratively finds for each example (xi, yi) the output \u02c6y associated with the most violated constraint. If the corresponding constraint is violated by more than we introduce \u02c6y into the working set Wi of active constraints for example i, and re-solve (3) using the updated W. It can be shown that Algorithm 1\"s outer loop is guaranteed to halt within a polynomial number of iterations for any desired precision . Theorem 1. Let \u00afR = maxi maxy \u03a8(xi, yi) \u2212 \u03a8(xi, y) , \u00af\u2206 = maxi maxy \u2206(yi, y), and for any > 0, Algorithm 1 terminates after adding at most max 2n \u00af\u2206 8C \u00af\u2206 \u00afR2 ff constraints to the working set W. (see for proof) However, within the inner loop of this algorithm we have to compute argmaxy\u2208Y H(y; w), where H(y; w) = \u2206(yi, y) + wT \u03a8(xi, y) \u2212 wT \u03a8(xi, yi), or equivalently, argmax y\u2208Y \u2206(yi, y) + wT \u03a8(xi, y), since wT \u03a8(xi, yi) is constant with respect to y. Though closely related to the classification procedure, this has the substantial complication that we must contend with the additional \u2206(yi, y) term. Without the ability to efficiently find the most violated constraint (i.e., solve argmaxy\u2208Y H(y, w)), the constraint generation procedure is not tractable. 3.2 Finding the Most Violated Constraint Using OP 1 and optimizing to ROCArea loss (\u2206roc), the problem of finding the most violated constraint, or solving argmaxy\u2208Y H(y, w) (henceforth argmax H), is addressed in . Solving argmax H for \u2206map is more difficult. This is primarily because ROCArea decomposes nicely into a sum of scores computed independently on each relative ordering of a relevant/non-relevant document pair. MAP, on the other hand, does not decompose in the same way as ROCArea. The main algorithmic contribution of this paper is an efficient method for solving argmax H for \u2206map. One useful property of \u2206map is that it is invariant to swapping two documents with equal relevance. For example, if documents da and db are both relevant, then swapping the positions of da and db in any ranking does not affect \u2206map. By extension, \u2206map is invariant to any arbitrary permutation of the relevant documents amongst themselves and of the non-relevant documents amongst themselves. However, this reshu\ufb04ing will affect the discriminant score, wT \u03a8(x, y). This leads us to Observation 1. Observation 1. Consider rankings which are constrained by fixing the relevance at each position in the ranking (e.g., the 3rd document in the ranking must be relevant). Every ranking which satisfies the same set of constraints will have the same \u2206map. If the relevant documents are sorted by wT \u03c6(x, d) in descending order, and the non-relevant documents are likewise sorted by wT \u03c6(x, d), then the interleaving of the two sorted lists which satisfies the constraints will maximize H for that constrained set of rankings. Observation 1 implies that in the ranking which maximizes H, the relevant documents will be sorted by wT \u03c6(x, d), and the non-relevant documents will also be sorted likewise. By first sorting the relevant and non-relevant documents, the problem is simplified to finding the optimal interleaving of two sorted lists. For the rest of our discussion, we assume that the relevant documents and non-relevant documents are both sorted by descending wT \u03c6(x, d). For convenience, we also refer to relevant documents as {dx 1 , . . . dx |Cx|} = Cx and non-relevant documents as {d\u00afx 1 , . . . d\u00afx |C\u00afx|} = C\u00afx We define \u03b4j(i1, i2), with i1 < i2, as the change in H from when the highest ranked relevant document ranked after d\u00afx is dx i1 to when it is dx i2 . For i2 = i1 + 1, we have \u03b4j(i, i + 1) = |Cx| j + i j \u2212 1 j + i \u2212 1 \u2212 2 \u00b7 (sx i \u2212 s\u00afx j ), (5) where si = wT \u03c6(x, di). The first term in (5) is the change in \u2206map when the ith relevant document has j non-relevant documents ranked before it, as opposed to j \u22121. The second term is the change in the discriminant score, wT \u03a8(x, y), when yij changes from +1 to \u22121. . . . , dx i , d\u00afx j , dx i+1, . . . . . . , d\u00afx j , dx i , dx i+1, . . . Figure 1: Example for \u03b4j(i, i + 1) Figure 1 gives a conceptual example for \u03b4j(i, i + 1). The bottom ranking differs from the top only where d\u00afx j slides up one rank. The difference in the value of H for these two rankings is exactly \u03b4j(i, i + 1). For any i1 < i2, we can then define \u03b4j(i1, i2) as \u03b4j(i1, i2) = i2\u22121 k=i1 \u03b4j(k, k + 1), (6) or equivalently, \u03b4j(i1, i2) = i2\u22121 k=i1 |Cx| j + k j \u2212 1 j + k \u2212 1 \u2212 2 \u00b7 (sx k \u2212 s\u00afx j ) Let o1, . . . , o|C\u00afx| encode the positions of the non-relevant documents, where dx oj is the highest ranked relevant document ranked after the jth non-relevant document. Due to Observation 1, this encoding uniquely identifies a complete ranking. We can recover the ranking as yij = >>>< >>>: 0 if i = j sign(si \u2212 sj) if di, dj equal relevance sign(oj \u2212 i \u2212 0.5) if di = dx i , dj = d\u00afx sign(j \u2212 oi + 0.5) if di = d\u00afx i , dj = dx . (7) We can now reformulate H into a new objective function, H (o1, . . . , o|C\u00afx||w) = H(\u00afy|w) + |C\u00afx k=1 \u03b4k(ok, |Cx | + 1), where \u00afy is the true (weak) ranking. Conceptually H starts with a perfect ranking \u00afy, and adds the change in H when each successive non-relevant document slides up the ranking. We can then reformulate the argmax H problem as argmax H = argmax o1,...,o|C\u00afx| |C\u00afx k=1 \u03b4k(ok, |Cx | + 1) (8) s.t. o1 \u2264 . . . \u2264 o|C\u00afx|. (9) Algorithm 2 describes the algorithm used to solve equation (8). Conceptually, Algorithm 2 starts with a perfect ranking. Then for each successive non-relevant document, the algorithm modifies the solution by sliding that document up the ranking to locally maximize H while keeping the positions of the other non-relevant documents constant. 3.2.1 Proof of Correctness Algorithm 2 is greedy in the sense that it finds the best position of each non-relevant document independently from the other non-relevant documents. In other words, the algorithm maximizes H for each non-relevant document, d\u00afx j , Algorithm 2 Finding the Most Violated Constraint (argmax H) for Algorithm 1 with \u2206map 1: Input: w, Cx , C\u00afx 2: sort Cx and C\u00afx in descending order of wT \u03c6(x, d) 3: sx i \u2190 wT \u03c6(x, dx i ), i = 1, . . . , |Cx 4: s\u00afx i \u2190 wT \u03c6(x, d\u00afx i ), i = 1, . . . , |C\u00afx 5: for j = 1, . . . , |C\u00afx | do 6: optj \u2190 argmaxk \u03b4j(k, |Cx | + 1) 7: end for 8: encode \u02c6y according to (7) 9: return \u02c6y without considering the positions of the other non-relevant documents, and thus ignores the constraints of (9). In order for the solution to be feasible, the jth non-relevant document must be ranked after the first j \u2212 1 non-relevant documents, thus satisfying opt1 \u2264 opt2 \u2264 . . . \u2264 opt|C\u00afx|. (10) If the solution is feasible, the it clearly solves (8). Therefore, it suffices to prove that Algorithm 2 satisfies (10). We first prove that \u03b4j(\u00b7, \u00b7) is monotonically decreasing in j. Lemma 1. For any 1 \u2264 i1 < i2 \u2264 |Cx | + 1 and 1 \u2264 j < |C\u00afx |, it must be the case that \u03b4j+1(i1, i2) \u2264 \u03b4j(i1, i2). Proof. Recall from (6) that both \u03b4j(i1, i2) and \u03b4j+1(i1, i2) are summations of i2 \u2212 i1 terms. We will show that each term in the summation of \u03b4j+1(i1, i2) is no greater than the corresponding term in \u03b4j(i1, i2), or \u03b4j+1(k, k + 1) \u2264 \u03b4j(k, k + 1) for k = i1, . . . , i2 \u2212 1. Each term in \u03b4j(k, k +1) and \u03b4j+1(k, k +1) can be further decomposed into two parts (see (5)). We will show that each part of \u03b4j+1(k, k + 1) is no greater than the corresponding part in \u03b4j(k, k + 1). In other words, we will show that both j + 1 j + k + 1 j + k j + k j \u2212 1 j + k \u2212 1 (11) and \u22122 \u00b7 (sx k \u2212 s\u00afx j+1) \u2264 \u22122 \u00b7 (sx k \u2212 s\u00afx j ) (12) are true for the aforementioned values of j and k. It is easy to see that (11) is true by observing that for any two positive integers 1 \u2264 a < b, a + 1 b + 1 a \u2212 1 b \u2212 1 and choosing a = j and b = j + k. The second inequality (12) holds because Algorithm 2 first sorts d\u00afx in descending order of s\u00afx , implying s\u00afx j+1 \u2264 s\u00afx j . Thus we see that each term in \u03b4j+1 is no greater than the corresponding term in \u03b4j, which completes the proof. The result of Lemma 1 leads directly to our main correctness result: Theorem 2. In Algorithm 2, the computed values of optj satisfy (10), implying that the solution returned by Algorithm 2 is feasible and thus optimal. Proof. We will prove that optj \u2264 optj+1 holds for any 1 \u2264 j < |C\u00afx |, thus implying (10). Since Algorithm 2 computes optj as optj = argmax \u03b4j(k, |Cx | + 1), (13) then by definition of \u03b4j (6), for any 1 \u2264 i < optj, \u03b4j(i, optj) = \u03b4j(i, |Cx | + 1) \u2212 \u03b4j(optj, |Cx | + 1) < 0. Using Lemma 1, we know that \u03b4j+1(i, optj) \u2264 \u03b4j(i, optj) < 0, which implies that for any 1 \u2264 i < optj, \u03b4j+1(i, |Cx | + 1) \u2212 \u03b4j+1(optj, |Cx | + 1) < 0. Suppose for contradiction that optj+1 < optj. Then \u03b4j+1(optj+1, |Cx | + 1) < \u03b4j+1(optj, |Cx | + 1), which contradicts (13). Therefore, it must be the case that optj \u2264 optj+1, which completes the proof. 3.2.2 Running Time The running time of Algorithm 2 can be split into two parts. The first part is the sort by wT \u03c6(x, d), which requires O(n log n) time, where n = |Cx | + |C\u00afx |. The second part computes each optj, which requires O(|Cx | \u00b7 |C\u00afx |) time. Though in the worst case this is O(n2 ), the number of relevant documents, |Cx |, is often very small (e.g., constant with respect to n), in which case the running time for the second part is simply O(n). For most real-world datasets, Algorithm 2 is dominated by the sort and has complexity O(n log n). Algorithm 1 is guaranteed to halt in a polynomial number of iterations , and each iteration runs Algorithm 2. Virtually all well-performing models were trained in a reasonable amount of time (usually less than one hour). Once training is complete, making predictions on query x using the resulting hypothesis h(x|w) requires only sorting by wT \u03c6(x, d). We developed our software using a Python interface3 to SVMstruct , since the Python language greatly simplified the coding process. To improve performance, it is advisable to use the standard C implementation4 of SVMstruct 4. EXPERIMENT SETUP The main goal of our experiments is to evaluate whether directly optimizing MAP leads to improved MAP performance compared to conventional SVM methods that optimize a substitute loss such as accuracy or ROCArea. We empirically evaluate our method using two sets of TREC Web Track queries, one each from TREC 9 and TREC 10 (topics 451-500 and 501-550), both of which used the WT10g corpus. For each query, TREC provides the relevance judgments of the documents. We generated our features using the scores of existing retrieval functions on these queries. While our method is agnostic to the meaning of the features, we chose to use existing retrieval functions as a simple yet effective way of acquiring useful features. As such, our . TREC 10 Submissions 18 900 Table 5: Dataset Statistics experiments essentially test our method\"s ability to re-rank the highly ranked documents (e.g., re-combine the scores of the retrieval functions) to improve MAP. We compare our method against the best retrieval functions trained on (henceforth base functions), as well as against previously proposed SVM methods. Comparing with the best base functions tests our method\"s ability to learn a useful combination. Comparing with previous SVM methods allows us to test whether optimizing directly for MAP (as opposed to accuracy or ROCArea) achieves a higher MAP score in practice. The rest of this section describes the base functions and the feature generation method in detail. 4.1 Choosing Retrieval Functions We chose two sets of base functions for our experiments. For the first set, we generated three indices over the WT10g corpus using Indri5 . The first index was generated using default settings, the second used Porter-stemming, and the last used Porter-stemming and Indri\"s default stopwords. For both TREC 9 and TREC 10, we used the description portion of each query and scored the documents using five of Indri\"s built-in retrieval methods, which are Cosine Similarity, TFIDF, Okapi, Language Model with Dirichlet Prior, and Language Model with Jelinek-Mercer Prior. All parameters were kept as their defaults. We computed the scores of these five retrieval methods over the three indices, giving 15 base functions in total. For each query, documents of each base function. For our second set of base functions, we used scores from the TREC 9 and TREC 10 Web Track submissions. We used only the non-manual, non-short submissions from both years. For TREC 9 and TREC 10, there were 53 and 18 such submissions, respectively. documents. b ca wT \u03c6(x,d) f(d|x) Figure 2: Example Feature Binning 4.2 Generating Features In order to generate input examples for our method, a concrete instantiation of \u03c6 must be provided. For each doc5 TREC 9 TREC 10 Model MAP W/L MAP W/L SVM\u2206 map 0.242 - 0.236Best Func. 0.204 39/11 ** 0.181 37/13 ** 2nd Best 0.199 38/12 ** 0.174 43/7 ** 3rd Best 0.188 34/16 ** 0.174 38/12 ** Table 6: Comparison with Indri Functions ument d scored by a set of retrieval functions F on query x, we generate the features as a vector \u03c6(x, d) = 1[f(d|x)>k] : \u2200f \u2208 F, \u2200k \u2208 Kf , where f(d|x) denotes the score that retrieval function f assigns to document d for query x, and each Kf is a set of real values. From a high level, we are expressing the score of each retrieval function using |Kf | + 1 bins. Since we are using linear kernels, one can think of the learning problem as finding a good piecewise-constant combination of the scores of the retrieval functions. Figure 2 shows an example of our feature mapping method. In this example we have a single feature F = {f}. Here, Kf = {a, b, c}, and the weight vector is w = wa, wb, wc . For any document d and query x, we have wT \u03c6(x, d) = >>< >>: 0 if f(d|x) < a wa if a \u2264 f(d|x) < b wa + wb if b \u2264 f(d|x) < c wa + wb + wc if c \u2264 f(d|x) This is expressed qualitatively in Figure 2, where wa and wb are positive, and wc is negative. We ran our main experiments using four choices of F: the set of aforementioned Indri retrieval functions for TREC 9 and TREC 10, and the Web Track submissions for TREC 9 and TREC 10. For each F and each function f \u2208 F, we chose 50 values for Kf which are reasonably spaced and capture the sensitive region of f. Using the four choices of F, we generated four datasets for our main experiments. Table 5 contains statistics of the generated datasets. There are many ways to generate features, and we are not advocating our method over others. This was simply an efficient means to normalize the outputs of different functions and allow for a more expressive model. 5. EXPERIMENTS For each dataset in Table 5, we performed 50 trials. For each trial, we train on 10 randomly selected queries, and select another 5 queries at random for a validation set. Models were trained using a wide range of C values. The model which performed best on the validation set was selected and tested on the remaining 35 queries. All queries were selected to be in the training, validation and test sets the same number of times. Using this setup, we performed the same experiments while using our method (SVM\u2206 map), an SVM optimizing for ROCArea (SVM\u2206 roc) , and a conventional classification SVM (SVMacc) . All SVM methods used a linear kernel. We reported the average performance of all models over the 50 trials. 5.1 Comparison with Base Functions In analyzing our results, the first question to answer is, can SVM\u2206 map learn a model which outperforms the best base TREC 9 TREC 10 Model MAP W/L MAP W/L SVM\u2206 map 0.290 - 0.287Best Func. 0.280 28/22 0.283 29/21 2nd Best 0.269 30/20 0.251 36/14 ** 3rd Best 0.266 30/20 0.233 36/14 ** Table 7: Comparison with TREC Submissions TREC 9 TREC 10 Model MAP W/L MAP W/L SVM\u2206 map 0.284 - 0.288Best Func. 0.280 27/23 0.283 31/19 2nd Best 0.269 30/20 0.251 36/14 ** 3rd Best 0.266 30/20 0.233 35/15 ** Table 8: Comparison with TREC Subm. (w/o best) functions? Table 6 presents the comparison of SVM\u2206 map with the best Indri base functions. Each column group contains the macro-averaged MAP performance of SVM\u2206 map or a base function. The W/L columns show the number of queries where SVM\u2206 map achieved a higher MAP score. Significance tests were performed using the two-tailed Wilcoxon signed rank test. Two stars indicate a significance level of 0.95. All tables displaying our experimental results are structured identically. Here, we find that SVM\u2206 map significantly outperforms the best base functions. Table 7 shows the comparison when trained on TREC submissions. While achieving a higher MAP score than the best base functions, the performance difference between SVM\u2206 map the base functions is not significant. Given that many of these submissions use scoring functions which are carefully crafted to achieve high MAP, it is possible that the best performing submissions use techniques which subsume the techniques of the other submissions. As a result, SVM\u2206 map would not be able to learn a hypothesis which can significantly out-perform the best submission. Hence, we ran the same experiments using a modified dataset where the features computed using the best submission were removed. Table 8 shows the results (note that we are still comparing against the best submission though we are not using it for training). Notice that while the performance of SVM\u2206 map degraded slightly, the performance was still comparable with that of the best submission. 5.2 Comparison w/ Previous SVM Methods The next question to answer is, does SVM\u2206 map produce higher MAP scores than previous SVM methods? Tables 9 and 10 present the results of SVM\u2206 map, SVM\u2206 roc, and SVMacc when trained on the Indri retrieval functions and TREC submissions, respectively. Table 11 contains the corresponding results when trained on the TREC submissions without the best submission. To start with, our results indicate that SVMacc was not competitive with SVM\u2206 map and SVM\u2206 roc, and at times underperformed dramatically. As such, we tried several approaches to improve the performance of SVMacc. 5.2.1 Alternate SVMacc Methods One issue which may cause SVMacc to underperform is the severe imbalance between relevant and non-relevant docTREC 9 TREC 10 Model MAP W/L MAP W/L SVM\u2206 map 0.242 - 0.236SVM\u2206 roc 0.237 29/21 0.234 24/26 SVMacc 0.147 47/3 ** 0.155 47/3 ** SVMacc2 0.219 39/11 ** 0.207 43/7 ** SVMacc3 0.113 49/1 ** 0.153 45/5 ** SVMacc4 0.155 48/2 ** 0.155 48/2 ** Table 9: Trained on Indri Functions TREC 9 TREC 10 Model MAP W/L MAP W/L SVM\u2206 map 0.290 - 0.287SVM\u2206 roc 0.282 29/21 0.278 35/15 ** SVMacc 0.213 49/1 ** 0.222 49/1 ** SVMacc2 0.270 34/16 ** 0.261 42/8 ** SVMacc3 0.133 50/0 ** 0.182 46/4 ** SVMacc4 0.233 47/3 ** 0.238 46/4 ** Table 10: Trained on TREC Submissions uments. The vast majority of the documents are not relevant. SVMacc2 addresses this problem by assigning more penalty to false negative errors. For each dataset, the ratio of the false negative to false positive penalties is equal to the ratio of the number non-relevant and relevant documents in that dataset. Tables 9, 10 and 11 indicate that SVMacc2 still performs significantly worse than SVM\u2206 map. Another possible issue is that SVMacc attempts to find just one discriminating threshold b that is query-invariant. It may be that different queries require different values of b. Having the learning method trying to find a good b value (when one does not exist) may be detrimental. We took two approaches to address this issue. The first method, SVMacc3, converts the retrieval function scores into percentiles. For example, for document d, query q and retrieval function f, if the score f(d|q) is in the top 90% of the scores f(\u00b7|q) for query q, then the converted score is f (d|q) = 0.9. Each Kf contains 50 evenly spaced values between 0 and 1. Tables 9, 10 and 11 show that the performance of SVMacc3 was also not competitive with SVM\u2206 map. The second method, SVMacc4, normalizes the scores given by f for each query. For example, assume for query q that f outputs scores in the range 0.2 to 0.7. Then for document d, if f(d|q) = 0.6, the converted score would be f (d|q) = (0.6 \u2212 0.2)/(0.7 \u2212 0.2) = 0.8. Each Kf contains 50 evenly spaced values between 0 and 1. Again, Tables 9, 10 and 11 show that SVMacc4 was not competitive with SVM\u2206 map 5.2.2 MAP vs ROCArea SVM\u2206 roc performed much better than SVMacc in our experiments. When trained on Indri retrieval functions (see Table 9), the performance of SVM\u2206 roc was slight, though not significantly, worse than the performances of SVM\u2206 map. However, Table 10 shows that SVM\u2206 map did significantly outperform SVM\u2206 roc when trained on the TREC submissions. Table 11 shows the performance of the models when trained on the TREC submissions with the best submission removed. The performance of most models degraded by a small amount, with SVM\u2206 map still having the best performance. TREC 9 TREC 10 Model MAP W/L MAP W/L SVM\u2206 map 0.284 - 0.288SVM\u2206 roc 0.274 31/19 ** 0.272 38/12 ** SVMacc 0.215 49/1 ** 0.211 50/0 ** SVMacc2 0.267 35/15 ** 0.258 44/6 ** SVMacc3 0.133 50/0 ** 0.174 46/4 ** SVMacc4 0.228 46/4 ** 0.234 45/5 ** Table 11: Trained on TREC Subm. (w/o Best) 6. CONCLUSIONS AND FUTURE WORK We have presented an SVM method that directly optimizes MAP. It provides a principled approach and avoids difficult to control heuristics. We formulated the optimization problem and presented an algorithm which provably finds the solution in polynomial time. We have shown empirically that our method is generally superior to or competitive with conventional SVMs methods. Our new method makes it conceptually just as easy to optimize SVMs for MAP as was previously possible only for Accuracy and ROCArea. The computational cost for training is very reasonable in practice. Since other methods typically require tuning multiple heuristics, we also expect to train fewer models before finding one which achieves good performance. The learning framework used by our method is fairly general. A natural extension of this framework would be to develop methods to optimize for other important IR measures, such as Normalized Discounted Cumulative Gain and Mean Reciprocal Rank. 7. ACKNOWLEDGMENTS This work was funded under NSF Award IIS- and a gift from Yahoo! Research. The third author was also partly supported by a Microsoft Research Fellowship.", "body1": "State of the art information retrieval systems commonly use machine learning techniques to learn ranking functions. Instead, current algorithms tend to take one of two general approaches. Learning a model to optimize for such measures might result in suboptimal MAP performance. In this paper, we present a general approach for learning ranking functions that maximize MAP performance. Specifically, we present an SVM algorithm that globally optimizes a hinge-loss relaxation of MAP. , our technique is computationally efficient while finding a globally optimal solution. Following the standard machine learning setup, our goal is to learn a function h : X \u2192 Y between an input space X (all possible queries) and output space Y (rankings over a corpus). \u2206(y, \u02c6y) quantifies the penalty for making prediction \u02c6y if the correct output is y. In the case of learning a ranked retrieval function, X denotes a space of queries, and Y the space of (possibly weak) rankings over some corpus of documents C = {d1, . We can define average precision loss as \u2206map(y, \u02c6y) = 1 \u2212 MAP(rank(y), rank(\u02c6y)), where rank(y) is a vector of the rank values of each document in C. For example, for a corpus of two documents, {d1, d2}, with d1 having higher rank than d2, rank(y ) = (1, 0). Let p = rank(y) and \u02c6p = rank(\u02c6y). ROCArea assigns equal penalty to each misordering of a relevant/non-relevant pair. Using our notation, ROCArea can be defined as ROC(p, \u02c6p) = rel \u00b7 (|C| \u2212 rel) i:pi=1 j:pj =0 1[\u02c6pi>\u02c6pj ], where p is the true (weak) ranking, \u02c6p is the predicted ranking, and 1[b] is the indicator function conditioned on b. Doc ID 1 2 3 4 5 6 7 8 p 1 0 0 0 0 1 1 0 rank(h1(x)) 8 7 6 5 4 3 2 1 rank(h2(x)) 1 2 3 4 5 6 7 8 Table 1: Toy Example and Models Suppose we have a hypothesis space with only two hypothesis functions, h1 and h2, as shown in Table 1. Hypothesis MAP ROCArea h1(x) 0.59 0.47 h2(x) 0.51 0.53 Table 2: Performance of Toy Models Table 2 shows the MAP and ROCArea scores of h1 and h2. 2.2 MAP vs Accuracy Using a very similar example, we now demonstrate how optimizing for accuracy might result in suboptimal MAP. Models which optimize for accuracy are not directly concerned with the ranking. Doc ID 1 2 3 4 5 6 7 8 9 10 11 p 1 0 0 0 0 1 1 1 1 0 0 rank(h1(x)) 11 10 9 8 7 6 5 4 3 2 1 rank(h2(x)) 1 2 3 4 5 6 7 8 9 10 11 Table 3: Toy Example and Models We consider again a hypothesis space with two hypotheses. Hypothesis MAP Best Acc. h1(q) 0.70 0.64 h2(q) 0.64 0.73 Table 4: Performance of Toy Models Table 4 shows the MAP and best accuracy scores of h1(q) and h2(q). We build upon the approach used by for optimizing ROCArea. We focus on functions which are parametrized by a weight vector w, and thus wish to find w to minimize the empirical risk, R\u2206 S (w) \u2261 R\u2206 S (h(\u00b7; w)). solving equation (1)) given query x reduces to picking each yij to maximize wT \u03a8(x, y). 3.1 Structural SVMs The above formulation is very similar to learning a straightforward linear model while training on the pairwise difference of relevant/non-relevant document pairings. We use the structural SVM formulation, presented in Optimization Problem 1, to learn a w \u2208 RN Optimization Problem 1. 1: Input: (x1, y1), . For each (xi, yi) in the training set, a set of constraints of the form in equation (4) is added to the optimization problem. This is stated formally in Proposition 1. Proposition 1. (see for proof) Proposition 1 shows that OP 1 learns a ranking function that optimizes an upper bound on MAP error on the training set. Algorithm 1 is a cutting plane algorithm, iteratively introducing constraints until we have solved the original problem within a desired tolerance . If the corresponding constraint is violated by more than we introduce \u02c6y into the working set Wi of active constraints for example i, and re-solve (3) using the updated W. It can be shown that Algorithm 1\"s outer loop is guaranteed to halt within a polynomial number of iterations for any desired precision . Theorem 1. 3.2 Finding the Most Violated Constraint Using OP 1 and optimizing to ROCArea loss (\u2206roc), the problem of finding the most violated constraint, or solving argmaxy\u2208Y H(y, w) (henceforth argmax H), is addressed in . By extension, \u2206map is invariant to any arbitrary permutation of the relevant documents amongst themselves and of the non-relevant documents amongst themselves. This leads us to Observation 1. Observation 1. By first sorting the relevant and non-relevant documents, the problem is simplified to finding the optimal interleaving of two sorted lists. . Figure 1: Example for \u03b4j(i, i + 1) Figure 1 gives a conceptual example for \u03b4j(i, i + 1). For any i1 < i2, we can then define \u03b4j(i1, i2) as \u03b4j(i1, i2) = i2\u22121 k=i1 \u03b4j(k, k + 1), (6) or equivalently, \u03b4j(i1, i2) = i2\u22121 k=i1 |Cx| j + k j \u2212 1 j + k \u2212 1 \u2212 2 \u00b7 (sx k \u2212 s\u00afx j ) Let o1, . o1 \u2264 . 3.2.1 Proof of Correctness Algorithm 2 is greedy in the sense that it finds the best position of each non-relevant document independently from the other non-relevant documents. In order for the solution to be feasible, the jth non-relevant document must be ranked after the first j \u2212 1 non-relevant documents, thus satisfying opt1 \u2264 opt2 \u2264 . Proof. Each term in \u03b4j(k, k +1) and \u03b4j+1(k, k +1) can be further decomposed into two parts (see (5)). The second inequality (12) holds because Algorithm 2 first sorts d\u00afx in descending order of s\u00afx , implying s\u00afx j+1 \u2264 s\u00afx j . Thus we see that each term in \u03b4j+1 is no greater than the corresponding term in \u03b4j, which completes the proof. The result of Lemma 1 leads directly to our main correctness result: Theorem 2. Proof. Since Algorithm 2 computes optj as optj = argmax \u03b4j(k, |Cx | + 1), (13) then by definition of \u03b4j (6), for any 1 \u2264 i < optj, \u03b4j(i, optj) = \u03b4j(i, |Cx | + 1) \u2212 \u03b4j(optj, |Cx | + 1) < 0. Using Lemma 1, we know that \u03b4j+1(i, optj) \u2264 \u03b4j(i, optj) < 0, which implies that for any 1 \u2264 i < optj, \u03b4j+1(i, |Cx | + 1) \u2212 \u03b4j+1(optj, |Cx | + 1) < 0. Suppose for contradiction that optj+1 < optj. 3.2.2 Running Time The running time of Algorithm 2 can be split into two parts. Algorithm 1 is guaranteed to halt in a polynomial number of iterations , and each iteration runs Algorithm 2. Virtually all well-performing models were trained in a reasonable amount of time (usually less than one hour). The main goal of our experiments is to evaluate whether directly optimizing MAP leads to improved MAP performance compared to conventional SVM methods that optimize a substitute loss such as accuracy or ROCArea. We compare our method against the best retrieval functions trained on (henceforth base functions), as well as against previously proposed SVM methods. For the first set, we generated three indices over the WT10g corpus using Indri5 . For both TREC 9 and TREC 10, we used the description portion of each query and scored the documents using five of Indri\"s built-in retrieval methods, which are Cosine Similarity, TFIDF, Okapi, Language Model with Dirichlet Prior, and Language Model with Jelinek-Mercer Prior. We computed the scores of these five retrieval methods over the three indices, giving 15 base functions in total. For our second set of base functions, we used scores from the TREC 9 and TREC 10 Web Track submissions. We used only the non-manual, non-short submissions from both years. b ca wT \u03c6(x,d) f(d|x) Figure 2: Example Feature Binning 4.2 Generating Features In order to generate input examples for our method, a concrete instantiation of \u03c6 must be provided. Since we are using linear kernels, one can think of the learning problem as finding a good piecewise-constant combination of the scores of the retrieval functions. Using the four choices of F, we generated four datasets for our main experiments. For each dataset in Table 5, we performed 50 trials. All queries were selected to be in the training, validation and test sets the same number of times. 5.1 Comparison with Base Functions In analyzing our results, the first question to answer is, can SVM\u2206 map learn a model which outperforms the best base TREC 9 TREC 10 Model MAP W/L MAP W/L SVM\u2206 map 0.290 - 0.287Best Func. Table 7 shows the comparison when trained on TREC submissions. Hence, we ran the same experiments using a modified dataset where the features computed using the best submission were removed. 5.2 Comparison w/ Previous SVM Methods The next question to answer is, does SVM\u2206 map produce higher MAP scores than previous SVM methods? 5.2.1 Alternate SVMacc Methods One issue which may cause SVMacc to underperform is the severe imbalance between relevant and non-relevant docTREC 9 TREC 10 Model MAP W/L MAP W/L SVM\u2206 map 0.242 - 0.236SVM\u2206 roc 0.237 29/21 0.234 24/26 SVMacc 0.147 47/3 ** 0.155 47/3 ** SVMacc2 0.219 39/11 ** 0.207 43/7 ** SVMacc3 0.113 49/1 ** 0.153 45/5 ** SVMacc4 0.155 48/2 ** 0.155 48/2 ** Table 9: Trained on Indri Functions TREC 9 TREC 10 Model MAP W/L MAP W/L SVM\u2206 map 0.290 - 0.287SVM\u2206 roc 0.282 29/21 0.278 35/15 ** SVMacc 0.213 49/1 ** 0.222 49/1 ** SVMacc2 0.270 34/16 ** 0.261 42/8 ** SVMacc3 0.133 50/0 ** 0.182 46/4 ** SVMacc4 0.233 47/3 ** 0.238 46/4 ** Table 10: Trained on TREC Submissions uments. It may be that different queries require different values of b. We took two approaches to address this issue. The second method, SVMacc4, normalizes the scores given by f for each query. Table 11 shows the performance of the models when trained on the TREC submissions with the best submission removed. The performance of most models degraded by a small amount, with SVM\u2206 map still having the best performance.", "body2": "However, most current approaches do not optimize for the evaluation measure most often used, namely Mean Average Precision (MAP). Performance measures optimized include accuracy , ROCArea or modifications of ROCArea , and NDCG . In fact, although some previous systems have obtained good MAP performance, it is known that neither achieving optimal accuracy nor ROCArea can guarantee optimal MAP performance. In this paper, we present a general approach for learning ranking functions that maximize MAP performance. In contrast to recent work directly optimizing for MAP performance by Metzler & Croft and Caruana et al. Like , our method learns a linear model, but is much more efficient in practice and, unlike , can handle many thousands of features. In order to quantify the quality of a prediction, \u02c6y = h(x), we will consider a loss function \u2206 : Y \u00d7 Y \u2192 . , n}, the performance of h on S can be measured by the empirical risk, R\u2206 S (h) = nX i=1 \u2206(yi, h(xi)). ,d|C|}. We further assume that all predicted rankings are complete rankings (no ties). While optimizing for these measures might achieve good MAP performance, we use two simple examples to show it can also be suboptimal in terms of MAP. In contrast, MAP assigns greater penalties to misorderings higher up in the predicted ranking. Using our notation, ROCArea can be defined as ROC(p, \u02c6p) = rel \u00b7 (|C| \u2212 rel) i:pi=1 j:pj =0 1[\u02c6pi>\u02c6pj ], where p is the true (weak) ranking, \u02c6p is the predicted ranking, and 1[b] is the indicator function conditioned on b. These two hypotheses predict a ranking for query x over a corpus of eight documents. Here, a learning method which optimizes for ROCArea would choose h2 since that results in a higher ROCArea score, but this yields a suboptimal MAP score. 2.2 MAP vs Accuracy Using a very similar example, we now demonstrate how optimizing for accuracy might result in suboptimal MAP. Instead, they learn a threshold such that documents scoring higher than the threshold can be classified as relevant and documents scoring lower as nonrelevant. Table 3 shows the predictions of the two hypotheses on a single query x. Hypothesis MAP Best Acc. A learning method which optimizes for accuracy would choose h2 since that results in a higher accuracy score, but this yields a suboptimal MAP score. Let Cx and C\u00afx denote the set of relevant and non-relevant documents of C for query x, respectively. Given a learned weight vector w, predicting a ranking (i.e. We will discuss later the choices of \u03c6 we used for our experiments. We now present a method based on structural SVMs to address this problem. Algorithm 1 Cutting plane algorithm for solving OP 1 within tolerance . , n do 5: H(y; w) \u2261 \u2206(yi, y) + wT \u03a8(xi, y) \u2212 wT \u03a8(xi, yi) 6: compute \u02c6y = argmaxy\u2208Y H(y; w) 7: compute \u03bei = max{0, maxy\u2208Wi H(y; w)} 8: if H(\u02c6y; w) > \u03bei + then 9: Wi \u2190 Wi \u222a {\u02c6y} 10: w \u2190 optimize (3) over W = i Wi 11: end if 12: end for 13: until no Wi has changed during iteration parameter that controls this tradeoff and can be tuned to achieve good performance in different training tasks. Therefore, the sum of slacks, \u03bei, upper bounds the MAP loss. This is stated formally in Proposition 1. Let \u03be\u2217 (w) be the optimal solution of the slack variables for OP 1 for a given weight vector w. Then Pn i=1 \u03bei is an upper bound on the empirical risk R\u2206 S (w). Unfortunately there is a problem: a constraint is required for every possible wrong output y, and the number of possible wrong outputs is exponential in the size of C. Fortunately, we may employ Algorithm 1 to solve OP 1. The algorithm starts with no constraints, and iteratively finds for each example (xi, yi) the output \u02c6y associated with the most violated constraint. If the corresponding constraint is violated by more than we introduce \u02c6y into the working set Wi of active constraints for example i, and re-solve (3) using the updated W. It can be shown that Algorithm 1\"s outer loop is guaranteed to halt within a polynomial number of iterations for any desired precision . Without the ability to efficiently find the most violated constraint (i.e., solve argmaxy\u2208Y H(y, w)), the constraint generation procedure is not tractable. For example, if documents da and db are both relevant, then swapping the positions of da and db in any ranking does not affect \u2206map. However, this reshu\ufb04ing will affect the discriminant score, wT \u03a8(x, y). This leads us to Observation 1. Observation 1 implies that in the ranking which maximizes H, the relevant documents will be sorted by wT \u03c6(x, d), and the non-relevant documents will also be sorted likewise. The second term is the change in the discriminant score, wT \u03a8(x, y), when yij changes from +1 to \u22121. . The difference in the value of H for these two rankings is exactly \u03b4j(i, i + 1). We can then reformulate the argmax H problem as argmax H = argmax o1,...,o|C\u00afx| |C\u00afx k=1 \u03b4k(ok, |Cx | + 1) (8) s.t. Then for each successive non-relevant document, the algorithm modifies the solution by sliding that document up the ranking to locally maximize H while keeping the positions of the other non-relevant documents constant. , |C\u00afx | do 6: optj \u2190 argmaxk \u03b4j(k, |Cx | + 1) 7: end for 8: encode \u02c6y according to (7) 9: return \u02c6y without considering the positions of the other non-relevant documents, and thus ignores the constraints of (9). For any 1 \u2264 i1 < i2 \u2264 |Cx | + 1 and 1 \u2264 j < |C\u00afx |, it must be the case that \u03b4j+1(i1, i2) \u2264 \u03b4j(i1, i2). , i2 \u2212 1. In other words, we will show that both j + 1 j + k + 1 j + k j + k j \u2212 1 j + k \u2212 1 (11) and \u22122 \u00b7 (sx k \u2212 s\u00afx j+1) \u2264 \u22122 \u00b7 (sx k \u2212 s\u00afx j ) (12) are true for the aforementioned values of j and k. It is easy to see that (11) is true by observing that for any two positive integers 1 \u2264 a < b, a + 1 b + 1 a \u2212 1 b \u2212 1 and choosing a = j and b = j + k. The second inequality (12) holds because Algorithm 2 first sorts d\u00afx in descending order of s\u00afx , implying s\u00afx j+1 \u2264 s\u00afx j . Thus we see that each term in \u03b4j+1 is no greater than the corresponding term in \u03b4j, which completes the proof. In Algorithm 2, the computed values of optj satisfy (10), implying that the solution returned by Algorithm 2 is feasible and thus optimal. We will prove that optj \u2264 optj+1 holds for any 1 \u2264 j < |C\u00afx |, thus implying (10). Since Algorithm 2 computes optj as optj = argmax \u03b4j(k, |Cx | + 1), (13) then by definition of \u03b4j (6), for any 1 \u2264 i < optj, \u03b4j(i, optj) = \u03b4j(i, |Cx | + 1) \u2212 \u03b4j(optj, |Cx | + 1) < 0. Using Lemma 1, we know that \u03b4j+1(i, optj) \u2264 \u03b4j(i, optj) < 0, which implies that for any 1 \u2264 i < optj, \u03b4j+1(i, |Cx | + 1) \u2212 \u03b4j+1(optj, |Cx | + 1) < 0. Therefore, it must be the case that optj \u2264 optj+1, which completes the proof. For most real-world datasets, Algorithm 2 is dominated by the sort and has complexity O(n log n). Algorithm 1 is guaranteed to halt in a polynomial number of iterations , and each iteration runs Algorithm 2. Once training is complete, making predictions on query x using the resulting hypothesis h(x|w) requires only sorting by wT \u03c6(x, d). As such, our . TREC 10 Submissions 18 900 Table 5: Dataset Statistics experiments essentially test our method\"s ability to re-rank the highly ranked documents (e.g., re-combine the scores of the retrieval functions) to improve MAP. 4.1 Choosing Retrieval Functions We chose two sets of base functions for our experiments. The first index was generated using default settings, the second used Porter-stemming, and the last used Porter-stemming and Indri\"s default stopwords. All parameters were kept as their defaults. For each query, documents of each base function. For our second set of base functions, we used scores from the TREC 9 and TREC 10 Web Track submissions. documents. From a high level, we are expressing the score of each retrieval function using |Kf | + 1 bins. For each F and each function f \u2208 F, we chose 50 values for Kf which are reasonably spaced and capture the sensitive region of f. This was simply an efficient means to normalize the outputs of different functions and allow for a more expressive model. The model which performed best on the validation set was selected and tested on the remaining 35 queries. We reported the average performance of all models over the 50 trials. Here, we find that SVM\u2206 map significantly outperforms the best base functions. As a result, SVM\u2206 map would not be able to learn a hypothesis which can significantly out-perform the best submission. Notice that while the performance of SVM\u2206 map degraded slightly, the performance was still comparable with that of the best submission. As such, we tried several approaches to improve the performance of SVMacc. Another possible issue is that SVMacc attempts to find just one discriminating threshold b that is query-invariant. Having the learning method trying to find a good b value (when one does not exist) may be detrimental. Tables 9, 10 and 11 show that the performance of SVMacc3 was also not competitive with SVM\u2206 map. However, Table 10 shows that SVM\u2206 map did significantly outperform SVM\u2206 roc when trained on the TREC submissions. Table 11 shows the performance of the models when trained on the TREC submissions with the best submission removed. The performance of most models degraded by a small amount, with SVM\u2206 map still having the best performance.", "introduction": "State of the art information retrieval systems commonly use machine learning techniques to learn ranking functions. However, most current approaches do not optimize for the evaluation measure most often used, namely Mean Average Precision (MAP). Instead, current algorithms tend to take one of two general approaches. The first approach is to learn a model that estimates the probability of a document being relevant given a query (e.g., ). If solved effectively, the ranking with best MAP performance can easily be derived from the probabilities of relevance. However, achieving high MAP only requires finding a good ordering of the documents. As a result, finding good probabilities requires solving a more difficult problem than necessary, likely requiring more training data to achieve the same MAP performance. The second common approach is to learn a function that maximizes a surrogate measure. Performance measures optimized include accuracy , ROCArea or modifications of ROCArea , and NDCG . Learning a model to optimize for such measures might result in suboptimal MAP performance. In fact, although some previous systems have obtained good MAP performance, it is known that neither achieving optimal accuracy nor ROCArea can guarantee optimal MAP performance. In this paper, we present a general approach for learning ranking functions that maximize MAP performance. Specifically, we present an SVM algorithm that globally optimizes a hinge-loss relaxation of MAP. This approach simplifies the process of obtaining ranking functions with high MAP performance by avoiding additional intermediate steps and heuristics. The new algorithm also makes it conceptually just as easy to optimize SVMs for MAP as was previously possible only for accuracy and ROCArea. In contrast to recent work directly optimizing for MAP performance by Metzler & Croft and Caruana et al. , our technique is computationally efficient while finding a globally optimal solution. Like , our method learns a linear model, but is much more efficient in practice and, unlike , can handle many thousands of features. We now describe the algorithm in detail and provide proof of correctness. Following this, we provide an analysis of running time. We finish with empirical results from experiments on the TREC 9 and TREC 10 Web Track corpus. We have also developed a software package implementing our algorithm that is available for public use1", "conclusion": "We have presented an SVM method that directly optimizes MAP.. It provides a principled approach and avoids difficult to control heuristics.. We formulated the optimization problem and presented an algorithm which provably finds the solution in polynomial time.. We have shown empirically that our method is generally superior to or competitive with conventional SVMs methods.. Our new method makes it conceptually just as easy to optimize SVMs for MAP as was previously possible only for Accuracy and ROCArea.. The computational cost for training is very reasonable in practice.. Since other methods typically require tuning multiple heuristics, we also expect to train fewer models before finding one which achieves good performance.. The learning framework used by our method is fairly general.. A natural extension of this framework would be to develop methods to optimize for other important IR measures, such as Normalized Discounted Cumulative Gain and Mean Reciprocal Rank.. ACKNOWLEDGMENTS This work was funded under NSF Award IIS- and a gift from Yahoo!. The third author was also partly supported by a Microsoft Research Fellowship."}
{"id": "J-62", "keywords": ["domin strategi", "mechan design", "strategyproof", "truth", "weak monoton"], "title": "Weak Monotonicity Suffices for Truthfulness on Convex Domains", "abstract": "Weak monotonicity is a simple necessary condition for a social choice function to be implementable by a truthful mechanism. Roberts [10] showed that it is sufficient for all social choice functions whose domain is unrestricted. Lavi, Mu'alem and Nisan [6] proved the sufficiency of weak monotonicity for functions over order-based domains and Gui, Muller and Vohra [5] proved sufficiency for order-based domains with range constraints and for domains defined by other special types of linear inequality constraints. Here we show the more general result, conjectured by Lavi, Mu'alem and Nisan [6], that weak monotonicity is sufficient for functions defined on any convex domain.", "references": ["Truthful mechanisms for one-parameter agents", "Incentive compatible multi unit combinatorial auctions", "Incentive compatibility in multi-unit auctions", "Competitive Auctions", "Dominant strategy mechanisms with multidimensional types", "Towards a characterization of truthful combinatorial auctions", "Truth revelation in approximately efficient combinatorial auctions", "Microeconomic Theory", "Algorithms for selfish agents", "The characterization of implementable choice rules", "A necessary and sufficient condition for rationalizability in a quasi-linear context", "Dominant strategy implementation with quasi-linear preferences"], "full_text": "1. INTRODUCTION Social choice theory centers around the general problem of selecting a single outcome out of a set A of alternative outcomes based on the individual preferences of a set P of players. A method for aggregating player preferences to select one outcome is called a social choice function. In this paper we assume that the range A is finite and that each player\"s preference is expressed by a valuation function which assigns to each possible outcome a real number representing the benefit the player derives from that outcome. The ensemble of player valuation functions is viewed as a valuation matrix with rows indexed by players and columns by outcomes. A major difficulty connected with social choice functions is that players can not be required to tell the truth about their preferences. Since each player seeks to maximize his own benefit, he may find it in his interest to misrepresent his valuation function. An important approach for dealing with this problem is to augment a given social choice function with a payment function, which assigns to each player a (positive or negative) payment as a function of all of the individual preferences. By carefully choosing the payment function, one can hope to entice each player to tell the truth. A social choice function augmented with a payment function is called a mechanism 1 and the mechanism is said to implement the social choice function. A mechanism is truthful (or to be strategyproof or to have a dominant strategy) if each player\"s best strategy, knowing the preferences of the others, is always to declare his own true preferences. A social choice function is truthfully implementable, or truthful if it has a truthful implementation. (The property of truthful implementability is sometimes called dominant strategy incentive compatibility). This framework leads naturally to the question: which social choice functions are truthful? This question is of the following general type: given a class of functions (here, social choice functions) and a property that holds for some of them (here, truthfulness), characterize the property. The definition of the property itself provides a characterization, so what more is needed? Here are some useful notions of characterization: \u2022 Recognition algorithm. Give an algorithm which, given an appropriate representation of a function in the class, determines whether the function has the property. \u2022 Parametric representation. Give an explicit parametrized family of functions and show that each function in the The usual definition of mechanism is more general than this (see Chapter 23.C or ); the mechanisms we consider here are usually called direct revelation mechanisms. 286 family has the property, and that every function with the property is in the family. A third notion applies in the case of hereditary properties of functions. A function g is a subfunction of function f, or f contains g, if g is obtained by restricting the domain of f. A property P of functions is hereditary if it is preserved under taking subfunctions. Truthfulness is easily seen to be hereditary. \u2022 Sets of obstructions. For a hereditary property P, a function g that does not have the property is an obstruction to the property in the sense that any function containing g doesn\"t have the property. An obstruction is minimal if every proper subfunction has the property. A set of obstructions is complete if every function that does not have the property contains one of them as a subfunction. The set of all functions that don\"t satisfy P is a complete (but trivial and uninteresting) set of obstructions; one seeks a set of small (ideally, minimal) obstructions. We are not aware of any work on recognition algorithms for the property of truthfulness, but there are significant results concerning parametric representations and obstruction characterizations of truthfulness. It turns out that the domain of the function, i.e., the set of allowed valuation matrices, is crucial. For functions with unrestricted domain, i.e., whose domain is the set of all real matrices, there are very good characterizations of truthfulness. For general domains, however, the picture is far from complete. Typically, the domains of social choice functions are specified by a system of constraints. For example, an order constraint requires that one specified entry in some row be larger than another in the same row, a range constraint places an upper or lower bound on an entry, and a zero constraint forces an entry to be 0. These are all examples of linear inequality constraints on the matrix entries. Building on work of Roberts , Lavi, Mu\"alem and Nisan defined a condition called weak monotonicity (WMON). (Independently, in the context of multi-unit auctions, Bikhchandani, Chatterji and Sen identified the same condition and called it nondecreasing in marginal utilities (NDMU).) The definition of W-MON can be formulated in terms of obstructions: for some specified simple set F of functions each having domains of size 2, a function satisfies W-MON if it contains no function from F. The functions in F are not truthful, and therefore W-MON is a necessary condition for truthfulness. Lavi, Mu\"alem and Nisan showed that W-MON is also sufficient for truthfulness for social choice functions whose domain is order-based, i.e., defined by order constraints and zero constraints, and Gui, Muller and Vohra extended this to other domains. The domain constraints considered in both papers are special cases of linear inequality constraints, and it is natural to ask whether W-MON is sufficient for any domain defined by such constraints. Lavi, Mu\"alem and Nisan conjectured that W-MON suffices for convex domains. The main result of this paper is an affirmative answer to this conjecture: Theorem 1. For any social choice function having convex domain and finite range, weak monotonicity is necessary and sufficient for truthfulness. Using the interpretation of weak monotonicity in terms of obstructions each having domain size 2, this provides a complete set of minimal obstructions for truthfulness within the class of social choice functions with convex domains. The two hypotheses on the social choice function, that the domain is convex and that the range is finite, can not be omitted as is shown by the examples given in section 7. 1.1 Related Work There is a simple and natural parametrized set of truthful social choice functions called affine maximizers. Roberts showed that for functions with unrestricted domain, every truthful function is an affine maximizer, thus providing a parametrized representation for truthful functions with unrestricted domain. There are many known examples of truthful functions over restricted domains that are not affine maximizers (see , , , and ). Each of these examples has a special structure and it seems plausible that there might be some mild restrictions on the class of all social choice functions such that all truthful functions obeying these restrictions are affine maximizers. Lavi, Mu\"alem and Nisan obtained a result in this direction by showing that for order-based domains, under certain technical assumptions, every truthful social choice function is almost an affine maximizer. There are a number of results about truthfulness that can be viewed as providing obstruction characterizations, although the notion of obstruction is not explicitly discussed. For a player i, a set of valuation matrices is said to be i-local if all of the matrices in the set are identical except for row i. Call a social choice function i-local if its domain is ilocal and call it local if it is i-local for some i. The following easily proved fact is used extensively in the literature: Proposition 2. The social choice function f is truthful if and only if every local subfunction of f is truthful. This implies that the set of all local non-truthful functions comprises a complete set of obstructions for truthfulness. This set is much smaller than the set of all non-truthful functions, but is still far from a minimal set of obstructions. Rochet , Rozenshtrom and Gui, Muller and Vohra identified a necessary and sufficient condition for truthfulness (see lemma 3 below) called the nonnegative cycle property. This condition can be viewed as providing a minimal complete set of non-truthful functions. As is required by proposition 2, each function in the set is local. Furthermore it is one-to-one. In particular its domain has size at most the number of possible outcomes |A|. As this complete set of obstructions consists of minimal non-truthful functions, this provides the optimal obstruction characterization of non-truthful functions within the class of all social choice functions. But by restricting attention to interesting subclasses of social choice functions, one may hope to get simpler sets of obstructions for truthfulness within that class. The condition of weak monotonicity mentioned earlier can be defined by a set of obstructions, each of which is a local function of domain size exactly 2. Thus the results of Lavi, Mu\"alem and Nisan , and of Gui, Muller and Vohra give a very simple set of obstructions for truthfulness within certain subclasses of social choice functions. Theorem 1 extends these results to a much larger subclass of functions. 287 1.2 Weak Monotonicity and the Nonnegative Cycle Property By proposition 2, a function is truthful if and only if each of its local subfunctions is truthful. Therefore, to get a set of obstructions for truthfulness, it suffices to obtain such a set for local functions. The domain of an i-local function consists of matrices that are fixed on all rows but row i. Fix such a function f and let D \u2286 RA be the set of allowed choices for row i. Since f depends only on row i and row i is chosen from D, we can view f as a function from D to A. Therefore, f is a social choice function having one player; we refer to such a function as a single player function. Associated to any single player function f with domain D we define an edge-weighted directed graph Hf whose vertex set is the image of f. For convenience, we assume that f is surjective and so this image is A. For each a, b \u2208 A, x \u2208 f\u22121 (a) there is an edge ex(a, b) from a to b with weight x(a) \u2212 x(b). The weight of a set of edges is just the sum of the weights of the edges. We say that f satisfies: \u2022 the nonnegative cycle property if every directed cycle has nonnegative weight. \u2022 the nonnegative two-cycle property if every directed cycle between two vertices has nonnegative weight. We say a local function g satisfies nonnegative cycle property/nonnegative two-cycle property if its associated single player function f does. The graph Hf has a possibly infinite number of edges between any two vertices. We define Gf to be the edgeweighted directed graph with exactly one edge from a to b, whose weight \u03b4ab is the infimum (possibly \u2212\u221e) of all of the edge weights ex(a, b) for x \u2208 f\u22121 (a). It is easy to see that Hf has the nonnegative cycle property/nonnegative two-cycle property if and only if Gf does. Gf is called the outcome graph of f. The weak monotonicity property mentioned earlier can be defined for arbitrary social choice functions by the condition that every local subfunction satisfies the nonnegative two-cycle property. The following result was obtained by Rochet in a slightly different form and rediscovered by Rozenshtrom and Gui, Muller and Vohra : Lemma 3. A local social choice function is truthful if and only if it has the nonnegative cycle property. Thus a social choice function is truthful if and only if every local subfunction satisfies the nonnegative cycle property. In light of this, theorem 1 follows from: Theorem 4. For any surjective single player function f : D \u2212\u2192 A where D is a convex subset of RA and A is finite, the nonnegative two-cycle property implies the nonnegative cycle property. This is the result we will prove. 1.3 Overview of the Proof of Theorem 4 Let D \u2286 RA be convex and let f : D \u2212\u2192 A be a single player function such that Gf has no negative two-cycles. We want to conclude that Gf has no negative cycles. For two vertices a, b, let \u03b4\u2217 ab denote the minimum weight of any path from a to b. Clearly \u03b4\u2217 ab \u2264 \u03b4ab. Our proof shows that the \u03b4\u2217 -weight of every cycle is exactly 0, from which theorem 4 follows. There seems to be no direct way to compute \u03b4\u2217 and so we proceed indirectly. Based on geometric considerations, we identify a subset of paths in Gf called admissible paths and a subset of admissible paths called straight paths. We prove that for any two outcomes a, b, there is a straight path from a to b (lemma 8 and corollary 10), and all straight paths from a to b have the same weight, which we denote \u03c1ab (theorem 12). We show that \u03c1ab \u2264 \u03b4ab (lemma 14) and that the \u03c1-weight of every cycle is 0. The key step to this proof is showing that the \u03c1-weight of every directed triangle is 0 (lemma 17). It turns out that \u03c1 is equal to \u03b4\u2217 (corollary 20), although this equality is not needed in the proof of theorem 4. To expand on the above summary, we give the definitions of an admissible path and a straight path. These are somewhat technical and rely on the geometry of f. We first observe that, without loss of generality, we can assume that D is (topologically) closed (section 2). In section 3, for each a \u2208 A, we enlarge the set f\u22121 (a) to a closed convex set Da \u2286 D in such a way that for a, b \u2208 A with a = b, Da and Db have disjoint interiors. We define an admissible path to be a sequence of outcomes (a1, . . . , ak) such that each of the sets Ij = Daj \u2229 Daj+1 is nonempty (section 4). An admissible path is straight if there is a straight line that meets one point from each of the sets I1, . . . , Ik\u22121 in order (section 5). Finally, we mention how the hypotheses of convex domain and finite range are used in the proof. Both hypotheses are needed to show: (1) the existence of a straight path from a to b for all a, b (lemma 8). (2) that the \u03c1-weight of a directed triangle is 0 (lemma 17). The convex domain hypothesis is also needed for the convexity of the sets Da (section 3). The finite range hypothesis is also needed to reduce theorem 4 to the case that D is closed (section 2) and to prove that every straight path from a to b has the same \u03b4-weight (theorem 12). 2. REDUCTION TO CLOSED DOMAIN We first reduce the theorem to the case that D is closed. Write DC for the closure of D. Since A is finite, DC \u222aa\u2208A(f\u22121 (a))C . Thus for each v \u2208 DC \u2212 D, there is an a = a(v) \u2208 A such that v \u2208 (f\u22121 (a))C . Extend f to the function g on DC by defining g(v) = a(v) for v \u2208 DC D and g(v) = f(v) for v \u2208 D. It is easy to check that \u03b4ab(g) = \u03b4ab(f) for all a, b \u2208 A and therefore it suffices to show that the nonnegative two-cycle property for g implies the nonnegative cycle property for g. Henceforth we assume D is convex and closed. 3. A DISSECTION OF THE DOMAIN In this section, we construct a family of closed convex sets {Da : a \u2208 A} with disjoint interiors whose union is D and satisfying f\u22121 (a) \u2286 Da for each a \u2208 A. Let Ra = {v : \u2200b \u2208 A, v(a) \u2212 v(b) \u2265 \u03b4ab}. Ra is a closed polyhedron containing f\u22121 (a). The next proposition implies that any two of these polyhedra intersect only on their boundary. Proposition 5. Let a, b \u2208 A. If v \u2208 Ra \u2229Rb then v(a)\u2212 v(b) = \u03b4ab = \u2212\u03b4ba. 288 Da Db Dc Dd De Figure 1: A 2-dimensional domain with 5 outcomes. Proof. v \u2208 Ra implies v(a) \u2212 v(b) \u2265 \u03b4ab and v \u2208 Rb implies v(b)\u2212v(a) \u2265 \u03b4ba which, by the nonnegative two-cycle property, implies v(a) \u2212 v(b) \u2264 \u03b4ab. Thus v(a) \u2212 v(b) = \u03b4ab and by symmetry v(b) \u2212 v(a) = \u03b4ba. Finally, we restrict the collection of sets {Ra : a \u2208 A} to the domain D by defining Da = Ra \u2229 D for each a \u2208 A. Clearly, Da is closed and convex, and contains f\u22121 (a). Therefore a\u2208A Da = D. Also, by proposition 5, any point v in Da \u2229 Db satisfies v(a) \u2212 v(b) = \u03b4ab = \u2212\u03b4ba. 4. PATHS AND D-SEQUENCES A path of size k is a sequence \u2212\u2192a = (a1, . . . , ak) with each ai \u2208 A (possibly with repetition). We call \u2212\u2192a an (a1, ak)path. For a path \u2212\u2192a , we write |\u2212\u2192a | for the size of \u2212\u2192a . \u2212\u2192a is simple if the ai\"s are distinct. For b, c \u2208 A we write Pbc for the set of (b, c)-paths and SPbc for the set of simple (b, c)-paths. The \u03b4-weight of path \u2212\u2192a is defined by \u03b4(\u2212\u2192a ) = k\u22121X i=1 \u03b4aiai+1 . A D-sequence of order k is a sequence \u2212\u2192u = (u0, . . . , uk) with each ui \u2208 D (possibly with repetition). We call \u2212\u2192u a (u0, uk)-sequence. For a D-sequence \u2212\u2192u , we write ord(u) for the order of \u2212\u2192u . For v, w \u2208 D we write Svw for the set of (v, w)-sequences. A compatible pair is a pair (\u2212\u2192a , \u2212\u2192u ) where \u2212\u2192a is a path and \u2212\u2192u is a D-sequence satisfying ord(\u2212\u2192u ) = |\u2212\u2192a | and for each i \u2208 [k], both ui\u22121 and ui belong to Dai . We write C(\u2212\u2192a ) for the set of D-sequences \u2212\u2192u that are compatible with \u2212\u2192a . We say that \u2212\u2192a is admissible if C(\u2212\u2192a ) is nonempty. For \u2212\u2192u \u2208 C(\u2212\u2192a ) we define \u2206\u2212\u2192a (\u2212\u2192u ) = |\u2212\u2192a |\u22121 i=1 (ui(ai) \u2212 ui(ai+1)). For v, w \u2208 D and b, c \u2208 A, we define Cvw bc to be the set of compatible pairs (\u2212\u2192a , \u2212\u2192u ) such that \u2212\u2192a \u2208 Pbc and \u2212\u2192u \u2208 Svw To illustrate these definitions, figure 1 gives the dissection of a domain, a 2-dimensional plane, into five regions Da, Db, Dc, Dd, De. D-sequence (v, w, x, y, z) is compatible with both path (a, b, c, e) and path (a, b, d, e); D-sequence (v, w, u, y, z) is compatible with a unique path (a, b, d, e). D-sequence (x, w, p, y, z) is compatible with a unique path (b, a, d, e). Hence (a, b, c, e), (a, b, d, e) and (b, a, d, e) are admissible paths. However, path (a, c, d) or path (b, e) is not admissible. Proposition 6. For any compatible pair (\u2212\u2192a , \u2212\u2192u ), \u2206\u2212\u2192a (\u2212\u2192u ) = \u03b4(\u2212\u2192a ). Proof. Let k = ord(\u2212\u2192u ) = |\u2212\u2192a |. By the definition of a compatible pair, ui \u2208 Dai \u2229 Dai+1 for i \u2208 [k \u2212 1]. ui(ai) \u2212 ui(ai+1) = \u03b4aiai+1 from proposition 5. Therefore, \u2206\u2212\u2192a (\u2212\u2192u ) = k\u22121X i=1 (ui(ai) \u2212 ui(ai+1)) = k\u22121X i=1 \u03b4aiai+1 = \u03b4(\u2212\u2192a ). Lemma 7. Let b, c \u2208 A and let \u2212\u2192a , \u2212\u2192a \u2208 Pbc. If C(\u2212\u2192a ) \u2229 C(\u2212\u2192a ) = \u2205 then \u03b4(\u2212\u2192a ) = \u03b4(\u2212\u2192a ). Proof. Let \u2212\u2192u be a D-sequence in C(\u2212\u2192a ) \u2229 C(\u2212\u2192a ). By proposition 6, \u03b4(\u2212\u2192a ) = \u2206\u2212\u2192a (\u2212\u2192u ) and \u03b4(\u2212\u2192a ) = \u2206\u2212\u2192a (\u2212\u2192u ), it suffices to show \u2206\u2212\u2192a (\u2212\u2192u ) = \u2206\u2212\u2192a (\u2212\u2192u ). Let k = ord(\u2212\u2192u ) = |\u2212\u2192a | = |\u2212\u2192a |. Since \u2206\u2212\u2192a (\u2212\u2192u ) = k\u22121X i=1 (ui(ai) \u2212 ui(ai+1)) = u1(a1) + k\u22121X i=2 (ui(ai) \u2212 ui\u22121(ai)) \u2212 uk\u22121(ak) = u1(b) + k\u22121X i=2 (ui(ai) \u2212 ui\u22121(ai)) \u2212 uk\u22121(c), \u2206\u2212\u2192a (\u2212\u2192u ) \u2212 \u2206\u2212\u2192a (\u2212\u2192u ) k\u22121X i=2 ((ui(ai) \u2212 ui\u22121(ai)) \u2212 (ui(ai) \u2212 ui\u22121(ai))) k\u22121X i=2 ((ui(ai) \u2212 ui(ai)) \u2212 (ui\u22121(ai) \u2212 ui\u22121(ai))). Noticing both ui\u22121 and ui belong to Dai \u2229 Dai , we have by proposition 5 ui\u22121(ai) \u2212 ui\u22121(ai) = \u03b4aiai = ui(ai) \u2212 ui(ai). Hence \u2206\u2212\u2192a (\u2212\u2192u ) \u2212 \u2206\u2212\u2192a (\u2212\u2192u ) = 0. 5. LINEAR D-SEQUENCES AND STRAIGHT PATHS For v, w \u2208 D we write vw for the (closed) line segment joining v and w. A D-sequence \u2212\u2192u of order k is linear provided that there is a sequence of real numbers 0 = \u03bb0 \u2264 \u03bb1 \u2264 . . . \u2264 \u03bbk = 1 such that ui = (1 \u2212 \u03bbi)u0 + \u03bbiuk. In particular, each ui belongs to u0uk. For v, w \u2208 D we write Lvw for the set of linear (v, w)-sequences. For b, c \u2208 A and v, w \u2208 D we write LCvw bc for the set of compatible pairs (\u2212\u2192a , \u2212\u2192u ) such that \u2212\u2192a \u2208 Pbc and \u2212\u2192u \u2208 Lvw For a path \u2212\u2192a , we write L(\u2212\u2192a ) for the set of linear sequences compatible with \u2212\u2192a . We say that \u2212\u2192a is straight if L(\u2212\u2192a ) = \u2205. For example, in figure 1, D-sequence (v, w, x, y, z) is linear while (v, w, u, y, z), (x, w, p, y, z), and (x, v, w, y, z) are not. Hence path (a, b, c, e) and (a, b, d, e) are both straight. However, path (b, a, d, e) is not straight. 289 Lemma 8. Let b, c \u2208 A and v \u2208 Db, w \u2208 Dc. There is a simple path \u2212\u2192a and D-sequence \u2212\u2192u such that (\u2212\u2192a , \u2212\u2192u ) \u2208 LCvw bc . Furthermore, for any such path \u2212\u2192a , \u03b4(\u2212\u2192a ) \u2264 v(b) \u2212 v(c). Proof. By the convexity of D, any sequence of points on vw is a D-sequence. If b = c, singleton path \u2212\u2192a = (b) and D-sequence \u2212\u2192u = (v, w) are obviously compatible. \u03b4(\u2212\u2192a ) = 0 = v(b) \u2212 v(c). So assume b = c. If Db \u2229Dc \u2229vw = \u2205, we pick an arbitrary x from this set and let \u2212\u2192a = (b, c) \u2208 SPbc, \u2212\u2192u = (v, x, w) \u2208 Lvw . Again it is easy to check the compatibility of (\u2212\u2192a , \u2212\u2192u ). Since v \u2208 Db, v(b) \u2212 v(c) \u2265 \u03b4bc = \u03b4(\u2212\u2192a ). For the remaining case b = c and Db \u2229Dc \u2229vw = \u2205, notice v = w otherwise v = w \u2208 Db \u2229 Dc \u2229 vw. So we can define \u03bbx for every point x on vw to be the unique number in such that x = (1 \u2212 \u03bbx)v + \u03bbxw. For convenience, we write x \u2264 y for \u03bbx \u2264 \u03bby. Let Ia = Da \u2229 vw for each a \u2208 A. Since D = \u222aa\u2208ADa, we have vw = \u222aa\u2208AIa. Moreover, by the convexity of Da and vw, Ia is a (possibly trivial) closed interval. We begin by considering the case that Ib and Ic are each a single point, that is, Ib = {v} and Ic = {w}. Let S be a minimal subset of A satisfying \u222as\u2208SIs = vw. For each s \u2208 S, Is is maximal, i.e., not contained in any other It, for t \u2208 S. In particular, the intervals {Is : s \u2208 S} have all left endpoints distinct and all right endpoints distinct and the order of the left endpoints is the same as that of the right endpoints. Let k = |S| + 2 and index S as a2, . . . , ak\u22121 in the order defined by the right endpoints. Denote the interval Iai by [li, ri]. Thus l2 < l3 < . . . < lk\u22121, r2 < r3 < . . . < rk\u22121 and the fact that these intervals cover vw implies l2 = v, rk\u22121 = w and for all 2 \u2264 i \u2264 k \u2212 2, li+1 \u2264 ri which further implies li < ri. Now we define the path \u2212\u2192a = (a1, a2, . . . , ak\u22121, ak) with a1 = b, ak = c and a2, a3, . . . , ak\u22121 as above. Define the linear D-sequence \u2212\u2192u = (u0, u1, . . . , uk) by u0 = u1 = v, uk = w and for 2 \u2264 i \u2264 k\u22121, ui = ri. It follows immediately that (\u2212\u2192a , \u2212\u2192u ) \u2208 LCvw bc . Neither b nor c is in S since lb = rb and lc = rc. Thus \u2212\u2192a is simple. Finally to show \u03b4(\u2212\u2192a ) \u2264 v(b) \u2212 v(c), we note v(b) \u2212 v(c) = v(a1) \u2212 v(ak) = k\u22121X i=1 (v(ai) \u2212 v(ai+1)) and \u03b4(\u2212\u2192a ) = \u2206\u2212\u2192a (\u2212\u2192u ) = k\u22121X i=1 (ui(ai) \u2212 ui(ai+1)) = v(a1) \u2212 v(a2) + k\u22121X i=2 (ri(ai) \u2212 ri(ai+1)). For two outcomes d, e \u2208 A, let us define fde(z) = z(d)\u2212z(e) for all z \u2208 D. It suffices to show faiai+1 (ri) \u2264 faiai+1 (v) for 2 \u2264 i \u2264 k \u2212 1. Fact 9. For d, e \u2208 A, fde(z) is a linear function of z. Furthermore, if x \u2208 Dd and y \u2208 De with x = y, then fde(x) = x(d) \u2212 x(e) \u2265 \u03b4de \u2265 \u2212\u03b4ed \u2265 \u2212(y(e) \u2212 y(d)) = fde(y). Therefore fde(z) is monotonically nonincreasing along the line \u2190\u2192 xy as z moves in the direction from x to y. Applying this fact with d = ai, e = ai+1, x = li and y = ri gives the desired conclusion. This completes the proof for the case that Ib = {v} and Ic = {w}. For general Ib, Ic, rb < lc otherwise Db \u2229 Dc \u2229 vw = Ib \u2229 Ic = \u2205. Let v = rb and w = lc. Clearly we can apply the above conclusion to v \u2208 Db, w \u2208 Dc and get a compatible pair (\u2212\u2192a , \u2212\u2192u ) \u2208 LCv w bc with \u2212\u2192a simple and \u03b4(\u2212\u2192a ) \u2264 v (b) \u2212 v (c). Define the linear D-sequence \u2212\u2192u by u0 = v, uk = w and ui = ui for i \u2208 [k \u2212 1]. (\u2212\u2192a , \u2212\u2192u ) \u2208 LCvw bc is evident. Moreover, applying the above fact with d = b, e = c, x = v and y = w, we get v(b) \u2212 v(c) \u2265 v (b) \u2212 v (c) \u2265 \u03b4(\u2212\u2192a ). Corollary 10. For any b, c \u2208 A there is a straight (b, c)path. The main result of this section (theorem 12) says that for any b, c \u2208 A, every straight (b, c)-path has the same \u03b4-weight. To prove this, we first fix v \u2208 Db and w \u2208 Dc and show (lemma 11) that every straight (b, c)-path compatible with some linear (v, w)-sequence has the same \u03b4-weight \u03c1bc(v, w). We then show in theorem 12 that \u03c1bc(v, w) is the same for all choices of v \u2208 Db and w \u2208 Dc. Lemma 11. For b, c \u2208 A, there is a function \u03c1bc : Db \u00d7 Dc \u2212\u2192 R satisfying that for any (\u2212\u2192a , \u2212\u2192u ) \u2208 LCvw bc , \u03b4(\u2212\u2192a ) = \u03c1bc(v, w). Proof. Let (\u2212\u2192a , \u2212\u2192u ), (\u2212\u2192a , \u2212\u2192u ) \u2208 LCvw bc . It suffices to show \u03b4(\u2212\u2192a ) = \u03b4(\u2212\u2192a ). To do this we construct a linear (v, w)-sequence \u2212\u2192u and paths \u2212\u2192a \u2217 , \u2212\u2192a \u2217\u2217 \u2208 Pbc, both compatible with \u2212\u2192u , satisfying \u03b4(\u2212\u2192a \u2217 ) = \u03b4(\u2212\u2192a ) and \u03b4(\u2212\u2192a \u2217\u2217 ) = \u03b4(\u2212\u2192a ). Lemma 7 implies \u03b4(\u2212\u2192a \u2217 ) = \u03b4(\u2212\u2192a \u2217\u2217 ), which will complete the proof. Let |\u2212\u2192a | = ord(\u2212\u2192u ) = k and |\u2212\u2192a | = ord(\u2212\u2192u ) = l. We select \u2212\u2192u to be any linear (v, w)-sequence (u0, u1, . . . , ut) such that \u2212\u2192u and \u2212\u2192u are both subsequences of \u2212\u2192u , i.e., there are indices 0 = i0 < i1 < \u00b7 \u00b7 \u00b7 < ik = t and 0 = j0 < j1 < \u00b7 \u00b7 \u00b7 < jl = t such that \u2212\u2192u = (ui0 , ui1 , . . . , uik ) and \u2212\u2192u = (uj0 , uj1 , . . . , ujl ). We now construct a (b, c)-path \u2212\u2192a \u2217 compatible with \u2212\u2192u such that \u03b4(\u2212\u2192a \u2217 ) = \u03b4(\u2212\u2192a ). (An analogous construction gives \u2212\u2192a \u2217\u2217 compatible with \u2212\u2192u such that \u03b4(\u2212\u2192a \u2217\u2217 ) = \u03b4(\u2212\u2192a ).) This will complete the proof. \u2212\u2192a \u2217 is defined as follows: for 1 \u2264 j \u2264 t, a\u2217 j = ar where r is the unique index satisfying ir\u22121 < j \u2264 ir. Since both uir\u22121 = ur\u22121 and uir = ur belong to Dar , uj \u2208 Dar for ir\u22121 \u2264 j \u2264 ir by the convexity of Dar . The compatibility of (\u2212\u2192a \u2217 , \u2212\u2192u ) follows immediately. Clearly, a\u2217 1 = a1 = b and a\u2217 t = ak = c, so \u2212\u2192a \u2217 \u2208 Pbc. Furthermore, as \u03b4a\u2217 j a\u2217 j+1 = \u03b4arar = 0 for each r \u2208 [k], ir\u22121 < j < ir, \u03b4(\u2212\u2192a \u2217 ) = k\u22121X r=1 \u03b4a\u2217 ir a\u2217 ir+1 k\u22121X r=1 \u03b4arar+1 = \u03b4(\u2212\u2192a ). We are now ready for the main theorem of the section: Theorem 12. \u03c1bc is a constant map on Db \u00d7 Dc. Thus for any b, c \u2208 A, every straight (b, c)-path has the same \u03b4weight. Proof. For a path \u2212\u2192a , (v, w) is compatible with \u2212\u2192a if there is a linear (v, w)-sequence compatible with \u2212\u2192a . We write CP(\u2212\u2192a ) for the set of pairs (v, w) compatible with \u2212\u2192a . \u03c1bc is constant on CP(\u2212\u2192a ) because for each (v, w) \u2208 CP(\u2212\u2192a ), \u03c1bc(v, w) = \u03b4(\u2212\u2192a ). By lemma 8, we also haveS \u2212\u2192a \u2208SPbc CP(\u2212\u2192a ) = Db \u00d7Dc. Since A is finite, SPbc, the set of simple paths from b to c, is finite as well. 290 Next we prove that for any path \u2212\u2192a , CP(\u2212\u2192a ) is closed. Let ((vn , wn ) : n \u2208 N) be a convergent sequence in CP(\u2212\u2192a ) and let (v, w) be the limit. We want to show that (v, w) \u2208 CP(\u2212\u2192a ). For each n \u2208 N, since (vn , wn ) \u2208 CP(\u2212\u2192a ), there is a linear (vn , wn )-sequence un compatible with \u2212\u2192a , i.e., there are 0 = \u03bbn 0 \u2264 \u03bbn 1 \u2264 . . . \u2264 \u03bbn k = 1 (k = |\u2212\u2192a |) such that un j = (1 \u2212 \u03bbn j )vn + \u03bbn j wn (j = 0, 1, . . . , k). Since for each n, \u03bbn = (\u03bbn 0 , \u03bbn 1 , . . . , \u03bbn k ) belongs to the closed bounded set k+1 we can choose an infinite subset I \u2286 N such that the sequence (\u03bbn : n \u2208 I) converges. Let \u03bb = (\u03bb0, \u03bb1, . . . , \u03bbk) be the limit. Clearly 0 = \u03bb0 \u2264 \u03bb1 \u2264 \u00b7 \u00b7 \u00b7 \u2264 \u03bbk = 1. Define the linear (v, w)-sequence \u2212\u2192u by uj = (1 \u2212 \u03bbj )v + \u03bbj w (j = 0, 1, . . . , k). Then for each j \u2208 {0, . . . , k}, uj is the limit of the sequence (un j : n \u2208 I). For j > 0, each un belongs to the closed set Daj , so uj \u2208 Daj . Similarly, for j < k each un j belongs to the closed set Daj+1 , so uj \u2208 Daj+1 . Hence (\u2212\u2192a , \u2212\u2192u ) is compatible, implying (v, w) \u2208 CP(\u2212\u2192a ). Now we have Db \u00d7 Dc covered by finitely many closed subsets on each of them \u03c1bc is a constant. Suppose for contradiction that there are (v, w), (v , w ) \u2208 Db \u00d7 Dc such that \u03c1bc(v, w) = \u03c1bc(v , w ). L = {((1 \u2212 \u03bb)v + \u03bbv , (1 \u2212 \u03bb)w + \u03bbw ) : \u03bb \u2208 } is a line segment in Db \u00d7Dc by the convexity of Db, Dc. Let L1 = {(x, y) \u2208 L : \u03c1bc(x, y) = \u03c1bc(v, w)} and L2 = L \u2212 L1. Clearly (v, w) \u2208 L1, (v , w ) \u2208 L2. Let P = {\u2212\u2192a \u2208 SPbc : \u03b4(\u2212\u2192a ) = \u03c1bc(v, w)}. L1 = `S \u2212\u2192a \u2208P CP(\u2212\u2192a ) \u2229 L, L2 = \u2212\u2192a \u2208SPbc\u2212P CP(\u2212\u2192a ) \u2229 L are closed by the finiteness of P. This is a contradiction, since it is well known (and easy to prove) that a line segment can not be expressed as the disjoint union of two nonempty closed sets. Summarizing corollary 10, lemma 11 and theorem 12, we have Corollary 13. For any b, c \u2208 A, there is a real number \u03c1bc with the property that (1) There is at least one straight (b, c)-path of \u03b4-weight \u03c1bc and (2) Every straight (b, c)-path has \u03b4-weight \u03c1bc. 6. PROOF OF THEOREM 4 Lemma 14. \u03c1bc \u2264 \u03b4bc for all b, c \u2208 A. Proof. For contradiction, suppose \u03c1bc \u2212 \u03b4bc = > 0. By the definition of \u03b4bc, there exists v \u2208 f\u22121 (b) \u2286 Db with v(b) \u2212 v(c) < \u03b4bc + = \u03c1bc. Pick an arbitrary w \u2208 Dc. By lemma 8, there is a compatible pair (\u2212\u2192a , \u2212\u2192u ) \u2208 LCvw bc with \u03b4(\u2212\u2192a ) \u2264 v(b) \u2212 v(c). Since \u2212\u2192a is a straight (b, c)-path, \u03c1bc = \u03b4(\u2212\u2192a ) \u2264 v(b) \u2212 v(c), leading to a contradiction. Define another edge-weighted complete directed graph Gf on vertex set A where the weight of arc (a, b) is \u03c1ab. Immediately from lemma 14, the weight of every directed cycle in Gf is bounded below by its weight in Gf . To prove theorem 4, it suffices to show the zero cycle property of Gf , i.e., every directed cycle has weight zero. We begin by considering two-cycles. Lemma 15. \u03c1bc + \u03c1cb = 0 for all b, c \u2208 A. Proof. Let \u2212\u2192a be a straight (b, c)-path compatible with linear sequence \u2212\u2192u . let \u2212\u2192a be the reverse of \u2212\u2192a and \u2212\u2192u the reverse of \u2212\u2192u . Obviously, (\u2212\u2192a , \u2212\u2192u ) is compatible as well and thus \u2212\u2192a is a straight (c, b)-path. Therefore, \u03c1bc + \u03c1cb = \u03b4(\u2212\u2192a ) + \u03b4(\u2212\u2192a ) = k\u22121X i=1 \u03b4aiai+1 + k\u22121X i=1 \u03b4ai+1ai k\u22121X i=1 (\u03b4aiai+1 + \u03b4ai+1ai ) = 0, where the final equality uses proposition 5. Next, for three cycles, we first consider those compatible with linear triples. Lemma 16. If there are collinear points u \u2208 Da, v \u2208 Db, w \u2208 Dc (a, b, c \u2208 A), \u03c1ab + \u03c1bc + \u03c1ca = 0. Proof. First, we prove for the case where v is between u and w. From lemma 8, there are compatible pairs (\u2212\u2192a , \u2212\u2192u ) \u2208 LCuv ab , (\u2212\u2192a , \u2212\u2192u ) \u2208 LCvw bc . Let |\u2212\u2192a | = ord(\u2212\u2192u ) = k and |\u2212\u2192a | = ord(\u2212\u2192u ) = l. We paste \u2212\u2192a and \u2212\u2192a together as \u2212\u2192a = (a = a1, a2, . . . , ak\u22121, ak, a1 , . . . , al = c), \u2212\u2192u and \u2212\u2192u as \u2212\u2192u = (u = u0, u1, . . . , uk = v = u0 , u1 , . . . , ul = w). Clearly (\u2212\u2192a , \u2212\u2192u ) \u2208 LCuw ac and \u03b4(\u2212\u2192a ) = k\u22121X i=1 \u03b4aiai+1 + \u03b4ak a1 l\u22121X i=1 \u03b4ai ai+1 = \u03b4(\u2212\u2192a ) + \u03b4bb + \u03b4(\u2212\u2192a ) = \u03b4(\u2212\u2192a ) + \u03b4(\u2212\u2192a ). Therefore, \u03c1ac = \u03b4(\u2212\u2192a ) = \u03b4(\u2212\u2192a ) + \u03b4(\u2212\u2192a ) = \u03c1ab + \u03c1bc. Moreover, \u03c1ac = \u2212\u03c1ca by lemma 15, so we get \u03c1ab + \u03c1bc + \u03c1ca = 0. Now suppose w is between u and v. By the above argument, we have \u03c1ac + \u03c1cb + \u03c1ba = 0 and by lemma 15, \u03c1ab + \u03c1bc + \u03c1ca = \u2212\u03c1ba \u2212 \u03c1cb \u2212 \u03c1ac = 0. The case that u is between v and w is similar. Now we are ready for the zero three-cycle property: Lemma 17. \u03c1ab + \u03c1bc + \u03c1ca = 0 for all a, b, c \u2208 A. Proof. Let S = {(a, b, c) : \u03c1ab + \u03c1bc + \u03c1ca = 0} and for contradiction, suppose S = \u2205. S is finite. For each a \u2208 A, choose va \u2208 Da arbitrarily and let T be the convex hull of {va : a \u2208 A}. For each (a, b, c) \u2208 S, let Rabc = Da \u00d7 Db \u00d7 Dc \u2229 T3 . Clearly, each Rabc is nonempty and compact. Moreover, by lemma 16, no (u, v, w) \u2208 Rabc is collinear. Define f : D3 \u2192 R by f(u, v, w) = |v\u2212u|+|w\u2212v|+|u\u2212w|. For (a, b, c) \u2208 S, the restriction of f to the compact set Rabc attains a minimum m(a, b, c) at some point (u, v, w) \u2208 Rabc by the continuity of f, i.e., there exists a triangle \u2206uvw of minimum perimeter within T with u \u2208 Da, v \u2208 Db, w \u2208 Dc. Choose (a\u2217 , b\u2217 , c\u2217 ) \u2208 S so that m(a\u2217 , b\u2217 , c\u2217 ) is minimum and let (u\u2217 , v\u2217 , w\u2217 ) \u2208 Ra\u2217b\u2217c\u2217 be a triple achieving it. Pick an arbitrary point p in the interior of \u2206u\u2217 v\u2217 w\u2217 . By the convexity of domain D, there is d \u2208 A such that p \u2208 Dd. 291 Consider triangles \u2206u\u2217 pw\u2217 , \u2206w\u2217 pv\u2217 and \u2206v\u2217 pu\u2217 . Since each of them has perimeter less than that of \u2206u\u2217 v\u2217 w\u2217 and all three triangles are contained in T, by the minimality of \u2206u\u2217 v\u2217 w\u2217 , (a\u2217 , d, c\u2217 ), (c\u2217 , d, b\u2217 ), (b\u2217 , d, a\u2217 ) \u2208 S. Thus \u03c1a\u2217d + \u03c1dc\u2217 + \u03c1c\u2217a\u2217 = 0, \u03c1c\u2217d + \u03c1db\u2217 + \u03c1b\u2217c\u2217 = 0, \u03c1b\u2217d + \u03c1da\u2217 + \u03c1a\u2217b\u2217 = 0. Summing up the three equalities, (\u03c1a\u2217d + \u03c1dc\u2217 + \u03c1c\u2217d + \u03c1db\u2217 + \u03c1b\u2217d + \u03c1da\u2217 ) +(\u03c1c\u2217a\u2217 + \u03c1b\u2217c\u2217 + \u03c1a\u2217b\u2217 ) = 0, which yields a contradiction \u03c1a\u2217b\u2217 + \u03c1b\u2217c\u2217 + \u03c1c\u2217a\u2217 = 0. With the zero two-cycle and three-cycle properties, the zero cycle property of Gf is immediate. As noted earlier, this completes the proof of theorem 4. Theorem 18. Every directed cycle of Gf has weight zero. Proof. Clearly, zero two-cycle and three-cycle properties imply triangle equality \u03c1ab +\u03c1bc = \u03c1ac for all a, b, c \u2208 A. For a directed cycle C = a1a2 . . . aka1, by inductively applying triangle equality, we have Pk\u22121 i=1 \u03c1aiai+1 = \u03c1a1ak . Therefore, the weight of C is k\u22121X i=1 \u03c1aiai+1 + \u03c1aka1 = \u03c1a1ak + \u03c1aka1 = 0. As final remarks, we note that our result implies the following strengthenings of theorem 12: Corollary 19. For any b, c \u2208 A, every admissible (b, c)path has the same \u03b4-weight \u03c1bc. Proof. First notice that for any b, c \u2208 A, if Db \u2229Dc = \u2205, \u03b4bc = \u03c1bc. To see this, pick v \u2208 Db \u2229 Dc arbitrarily. Obviously, path \u2212\u2192a = (b, c) is compatible with linear sequence \u2212\u2192u = (v, v, v) and is thus a straight (b, c)-path. Hence \u03c1bc = \u03b4(\u2212\u2192a ) = \u03b4bc. Now for any b, c \u2208 A and any (b, c)-path \u2212\u2192a with C(\u2212\u2192a ) = \u2205, let \u2212\u2192u \u2208 C(\u2212\u2192a ). Since ui \u2208 Dai \u2229 Dai+1 for i \u2208 [|\u2212\u2192a | \u2212 1], \u03b4(\u2212\u2192a ) = |\u2212\u2192a |\u22121 i=1 \u03b4aiai+1 = |\u2212\u2192a |\u22121 i=1 \u03c1aiai+1 , which by theorem 18, = \u2212\u03c1a|\u2212\u2192a |a1 = \u03c1a1a|\u2212\u2192a | = \u03c1bc. Corollary 20. For any b, c \u2208 A, \u03c1bc is equal to \u03b4\u2217 bc, the minimum \u03b4-weight over all (b, c)-paths. Proof. Clearly \u03c1bc \u2265 \u03b4\u2217 bc by corollary 13. On the other hand, for every (b, c)-path \u2212\u2192a = (b = a1, a2, . . . , ak = c), by lemma 14, \u03b4(\u2212\u2192a ) = k\u22121X i=1 \u03b4aiai+1 \u2265 k\u22121X i=1 \u03c1aiai+1 , which by theorem 18, = \u2212\u03c1aka1 = \u03c1a1ak = \u03c1bc. Hence \u03c1bc \u2264 \u03b4\u2217 bc, which completes the proof. 7. COUNTEREXAMPLES TO STRONGER FORMS OF THEOREM 4 Theorem 4 applies to social choice functions with convex domain and finite range. We now show that neither of these hypotheses can be omitted. Our examples are single player functions. The first example illustrates that convexity can not be omitted. We present an untruthful single player social choice function with three outcomes a, b, c satisfying W-MON on a path-connected but non-convex domain. The domain is the boundary of a triangle whose vertices are x = (0, 1, \u22121), y = (\u22121, 0, 1) and z = (1, \u22121, 0). x and the open line segment zx is assigned outcome a, y and the open line segment xy is assigned outcome b, and z and the open line segment yz is assigned outcome c. Clearly, \u03b4ab = \u2212\u03b4ba = \u03b4bc = \u2212\u03b4cb = \u03b4ca = \u2212\u03b4ac = \u22121, W-MON (the nonnegative twocycle property) holds. Since there is a negative cycle \u03b4ab + \u03b4bc + \u03b4ca = \u22123, by lemma 3, this is not a truthful choice function. We now show that the hypothesis of finite range can not be omitted. We construct a family of single player social choice functions each having a convex domain and an infinite number of outcomes, and satisfying weak monotonicity but not truthfulness. Our examples will be specified by a positive integer n and an n \u00d7 n matrix M satisfying the following properties: (1) M is non-singular. (2) M is positive semidefinite. (3) There are distinct i1, i2, . . . , ik \u2208 [n] satisfying k\u22121X j=1 (M(ij, ij) \u2212 M(ij , ij+1)) + (M(ik, ik) \u2212 M(ik, i1)) < 0. Here is an example matrix with n = 3 and (i1, i2, i3) = (1, 2, 3): 0 1 \u22121 \u22121 0 1 1 \u22121 0 Let e1, e2, . . . , en denote the standard basis of Rn . Let Sn denote the convex hull of {e1, e2 . . . , en}, which is the set of vectors in Rn with nonnegative coordinates that sum to 1. The range of our social choice function will be the set Sn and the domain D will be indexed by Sn, that is D = {y\u03bb : \u03bb \u2208 Sn}, where y\u03bb is defined below. The function f maps y\u03bb to \u03bb. Next we specify y\u03bb. By definition, D must be a set of functions from Sn to R. For \u03bb \u2208 Sn, the domain element y\u03bb : Sn \u2212\u2192 R is defined by y\u03bb(\u03b1) = \u03bbT M\u03b1. The nonsingularity of M guarantees that y\u03bb = y\u00b5 for \u03bb = \u00b5 \u2208 Sn. It is easy to see that D is a convex subset of the set of all functions from Sn to R. The outcome graph Gf is an infinite graph whose vertex set is the outcome set A = Sn. For outcomes \u03bb, \u00b5 \u2208 A, the edge weight \u03b4\u03bb\u00b5 is equal to \u03b4\u03bb\u00b5 = inf{v(\u03bb) \u2212 v(\u00b5) : f(v) = \u03bb} = y\u03bb(\u03bb) \u2212 y\u03bb(\u00b5) = \u03bbT M\u03bb \u2212 \u03bbT M\u00b5 = \u03bbT M(\u03bb \u2212 \u00b5). We claim that Gf satisfies the nonnegative two-cycle property (W-MON) but has a negative cycle (and hence is not truthful). For outcomes \u03bb, \u00b5 \u2208 A, \u03b4\u03bb\u00b5 +\u03b4\u00b5\u03bb = \u03bbT M(\u03bb\u2212\u00b5)+\u00b5T M(\u00b5\u2212\u03bb) = (\u03bb\u2212\u00b5)T M(\u03bb\u2212\u00b5), 292 which is nonnegative since M is positive semidefinite. Hence the nonnegative two-cycle property holds. Next we show that Gf has a negative cycle. Let i1, i2, . . . , ik be a sequence of indices satisfying property 3 of M. We claim ei1 ei2 . . . eik ei1 is a negative cycle. Since \u03b4eiej = eT i M(ei \u2212 ej) = M(i, i) \u2212 M(i, j) for any i, j \u2208 [k], the weight of the cycle k\u22121X j=1 \u03b4eij eij+1 + \u03b4eik ei1 k\u22121X j=1 (M(ij , ij ) \u2212 M(ij, ij+1)) + (M(ik, ik) \u2212 M(ik, i1)) < 0, which completes the proof. Finally, we point out that the third property imposed on the matrix M has the following interpretation. Let R(M) = {r1, r2, . . . , rn} be the set of row vectors of M and let hM be the single player social choice function with domain R(M) and range {1, 2, . . . , n} mapping ri to i. Property 3 is equivalent to the condition that the outcome graph GhM has a negative cycle. By lemma 3, this is equivalent to the condition that hM is untruthful. 8. FUTURE WORK As stated in the introduction, the goal underlying the work in this paper is to obtain useful and general characterizations of truthfulness. Let us say that a set D of P \u00d7 A real valuation matrices is a WM-domain if any social choice function on D satisfying weak monotonicity is truthful. In this paper, we showed that for finite A, any convex D is a WM-domain. Typically, the domains of social choice functions considered in mechanism design are convex, but there are interesting examples with non-convex domains, e.g., combinatorial auctions with unknown single-minded bidders. It is intriguing to find the most general conditions under which a set D of real matrices is a WM-domain. We believe that convexity is the main part of the story, i.e., a WM-domain is, after excluding some exceptional cases, essentially a convex set. Turning to parametric representations, let us say a set D of P \u00d7 A matrices is an AM-domain if any truthful social choice function with domain D is an affine maximizer. Roberts\" theorem says that the unrestricted domain is an AM-domain. What are the most general conditions under which a set D of real matrices is an AM-domain? Acknowledgments We thank Ron Lavi for helpful discussions and the two anonymous referees for helpful comments.", "body1": "Social choice theory centers around the general problem of selecting a single outcome out of a set A of alternative outcomes based on the individual preferences of a set P of players. A major difficulty connected with social choice functions is that players can not be required to tell the truth about their preferences. A social choice function augmented with a payment function is called a mechanism 1 and the mechanism is said to implement the social choice function. 286 family has the property, and that every function with the property is in the family. A third notion applies in the case of hereditary properties of functions. \u2022 Sets of obstructions. We are not aware of any work on recognition algorithms for the property of truthfulness, but there are significant results concerning parametric representations and obstruction characterizations of truthfulness. Building on work of Roberts , Lavi, Mu\"alem and Nisan defined a condition called weak monotonicity (WMON). The two hypotheses on the social choice function, that the domain is convex and that the range is finite, can not be omitted as is shown by the examples given in section 7. 1.1 Related Work There is a simple and natural parametrized set of truthful social choice functions called affine maximizers. For a player i, a set of valuation matrices is said to be i-local if all of the matrices in the set are identical except for row i. This set is much smaller than the set of all non-truthful functions, but is still far from a minimal set of obstructions. Rochet , Rozenshtrom and Gui, Muller and Vohra identified a necessary and sufficient condition for truthfulness (see lemma 3 below) called the nonnegative cycle property. As this complete set of obstructions consists of minimal non-truthful functions, this provides the optimal obstruction characterization of non-truthful functions within the class of all social choice functions. The condition of weak monotonicity mentioned earlier can be defined by a set of obstructions, each of which is a local function of domain size exactly 2. The domain of an i-local function consists of matrices that are fixed on all rows but row i. Associated to any single player function f with domain D we define an edge-weighted directed graph Hf whose vertex set is the image of f. For convenience, we assume that f is surjective and so this image is A. We say a local function g satisfies nonnegative cycle property/nonnegative two-cycle property if its associated single player function f does. The graph Hf has a possibly infinite number of edges between any two vertices. The weak monotonicity property mentioned earlier can be defined for arbitrary social choice functions by the condition that every local subfunction satisfies the nonnegative two-cycle property. This is the result we will prove. 1.3 Overview of the Proof of Theorem 4 Let D \u2286 RA be convex and let f : D \u2212\u2192 A be a single player function such that Gf has no negative two-cycles. There seems to be no direct way to compute \u03b4\u2217 and so we proceed indirectly. To expand on the above summary, we give the definitions of an admissible path and a straight path. Finally, we mention how the hypotheses of convex domain and finite range are used in the proof. We first reduce the theorem to the case that D is closed. Write DC for the closure of D. Since A is finite, DC \u222aa\u2208A(f\u22121 (a))C . In this section, we construct a family of closed convex sets {Da : a \u2208 A} with disjoint interiors whose union is D and satisfying f\u22121 (a) \u2286 Da for each a \u2208 A. Let Ra = {v : \u2200b \u2208 A, v(a) \u2212 v(b) \u2265 \u03b4ab}. Proposition 5. Proof. Therefore a\u2208A Da = D. Also, by proposition 5, any point v in Da \u2229 Db satisfies v(a) \u2212 v(b) = \u03b4ab = \u2212\u03b4ba. A path of size k is a sequence \u2212\u2192a = (a1, . A D-sequence of order k is a sequence \u2212\u2192u = (u0, . We write C(\u2212\u2192a ) for the set of D-sequences \u2212\u2192u that are compatible with \u2212\u2192a . D-sequence (x, w, p, y, z) is compatible with a unique path (b, a, d, e). Lemma 7. Noticing both ui\u22121 and ui belong to Dai \u2229 Dai , we have by proposition 5 ui\u22121(ai) \u2212 ui\u22121(ai) = \u03b4aiai = ui(ai) \u2212 ui(ai). Hence \u2206\u2212\u2192a (\u2212\u2192u ) \u2212 \u2206\u2212\u2192a (\u2212\u2192u ) = 0. PATHS For v, w \u2208 D we write vw for the (closed) line segment joining v and w. A D-sequence \u2212\u2192u of order k is linear provided that there is a sequence of real numbers 0 = \u03bb0 \u2264 \u03bb1 \u2264 . For example, in figure 1, D-sequence (v, w, x, y, z) is linear while (v, w, u, y, z), (x, w, p, y, z), and (x, v, w, y, z) are not. However, path (b, a, d, e) is not straight. 289 Lemma 8. If b = c, singleton path \u2212\u2192a = (b) and D-sequence \u2212\u2192u = (v, w) are obviously compatible. So assume b = c. If Db \u2229Dc \u2229vw = \u2205, we pick an arbitrary x from this set and let \u2212\u2192a = (b, c) \u2208 SPbc, \u2212\u2192u = (v, x, w) \u2208 Lvw . Since v \u2208 Db, v(b) \u2212 v(c) \u2265 \u03b4bc = \u03b4(\u2212\u2192a ). For the remaining case b = c and Db \u2229Dc \u2229vw = \u2205, notice v = w otherwise v = w \u2208 Db \u2229 Dc \u2229 vw. Let Ia = Da \u2229 vw for each a \u2208 A. Let S be a minimal subset of A satisfying \u222as\u2208SIs = vw. For each s \u2208 S, Is is maximal, i.e., not contained in any other It, for t \u2208 S. In particular, the intervals {Is : s \u2208 S} have all left endpoints distinct and all right endpoints distinct and the order of the left endpoints is the same as that of the right endpoints. Denote the interval Iai by [li, ri]. For two outcomes d, e \u2208 A, let us define fde(z) = z(d)\u2212z(e) for all z \u2208 D. It suffices to show faiai+1 (ri) \u2264 faiai+1 (v) for 2 \u2264 i \u2264 k \u2212 1. Fact 9. Furthermore, if x \u2208 Dd and y \u2208 De with x = y, then fde(x) = x(d) \u2212 x(e) \u2265 \u03b4de \u2265 \u2212\u03b4ed \u2265 \u2212(y(e) \u2212 y(d)) = fde(y). Applying this fact with d = ai, e = ai+1, x = li and y = ri gives the desired conclusion. For general Ib, Ic, rb < lc otherwise Db \u2229 Dc \u2229 vw = Ib \u2229 Ic = \u2205. Corollary 10. The main result of this section (theorem 12) says that for any b, c \u2208 A, every straight (b, c)-path has the same \u03b4-weight. To prove this, we first fix v \u2208 Db and w \u2208 Dc and show (lemma 11) that every straight (b, c)-path compatible with some linear (v, w)-sequence has the same \u03b4-weight \u03c1bc(v, w). We then show in theorem 12 that \u03c1bc(v, w) is the same for all choices of v \u2208 Db and w \u2208 Dc. Lemma 11. Let |\u2212\u2192a | = ord(\u2212\u2192u ) = k and |\u2212\u2192a | = ord(\u2212\u2192u ) = l. We select \u2212\u2192u to be any linear (v, w)-sequence (u0, u1, . \u2212\u2192a \u2217 is defined as follows: for 1 \u2264 j \u2264 t, a\u2217 j = ar where r is the unique index satisfying ir\u22121 < j \u2264 ir. Let ((vn , wn ) : n \u2208 N) be a convergent sequence in CP(\u2212\u2192a ) and let (v, w) be the limit. Define the linear (v, w)-sequence \u2212\u2192u by uj = (1 \u2212 \u03bbj )v + \u03bbj w (j = 0, 1, . Now we have Db \u00d7 Dc covered by finitely many closed subsets on each of them \u03c1bc is a constant. Suppose for contradiction that there are (v, w), (v , w ) \u2208 Db \u00d7 Dc such that \u03c1bc(v, w) = \u03c1bc(v , w ). L = {((1 \u2212 \u03bb)v + \u03bbv , (1 \u2212 \u03bb)w + \u03bbw ) : \u03bb \u2208 } is a line segment in Db \u00d7Dc by the convexity of Db, Dc. Summarizing corollary 10, lemma 11 and theorem 12, we have Corollary 13. Lemma 14. \u03c1bc \u2264 \u03b4bc for all b, c \u2208 A. By the definition of \u03b4bc, there exists v \u2208 f\u22121 (b) \u2286 Db with v(b) \u2212 v(c) < \u03b4bc + = \u03c1bc. By lemma 8, there is a compatible pair (\u2212\u2192a , \u2212\u2192u ) \u2208 LCvw bc with \u03b4(\u2212\u2192a ) \u2264 v(b) \u2212 v(c). Define another edge-weighted complete directed graph Gf on vertex set A where the weight of arc (a, b) is \u03c1ab. Immediately from lemma 14, the weight of every directed cycle in Gf is bounded below by its weight in Gf . Lemma 16. Therefore, \u03c1ac = \u03b4(\u2212\u2192a ) = \u03b4(\u2212\u2192a ) + \u03b4(\u2212\u2192a ) = \u03c1ab + \u03c1bc. Moreover, \u03c1ac = \u2212\u03c1ca by lemma 15, so we get \u03c1ab + \u03c1bc + \u03c1ca = 0. Now suppose w is between u and v. By the above argument, we have \u03c1ac + \u03c1cb + \u03c1ba = 0 and by lemma 15, \u03c1ab + \u03c1bc + \u03c1ca = \u2212\u03c1ba \u2212 \u03c1cb \u2212 \u03c1ac = 0. The case that u is between v and w is similar. Now we are ready for the zero three-cycle property: Lemma 17. \u03c1ab + \u03c1bc + \u03c1ca = 0 for all a, b, c \u2208 A. For (a, b, c) \u2208 S, the restriction of f to the compact set Rabc attains a minimum m(a, b, c) at some point (u, v, w) \u2208 Rabc by the continuity of f, i.e., there exists a triangle \u2206uvw of minimum perimeter within T with u \u2208 Da, v \u2208 Db, w \u2208 Dc. Choose (a\u2217 , b\u2217 , c\u2217 ) \u2208 S so that m(a\u2217 , b\u2217 , c\u2217 ) is minimum and let (u\u2217 , v\u2217 , w\u2217 ) \u2208 Ra\u2217b\u2217c\u2217 be a triple achieving it. Summing up the three equalities, (\u03c1a\u2217d + \u03c1dc\u2217 + \u03c1c\u2217d + \u03c1db\u2217 + \u03c1b\u2217d + \u03c1da\u2217 ) +(\u03c1c\u2217a\u2217 + \u03c1b\u2217c\u2217 + \u03c1a\u2217b\u2217 ) = 0, which yields a contradiction \u03c1a\u2217b\u2217 + \u03c1b\u2217c\u2217 + \u03c1c\u2217a\u2217 = 0. With the zero two-cycle and three-cycle properties, the zero cycle property of Gf is immediate. Theorem 18. Now for any b, c \u2208 A and any (b, c)-path \u2212\u2192a with C(\u2212\u2192a ) = \u2205, let \u2212\u2192u \u2208 C(\u2212\u2192a ). Corollary 20. Theorem 4 applies to social choice functions with convex domain and finite range. The first example illustrates that convexity can not be omitted. Our examples will be specified by a positive integer n and an n \u00d7 n matrix M satisfying the following properties: (1) M is non-singular. Here is an example matrix with n = 3 and (i1, i2, i3) = (1, 2, 3): 0 1 \u22121 \u22121 0 1 1 \u22121 0 Let e1, e2, . Next we specify y\u03bb. The outcome graph Gf is an infinite graph whose vertex set is the outcome set A = Sn. We claim that Gf satisfies the nonnegative two-cycle property (W-MON) but has a negative cycle (and hence is not truthful). For outcomes \u03bb, \u00b5 \u2208 A, \u03b4\u03bb\u00b5 +\u03b4\u00b5\u03bb = \u03bbT M(\u03bb\u2212\u00b5)+\u00b5T M(\u00b5\u2212\u03bb) = (\u03bb\u2212\u00b5)T M(\u03bb\u2212\u00b5), 292 which is nonnegative since M is positive semidefinite. Finally, we point out that the third property imposed on the matrix M has the following interpretation.", "body2": "The ensemble of player valuation functions is viewed as a valuation matrix with rows indexed by players and columns by outcomes. By carefully choosing the payment function, one can hope to entice each player to tell the truth. Give an explicit parametrized family of functions and show that each function in the The usual definition of mechanism is more general than this (see Chapter 23.C or ); the mechanisms we consider here are usually called direct revelation mechanisms. 286 family has the property, and that every function with the property is in the family. Truthfulness is easily seen to be hereditary. The set of all functions that don\"t satisfy P is a complete (but trivial and uninteresting) set of obstructions; one seeks a set of small (ideally, minimal) obstructions. These are all examples of linear inequality constraints on the matrix entries. Using the interpretation of weak monotonicity in terms of obstructions each having domain size 2, this provides a complete set of minimal obstructions for truthfulness within the class of social choice functions with convex domains. The two hypotheses on the social choice function, that the domain is convex and that the range is finite, can not be omitted as is shown by the examples given in section 7. There are a number of results about truthfulness that can be viewed as providing obstruction characterizations, although the notion of obstruction is not explicitly discussed. This implies that the set of all local non-truthful functions comprises a complete set of obstructions for truthfulness. This set is much smaller than the set of all non-truthful functions, but is still far from a minimal set of obstructions. In particular its domain has size at most the number of possible outcomes |A|. But by restricting attention to interesting subclasses of social choice functions, one may hope to get simpler sets of obstructions for truthfulness within that class. Therefore, to get a set of obstructions for truthfulness, it suffices to obtain such a set for local functions. Therefore, f is a social choice function having one player; we refer to such a function as a single player function. \u2022 the nonnegative two-cycle property if every directed cycle between two vertices has nonnegative weight. We say a local function g satisfies nonnegative cycle property/nonnegative two-cycle property if its associated single player function f does. Gf is called the outcome graph of f. For any surjective single player function f : D \u2212\u2192 A where D is a convex subset of RA and A is finite, the nonnegative two-cycle property implies the nonnegative cycle property. This is the result we will prove. Our proof shows that the \u03b4\u2217 -weight of every cycle is exactly 0, from which theorem 4 follows. It turns out that \u03c1 is equal to \u03b4\u2217 (corollary 20), although this equality is not needed in the proof of theorem 4. , Ik\u22121 in order (section 5). The finite range hypothesis is also needed to reduce theorem 4 to the case that D is closed (section 2) and to prove that every straight path from a to b has the same \u03b4-weight (theorem 12). We first reduce the theorem to the case that D is closed. Extend f to the function g on DC by defining g(v) = a(v) for v \u2208 DC D and g(v) = f(v) for v \u2208 D. It is easy to check that \u03b4ab(g) = \u03b4ab(f) for all a, b \u2208 A and therefore it suffices to show that the nonnegative two-cycle property for g implies the nonnegative cycle property for g. Henceforth we assume D is convex and closed. In this section, we construct a family of closed convex sets {Da : a \u2208 A} with disjoint interiors whose union is D and satisfying f\u22121 (a) \u2286 Da for each a \u2208 A. The next proposition implies that any two of these polyhedra intersect only on their boundary. 288 Da Db Dc Dd De Figure 1: A 2-dimensional domain with 5 outcomes. Clearly, Da is closed and convex, and contains f\u22121 (a). Therefore a\u2208A Da = D. Also, by proposition 5, any point v in Da \u2229 Db satisfies v(a) \u2212 v(b) = \u03b4ab = \u2212\u03b4ba. The \u03b4-weight of path \u2212\u2192a is defined by \u03b4(\u2212\u2192a ) = k\u22121X i=1 \u03b4aiai+1 . A compatible pair is a pair (\u2212\u2192a , \u2212\u2192u ) where \u2212\u2192a is a path and \u2212\u2192u is a D-sequence satisfying ord(\u2212\u2192u ) = |\u2212\u2192a | and for each i \u2208 [k], both ui\u22121 and ui belong to Dai . D-sequence (v, w, x, y, z) is compatible with both path (a, b, c, e) and path (a, b, d, e); D-sequence (v, w, u, y, z) is compatible with a unique path (a, b, d, e). For any compatible pair (\u2212\u2192a , \u2212\u2192u ), \u2206\u2212\u2192a (\u2212\u2192u ) = \u03b4(\u2212\u2192a ). Therefore, \u2206\u2212\u2192a (\u2212\u2192u ) = k\u22121X i=1 (ui(ai) \u2212 ui(ai+1)) = k\u22121X i=1 \u03b4aiai+1 = \u03b4(\u2212\u2192a ). If C(\u2212\u2192a ) \u2229 C(\u2212\u2192a ) = \u2205 then \u03b4(\u2212\u2192a ) = \u03b4(\u2212\u2192a ). Since \u2206\u2212\u2192a (\u2212\u2192u ) = k\u22121X i=1 (ui(ai) \u2212 ui(ai+1)) = u1(a1) + k\u22121X i=2 (ui(ai) \u2212 ui\u22121(ai)) \u2212 uk\u22121(ak) = u1(b) + k\u22121X i=2 (ui(ai) \u2212 ui\u22121(ai)) \u2212 uk\u22121(c), \u2206\u2212\u2192a (\u2212\u2192u ) \u2212 \u2206\u2212\u2192a (\u2212\u2192u ) k\u22121X i=2 ((ui(ai) \u2212 ui\u22121(ai)) \u2212 (ui(ai) \u2212 ui\u22121(ai))) k\u22121X i=2 ((ui(ai) \u2212 ui(ai)) \u2212 (ui\u22121(ai) \u2212 ui\u22121(ai))). Noticing both ui\u22121 and ui belong to Dai \u2229 Dai , we have by proposition 5 ui\u22121(ai) \u2212 ui\u22121(ai) = \u03b4aiai = ui(ai) \u2212 ui(ai). Hence \u2206\u2212\u2192a (\u2212\u2192u ) \u2212 \u2206\u2212\u2192a (\u2212\u2192u ) = 0. PATHS For v, w \u2208 D we write vw for the (closed) line segment joining v and w. We say that \u2212\u2192a is straight if L(\u2212\u2192a ) = \u2205. Hence path (a, b, c, e) and (a, b, d, e) are both straight. However, path (b, a, d, e) is not straight. By the convexity of D, any sequence of points on vw is a D-sequence. \u03b4(\u2212\u2192a ) = 0 = v(b) \u2212 v(c). Again it is easy to check the compatibility of (\u2212\u2192a , \u2212\u2192u ). Since v \u2208 Db, v(b) \u2212 v(c) \u2265 \u03b4bc = \u03b4(\u2212\u2192a ). For convenience, we write x \u2264 y for \u03bbx \u2264 \u03bby. We begin by considering the case that Ib and Ic are each a single point, that is, Ib = {v} and Ic = {w}. Let S be a minimal subset of A satisfying \u222as\u2208SIs = vw. , ak\u22121 in the order defined by the right endpoints. Finally to show \u03b4(\u2212\u2192a ) \u2264 v(b) \u2212 v(c), we note v(b) \u2212 v(c) = v(a1) \u2212 v(ak) = k\u22121X i=1 (v(ai) \u2212 v(ai+1)) and \u03b4(\u2212\u2192a ) = \u2206\u2212\u2192a (\u2212\u2192u ) = k\u22121X i=1 (ui(ai) \u2212 ui(ai+1)) = v(a1) \u2212 v(a2) + k\u22121X i=2 (ri(ai) \u2212 ri(ai+1)). For two outcomes d, e \u2208 A, let us define fde(z) = z(d)\u2212z(e) for all z \u2208 D. It suffices to show faiai+1 (ri) \u2264 faiai+1 (v) for 2 \u2264 i \u2264 k \u2212 1. For d, e \u2208 A, fde(z) is a linear function of z. Therefore fde(z) is monotonically nonincreasing along the line \u2190\u2192 xy as z moves in the direction from x to y. This completes the proof for the case that Ib = {v} and Ic = {w}. Moreover, applying the above fact with d = b, e = c, x = v and y = w, we get v(b) \u2212 v(c) \u2265 v (b) \u2212 v (c) \u2265 \u03b4(\u2212\u2192a ). For any b, c \u2208 A there is a straight (b, c)path. The main result of this section (theorem 12) says that for any b, c \u2208 A, every straight (b, c)-path has the same \u03b4-weight. To prove this, we first fix v \u2208 Db and w \u2208 Dc and show (lemma 11) that every straight (b, c)-path compatible with some linear (v, w)-sequence has the same \u03b4-weight \u03c1bc(v, w). We then show in theorem 12 that \u03c1bc(v, w) is the same for all choices of v \u2208 Db and w \u2208 Dc. For b, c \u2208 A, there is a function \u03c1bc : Db \u00d7 Dc \u2212\u2192 R satisfying that for any (\u2212\u2192a , \u2212\u2192u ) \u2208 LCvw bc , \u03b4(\u2212\u2192a ) = \u03c1bc(v, w). Lemma 7 implies \u03b4(\u2212\u2192a \u2217 ) = \u03b4(\u2212\u2192a \u2217\u2217 ), which will complete the proof. This will complete the proof. Thus for any b, c \u2208 A, every straight (b, c)-path has the same \u03b4weight. 290 Next we prove that for any path \u2212\u2192a , CP(\u2212\u2192a ) is closed. Clearly 0 = \u03bb0 \u2264 \u03bb1 \u2264 \u00b7 \u00b7 \u00b7 \u2264 \u03bbk = 1. Hence (\u2212\u2192a , \u2212\u2192u ) is compatible, implying (v, w) \u2208 CP(\u2212\u2192a ). Now we have Db \u00d7 Dc covered by finitely many closed subsets on each of them \u03c1bc is a constant. Suppose for contradiction that there are (v, w), (v , w ) \u2208 Db \u00d7 Dc such that \u03c1bc(v, w) = \u03c1bc(v , w ). L1 = `S \u2212\u2192a \u2208P CP(\u2212\u2192a ) \u2229 L, L2 = \u2212\u2192a \u2208SPbc\u2212P CP(\u2212\u2192a ) \u2229 L are closed by the finiteness of P. This is a contradiction, since it is well known (and easy to prove) that a line segment can not be expressed as the disjoint union of two nonempty closed sets. For any b, c \u2208 A, there is a real number \u03c1bc with the property that (1) There is at least one straight (b, c)-path of \u03b4-weight \u03c1bc and (2) Every straight (b, c)-path has \u03b4-weight \u03c1bc. For contradiction, suppose \u03c1bc \u2212 \u03b4bc = > 0. Pick an arbitrary w \u2208 Dc. Since \u2212\u2192a is a straight (b, c)-path, \u03c1bc = \u03b4(\u2212\u2192a ) \u2264 v(b) \u2212 v(c), leading to a contradiction. Define another edge-weighted complete directed graph Gf on vertex set A where the weight of arc (a, b) is \u03c1ab. Lemma 15. \u03c1bc + \u03c1cb = 0 for all b, c \u2208 A. Next, for three cycles, we first consider those compatible with linear triples. If there are collinear points u \u2208 Da, v \u2208 Db, w \u2208 Dc (a, b, c \u2208 A), \u03c1ab + \u03c1bc + \u03c1ca = 0. Clearly (\u2212\u2192a , \u2212\u2192u ) \u2208 LCuw ac and \u03b4(\u2212\u2192a ) = k\u22121X i=1 \u03b4aiai+1 + \u03b4ak a1 l\u22121X i=1 \u03b4ai ai+1 = \u03b4(\u2212\u2192a ) + \u03b4bb + \u03b4(\u2212\u2192a ) = \u03b4(\u2212\u2192a ) + \u03b4(\u2212\u2192a ). Therefore, \u03c1ac = \u03b4(\u2212\u2192a ) = \u03b4(\u2212\u2192a ) + \u03b4(\u2212\u2192a ) = \u03c1ab + \u03c1bc. Moreover, \u03c1ac = \u2212\u03c1ca by lemma 15, so we get \u03c1ab + \u03c1bc + \u03c1ca = 0. Now suppose w is between u and v. By the above argument, we have \u03c1ac + \u03c1cb + \u03c1ba = 0 and by lemma 15, \u03c1ab + \u03c1bc + \u03c1ca = \u2212\u03c1ba \u2212 \u03c1cb \u2212 \u03c1ac = 0. The case that u is between v and w is similar. Now we are ready for the zero three-cycle property: Lemma 17. \u03c1ab + \u03c1bc + \u03c1ca = 0 for all a, b, c \u2208 A. Define f : D3 \u2192 R by f(u, v, w) = |v\u2212u|+|w\u2212v|+|u\u2212w|. For (a, b, c) \u2208 S, the restriction of f to the compact set Rabc attains a minimum m(a, b, c) at some point (u, v, w) \u2208 Rabc by the continuity of f, i.e., there exists a triangle \u2206uvw of minimum perimeter within T with u \u2208 Da, v \u2208 Db, w \u2208 Dc. Since each of them has perimeter less than that of \u2206u\u2217 v\u2217 w\u2217 and all three triangles are contained in T, by the minimality of \u2206u\u2217 v\u2217 w\u2217 , (a\u2217 , d, c\u2217 ), (c\u2217 , d, b\u2217 ), (b\u2217 , d, a\u2217 ) \u2208 S. Thus \u03c1a\u2217d + \u03c1dc\u2217 + \u03c1c\u2217a\u2217 = 0, \u03c1c\u2217d + \u03c1db\u2217 + \u03c1b\u2217c\u2217 = 0, \u03c1b\u2217d + \u03c1da\u2217 + \u03c1a\u2217b\u2217 = 0. Summing up the three equalities, (\u03c1a\u2217d + \u03c1dc\u2217 + \u03c1c\u2217d + \u03c1db\u2217 + \u03c1b\u2217d + \u03c1da\u2217 ) +(\u03c1c\u2217a\u2217 + \u03c1b\u2217c\u2217 + \u03c1a\u2217b\u2217 ) = 0, which yields a contradiction \u03c1a\u2217b\u2217 + \u03c1b\u2217c\u2217 + \u03c1c\u2217a\u2217 = 0. As noted earlier, this completes the proof of theorem 4. Every directed cycle of Gf has weight zero. For any b, c \u2208 A, every admissible (b, c)path has the same \u03b4-weight \u03c1bc. Hence \u03c1bc = \u03b4(\u2212\u2192a ) = \u03b4bc. Since ui \u2208 Dai \u2229 Dai+1 for i \u2208 [|\u2212\u2192a | \u2212 1], \u03b4(\u2212\u2192a ) = |\u2212\u2192a |\u22121 i=1 \u03b4aiai+1 = |\u2212\u2192a |\u22121 i=1 \u03c1aiai+1 , which by theorem 18, = \u2212\u03c1a|\u2212\u2192a |a1 = \u03c1a1a|\u2212\u2192a | = \u03c1bc. For any b, c \u2208 A, \u03c1bc is equal to \u03b4\u2217 bc, the minimum \u03b4-weight over all (b, c)-paths. Hence \u03c1bc \u2264 \u03b4\u2217 bc, which completes the proof. Our examples are single player functions. We construct a family of single player social choice functions each having a convex domain and an infinite number of outcomes, and satisfying weak monotonicity but not truthfulness. , ik \u2208 [n] satisfying k\u22121X j=1 (M(ij, ij) \u2212 M(ij , ij+1)) + (M(ik, ik) \u2212 M(ik, i1)) < 0. The function f maps y\u03bb to \u03bb. It is easy to see that D is a convex subset of the set of all functions from Sn to R. For outcomes \u03bb, \u00b5 \u2208 A, the edge weight \u03b4\u03bb\u00b5 is equal to \u03b4\u03bb\u00b5 = inf{v(\u03bb) \u2212 v(\u00b5) : f(v) = \u03bb} = y\u03bb(\u03bb) \u2212 y\u03bb(\u00b5) = \u03bbT M\u03bb \u2212 \u03bbT M\u00b5 = \u03bbT M(\u03bb \u2212 \u00b5). We claim that Gf satisfies the nonnegative two-cycle property (W-MON) but has a negative cycle (and hence is not truthful). Since \u03b4eiej = eT i M(ei \u2212 ej) = M(i, i) \u2212 M(i, j) for any i, j \u2208 [k], the weight of the cycle k\u22121X j=1 \u03b4eij eij+1 + \u03b4eik ei1 k\u22121X j=1 (M(ij , ij ) \u2212 M(ij, ij+1)) + (M(ik, ik) \u2212 M(ik, i1)) < 0, which completes the proof. By lemma 3, this is equivalent to the condition that hM is untruthful.", "introduction": "Social choice theory centers around the general problem of selecting a single outcome out of a set A of alternative outcomes based on the individual preferences of a set P of players. A method for aggregating player preferences to select one outcome is called a social choice function. In this paper we assume that the range A is finite and that each player\"s preference is expressed by a valuation function which assigns to each possible outcome a real number representing the benefit the player derives from that outcome. The ensemble of player valuation functions is viewed as a valuation matrix with rows indexed by players and columns by outcomes. A major difficulty connected with social choice functions is that players can not be required to tell the truth about their preferences. Since each player seeks to maximize his own benefit, he may find it in his interest to misrepresent his valuation function. An important approach for dealing with this problem is to augment a given social choice function with a payment function, which assigns to each player a (positive or negative) payment as a function of all of the individual preferences. By carefully choosing the payment function, one can hope to entice each player to tell the truth. A social choice function augmented with a payment function is called a mechanism 1 and the mechanism is said to implement the social choice function. A mechanism is truthful (or to be strategyproof or to have a dominant strategy) if each player\"s best strategy, knowing the preferences of the others, is always to declare his own true preferences. A social choice function is truthfully implementable, or truthful if it has a truthful implementation. (The property of truthful implementability is sometimes called dominant strategy incentive compatibility). This framework leads naturally to the question: which social choice functions are truthful? This question is of the following general type: given a class of functions (here, social choice functions) and a property that holds for some of them (here, truthfulness), characterize the property. The definition of the property itself provides a characterization, so what more is needed? Here are some useful notions of characterization: \u2022 Recognition algorithm. Give an algorithm which, given an appropriate representation of a function in the class, determines whether the function has the property. Give an explicit parametrized family of functions and show that each function in the The usual definition of mechanism is more general than this (see Chapter 23.C or ); the mechanisms we consider here are usually called direct revelation mechanisms. 286 family has the property, and that every function with the property is in the family. A third notion applies in the case of hereditary properties of functions. A function g is a subfunction of function f, or f contains g, if g is obtained by restricting the domain of f. A property P of functions is hereditary if it is preserved under taking subfunctions. Truthfulness is easily seen to be hereditary. For a hereditary property P, a function g that does not have the property is an obstruction to the property in the sense that any function containing g doesn\"t have the property. An obstruction is minimal if every proper subfunction has the property. A set of obstructions is complete if every function that does not have the property contains one of them as a subfunction. The set of all functions that don\"t satisfy P is a complete (but trivial and uninteresting) set of obstructions; one seeks a set of small (ideally, minimal) obstructions. We are not aware of any work on recognition algorithms for the property of truthfulness, but there are significant results concerning parametric representations and obstruction characterizations of truthfulness. It turns out that the domain of the function, i.e., the set of allowed valuation matrices, is crucial. For functions with unrestricted domain, i.e., whose domain is the set of all real matrices, there are very good characterizations of truthfulness. For general domains, however, the picture is far from complete. Typically, the domains of social choice functions are specified by a system of constraints. For example, an order constraint requires that one specified entry in some row be larger than another in the same row, a range constraint places an upper or lower bound on an entry, and a zero constraint forces an entry to be 0. These are all examples of linear inequality constraints on the matrix entries. Building on work of Roberts , Lavi, Mu\"alem and Nisan defined a condition called weak monotonicity (WMON). (Independently, in the context of multi-unit auctions, Bikhchandani, Chatterji and Sen identified the same condition and called it nondecreasing in marginal utilities (NDMU).) The definition of W-MON can be formulated in terms of obstructions: for some specified simple set F of functions each having domains of size 2, a function satisfies W-MON if it contains no function from F. The functions in F are not truthful, and therefore W-MON is a necessary condition for truthfulness. Lavi, Mu\"alem and Nisan showed that W-MON is also sufficient for truthfulness for social choice functions whose domain is order-based, i.e., defined by order constraints and zero constraints, and Gui, Muller and Vohra extended this to other domains. The domain constraints considered in both papers are special cases of linear inequality constraints, and it is natural to ask whether W-MON is sufficient for any domain defined by such constraints. Lavi, Mu\"alem and Nisan conjectured that W-MON suffices for convex domains. The main result of this paper is an affirmative answer to this conjecture: Theorem 1. For any social choice function having convex domain and finite range, weak monotonicity is necessary and sufficient for truthfulness. Using the interpretation of weak monotonicity in terms of obstructions each having domain size 2, this provides a complete set of minimal obstructions for truthfulness within the class of social choice functions with convex domains. The two hypotheses on the social choice function, that the domain is convex and that the range is finite, can not be omitted as is shown by the examples given in section 7. 1.1 Related Work There is a simple and natural parametrized set of truthful social choice functions called affine maximizers. Roberts showed that for functions with unrestricted domain, every truthful function is an affine maximizer, thus providing a parametrized representation for truthful functions with unrestricted domain. There are many known examples of truthful functions over restricted domains that are not affine maximizers (see , , , and ). Each of these examples has a special structure and it seems plausible that there might be some mild restrictions on the class of all social choice functions such that all truthful functions obeying these restrictions are affine maximizers. Lavi, Mu\"alem and Nisan obtained a result in this direction by showing that for order-based domains, under certain technical assumptions, every truthful social choice function is almost an affine maximizer. There are a number of results about truthfulness that can be viewed as providing obstruction characterizations, although the notion of obstruction is not explicitly discussed. For a player i, a set of valuation matrices is said to be i-local if all of the matrices in the set are identical except for row i. Call a social choice function i-local if its domain is ilocal and call it local if it is i-local for some i. The following easily proved fact is used extensively in the literature: Proposition 2. The social choice function f is truthful if and only if every local subfunction of f is truthful. This implies that the set of all local non-truthful functions comprises a complete set of obstructions for truthfulness. This set is much smaller than the set of all non-truthful functions, but is still far from a minimal set of obstructions. Rochet , Rozenshtrom and Gui, Muller and Vohra identified a necessary and sufficient condition for truthfulness (see lemma 3 below) called the nonnegative cycle property. This condition can be viewed as providing a minimal complete set of non-truthful functions. As is required by proposition 2, each function in the set is local. In particular its domain has size at most the number of possible outcomes |A|. As this complete set of obstructions consists of minimal non-truthful functions, this provides the optimal obstruction characterization of non-truthful functions within the class of all social choice functions. But by restricting attention to interesting subclasses of social choice functions, one may hope to get simpler sets of obstructions for truthfulness within that class. The condition of weak monotonicity mentioned earlier can be defined by a set of obstructions, each of which is a local function of domain size exactly 2. Thus the results of Lavi, Mu\"alem and Nisan , and of Gui, Muller and Vohra give a very simple set of obstructions for truthfulness within certain subclasses of social choice functions. Theorem 1 extends these results to a much larger subclass of functions. 287 1.2 Weak Monotonicity and the Nonnegative Cycle Property By proposition 2, a function is truthful if and only if each of its local subfunctions is truthful. Therefore, to get a set of obstructions for truthfulness, it suffices to obtain such a set for local functions. The domain of an i-local function consists of matrices that are fixed on all rows but row i. Fix such a function f and let D \u2286 RA be the set of allowed choices for row i. Since f depends only on row i and row i is chosen from D, we can view f as a function from D to A. Therefore, f is a social choice function having one player; we refer to such a function as a single player function. Associated to any single player function f with domain D we define an edge-weighted directed graph Hf whose vertex set is the image of f. For convenience, we assume that f is surjective and so this image is A. For each a, b \u2208 A, x \u2208 f\u22121 (a) there is an edge ex(a, b) from a to b with weight x(a) \u2212 x(b). The weight of a set of edges is just the sum of the weights of the edges. We say that f satisfies: \u2022 the nonnegative cycle property if every directed cycle has nonnegative weight. \u2022 the nonnegative two-cycle property if every directed cycle between two vertices has nonnegative weight. We say a local function g satisfies nonnegative cycle property/nonnegative two-cycle property if its associated single player function f does. The graph Hf has a possibly infinite number of edges between any two vertices. We define Gf to be the edgeweighted directed graph with exactly one edge from a to b, whose weight \u03b4ab is the infimum (possibly \u2212\u221e) of all of the edge weights ex(a, b) for x \u2208 f\u22121 (a). It is easy to see that Hf has the nonnegative cycle property/nonnegative two-cycle property if and only if Gf does. Gf is called the outcome graph of f. The weak monotonicity property mentioned earlier can be defined for arbitrary social choice functions by the condition that every local subfunction satisfies the nonnegative two-cycle property. The following result was obtained by Rochet in a slightly different form and rediscovered by Rozenshtrom and Gui, Muller and Vohra : Lemma 3. A local social choice function is truthful if and only if it has the nonnegative cycle property. Thus a social choice function is truthful if and only if every local subfunction satisfies the nonnegative cycle property. In light of this, theorem 1 follows from: Theorem 4. For any surjective single player function f : D \u2212\u2192 A where D is a convex subset of RA and A is finite, the nonnegative two-cycle property implies the nonnegative cycle property. This is the result we will prove. 1.3 Overview of the Proof of Theorem 4 Let D \u2286 RA be convex and let f : D \u2212\u2192 A be a single player function such that Gf has no negative two-cycles. We want to conclude that Gf has no negative cycles. For two vertices a, b, let \u03b4\u2217 ab denote the minimum weight of any path from a to b. Our proof shows that the \u03b4\u2217 -weight of every cycle is exactly 0, from which theorem 4 follows. There seems to be no direct way to compute \u03b4\u2217 and so we proceed indirectly. Based on geometric considerations, we identify a subset of paths in Gf called admissible paths and a subset of admissible paths called straight paths. We prove that for any two outcomes a, b, there is a straight path from a to b (lemma 8 and corollary 10), and all straight paths from a to b have the same weight, which we denote \u03c1ab (theorem 12). We show that \u03c1ab \u2264 \u03b4ab (lemma 14) and that the \u03c1-weight of every cycle is 0. The key step to this proof is showing that the \u03c1-weight of every directed triangle is 0 (lemma 17). It turns out that \u03c1 is equal to \u03b4\u2217 (corollary 20), although this equality is not needed in the proof of theorem 4. To expand on the above summary, we give the definitions of an admissible path and a straight path. These are somewhat technical and rely on the geometry of f. We first observe that, without loss of generality, we can assume that D is (topologically) closed (section 2). In section 3, for each a \u2208 A, we enlarge the set f\u22121 (a) to a closed convex set Da \u2286 D in such a way that for a, b \u2208 A with a = b, Da and Db have disjoint interiors. We define an admissible path to be a sequence of outcomes (a1, . , ak) such that each of the sets Ij = Daj \u2229 Daj+1 is nonempty (section 4). An admissible path is straight if there is a straight line that meets one point from each of the sets I1, . , Ik\u22121 in order (section 5). Finally, we mention how the hypotheses of convex domain and finite range are used in the proof. Both hypotheses are needed to show: (1) the existence of a straight path from a to b for all a, b (lemma 8). (2) that the \u03c1-weight of a directed triangle is 0 (lemma 17). The convex domain hypothesis is also needed for the convexity of the sets Da (section 3). The finite range hypothesis is also needed to reduce theorem 4 to the case that D is closed (section 2) and to prove that every straight path from a to b has the same \u03b4-weight (theorem 12).", "conclusion": "As stated in the introduction, the goal underlying the work in this paper is to obtain useful and general characterizations of truthfulness.. Let us say that a set D of P \u00d7 A real valuation matrices is a WM-domain if any social choice function on D satisfying weak monotonicity is truthful.. In this paper, we showed that for finite A, any convex D is a WM-domain.. Typically, the domains of social choice functions considered in mechanism design are convex, but there are interesting examples with non-convex domains, e.g., combinatorial auctions with unknown single-minded bidders.. It is intriguing to find the most general conditions under which a set D of real matrices is a WM-domain.. We believe that convexity is the main part of the story, i.e., a WM-domain is, after excluding some exceptional cases, essentially a convex set.. Turning to parametric representations, let us say a set D of P \u00d7 A matrices is an AM-domain if any truthful social choice function with domain D is an affine maximizer.. Roberts\" theorem says that the unrestricted domain is an AM-domain.. What are the most general conditions under which a set D of real matrices is an AM-domain?. Acknowledgments We thank Ron Lavi for helpful discussions and the two anonymous referees for helpful comments."}
{"id": "C-50", "keywords": ["search and rescu", "sensor network"], "title": "CenWits: A Sensor-Based Loosely Coupled Search and Rescue System Using Witnesses", "abstract": "This paper describes the design, implementation and evaluation of a search and rescue system called CenWits. CenWits uses several small, commonly-available RF-based sensors, and a small number of storage and processing devices. It is designed for search and rescue of people in emergency situations in wilderness areas. A key feature of CenWits is that it does not require a continuously connected sensor network for its operation. It is designed for an intermittently connected network that provides only occasional connectivity. It makes a judicious use of the combined storage capability of sensors to filter, organize and store important information, combined battery power of sensors to ensure that the system remains operational for longer time periods, and intermittent network connectivity to propagate information to a processing center. A prototype of CenWits has been implemented using Berkeley Mica2 motes. The paper describes this implementation and reports on the performance measured from it.", "references": ["based tracking system", "Brent geese 2002", "The onstar system", "Personal locator beacons with GPS receiver and satellite transmitter", "Personal tracking using GPS and GSM system", "Rf based kid tracking system", "Performance measurements with motes technology", "RADAR: An in-building RF-based user location and tracking system", "A delay-tolerant network architecture for challenged internets", "Radio triggered wake-up capability for sensor networks", "Location systems for ubiquitous computing", "Lifetch life saving system", "Energy-efficient computing for wildlife tracking: design tradeoffs and early experiences with ZebraNet", "Energy harvesting aware power management", "Embedding the internet: wireless integrated network sensors", "A study of low-level vibrations as a power source for wireless sensor networks", "Locationing in distributed ad-hoc wireless sensor networks", "Simulating the power consumption of large-scale sensor network applications", "Active badges and personal interactive computing objects", "Programming sensor networks using abstract regions"], "full_text": "1. INTRODUCTION Search and rescue of people in emergency situation in a timely manner is an extremely important service. It has been difficult to provide such a service due to lack of timely information needed to determine the current location of a person who may be in an emergency situation. With the emergence of pervasive computing, several systems have been developed over the last few years that make use of small devices such as cell phones, sensors, etc. All these systems require a connected network via satellites, GSM base stations, or mobile devices. This requirement severely limits their applicability, particularly in remote wilderness areas where maintaining a connected network is very difficult. For example, a GSM transmitter has to be in the range of a base station to transmit. As a result, it cannot operate in most wilderness areas. While a satellite transmitter is the only viable solution in wilderness areas, it is typically expensive and cumbersome. Furthermore, a line of sight is required to transmit to satellite, and that makes it infeasible to stay connected in narrow canyons, large cities with skyscrapers, rain forests, or even when there is a roof or some other obstruction above the transmitter, e.g. in a car. An RF transmitter has a relatively smaller range of transmission. So, while an in-situ sensor is cheap as a single unit, it is expensive to build a large network that can provide connectivity over a large wilderness area. In a mobile environment where sensors are carried by moving people, power-efficient routing is difficult to implement and maintain over a large wilderness area. In fact, building an adhoc sensor network using only the sensors worn by hikers is nearly impossible due to a relatively small number of sensors spread over a large wilderness area. In this paper, we describe the design, implementation and evaluation of a search and rescue system called CenWits (Connection-less Sensor-Based Tracking System Using Witnesses). CenWits is comprised of mobile, in-situ sensors that are worn by subjects (people, wild animals, or in-animate objects), access points (AP) that collect information from these sensors, and GPS receivers and location points (LP) that provide location information to the sensors. A subject uses GPS receivers (when it can connect to a satellite) and LPs to determine its current location. The key idea of CenWits is that it uses a concept of witnesses to convey a subject\"s movement and location information to the outside world. This averts a need for maintaining a connected network to transmit location information to the outside world. In particular, there is no need for expensive GSM or satellite transmitters, or maintaining an adhoc network of in-situ sensors in CenWits. 180 CenWits employs several important mechanisms to address the key problem of resource constraints (low signal strength, low power and limited memory) in sensors. In particular, it makes a judicious use of the combined storage capability of sensors to filter, organize and store important information, combined battery power of sensors to ensure that the system remains operational for longer time periods, and intermittent network connectivity to propagate information to a processing center. The problem of low signal strengths (short range RF communication) is addressed by avoiding a need for maintaining a connected network. Instead, CenWits propagates the location information of sensors using the concept of witnesses through an intermittently connected network. As a result, this system can be deployed in remote wilderness areas, as well as in large urban areas with skyscrapers and other tall structures. Also, this makes CenWits cost-effective. A subject only needs to wear light-weight and low-cost sensors that have GPS receivers but no expensive GSM or satellite transmitters. Furthermore, since there is no need for a connected sensor network, there is no need to deploy sensors in very large numbers. The problem of limited battery life and limited memory of a sensor is addressed by incorporating the concepts of groups and partitions. Groups and partitions allow sensors to stay in sleep or receive modes most of the time. Using groups and partitions, the location information collected by a sensor can be distributed among several sensors, thereby reducing the amount of memory needed in one sensor to store that information. In fact, CenWits provides an adaptive tradeoff between memory and power consumption of sensors. Each sensor can dynamically adjust its power and memory consumption based on its remaining power or available memory. It has amply been noted that the strength of sensor networks comes from the fact that several sensor nodes can be distributed over a relatively large area to construct a multihop network. This paper demonstrates that important large-scale applications can be built using sensors by judiciously integrating the storage, communication and computation capabilities of sensors. The paper describes important techniques to combine memory, transmission and battery power of many sensors to address resource constraints in the context of a search and rescue application. However, these techniques are quite general. We discuss several other sensor-based applications that can employ these techniques. While CenWits addresses the general location tracking and reporting problem in a wide-area network, there are two important differences from the earlier work done in this area. First, unlike earlier location tracking solutions, CenWits does not require a connected network. Second, unlike earlier location tracking solutions, CenWits does not aim for a very high accuracy of localization. Instead, the main goal is to provide an approximate, small area where search and rescue efforts can be concentrated. The rest of this paper is organized as follows. In Section 2, we overview some of the recent projects and technologies related to movement and location tracking, and search and rescue systems. In Section 3, we describe the overall architecture of CenWits, and provide a high-level description of its functionality. In the next section, Section 4, we discuss power and memory management in CenWits. To simplify our presentation, we will focus on a specific application of tracking lost/injured hikers in all these sections. In Section 6, we describe a prototype implementation of CenWits and present performance measured from this implementation. We discuss how the ideas of CenWits can be used to build several other applications in Section 7. Finally, in Section 8, we discuss some related issues and conclude the paper. 2. RELATED WORK A survey of location systems for ubiquitous computing is provided in . A location tracking system for adhoc sensor networks using anchor sensors as reference to gain location information and spread it out to outer node is proposed in . Most location tracking systems in adhoc sensor networks are for benefiting geographic-aware routing. They don\"t fit well for our purposes. The well-known active badge system lets a user carry a badge around. An infrared sensor in the room can detect the presence of a badge and determine the location and identification of the person. This is a useful system for indoor environment, where GPS doesn\"t work. Locationing using 802.11 devices is probably the cheapest solution for indoor position tracking . Because of the popularity and low cost of 802.11 devices, several business solutions based on this technology have been developed. A system that combines two mature technologies and is viable in suburban area where a user can see clear sky and has GSM cellular reception at the same time is currently available. This system receives GPS signal from a satellite and locates itself, draws location on a map, and sends location information through GSM network to the others who are interested in the user\"s location. A very simple system to monitor children consists an RF transmitter and a receiver. The system alarms the holder of the receiver when the transmitter is about to run out of range . Personal Locater Beacons (PLB) has been used for avalanche rescuing for years. A skier carries an RF transmitter that emits beacons periodically, so that a rescue team can find his/her location based on the strength of the RF signal. Luxury version of PLB combines a GPS receiver and a COSPASSARSAT satellite transmitter that can transmit user\"s location in latitude and longitude to the rescue team whenever an accident happens . However, the device either is turned on all the time resulting in fast battery drain, or must be turned on after the accident to function. Another related technology in widespread use today is the ONSTAR system , typically used in several luxury cars. In this system, a GPS unit provides position information, and a powerful transmitter relays that information via satellite to a customer service center. Designed for emergencies, the system can be triggered either by the user with the push of a button, or by a catastrophic accident. Once the system has been triggered, a human representative attempts to gain communication with the user via a cell phone built as an incar device. If contact cannot be made, emergency services are dispatched to the location provided by GPS. Like PLBs, this system has several limitations. First, it is heavy and expensive. It requires a satellite transmitter and a connected network. If connectivity with either the GPS network or a communication satellite cannot be maintained, the system fails. Unfortunately, these are common obstacles encountered in deep canyons, narrow streets in large cities, parking garages, and a number of other places. 181 The Lifetch system uses GPS receiver board combined with a GSM/GPRS transmitter and an RF transmitter in one wireless sensor node called Intelligent Communication Unit (ICU). An ICU first attempts to transmit its location to a control center through GSM/GPRS network. If that fails, it connects with other ICUs (adhoc network) to forward its location information until the information reaches an ICU that has GSM/GPRS reception. This ICU then transmits the location information of the original ICU via the GSM/GPRS network. ZebraNet is a system designed to study the moving patterns of zebras . It utilizes two protocols: History-based protocol and flooding protocol. History-based protocol is used when the zebras are grazing and not moving around too much. While this might be useful for tracking zebras, it\"s not suitable for tracking hikers because two hikers are most likely to meet each other only once on a trail. In the flooding protocol, a node dumps its data to a neighbor whenever it finds one and doesn\"t delete its own copy until it finds a base station. Without considering routing loops, packet filtering and grouping, the size of data on a node will grow exponentially and drain the power and memory of a sensor node with in a short time. Instead, Cenwits uses a four-phase hand-shake protocol to ensure that a node transmits only as much information as the other node is willing to receive. While ZebraNet is designed for a big group of sensors moving together in the same direction with same speed, Cenwits is designed to be used in the scenario where sensors move in different directions at different speeds. Delay tolerant network architecture addresses some important problems in challenged (resource-constrained) networks . While this work is mainly concerned with interoperability of challenged networks, some problems related to occasionally-connected networks are similar to the ones we have addressed in CenWits. Among all these systems, luxury PLB and Lifetch are designed for location tracking in wilderness areas. However, both of these systems require a connected network. Luxury PLB requires the user to transmit a signal to a satellite, while Lifetch requires connection to GSM/GPRS network. Luxury PLB transmits location information, only when an accident happens. However, if the user is buried in the snow or falls into a deep canyon, there is almost no chance for the signal to go through and be relayed to the rescue team. This is because satellite transmission needs line of sight. Furthermore, since there is no known history of user\"s location, it is not possible for the rescue team to infer the current location of the user. Another disadvantage of luxury PLB is that a satellite transmitter is very expensive, costing in the range of $750. Lifetch attempts to transmit the location information by GSM/GPRS and adhoc sensor network that uses AODV as the routing protocol. However, having a cellular reception in remote areas in wilderness areas, e.g. American national parks is unlikely. Furthermore, it is extremely unlikely that ICUs worn by hikers will be able to form an adhoc network in a large wilderness area. This is because the hikers are mobile and it is very unlikely to have several ICUs placed dense enough to forward packets even on a very popular hike route. CenWits is designed to address the limitations of systems such as luxury PLB and Lifetch. It is designed to provide hikers, skiers, and climbers who have their activities mainly in wilderness areas a much higher chance to convey their location information to a control center. It is not reliant upon constant connectivity with any communication medium. Rather, it communicates information along from user to user, finally arriving at a control center. Unlike several of the systems discussed so far, it does not require that a user\"s unit is constantly turned on. In fact, it can discover a victim\"s location, even if the victim\"s sensor was off at the time of accident and has remained off since then. CenWits solves one of the greatest problems plaguing modern search and rescue systems: it has an inherent on-site storage capability. This means someone within the network will have access to the last-known-location information of a victim, and perhaps his bearing and speed information as well. Figure 1: Hiker A and Hiker B are are not in the range of each other 3. CENWITS We describe CenWits in the context of locating lost/injured hikers in wilderness areas. Each hiker wears a sensor (MICA2 motes in our prototype) equipped with a GPS receiver and an RF transmitter. Each sensor is assigned a unique ID and maintains its current location based on the signal received by its GPS receiver. It also emits beacons periodically. When any two sensors are in the range of one another, they record the presence of each other (witness information), and also exchange the witness information they recorded earlier. The key idea here is that if two sensors come with in range of each other at any time, they become each other\"s witnesses. Later on, if the hiker wearing one of these sensors is lost, the other sensor can convey the last known (witnessed) location of the lost hiker. Furthermore, by exchanging the witness information that each sensor recorded earlier, the witness information is propagated beyond a direct contact between two sensors. To convey witness information to a processing center or to a rescue team, access points are established at well-known locations that the hikers are expected to pass through, e.g. at the trail heads, trail ends, intersection of different trails, scenic view points, resting areas, and so on. Whenever a sensor node is in the vicinity of an access point, all witness information stored in that sensor is automatically dumped to the access point. Access points are connected to a processing center via satellite or some other network1 . The witness information is downloaded to the processing center from various access points at regular intervals. In case, connection to an access point is lost, the information from that A connection is needed only between access points and a processing center. There is no need for any connection between different access points. 182 access point can be downloaded manually, e.g. by UAVs. To estimate the speed, location and direction of a hiker at any point in time, all witness information of that hiker that has been collected from various access points is processed. Figure 2: Hiker A and Hiker B are in the range of each other. A records the presence of B and B records the presence of A. A and B become each other\"s witnesses. Figure 3: Hiker A is in the range of an access point. It uploads its recorded witness information and clears its memory. An example of how CenWits operates is illustrated in Figures 1, 2 and 3. First, hikers A and B are on two close trails, but out of range of each other (Figure 1). This is a very common scenario during a hike. For example, on a popular four-hour hike, a hiker might run into as many as 20 other hikers. This accounts for one encounter every 12 minutes on average. A slow hiker can go 1 mile (5,280 feet) per hour. feet. This implies that if we were to put 20 hikers on a 4-hour, one-way hike evenly, feet for them to communicate with one another continuously. The signal strength starts dropping rapidly for two Mica2 nodes to communicate with each other when they are 180 feet away, and is completely lost when they are 230 feet away from each other. So, for the sensors to form a sensor network on a 4-hour hiking trail, there should be at least 120 hikers scattered evenly. Clearly, this is extremely unlikely. In fact, in a 4-hour, less-popular hiking trail, one might only run into say five other hikers. CenWits takes advantage of the fact that sensors can communicate with one another and record their presence. Given a walking speed of one mile per hour (88 feet per minute) and Mica2 range of about 150 feet for non-line-of-sight radio transmission, two hikers have about 150/88 = 1.7 minutes to discover the presence of each other and exchange their witness information. We therefore design our system to have each sensor emit a beacon every one-and-a-half minute. In Figure 2, hiker B\"s sensor emits a beacon when A is in range, this triggers A to exchange data with B. A communicates the following information to B: My ID is A; I saw C at 1:23 PM at (39\u25e6655\", 105\u25e6776\"), I saw E at 3:09 PM at (40\u25e6879\", 105\u25e6168\"). B then replies with My ID is B; I saw K at 11:20 AM at (39\u25e6655\", 105\u25e6223\"). In addition, A records I saw B at 4:17 PM at (41\u25e6354\", 105\u25e6211\") and B records I saw A at 4:17 PM at (41\u25e6354\", 105\u25e6211\"). B goes on his way to overnight camping while A heads back to trail head where there is an AP, which emits beacon every 5 seconds to avoid missing any hiker. A dumps all witness information it has collected to the access point. This is shown in Figure 3. 3.1 Witness Information: Storage A critical concern is that there is limited amount of memory available on motes (4 KB SDRAM memory, 128 KB flash memory, and 4-512 KB EEPROM). So, it is important to organize witness information efficiently. CenWits stores witness information at each node as a set of witness records (Format is shown in Figure 4. 1 B Node ID Record Time X, Y Location Time Hop Count 1 B 3 B 8 B 3 B Figure 4: Format of a witness record. When two nodes i and j encounter each other, each node generates a new witness record. In the witness record generated by i, Node ID is j, Record Time is the current time in i\"s clock, (X,Y) are the coordinates of the location of i that i recorded most recently (either from satellite or an LP), Location Time is the time when the this location was recorded, and Hop Count is 0. Each node is assigned a unique Node Id when it enters a trail. In our current prototype, we have allocated one byte for Node Id, although this can be increased to two or more bytes if a large number of hikers are expected to be present at the same time. We can represent time in 17 bits to a second precision. So, we have allocated 3 bytes each for Record Time and Location Time. The circumference of the Earth is approximately 40,075 KM. If we use a 32-bit number to represent both longitude and latitude, the precision we get is 40,075,000/232 = meter = 0.37 inches, which is quite precise for our needs. So, we have allocated 4 bytes each for X and Y coordinates of the location of a node. In fact, a foot precision can be achieved by using only 27 bits. 3.2 Location Point and Location Inference Although a GPS receiver provides an accurate location information, it has it\"s limitation. In canyons and rainy forests, a GPS receiver does not work. When there is a heavy cloud cover, GPS users have experienced inaccuracy in the reported location as well. Unfortunately, a lot of hiking trails are in dense forests and canyons, and it\"s not that uncommon to rain after hikers start hiking. To address this, CenWits incorporates the idea of location points (LP). A location point can update a sensor node with its current location whenever the node is near that LP. LPs are placed at different locations in a wilderness area where GPS receivers don\"t work. An LP is a very simple device that emits prerecorded location information at some regular time interval. It can be placed in difficult-to-reach places such as deep canyons and dense rain forests by simply dropping them from an airplane. LPs allow a sensor node to determine its current location more accurately. However, they are not 183 an essential requirement of CenWits. If an LP runs out of power, the CenWits will continue to work correctly. Figure 5: GPS receiver not working correctly. Sensors then have to rely on LP to provide coordination In Figure 5, B cannot get GPS reception due to bad weather. It then runs into A on the trail who doesn\"t have GPS reception either. Their sensors record the presence of each other. After 10 minutes, A is in range of an LP that provides an accurate location information to A. When A returns to trail head and uploads its data (Figure 6), the system can draw a circle centered at the LP from which A fetched location information for the range of encounter location of A and B. By Overlapping this circle with the trail map, two or three possible location of encounter can be inferred. Thus when a rescue is required, the possible location of B can be better inferred (See Figures 7 and 8). Figure 6: A is back to trail head, It reports the time of encounter with B to AP, but no location information to AP Figure 7: B is still missing after sunset. CenWits infers the last contact point and draws the circle of possible current locations based on average hiking speed CenWits requires that the clocks of different sensor nodes be loosely synchronized with one another. Such a synchronization is trivial when GPS coverage is available. In addition, sensor nodes in CenWits synchronize their clocks whenever they are in the range of an AP or an LP. The Figure 8: Based on overlapping landscape, B might have hiked to wrong branch and fallen off a cliff. Hot rescue areas can thus be determined synchronization accuracy Cenwits needs is of the order of a second or so. As long as the clocks are synchronized with in one second range, whether A met B at 12:37\"45 or 12:37\"46 doesn\"t matter in the ordering of witness events and inferring the path. 4. MEMORY AND POWER MANAGEMENT CenWits employs several important mechanisms to conserve power and memory. It is important to note while current sensor nodes have limited amount of memory, future sensor nodes are expected to have much more memory. With this in mind, the main focus in our design is to provide a tradeoff between the amount of memory available and amount of power consumption. 4.1 Memory Management The size of witness information stored at a node can get very large. This is because the node may come across several other nodes during a hike, and may end up accumulating a large amount of witness information over time. To address this problem, CenWits allows a node to pro-actively free up some parts of its memory periodically. This raises an interesting question of when and which witness record should be deleted from the memory of a node? CenWits uses three criteria to determine this: record count, hop count, and record gap. Record count refers to the number of witness records with same node id that a node has stored in its memory. A node maintains an integer parameter MAX RECORD COUNT. It stores at most MAX RECORD COUNT witness records of any node. Every witness record has a hop count field that stores the number times (hops) this record has been transferred since being created. Initially this field is set to 0. Whenever a node receives a witness record from another node, it increments the hop count of that record by 1. A node maintains an integer parameter called MAX HOP COUNT. It keeps only those witness records in its memory, whose hop count is less than MAX HOP COUNT. The MAX HOP COUNT parameter provides a balance between two conflicting goals: (1) To ensure that a witness record has been propagated to and thus stored at as many nodes as possible, so that it has a high probability of being dumped at some AP as quickly as possible; and (2) To ensure that a witness record is stored only at a few nodes, so that it does not clog up too much of the combined memory of all sensor nodes. We chose to use hop count instead of time-to-live to decide when to drop a packet. The main reason for this is that the probability of a packet reaching an AP goes up as the hop count adds up. For example, when the hop count if 5 for a specific record, 184 the record is in at least 5 sensor nodes. On the other hand, if we discard old records, without considering hop count, there is no guarantee that the record is present in any other sensor node. Record gap refers to the time difference between the record times of two witness records with the same node id. To save memory, a node n ensures the the record gap between any two witness records with the same node id is at least MIN RECORD GAP. For each node id i, n stores the witness record with the most recent record time rti, the witness with most recent record time that is at least MIN RECORD GAP time units before rti, and so on until the record count limit (MAX RECORD COUNT) is reached. When a node is tight in memory, it adjusts the three parameters, MAX RECORD COUNT, MAX HOP COUNT and MIN RECORD GAP to free up some memory. It decrements MAX RECORD COUNT and MAX HOP COUNT, and increments MIN RECORD GAP. It then first erases all witness records whose hop count exceeds the reduced MAX HOP COUNT value, and then erases witness records to satisfy the record gap criteria. Also, when a node has extra memory space available, e.g. after dumping its witness information at an access point, it resets MAX RECORD COUNT, MAX HOP COUNT and MIN RECORD GAP to some predefined values. 4.2 Power Management An important advantage of using sensors for tracking purposes is that we can regulate the behavior of a sensor node based on current conditions. For example, we mentioned earlier that a sensor should emit a beacon every 1.7 minute, given a hiking speed of 1 mile/hour. However, if a user is moving 10 feet/sec, a beacon should be emitted every 10 seconds. If a user is not moving at all, a beacon can be emitted every 10 minutes. In the night, a sensor can be put into sleep mode to save energy, when a user is not likely to move at all for a relatively longer period of time. If a user is active for only eight hours in a day, we can put the sensor into sleep mode for the other 16 hours and thus save 2/3rd of the energy. In addition, a sensor node can choose to not send any beacons during some time intervals. For example, suppose hiker A has communicated its witness information to three other hikers in the last five minutes. If it is running low on power, it can go to receive mode or sleep mode for the next ten minutes. It goes to receive mode if it is still willing to receive additional witness information from hikers that it encounters in the next ten minutes. It goes to sleep mode if it is extremely low on power. The bandwidth and energy limitations of sensor nodes require that the amount of data transferred among the nodes be reduced to minimum. instructions could be executed for the same energy cost of sending a bit 100m by radio . To reduce the amount of data transfer, CenWits employs a handshake protocol that two nodes execute when they encounter one another. The goal of this protocol is to ensure that a node transmits only as much witness information as the other node is willing to receive. This protocol is initiated when a node i receives a beacon containing the node ID of the sender node j and i has not exchanged witness information with j in the last \u03b4 time units. Assume that i < j. The protocol consists of four phases (See Figure 9): 1. Phase I: Node i sends its receive constraints and the number of witness records it has in its memory. 2. Phase II: On receiving this message from i, j sends its receive constraints and the number of witness records it has in its memory. 3. Phase III: On receiving the above message from j, i sends its witness information (filtered based on receive constraints received in phase II). 4. Phase IV: After receiving the witness records from i, j sends its witness information (filtered based on receive constraints received in phase I). <Constaints, Witness info size> <Constaints, Witness info size> <Filtered Witness info> <Filtered Witness info> i j Figure 9: Four-Phase Hand Shake Protocol (i < j) Receive constraints are a function of memory and power. In the most general case, they are comprised of the three parameters (record count, hop count and record gap) used for memory management. If i is low on memory, it specifies the maximum number of records it is willing to accept from j. Similarly, i can ask j to send only those records that have hop count value less than MAX HOP COUNT \u2212 1. Finally, i can include its MIN RECORD GAP value in its receive constraints. Note that the handshake protocol is beneficial to both i and j. They save memory by receiving only as much information as they are willing to accept and conserve energy by sending only as many witness records as needed. It turns out that filtering witness records based on MIN RECORD GAP is complex. It requires that the witness records of any given node be arranged in an order sorted by their record time values. Maintaining this sorted order is complex in memory, because new witness records with the same node id can arrive later that may have to be inserted in between to preserve the sorted order. For this reason, the receive constraints in the current CenWits prototype do not include record gap. Suppose i specifies a hop count value of 3. In this case, j checks the hop count field of every witness record before sending them. If the hop count value is greater than 3, the record is not transmitted. 4.3 Groups and Partitions To further reduce communication and increase the lifetime of our system, we introduce the notion of groups. The idea is based on the concept of abstract regions presented in . A group is a set of n nodes that can be defined in terms of radio connectivity, geographic location, or other properties of nodes. All nodes within a group can communicate directly with one another and they share information to maintain their view of the external world. At any point in time, a group has exactly one leader that communicates 185 with external nodes on behalf of the entire group. A group can be static, meaning that the group membership does not change over the period of time, or it could be dynamic in which case nodes can leave or join the group. To make our analysis simple and to explain the advantages of group, we first discuss static groups. A static group is formed at the start of a hiking trail or ski slope. Suppose there are five family members who want to go for a hike in the Rocky Mountain National Park. Before these members start their hike, each one of them is given a sensor node and the information is entered in the system that the five nodes form a group. Each group member is given a unique id and every group member knows about other members of the group. The group, as a whole, is also assigned an id to distinguish it from other groups in the system. Figure 10: A group of five people. Node 2 is the group leader and it is communicating on behalf of the group with an external node 17. All other (shown in a lighter shade) are in sleep mode. As the group moves through the trail, it exchanges information with other nodes or groups that it comes across. At any point in time, only one group member, called the leader, sends and receives information on behalf of the group and all other n \u2212 1 group members are put in the sleep mode (See Figure 10). It is this property of groups that saves us energy. The group leadership is time-multiplexed among the group members. This is done to make sure that a single node does not run out of battery due to continuous exchange of information. Thus after every t seconds, the leadership is passed on to another node, called the successor, and the leader (now an ordinary member) is put to sleep. Since energy is dear, we do not implement an extensive election algorithm for choosing the successor. Instead, we choose the successor on the basis of node id. The node with the next highest id in the group is chosen as the successor. The last node, of course, chooses the node with the lowest id as its successor. We now discuss the data storage schemes for groups. Memory is a scarce resource in sensor nodes and it is therefore important that witness information be stored efficiently among group members. Efficient data storage is not a trivial task when it comes to groups. The tradeoff is between simplicity of the scheme and memory savings. A simpler scheme incurs lesser energy cost as compared to a more sophisticated scheme, but offers lesser memory savings as well. This is because in a more complicated scheme, the group members have to coordinate to update and store information. After considering a number of different schemes, we have come to a conclusion that there is no optimal storage scheme for groups. The system should be able to adapt according to its requirements. If group members are low on battery, then the group can adapt a scheme that is more energy efficient. Similarly, if the group members are running out of memory, they can adapt a scheme that is more memory efficient. We first present a simple scheme that is very energy efficient but does not offer significant memory savings. We then present an alternate scheme that is much more memory efficient. As already mentioned a group can receive information only through the group leader. Whenever the leader comes across an external node e, it receives information from that node and saves it. In our first scheme, when the timeslot for the leader expires, the leader passes this new information it received from e to its successor. This is important because during the next time slot, if the new leader comes across another external node, it should be able to pass information about all the external nodes this group has witnessed so far. Thus the information is fully replicated on all nodes to maintain the correct view of the world. Our first scheme does not offer any memory savings but is highly energy efficient and may be a good choice when the group members are running low on battery. Except for the time when the leadership is switched, all n \u2212 1 members are asleep at any given time. This means that a single member is up for t seconds once every n\u2217t seconds and therefore has to spend approximately only 1/nth of its energy. Thus, if there are 5 members in a group, we save 80% energy, which is huge. More energy can be saved by increasing the group size. We now present an alternate data storage scheme that aims at saving memory at the cost of energy. In this scheme we divide the group into what we call partitions. Partitions can be thought of as subgroups within a group. Each partition must have at least two nodes in it. The nodes within a partition are called peers. Each partition has one peer designated as partition leader. The partition leader stays in receive mode at all times, while all others peers a partition stay in the sleep mode. Partition leadership is time-multiplexed among the peers to make sure that a single node does not run out of battery. Like before, a group has exactly one leader and the leadership is time-multiplexed among partitions. The group leader also serves as the partition leader for the partition it belongs to (See Figure 11). In this scheme, all partition leaders participate in information exchange. Whenever a group comes across an external node e, every partition leader receives all witness information, but it only stores a subset of that information after filtering. Information is filtered in such a way that each partition leader has to store only B/K bytes of data, where K is the number of partitions and B is the total number of bytes received from e. Similarly when a group wants to send witness information to e, each partition leader sends only B/K bytes that are stored in the partition it belongs to. However, before a partition leader can send information, it must switch from receive mode to send mode. Also, partition leaders must coordinate with one another to ensure that they do not send their witness information at the same time, i.e. their message do not collide. All this is achieved by having the group leader send a signal to every partition leader in turn. 186 Figure 11: The figure shows a group of eight nodes divided into four partitions of 2 nodes each. Node 1 is the group leader whereas nodes 2, 9, and 7 are partition leaders. All other nodes are in the sleep mode. Since the partition leadership is time-multiplexed, it is important that any information received by the partition leader, p1, be passed on to the next leader, p2. This has to be done to make sure that p2 has all the information that it might need to send when it comes across another external node during its timeslot. One way of achieving this is to wake p2 up just before p1\"s timeslot expires and then have p1 transfer information only to p2. An alternate is to wake all the peers up at the time of leadership change, and then have p1 broadcast the information to all peers. Each peer saves the information sent by p1 and then goes back to sleep. In both cases, the peers send acknowledgement to the partition leader after receiving the information. In the former method, only one node needs to wake up at the time of leadership change, but the amount of information that has to be transmitted between the nodes increases as time passes. In the latter case, all nodes have to be woken up at the time of leadership change, but small piece of information has to be transmitted each time among the peers. Since communication is much more expensive than bringing the nodes up, we prefer the second method over the first one. A group can be divided into partitions in more than one way. For example, suppose we have a group of six members. We can divide this group into three partitions of two peers each, or two partitions with three peers each. The choice once again depends on the requirements of the system. A few big partitions will make the system more energy efficient. This is because in this configuration, a greater number of nodes will stay in sleep mode at any given point in time. On the other hand, several small partitions will make the system memory efficient, since each node will have to store lesser information (See Figure 12). A group that is divided into partitions must be able to readjust itself when a node leaves or runs out of battery. This is crucial because a partition must have at least two nodes at any point in time to tolerate failure of one node. For example, in figure 3 (a), if node 2 or node 5 dies, the partition is left with only one node. Later on, if that single node in the partition dies, all witness information stored in that partition will be lost. We have devised a very simple protocol to solve this problem. We first explain how partiFigure 12: The figure shows two different ways of partitioning a group of six nodes. In (a), a group is divided into three partitions of two nodes. Node 1 is the group leader, nodes 9 and 5 are partition leaders, and nodes 2, 3, and 6 are in sleep mode. In (b) the group is divided into two partitions of three nodes. Node 1 is the group leader, node 9 is the partition leader and nodes 2, 3, 5, and 6 are in sleep mode. tions are adjusted when a peer dies, and then explain what happens if a partition leader dies. Suppose node 2 in figure 3 (a) dies. When node 5, the partition leader, sends information to node 2, it does not receive an acknowledgement from it and concludes that node 2 has died2 . At this point, node 5 contacts other partition leaders (nodes 1 ... 9) using a broadcast message and informs them that one of its peers has died. Upon hearing this, each partition leader informs node 5 (i) the number of nodes in its partition, (ii) a candidate node that node 5 can take if the number of nodes in its partition is greater than 2, and (iii) the amount of witness information stored in its partition. Upon hearing from every leader, node 5 chooses the candidate node from the partition with maximum number (must be greater than 2) of peers, and sends a message back to all leaders. Node 5 then sends data to its new peer to make sure that the information is replicated within the partition. However, if all partitions have exactly two nodes, then node 5 must join another partition. It chooses the partition that has the least amount of witness information to join. It sends its witness information to the new partition leader. Witness information and membership update is propagated to all peers during the next partition leadership change. We now consider the case where the partition leader dies. If this happens, then we wait for the partition leadership to change and for the new partition leader to eventually find out that a peer has died. Once the new partition leader finds out that it needs more peers, it proceeds with the protocol explained above. However, in this case, we do lose information that the previous partition leader might have received just before it died. This problem can be solved by implementing a more rigorous protocol, but we have decided to give up on accuracy to save energy. Our current design uses time-division multiplexing to schedule wakeup and sleep modes in the sensor nodes. However, recent work on radio wakeup sensors can be used to do this scheduling more efficiently. we plan to incorporate radio wakeup sensors in CenWits when the hardware is mature. The algorithm to conclude that a node has died can be made more rigorous by having the partition leader query the suspected node a few times. 187 5. SYSTEM EVALUATION A sensor is constrained in the amount of memory and power. In general, the amount of memory needed and power consumption depends on a variety of factors such as node density, number of hiker encounters, and the number of access points. In this Section, we provide an estimate of how long the power of a MICA2 mote will last under certain assumtions. First, we assume that each sensor node carries about 100 witness records. On encountering another hiker, a sensor node transmits 50 witness records and receives 50 new witness records. Since, each record is 16 bytes long, it will take 0.34 seconds to transmit 50 records and another 0. bps link. The power consumption of MICA2 due to CPU processing, transmission and reception are approximately 8.0 mA, 7.0 mA and 8.5 mA per hour respectively ,mAh. Since the radio module of Mica2 is half-duplex and assuming that the CPU is always active when a node is awake, power consumption due to transmission is 8 + 8.5 = 16.5 mA per hour and due to reception is 8 + 7 = 15mA per hour. So, average power consumtion due to transmission and reception is (16.5 + 15)/2 = 15.75 mA per hour. mAh,/15.75 = 159 hours of transmission and reception. An encounter between two hikers results in exchange of about 50 witness records that takes about 0.68 seconds as calculated above. Thus, a single alkaline battery can last for (159 \u2217 60 \u2217 60)/0.68 = hiker encounters. Assuming that a node emits a beacon every 90 seconds and a hiker encounter occurs everytime a beacon is emitted (worst-case scenario), a single alkaline battery will last for ( \u2217 90)/(30 \u2217 24 \u2217 60 \u2217 60) = 29 days. Since, a Mica2 is equipped with two batteries, a Mica2 sensor can remain operation for about two months. Notice that this calculation is preliminary, because it assumes that hikers are active 24 hours of the day and a hiker encounter occurs every 90 seconds. In a more realistic scenario, power is expected to last for a much longer time period. Also, this time period will significantly increase when groups of hikers are moving together. Finally, the lifetime of a sensor running on two batteries can definitely be increased significantly by using energy scavenging techniques and energy harvesting techniques . 6. PROTOTYPE IMPLEMENTATION We have implemented a prototype of CenWits on MICA2 sensor 900MHz running Mantis OS 0.9.1b. One of the sensor is equipped with MTS420CA GPS module, which is capable of barometric pressure and two-axis acceleration sensing in addition to GPS location tracking. We use SiRF, the serial communication protocol, to control GPS module. SiRF has a rich command set, but we record only X and Y coordinates. A witness record is 16 bytes long. When a node starts up, it stores its current location and emits a beacon periodicallyin the prototype, a node emits a beacon every minute. We have conducted a number of experiments with this prototype. A detailed report on these experiments with the raw data collected and photographs of hikers, access points etc. is available at \u223chuangjh/ Cenwits.index.htm. Here we report results from three of them. In all these experiments, there are three access points (A, B and C) where nodes dump their witness information. These access points also provide location information to the nodes that come with in their range. We first show how CenWits can be used to determine the hiking trail a hiker is most likely on and the speed at which he is hiking, and identify hot search areas in case he is reported missing. Next, we show the results of power and memory management techniques of CenWits in conserving power and memory of a sensor node in one of our experiments. 6.1 Locating Lost Hikers The first experiment is called Direct Contact. It is a very simple experiment in which a single hiker starts from A, goes to B and then C, and finally returns to A (See Figure 13). The goal of this experiment is to illustrate that CenWits can deduce the trail a hiker takes by processing witness information. Figure 13: Direct Contact Experiment Node Id Record (X,Y) Location Hop Time Time Count 1 15 (12,7) 15 0 1 33 (31,17) 33 0 1 46 (12,23) 46 0 1 10 (12,7) 10 0 1 48 (12,23) 48 0 1 16 (12,7) 16 0 1 34 (31,17) 34 0 Table 1: Witness information collected in the direct contact experiment. The witness information dumped at the three access points was then collected and processed at a control center. Part of the witness information collected at the control center is shown in Table 1. The X,Y locations in this table correspond to the location information provided by access points A, B, and C. A is located at (12,7), B is located at (31,17) and C is located at (12,23). Three encounter points (between hiker 1 and the three access points) extracted from 188 this witness information are shown in Figure 13 (shown in rectangular boxes). For example, A,1 at 16 means 1 came in contact with A at time 16. Using this information, we can infer the direction in which hiker 1 was moving and speed at which he was moving. Furthermore, given a map of hiking trails in this area, it is clearly possible to identify the hiking trail that hiker 1 took. The second experiment is called Indirect Inference. This experiment is designed to illustrate that the location, direction and speed of a hiker can be inferred by CenWits, even if the hiker never comes in the range of any access point. It illustrates the importance of witness information in search and rescue applications. In this experiment, there are three hikers, 1, 2 and 3. Hiker 1 takes a trail that goes along access points A and B, while hiker 3 takes trail that goes along access points C and B. Hiker 2 takes a trail that does not come in the range of any access points. However, this hiker meets hiker 1 and 3 during his hike. This is illustrated in Figure 14. Figure 14: Indirect Inference Experiment Node Id Record (X,Y) Location Hop Time Time Count 2 16 (12,7) 6 0 2 15 (12,7) 6 0 1 4 (12,7) 4 0 1 6 (12,7) 6 0 1 29 (31,17) 29 0 1 31 (31,17) 31 0 Table 2: Witness information collected from hiker 1 in indirect inference experiment. Part of the witness information collected at the control center from access points A, B and C is shown in Tables 2 and 3. There are some interesting data in these tables. For example, the location time in some witness records is not the same as the record time. This means that the node that generated that record did not have its most up-to-date location at the encounter time. For example, when hikers 1 and 2 meet at time 16, the last recorded location time of Node Id Record (X,Y) Location Hop Time Time Count 3 78 (12,23) 78 0 3 107 (31,17) 107 0 3 106 (31,17) 106 0 3 76 (12,23) 76 0 3 79 (12,23) 79 0 2 94 (12,23) 79 0 1 16 (?,?) ? 1 1 15 (?,?) ? 1 Table 3: Witness information collected from hiker 3 in indirect inference experiment. hiker 1 is (12,7) recorded at time 6. So, node 1 generates a witness record with record time 16, location (12,7) and location time 6. In fact, the last two records in Table 3 have (?,?) as their location. This has happened because these witness records were generate by hiker 2 during his encounter with 1 at time 15 and 16. Until this time, hiker 2 hadn\"t come in contact with any location points. Interestingly, a more accurate location information of 1 and 2 encounter or 2 and 3 encounter can be computed by process the witness information at the control center. It took 25 units of time for hiker 1 to go from A (12,7) to B (31,17). Assuming a constant hiking speed and a relatively straight-line hike, it can be computed that at time 16, hiker 1 must have been at location (18,10). Thus (18,10) is a more accurate location of encounter between 1 and 2. Finally, our third experiment called Identifying Hot Search Areas is designed to determine the trail a hiker has taken and identify hot search areas for rescue after he is reported missing. There are six hikers (1, 2, 3, 4, 5 and 6) in this experiment. Figure 15 shows the trails that hikers 1, 2, 3, 4 and 5 took, along with the encounter points obtained from witness records collected at the control center. For brevity, we have not shown the entire witness information collected at the control center. This information is available at \u223chuangjh/Cenwits/index.htm. Figure 15: Identifying Hot Search Area Experiment (without hiker 6) 189 Now suppose hiker 6 is reported missing at time 260. To determine the hot search areas, the witness records of hiker 6 are processed to determine the trail he is most likely on, the speed at which he had been moving, direction in which he had been moving, and his last known location. Based on this information and the hiking trail map, hot search areas are identified. The hiking trail taken by hiker 6 as inferred by CenWits is shown by a dotted line and the hot search areas identified by CenWits are shown by dark lines inside the dotted circle in Figure 16. Figure 16: Identifying Hot Search Area Experiment (with hiker 6) 6.2 Results of Power and Memory Management The witness information shown in Tables 1, 2 and 3 has not been filtered using the three criteria described in Section 4.1. For example, the witness records generated by 3 at record times 76, 78 and 79 (see Table 3) have all been generated due a single contact between access point C and node 3. By applying the record gap criteria, two of these three records will be erased. Similarly, the witness records generated by 1 at record times 10, 15 and 16 (see Table 1) have all been generated due a single contact between access point A and node 1. Again, by applying the record gap criteria, two of these three records will be erased. Our experiments did not generate enough data to test the impact of record count or hop count criteria. To evaluate the impact of these criteria, we simulated CenWits to generate a significantly large number of records for a given number of hikers and access points. We generated witness records by having the hikers walk randomly. We applied the three criteria to measure the amount of memory savings in a sensor node. The results are shown in Table 4. The number of hikers in this simulation was 10 and the number of access points was 5. The number of witness records reported in this table is an average number of witness records a sensor node stored at the time of dump to an access point. These results show that the three memory management criteria significantly reduces the memory consumption of sensor nodes in CenWits. For example, they can reduce MAX MIN MAX # of RECORD RECORD HOP Witness COUNT GAP COUNT Records 5 5 5 628 4 5 5 421 3 5 5 316 5 10 5 311 5 20 5 207 5 5 4 462 5 5 3 341 3 20 3 161 Table 4: Impact of memory management techniques. the memory consumption by up to 75%. However, these results are premature at present for two reasons: (1) They are generated via simulation of hikers walking at random; and (2) It is not clear what impact the erasing of witness records has on the accuracy of inferred location/hot search areas of lost hikers. In our future work, we plan to undertake a major study to address these two concerns. 7. OTHER APPLICATIONS In addition to the hiking in wilderness areas, CenWits can be used in several other applications, e.g. skiing, climbing, wild life monitoring, and person tracking. Since CenWits relies only on intermittent connectivity, it can take advantage of the existing cheap and mature technologies, and thereby make tracking cheaper and fairly accurate. Since CenWits doesn\"t rely on keeping track of a sensor holder all time, but relies on maintaining witnesses, the system is relatively cheaper and widely applicable. For example, there are some dangerous cliffs in most ski resorts. But it is too expensive for a ski resort to deploy a connected wireless sensor network through out the mountain. Using CenWits, we can deploy some sensors at the cliff boundaries. These boundary sensors emit beacons quite frequently, e.g. every second, and so can record presence of skiers who cross the boundary and fall off the cliff. Ski patrols can cruise the mountains every hour, and automatically query the boundary sensor when in range using PDAs. If a PDA shows that a skier has been close to the boundary sensor, the ski patrol can use a long range walkie-talkie to query control center at the resort base to check the witness record of the skier. If there is no witness record after the recorded time in the boundary sensor, there is a high chance that a rescue is needed. In wildlife monitoring, a very popular method is to attach a GPS receiver on the animals. To collect data, either a satellite transmitter is used, or the data collector has to wait until the GPS receiver brace falls off (after a year or so) and then search for the GPS receiver. GPS transmitters are very expensive, e.g. the one used in geese tracking is $3,000 each . Also, it is not yet known if continuous radio signal is harmful to the birds. Furthermore, a GPS transmitter is quite bulky and uncomfortable, and as a result, birds always try to get rid of it. Using CenWits, not only can we record the presence of wildlife, we can also record the behavior of wild animals, e.g. lions might follow the migration of deers. CenWits does nor require any bulky and expensive satellite transmitters, nor is there a need to wait for a year and search for the braces. CenWits provides a very simple and cost-effective solution in this case. Also, access points 190 can be strategically located, e.g. near a water source, to increase chances of collecting up-to-date data. In fact, the access points need not be statically located. They can be placed in a low-altitude plane (e.g a UAV) and be flown over a wilderness area to collect data from wildlife. In large cities, CenWits can be used to complement GPS, since GPS doesn\"t work indoor and near skyscrapers. If a person A is reported missing, and from the witness records we find that his last contacts were C and D, we can trace an approximate location quickly and quite efficiently. 8. DISCUSSION AND FUTURE WORK This paper presents a new search and rescue system called CenWits that has several advantages over the current search and rescue systems. These advantages include a looselycoupled system that relies only on intermittent network connectivity, power and storage efficiency, and low cost. It solves one of the greatest problems plaguing modern search and rescue systems: it has an inherent on-site storage capability. This means someone within the network will have access to the last-known-location information of a victim, and perhaps his bearing and speed information as well. It utilizes the concept of witnesses to propagate information, infer current possible location and speed of a subject, and identify hot search and rescue areas in case of emergencies. A large part of CenWits design focuses on addressing the power and memory limitations of current sensor nodes. In fact, power and memory constraints depend on how much weight (of sensor node) a hiker is willing to carry and the cost of these sensors. An important goal of CenWits is build small chips that can be implanted in hiking boots or ski jackets. This goal is similar to the avalanche beacons that are currently implanted in ski jackets. We anticipate that power and memory will continue to be constrained in such an environment. While the paper focuses on the development of a search and rescue system, it also provides some innovative, systemlevel ideas for information processing in a sensor network system. We have developed and experimented with a basic prototype of CenWits at present. Future work includes developing a more mature prototype addressing important issues such as security, privacy, and high availability. There are several pressing concerns regarding security, privacy, and high availability in CenWits. For example, an adversary can sniff the witness information to locate endangered animals, females, children, etc. He may inject false information in the system. An individual may not be comfortable with providing his/her location and movement information, even though he/she is definitely interested in being located in a timely manner at the time of emergency. In general, people in hiking community are friendly and usually trustworthy. So, a bullet-proof security is not really required. However, when CenWits is used in the context of other applications, security requirements may change. Since the sensor nodes used in CenWits are fragile, they can fail. In fact, the nature and level of security, privacy and high availability support needed in CenWits strongly depends on the application for which it is being used and the individual subjects involved. Accordingly, we plan to design a multi-level support for security, privacy and high availability in CenWits. So far, we have experimented with CenWits in a very restricted environment with a small number of sensors. Our next goal is to deploy this system in a much larger and more realistic environment. In particular, discussions are currenly underway to deploy CenWits in the Rocky Mountain and Yosemite National Parks.", "body1": "Search and rescue of people in emergency situation in a timely manner is an extremely important service. For example, a GSM transmitter has to be in the range of a base station to transmit. In this paper, we describe the design, implementation and evaluation of a search and rescue system called CenWits (Connection-less Sensor-Based Tracking System Using Witnesses). The problem of low signal strengths (short range RF communication) is addressed by avoiding a need for maintaining a connected network. The problem of limited battery life and limited memory of a sensor is addressed by incorporating the concepts of groups and partitions. It has amply been noted that the strength of sensor networks comes from the fact that several sensor nodes can be distributed over a relatively large area to construct a multihop network. While CenWits addresses the general location tracking and reporting problem in a wide-area network, there are two important differences from the earlier work done in this area. The rest of this paper is organized as follows. A survey of location systems for ubiquitous computing is provided in . An infrared sensor in the room can detect the presence of a badge and determine the location and identification of the person. A very simple system to monitor children consists an RF transmitter and a receiver. Personal Locater Beacons (PLB) has been used for avalanche rescuing for years. Luxury version of PLB combines a GPS receiver and a COSPASSARSAT satellite transmitter that can transmit user\"s location in latitude and longitude to the rescue team whenever an accident happens . Another related technology in widespread use today is the ONSTAR system , typically used in several luxury cars. In this system, a GPS unit provides position information, and a powerful transmitter relays that information via satellite to a customer service center. 181 The Lifetch system uses GPS receiver board combined with a GSM/GPRS transmitter and an RF transmitter in one wireless sensor node called Intelligent Communication Unit (ICU). ZebraNet is a system designed to study the moving patterns of zebras . Among all these systems, luxury PLB and Lifetch are designed for location tracking in wilderness areas. Luxury PLB transmits location information, only when an accident happens. Furthermore, since there is no known history of user\"s location, it is not possible for the rescue team to infer the current location of the user. American national parks is unlikely. CenWits is designed to address the limitations of systems such as luxury PLB and Lifetch. We describe CenWits in the context of locating lost/injured hikers in wilderness areas. To convey witness information to a processing center or to a rescue team, access points are established at well-known locations that the hikers are expected to pass through, e.g. at the trail heads, trail ends, intersection of different trails, scenic view points, resting areas, and so on. To estimate the speed, location and direction of a hiker at any point in time, all witness information of that hiker that has been collected from various access points is processed. Figure 2: Hiker A and Hiker B are in the range of each other. An example of how CenWits operates is illustrated in Figures 1, 2 and 3. CenWits takes advantage of the fact that sensors can communicate with one another and record their presence. B goes on his way to overnight camping while A heads back to trail head where there is an AP, which emits beacon every 5 seconds to avoid missing any hiker. 3.1 Witness Information: Storage A critical concern is that there is limited amount of memory available on motes (4 KB SDRAM memory, 128 KB flash memory, and 4-512 KB EEPROM). When two nodes i and j encounter each other, each node generates a new witness record. Each node is assigned a unique Node Id when it enters a trail. 3.2 Location Point and Location Inference Although a GPS receiver provides an accurate location information, it has it\"s limitation. It can be placed in difficult-to-reach places such as deep canyons and dense rain forests by simply dropping them from an airplane. Sensors then have to rely on LP to provide coordination In Figure 5, B cannot get GPS reception due to bad weather. Figure 6: A is back to trail head, It reports the time of encounter with B to AP, but no location information to AP Figure 7: B is still missing after sunset. CenWits employs several important mechanisms to conserve power and memory. 4.1 Memory Management The size of witness information stored at a node can get very large. It stores at most MAX RECORD COUNT witness records of any node. Every witness record has a hop count field that stores the number times (hops) this record has been transferred since being created. Record gap refers to the time difference between the record times of two witness records with the same node id. When a node is tight in memory, it adjusts the three parameters, MAX RECORD COUNT, MAX HOP COUNT and MIN RECORD GAP to free up some memory. 4.2 Power Management An important advantage of using sensors for tracking purposes is that we can regulate the behavior of a sensor node based on current conditions. In addition, a sensor node can choose to not send any beacons during some time intervals. The bandwidth and energy limitations of sensor nodes require that the amount of data transferred among the nodes be reduced to minimum. 3. 4. <Constaints, Witness info size> <Constaints, Witness info size> <Filtered Witness info> <Filtered Witness info> i j Figure 9: Four-Phase Hand Shake Protocol (i < j) Receive constraints are a function of memory and power. In the most general case, they are comprised of the three parameters (record count, hop count and record gap) used for memory management. Finally, i can include its MIN RECORD GAP value in its receive constraints. It turns out that filtering witness records based on MIN RECORD GAP is complex. Suppose i specifies a hop count value of 3. 4.3 Groups and Partitions To further reduce communication and increase the lifetime of our system, we introduce the notion of groups. A static group is formed at the start of a hiking trail or ski slope. Figure 10: A group of five people. As the group moves through the trail, it exchanges information with other nodes or groups that it comes across. Memory is a scarce resource in sensor nodes and it is therefore important that witness information be stored efficiently among group members. Similarly, if the group members are running out of memory, they can adapt a scheme that is more memory efficient. As already mentioned a group can receive information only through the group leader. Our first scheme does not offer any memory savings but is highly energy efficient and may be a good choice when the group members are running low on battery. We now present an alternate data storage scheme that aims at saving memory at the cost of energy. In this scheme, all partition leaders participate in information exchange. 186 Figure 11: The figure shows a group of eight nodes divided into four partitions of 2 nodes each. Since the partition leadership is time-multiplexed, it is important that any information received by the partition leader, p1, be passed on to the next leader, p2. We can divide this group into three partitions of two peers each, or two partitions with three peers each. On the other hand, several small partitions will make the system memory efficient, since each node will have to store lesser information (See Figure 12). A group that is divided into partitions must be able to readjust itself when a node leaves or runs out of battery. This is crucial because a partition must have at least two nodes at any point in time to tolerate failure of one node. For example, in figure 3 (a), if node 2 or node 5 dies, the partition is left with only one node. Suppose node 2 in figure 3 (a) dies. However, if all partitions have exactly two nodes, then node 5 must join another partition. We now consider the case where the partition leader dies. If this happens, then we wait for the partition leadership to change and for the new partition leader to eventually find out that a peer has died. Our current design uses time-division multiplexing to schedule wakeup and sleep modes in the sensor nodes. A sensor is constrained in the amount of memory and power. First, we assume that each sensor node carries about 100 witness records. mAh,/15.75 = 159 hours of transmission and reception. Assuming that a node emits a beacon every 90 seconds and a hiker encounter occurs everytime a beacon is emitted (worst-case scenario), a single alkaline battery will last for ( \u2217 90)/(30 \u2217 24 \u2217 60 \u2217 60) = 29 days. We have implemented a prototype of CenWits on MICA2 sensor 900MHz running Mantis OS 0.9.1b. We have conducted a number of experiments with this prototype. These access points also provide location information to the nodes that come with in their range. 6.1 Locating Lost Hikers The first experiment is called Direct Contact. The witness information dumped at the three access points was then collected and processed at a control center. The second experiment is called Indirect Inference. Part of the witness information collected at the control center from access points A, B and C is shown in Tables 2 and 3. For example, the location time in some witness records is not the same as the record time. hiker 1 is (12,7) recorded at time 6. Interestingly, a more accurate location information of 1 and 2 encounter or 2 and 3 encounter can be computed by process the witness information at the control center. Finally, our third experiment called Identifying Hot Search Areas is designed to determine the trail a hiker has taken and identify hot search areas for rescue after he is reported missing. Figure 15: Identifying Hot Search Area Experiment (without hiker 6) 189 Now suppose hiker 6 is reported missing at time 260. Figure 16: Identifying Hot Search Area Experiment (with hiker 6) 6.2 Results of Power and Memory Management The witness information shown in Tables 1, 2 and 3 has not been filtered using the three criteria described in Section 4.1. To evaluate the impact of these criteria, we simulated CenWits to generate a significantly large number of records for a given number of hikers and access points. the memory consumption by up to 75%. In addition to the hiking in wilderness areas, CenWits can be used in several other applications, e.g. In wildlife monitoring, a very popular method is to attach a GPS receiver on the animals.", "body2": "This requirement severely limits their applicability, particularly in remote wilderness areas where maintaining a connected network is very difficult. In fact, building an adhoc sensor network using only the sensors worn by hikers is nearly impossible due to a relatively small number of sensors spread over a large wilderness area. In particular, it makes a judicious use of the combined storage capability of sensors to filter, organize and store important information, combined battery power of sensors to ensure that the system remains operational for longer time periods, and intermittent network connectivity to propagate information to a processing center. Furthermore, since there is no need for a connected sensor network, there is no need to deploy sensors in very large numbers. Each sensor can dynamically adjust its power and memory consumption based on its remaining power or available memory. We discuss several other sensor-based applications that can employ these techniques. Instead, the main goal is to provide an approximate, small area where search and rescue efforts can be concentrated. Finally, in Section 8, we discuss some related issues and conclude the paper. The well-known active badge system lets a user carry a badge around. This system receives GPS signal from a satellite and locates itself, draws location on a map, and sends location information through GSM network to the others who are interested in the user\"s location. The system alarms the holder of the receiver when the transmitter is about to run out of range . A skier carries an RF transmitter that emits beacons periodically, so that a rescue team can find his/her location based on the strength of the RF signal. However, the device either is turned on all the time resulting in fast battery drain, or must be turned on after the accident to function. Another related technology in widespread use today is the ONSTAR system , typically used in several luxury cars. Unfortunately, these are common obstacles encountered in deep canyons, narrow streets in large cities, parking garages, and a number of other places. This ICU then transmits the location information of the original ICU via the GSM/GPRS network. While this work is mainly concerned with interoperability of challenged networks, some problems related to occasionally-connected networks are similar to the ones we have addressed in CenWits. Luxury PLB requires the user to transmit a signal to a satellite, while Lifetch requires connection to GSM/GPRS network. This is because satellite transmission needs line of sight. However, having a cellular reception in remote areas in wilderness areas, e.g. This is because the hikers are mobile and it is very unlikely to have several ICUs placed dense enough to forward packets even on a very popular hike route. This means someone within the network will have access to the last-known-location information of a victim, and perhaps his bearing and speed information as well. Furthermore, by exchanging the witness information that each sensor recorded earlier, the witness information is propagated beyond a direct contact between two sensors. To convey witness information to a processing center or to a rescue team, access points are established at well-known locations that the hikers are expected to pass through, e.g. by UAVs. To estimate the speed, location and direction of a hiker at any point in time, all witness information of that hiker that has been collected from various access points is processed. It uploads its recorded witness information and clears its memory. In fact, in a 4-hour, less-popular hiking trail, one might only run into say five other hikers. In addition, A records I saw B at 4:17 PM at (41\u25e6354\", 105\u25e6211\") and B records I saw A at 4:17 PM at (41\u25e6354\", 105\u25e6211\"). This is shown in Figure 3. 1 B Node ID Record Time X, Y Location Time Hop Count 1 B 3 B 8 B 3 B Figure 4: Format of a witness record. In the witness record generated by i, Node ID is j, Record Time is the current time in i\"s clock, (X,Y) are the coordinates of the location of i that i recorded most recently (either from satellite or an LP), Location Time is the time when the this location was recorded, and Hop Count is 0. In fact, a foot precision can be achieved by using only 27 bits. An LP is a very simple device that emits prerecorded location information at some regular time interval. Figure 5: GPS receiver not working correctly. Thus when a rescue is required, the possible location of B can be better inferred (See Figures 7 and 8). As long as the clocks are synchronized with in one second range, whether A met B at 12:37\"45 or 12:37\"46 doesn\"t matter in the ordering of witness events and inferring the path. With this in mind, the main focus in our design is to provide a tradeoff between the amount of memory available and amount of power consumption. A node maintains an integer parameter MAX RECORD COUNT. It stores at most MAX RECORD COUNT witness records of any node. On the other hand, if we discard old records, without considering hop count, there is no guarantee that the record is present in any other sensor node. For each node id i, n stores the witness record with the most recent record time rti, the witness with most recent record time that is at least MIN RECORD GAP time units before rti, and so on until the record count limit (MAX RECORD COUNT) is reached. after dumping its witness information at an access point, it resets MAX RECORD COUNT, MAX HOP COUNT and MIN RECORD GAP to some predefined values. If a user is active for only eight hours in a day, we can put the sensor into sleep mode for the other 16 hours and thus save 2/3rd of the energy. It goes to sleep mode if it is extremely low on power. Phase II: On receiving this message from i, j sends its receive constraints and the number of witness records it has in its memory. Phase III: On receiving the above message from j, i sends its witness information (filtered based on receive constraints received in phase II). Phase IV: After receiving the witness records from i, j sends its witness information (filtered based on receive constraints received in phase I). <Constaints, Witness info size> <Constaints, Witness info size> <Filtered Witness info> <Filtered Witness info> i j Figure 9: Four-Phase Hand Shake Protocol (i < j) Receive constraints are a function of memory and power. Similarly, i can ask j to send only those records that have hop count value less than MAX HOP COUNT \u2212 1. They save memory by receiving only as much information as they are willing to accept and conserve energy by sending only as many witness records as needed. For this reason, the receive constraints in the current CenWits prototype do not include record gap. If the hop count value is greater than 3, the record is not transmitted. To make our analysis simple and to explain the advantages of group, we first discuss static groups. The group, as a whole, is also assigned an id to distinguish it from other groups in the system. All other (shown in a lighter shade) are in sleep mode. We now discuss the data storage schemes for groups. If group members are low on battery, then the group can adapt a scheme that is more energy efficient. We then present an alternate scheme that is much more memory efficient. Thus the information is fully replicated on all nodes to maintain the correct view of the world. More energy can be saved by increasing the group size. The group leader also serves as the partition leader for the partition it belongs to (See Figure 11). All this is achieved by having the group leader send a signal to every partition leader in turn. All other nodes are in the sleep mode. For example, suppose we have a group of six members. This is because in this configuration, a greater number of nodes will stay in sleep mode at any given point in time. On the other hand, several small partitions will make the system memory efficient, since each node will have to store lesser information (See Figure 12). A group that is divided into partitions must be able to readjust itself when a node leaves or runs out of battery. This is crucial because a partition must have at least two nodes at any point in time to tolerate failure of one node. tions are adjusted when a peer dies, and then explain what happens if a partition leader dies. Node 5 then sends data to its new peer to make sure that the information is replicated within the partition. Witness information and membership update is propagated to all peers during the next partition leadership change. We now consider the case where the partition leader dies. This problem can be solved by implementing a more rigorous protocol, but we have decided to give up on accuracy to save energy. The algorithm to conclude that a node has died can be made more rigorous by having the partition leader query the suspected node a few times. In this Section, we provide an estimate of how long the power of a MICA2 mote will last under certain assumtions. So, average power consumtion due to transmission and reception is (16.5 + 15)/2 = 15.75 mA per hour. Thus, a single alkaline battery can last for (159 \u2217 60 \u2217 60)/0.68 = hiker encounters. Finally, the lifetime of a sensor running on two batteries can definitely be increased significantly by using energy scavenging techniques and energy harvesting techniques . When a node starts up, it stores its current location and emits a beacon periodicallyin the prototype, a node emits a beacon every minute. In all these experiments, there are three access points (A, B and C) where nodes dump their witness information. Next, we show the results of power and memory management techniques of CenWits in conserving power and memory of a sensor node in one of our experiments. Figure 13: Direct Contact Experiment Node Id Record (X,Y) Location Hop Time Time Count 1 15 (12,7) 15 0 1 33 (31,17) 33 0 1 46 (12,23) 46 0 1 10 (12,7) 10 0 1 48 (12,23) 48 0 1 16 (12,7) 16 0 1 34 (31,17) 34 0 Table 1: Witness information collected in the direct contact experiment. Furthermore, given a map of hiking trails in this area, it is clearly possible to identify the hiking trail that hiker 1 took. Figure 14: Indirect Inference Experiment Node Id Record (X,Y) Location Hop Time Time Count 2 16 (12,7) 6 0 2 15 (12,7) 6 0 1 4 (12,7) 4 0 1 6 (12,7) 6 0 1 29 (31,17) 29 0 1 31 (31,17) 31 0 Table 2: Witness information collected from hiker 1 in indirect inference experiment. There are some interesting data in these tables. 1 Table 3: Witness information collected from hiker 3 in indirect inference experiment. Until this time, hiker 2 hadn\"t come in contact with any location points. Thus (18,10) is a more accurate location of encounter between 1 and 2. This information is available at \u223chuangjh/Cenwits/index.htm. The hiking trail taken by hiker 6 as inferred by CenWits is shown by a dotted line and the hot search areas identified by CenWits are shown by dark lines inside the dotted circle in Figure 16. Our experiments did not generate enough data to test the impact of record count or hop count criteria. For example, they can reduce MAX MIN MAX # of RECORD RECORD HOP Witness COUNT GAP COUNT Records 5 5 5 628 4 5 5 421 3 5 5 316 5 10 5 311 5 20 5 207 5 5 4 462 5 5 3 341 3 20 3 161 Table 4: Impact of memory management techniques. In our future work, we plan to undertake a major study to address these two concerns. If there is no witness record after the recorded time in the boundary sensor, there is a high chance that a rescue is needed. If a person A is reported missing, and from the witness records we find that his last contacts were C and D, we can trace an approximate location quickly and quite efficiently.", "introduction": "Search and rescue of people in emergency situation in a timely manner is an extremely important service. It has been difficult to provide such a service due to lack of timely information needed to determine the current location of a person who may be in an emergency situation. With the emergence of pervasive computing, several systems have been developed over the last few years that make use of small devices such as cell phones, sensors, etc. All these systems require a connected network via satellites, GSM base stations, or mobile devices. This requirement severely limits their applicability, particularly in remote wilderness areas where maintaining a connected network is very difficult. For example, a GSM transmitter has to be in the range of a base station to transmit. As a result, it cannot operate in most wilderness areas. While a satellite transmitter is the only viable solution in wilderness areas, it is typically expensive and cumbersome. Furthermore, a line of sight is required to transmit to satellite, and that makes it infeasible to stay connected in narrow canyons, large cities with skyscrapers, rain forests, or even when there is a roof or some other obstruction above the transmitter, e.g. An RF transmitter has a relatively smaller range of transmission. So, while an in-situ sensor is cheap as a single unit, it is expensive to build a large network that can provide connectivity over a large wilderness area. In a mobile environment where sensors are carried by moving people, power-efficient routing is difficult to implement and maintain over a large wilderness area. In fact, building an adhoc sensor network using only the sensors worn by hikers is nearly impossible due to a relatively small number of sensors spread over a large wilderness area. In this paper, we describe the design, implementation and evaluation of a search and rescue system called CenWits (Connection-less Sensor-Based Tracking System Using Witnesses). CenWits is comprised of mobile, in-situ sensors that are worn by subjects (people, wild animals, or in-animate objects), access points (AP) that collect information from these sensors, and GPS receivers and location points (LP) that provide location information to the sensors. A subject uses GPS receivers (when it can connect to a satellite) and LPs to determine its current location. The key idea of CenWits is that it uses a concept of witnesses to convey a subject\"s movement and location information to the outside world. This averts a need for maintaining a connected network to transmit location information to the outside world. In particular, there is no need for expensive GSM or satellite transmitters, or maintaining an adhoc network of in-situ sensors in CenWits. 180 CenWits employs several important mechanisms to address the key problem of resource constraints (low signal strength, low power and limited memory) in sensors. In particular, it makes a judicious use of the combined storage capability of sensors to filter, organize and store important information, combined battery power of sensors to ensure that the system remains operational for longer time periods, and intermittent network connectivity to propagate information to a processing center. The problem of low signal strengths (short range RF communication) is addressed by avoiding a need for maintaining a connected network. Instead, CenWits propagates the location information of sensors using the concept of witnesses through an intermittently connected network. As a result, this system can be deployed in remote wilderness areas, as well as in large urban areas with skyscrapers and other tall structures. A subject only needs to wear light-weight and low-cost sensors that have GPS receivers but no expensive GSM or satellite transmitters. Furthermore, since there is no need for a connected sensor network, there is no need to deploy sensors in very large numbers. The problem of limited battery life and limited memory of a sensor is addressed by incorporating the concepts of groups and partitions. Groups and partitions allow sensors to stay in sleep or receive modes most of the time. Using groups and partitions, the location information collected by a sensor can be distributed among several sensors, thereby reducing the amount of memory needed in one sensor to store that information. In fact, CenWits provides an adaptive tradeoff between memory and power consumption of sensors. Each sensor can dynamically adjust its power and memory consumption based on its remaining power or available memory. It has amply been noted that the strength of sensor networks comes from the fact that several sensor nodes can be distributed over a relatively large area to construct a multihop network. This paper demonstrates that important large-scale applications can be built using sensors by judiciously integrating the storage, communication and computation capabilities of sensors. The paper describes important techniques to combine memory, transmission and battery power of many sensors to address resource constraints in the context of a search and rescue application. However, these techniques are quite general. We discuss several other sensor-based applications that can employ these techniques. While CenWits addresses the general location tracking and reporting problem in a wide-area network, there are two important differences from the earlier work done in this area. First, unlike earlier location tracking solutions, CenWits does not require a connected network. Second, unlike earlier location tracking solutions, CenWits does not aim for a very high accuracy of localization. Instead, the main goal is to provide an approximate, small area where search and rescue efforts can be concentrated. The rest of this paper is organized as follows. In Section 2, we overview some of the recent projects and technologies related to movement and location tracking, and search and rescue systems. In Section 3, we describe the overall architecture of CenWits, and provide a high-level description of its functionality. In the next section, Section 4, we discuss power and memory management in CenWits. To simplify our presentation, we will focus on a specific application of tracking lost/injured hikers in all these sections. In Section 6, we describe a prototype implementation of CenWits and present performance measured from this implementation. We discuss how the ideas of CenWits can be used to build several other applications in Section 7. Finally, in Section 8, we discuss some related issues and conclude the paper.", "conclusion": "This paper presents a new search and rescue system called CenWits that has several advantages over the current search and rescue systems.. These advantages include a looselycoupled system that relies only on intermittent network connectivity, power and storage efficiency, and low cost.. It solves one of the greatest problems plaguing modern search and rescue systems: it has an inherent on-site storage capability.. This means someone within the network will have access to the last-known-location information of a victim, and perhaps his bearing and speed information as well.. It utilizes the concept of witnesses to propagate information, infer current possible location and speed of a subject, and identify hot search and rescue areas in case of emergencies.. A large part of CenWits design focuses on addressing the power and memory limitations of current sensor nodes.. In fact, power and memory constraints depend on how much weight (of sensor node) a hiker is willing to carry and the cost of these sensors.. An important goal of CenWits is build small chips that can be implanted in hiking boots or ski jackets.. This goal is similar to the avalanche beacons that are currently implanted in ski jackets.. We anticipate that power and memory will continue to be constrained in such an environment.. While the paper focuses on the development of a search and rescue system, it also provides some innovative, systemlevel ideas for information processing in a sensor network system.. We have developed and experimented with a basic prototype of CenWits at present.. Future work includes developing a more mature prototype addressing important issues such as security, privacy, and high availability.. There are several pressing concerns regarding security, privacy, and high availability in CenWits.. For example, an adversary can sniff the witness information to locate endangered animals, females, children, etc.. He may inject false information in the system.. An individual may not be comfortable with providing his/her location and movement information, even though he/she is definitely interested in being located in a timely manner at the time of emergency.. In general, people in hiking community are friendly and usually trustworthy.. So, a bullet-proof security is not really required.. However, when CenWits is used in the context of other applications, security requirements may change.. Since the sensor nodes used in CenWits are fragile, they can fail.. In fact, the nature and level of security, privacy and high availability support needed in CenWits strongly depends on the application for which it is being used and the individual subjects involved.. Accordingly, we plan to design a multi-level support for security, privacy and high availability in CenWits.. So far, we have experimented with CenWits in a very restricted environment with a small number of sensors.. Our next goal is to deploy this system in a much larger and more realistic environment.. In particular, discussions are currenly underway to deploy CenWits in the Rocky Mountain and Yosemite National Parks."}
{"id": "I-1", "keywords": ["agent", "reactiv and delib architectur", "formal model of agenc+agenc formal model"], "title": "Aborting Tasks in BDI Agents", "abstract": "Intelligent agents that are intended to work in dynamic environments must be able to gracefully handle unsuccessful tasks and plans. In addition, such agents should be able to make rational decisions about an appropriate course of action, which may include aborting a task or plan, either as a result of the agent's own deliberations, or potentially at the request of another agent. In this paper we investigate the incorporation of aborts into a BDI-style architecture. We discuss some conditions under which aborting a task or plan is appropriate, and how to determine the consequences of such a decision. We augment each plan with an optional abort-method, analogous to the failure method found in some agent programming languages. We provide an operational semantics for the execution cycle in the presence of aborts in the abstract agent language CAN, which enables us to specify a BDI-based execution model without limiting our attention to a particular agent system (such as JACK, Jadex, Jason, or SPARK). A key technical challenge we address is the presence of parallel execution threads and of sub-tasks, which require the agent to ensure that the abort methods for each plan are carried out in an appropriate sequence.", "references": ["Goal representation for BDI Agent systems", "JACK intelligent agents - components for intelligent agents in Java", "Extending the concept of transaction compensation", "Goal types in agent programming", "Aoex: An agent-based exception handling framework for building reliable, distributed, open software systems", "Programming declarative goals using plan patterns", "The Psi calculus: an algebraic agent language", "Using domain-independent exception handling services to enable robust open multi-agent systems: The case of agent death", "The SPARK agent framework", "Continuous refinement of agent resource estimates", "An intelligent personal assistant for task and time management", "A cognitive framework for delegation to an assistive user agent", "Developing Intelligent Agent Systems: A Practical Guide", "Jadex: A BDI reasoning engine", "AgentSpeak(L): BDI agents speak out in a logical computable language", "An abstract architecture for rational agents", "Hierarchical planning in BDI agent programming languages: a formal approach", "Goals in the context of bdi plan failure and planning", "Detecting and exploiting positive goal interaction in intelligent agents", "Avoiding resource conflicts in intelligent agents", "A framework for goal-based semantic compensation in agent systems", "Semantic-compensation-based recovery management in multi-agent systems", "Declarative and procedural goals in intelligent agent systems"], "full_text": "1. INTRODUCTION Intelligent agents generally work in complex, dynamic environments, such as air traffic control or robot navigation, in which the success of any particular action or plan cannot be guaranteed . Accordingly, dealing with failure is fundamental to agent programming, and is an important element of agent characteristics such as robustness, flexibility, and persistence . In agent architectures inspired by the Belief-Desire-Intention (BDI) model , these properties are often characterized by the interactions between beliefs, goals, and plans .1 In general, an agent that wishes to achieve a particular set of tasks will pursue a number of plans concurrently. When failures occur, the choice of plans will be reviewed. This may involve seeking alternative plans for a particular task, re-scheduling tasks to better comply with resource constraints, dropping some tasks, or some other decision that will increase the likelihood of success . Failures can occur for a number of reasons, and it is often not possible to predict these in advance, either because of the complexity of the system or because changes in the environment invalidate some earlier decisions. Given this need for deliberation about failed tasks or plans, failure deliberation is commonly built into the agent\"s execution cycle. Besides dealing with failure, an important capability of an intelligent agent is to be able to abort a particular task or plan. This decision may be due to an internal deliberation (such as the agent believing the task can no longer be achieved, or that some conflicting task now has a higher priority) or due to an external factor (such as another agent altering a commitment, or a change in the environment). Aborting a task or plan is distinct from its failure. Failure reflects an inability to perform and does not negate the need to perform - for example, a reasonable response to failure may be to try again. In contrast, aborting says nothing about the ability to perform; it merely eliminates the need. Failure propagates from the bottom up, whereas aborting propagates from the top down. The potential for concurrently executing sub-plans introduces different complexities for aborting and failure. For aborting, it means that multiple concurrent sub-plans may need to be aborted as the abort is propagated down. For failure, it means that parallel-sibling plans may need to be aborted as the failure is propagated up. There has been a considerable amount of work on plan failures (such as detecting and resolving resource conflicts ) and most agent systems incorporate some notion of failure handling. However, there has been relatively little work on the development of abort techniques beyond simple dropping of currently intended plans and tasks, which does not deal with the clean-up required. As one consequence, most agent systems are quite limited in their treatment of the situation where one branch of a parallel construct One can consider both tasks to be performed and goals to achieve a certain state of the world. A task can be considered a goal of achieving the state of the task having been performed, and a goal can be considered a task of bringing about that state of the world. We adopt the latter view and use task to also refer to goals. 978-81--7-5 (RPS) IFAAMAS fails (common approaches include either letting the other branch run to completion unhindered or dropping it completely). In this paper we discuss in detail the incorporation of abort cleanup methods into the agent execution cycle, providing a unified approach to failure and abort. A key feature of our procedure-based approach is that we allow each plan to execute some particular code on a failure and on an abort. This allows a plan to attempt to ensure a stable, known state, and possibly to recover some resources and otherwise clean up before exiting. Accordingly, a central technical challenge is to manage the orderly execution of the appropriate clean-up code. We show how aborts can be smoothly introduced into a BDI-style architecture, and for the first time we give an operational semantics for aborting in the abstract agent language CAN . This allows us to specify an appropriate level of detail for the execution model, without focusing on the specific constructs of any one agent system such as JACK , Jadex , Jason , or SPARK . Our focus is on a single agent, complementary to related work that considers exception handling for single- and multiagent systems (e.g., ). This paper is organized as follows. In Section 2 we give an example of the consequences of aborting a task, and in Section 3 we discuss some circumstances under which aborts should occur, and the appropriate representation and invocation procedures. In Section 4 we show how we can use CAN to formally specify the behaviour of an aborted plan. Section 5 discusses related work, and in Section 6 we present our conclusions and future work. 2. MOTIVATING EXAMPLE Alice is a knowledge worker assisted by a learning, personal assistive agent such as CALO . Alice plans to attend the IJCAI conference later in the year, and her CALO agent adopts the task of Support Meeting Submission (SMS) to assist her. CALO\"s plan for an SMS task in the case of a conference submission consists of the following sub-tasks: 1. Allocate a Paper Number (APN) to be used for administrative purposes in the company. 2. Track Writing Abstract (TWA): keep track of Alice\"s progress in preparing an abstract. 3. Apply For Clearance (AFC) for publication from Alice\"s manager based on the abstract and conference details. 4. Track Writing Paper (TWP): keep track of Alice\"s progress in writing the paper. 5. Handle Paper Submission (HPS): follow company internal procedures for submitting a paper to a conference. These steps must be performed in order, with the exception of steps 3 (AFC) and 4 (TWP), which may be performed in parallel. Similarly, CALO can perform the task Apply For Clearance (AFC) by a plan consisting of: 1. Send Clearance Request (SCR) to Alice\"s manager. 2. Wait For Response (WFR) from the manager. 3. Confirm that the response was positive, and fail otherwise. Now suppose that a change in circumstances causes Alice to reconsider her travel plans while she is writing the paper. Alice will no longer be able to attend IJCAI. She therefore instructs her CALO agent to abort the SMS task. Aborting the task implies aborting both the SMS plan and the AFC subplan. Aborting the first plan requires CALO to notify the paper number registry that the allocated paper number is obsolete, which it can achieve by the Cancel Paper Number (CPN) task.2 Aborting the second plan requires CALO to notify Alice\"s manager that Alice no longer requires clearance for publication, which CALO can achieve by invoking the Cancel Clearance Request (CCR) task. We note a number of important observations from the example. First, the decision to abort a particular course of action can come from the internal deliberations of the agent (such as reasoning about priorities in a conflict over resources), or from external sources (such as another agent cancelling a commitment), as in this example. In this paper we only touch on the problem of determining whether a task or plan should be aborted, instead concentrating on determining the appropriate actions once this decision is made. Hence, our objective is to determine how to incorporate aborting mechanisms into the standard execution cycle rather than determine what should be aborted and when. Second, once the decision is made to abort the attempt to submit a paper, there are some actions the agent should take, such as cancelling the clearance request. In other words, aborting a task is not simply a matter of dropping the task and associated active plans: there are some clean up actions that may need to be done. This is similar to the case for failure, in that there may also be actions to take when a task or plan fails. In both cases, note that it is not simply a matter of the agent undo-ing its actions to date; indeed, this may be neither possible (since the agent acts in a situated world and its actions change world state) nor desirable (depending on the semantics of the task). Rather, cleaning up involves compensation via forward recovery actions . Third, there is a distinction between aborting a task and aborting a plan. In the former case, it is clear that all plans being executed to perform the task should be aborted; in the latter case, it may be that there are better alternatives to the current plan and one of these should be attempted. Hence, plan aborting or failure does not necessarily lead to task aborting or failure. Fourth, given that tasks may contain sub-tasks, which may contain further sub-tasks, it is necessary for a parent task to wait until its children have finished their abort methods. This is the source of one of the technical challenges that we address: determining the precise sequence of actions once a parent task or plan is aborted. 3. ABORTING TASKS AND PLANS As we have alluded to, failure and aborting are related concepts. They both cause the execution of existing plans to cease and, consequentially, the agent to reflect over its current tasks and intentions. Failure and aborting, however, differ in the way they arise. In the case of failure, the trigger to cease execution of a task or plan comes from below, that is, the failure of sub-tasks or lower-level plans. In the case of aborting, the trigger comes from above, that is, the tasks and the parent plans that initiated a plan. In BDI-style systems such as JACK and SPARK, an agent\"s domain knowledge includes a pre-defined plan library of plan clauses. Each plan clause has a plan body, which is a program (i.e., combination of primitive actions, sub-tasks, etc.) that can be executed in response to a task or other event should the plan clause\"s context condition be satisfied. The agent selects and executes instances of plan clauses to perform its tasks. There can be more than one applicable plan clause and, in the event that one fails, another applicable one may be attempted. Plans may have sub-tasks that must succeed CALO needs only drop the TWA and TWP tasks to abort them: for the sake of simplicity we suppose no explicit clean up of its internal state is necessary. The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 9 for the plan to succeed. In such systems, a plan failure occurs if one of the actions or sub-tasks within the plan fails. The agent\"s action upon plan failure depends on its nature: for example, the agent may declare the task to have failed if one plan has been tried and resulted in failure, or it may retry alternate plans and declare (indeed, must declare) task failure only if all possible alternate plans to perform the task have been tried and resulted in failure. Observe that, while task failure can follow from plan failure or a sequence of plan failures, plan failure need not lead to task failure provided the agent can successfully complete an alternate plan. Moreover, task failure can also arise separately from plan failure, if the agent decides to abort the task. Our approach associates an abort-method with each plan. This enables the programmer to specify dedicated compensation actions according to how the agent is attempting to perform the task. Note that our abort-methods can be arbitrary programs and so can invoke tasks that may be performed dynamically in the usual BDI fashion, i.e., the clean-up is not limited to executing a predetermined set of actions. The question remains which abort-method should be invoked, and in what manner. Given the complexity of agent action spaces, it is not possible nor desirable to enumerate a static set of rules. Rather, the agent will invoke its abort-methods dynamically according to the state of execution and its own internal events. An alternative to attaching an abort-method to each plan is to attach such methods to each atomic action. We choose the former because: (1) action-level abort-methods would incur a greater overhead, (2) plans are meant to be designed as single cohesive units and are the unit of deliberation in BDI systems, and (3) the cleanup methods for failure in current systems are attached to plans. In order to understand how the agent\"s abort processing should function, we consider three situations where it is sensible for an agent to consider aborting some of its tasks and plans: 1. When a task succeeds or fails because of an external factor other than the agent itself, the plan currently executed to perform the task should be aborted. For example, suppose company policy changes so that employees of Alice\"s seniority automatically have clearance for publishing papers. Since Alice now has clearance for publishing her paper, CALO can abort the plan for Apply For Clearance. In doing so it must invoke the abort-method, in this case thus performing Cancel Clearance Request.3 2. When two or more sub-programs are executed in parallel, if one fails then the others should be aborted, given that the failure of one branch leads to the failure of the overall task. For example, suppose that part-way through writing the paper, Alice realizes that there is a fatal flaw in her results, and so notifies CALO that she will not be able to complete the paper by the deadline. The failure of the Track Writing Paper task should cause the Apply For Clearance task being executed in parallel to be aborted. 3. When an execution event alters the importance of an existing task or intention, the agent should deliberate over whether the existing plan(s) should continue. For example, suppose that Alice tasks CALO with a new, high-priority task to purchase a replacement laptop, but that Alice lacks enough funds to both purchase the laptop and to attend IJCAI. Reasoning over resource requirements will cause the agent to realize If there is any difference between how to abort a task that is externally performed versus how to abort one that is now known to be impossible, the abort-method can detect the circumstances and handle the situation as appropriate. that it cannot successfully complete both tasks. Given that the new task has greater importance, a rational agent will evaluate its best course of action and may decide to abortor at least suspend - the existing task of submitting a paper and intentions derived from it . The operational semantics we provide in Section 4 for aborting tasks and plans captures the first two situations above. The third situation involves deliberating over the importance of a task, which depends on various factors such as task priority. Although this deliberation is beyond the scope of this paper, it is a complementary topic of our future work. Note that the above situations apply to achievement goals, for which the task is completed when a particular state of the world is brought about (e.g., ensure we have clearance). Different forms of reasoning apply to other goal types such as maintenance goals , where the goal is satisfied by maintaining a state of the world for some period of time (e.g., maintain $100 in cash). Abort Method Representation The intent of aborting a task or plan is that the task or plan and all its children cease to execute, and that appropriate clean-up methods are performed as required. In contrast to offline planning systems, BDI agents are situated: they perform online deliberation and their actions change the state of the world. As a result, the effects of many actions cannot be simply undone. Moreover, the undo process may cause adverse effects. Therefore, the clean-up methods that we specify are forward recovery procedures that attempt to ensure a stable state and that also may, if possible, recover resources. The common plan representation in BDI-style systems such as JACK and SPARK includes a failure-method, which is the designated clean-up method invoked when the plan fails. To this, we add the abort-method, which is invoked if the plan is to be aborted. In our example, the abort-method for the plan for Support Meeting Submission consists of invoking the sub-task Cancel Paper Number. The abort-method need not explicitly abort Apply For Clearance, because the agent will invoke the abort-method for the subtask appropriately, as we outline below. The assumption here is that, like the failure-method, the programmer of the agent system has the opportunity to specify a sensible abort-method that takes into consideration the point in the plan at which the abort is to be executed. For any plan, the abort-method is optional: if no abort-method is specified, the agent takes no specific action for this plan. However, the agent\"s default behavioural rules still apply, for example, whether to retry an alternate plan for the parent task. Note that an explicit representation of the clean-up methods for tasks is not required, since tasks are performed by executing some plan or plans. Hence, aborting a task means aborting the current plan that is executed to perform that task, as we next describe. Abort Method Invocation We now informally lay out the agent\"s action upon aborting plans and tasks. When a plan P is aborted: 1. Abort each sub-task that is an active child of P. An active child is one that was triggered by P and is currently in execution. 2. When there are no more active children, invoke the abort method of plan P. 3. Indicate a plan failure to TP , the parent task of P. We note here that if the parent task TP is not to be aborted then the agent may choose another applicable plan to satisfy TP . 10 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) When a task (or sub-task) T is aborted: 1. Abort the current active plan to satisfy T (if any). 2. When there are no more active child processes, drop the task. The agent thus no longer pursues T. 3. Note here that when the current active plan for performing T is aborted, no other applicable plans to perform T should be tried as it is the task that is to be aborted. In order to prevent infinitely cascading clean-up efforts, we assume that abort-methods will never be aborted nor fail. In reality, however, an abort-method may fail. In this case, lacking a more sophisticated handling mechanism, the agent simply stops executing the failed abort-method with no further deliberation. The assumption we make is thus not a reflection of the full complexity of reality, but one that is pragmatic in terms of the agent execution cycle; the approach to failure-handling of makes the same assumption. In systems such as SPARK, the programmer can specify an alternative behaviour for a failed failure- or abort-method by means of meta-level procedures. We also assume that failure- and abort-methods terminate in finite time. 4. OPERATIONAL SEMANTICS We provide the semantics for the task and plan failure and aborting processes outlined above. We use the CAN language initially defined in and later extended as CANPLAN in to include a planning component and then as CANPLAN2 in to improve the goal adoption and dropping mechanisms. The extensions also simplified the semantics in the earlier work. We use some of these simplifications for providing a brief summary of the CAN language in Section 4.1. Following a presentation of the operational semantics of our approach in Section 4.2, in Section 4.3 we provide a worked example to clarify the semantics that we present. 4.1 CAN Language CAN is a high-level agent language, in a spirit similar to that of AgentSpeak and Kinny\"s \u03a8 , both of which attempt to extract the essence of a class of implemented BDI agent systems. CAN provides an explicit goal construct that captures both the declarative and procedural aspects of a goal. Goals are persistent in CAN in that, when a plan fails, another applicable plan is attempted. This equates to the default failure handling mechanism typically found in implemented BDI systems such as JACK . In practical systems, tasks are typically translated into events that trigger the execution of some plans. This is also true in the CAN language, but, in order to maintain the persistence of goals, a goal construct is introduced. This is denoted by Goal \u03c6s, P, \u03c6f , where \u03c6s is the success condition that determines when the goal is considered achieved, \u03c6f is a fail condition under which it is considered the goal is no longer achievable or relevant, and P is a program for achieving the goal, which will be aborted once \u03c6s or \u03c6f become true. An agent\"s behavior is specified by a plan library, denoted by \u03a0, that consists of a collection of plan clauses of the form e : c \u2190 P, where e is an event, c is a context condition (a logical formula over the agent\"s beliefs that must be true in order for the plan to be applicable)4 and P is the plan body. The plan body is a program that is defined recursively as follows: P ::= act | +b | \u2212b | ?\u03c6 | !e | P1; P2 | P1 P2 | Goal \u03c6s, P1, \u03c6f | P1 P2 | {\u03c81 : P1, . . . , \u03c8n : Pn} | nil An omitted c is equivalent to true. \u0394 = {\u03c8i\u03b8 : Pi\u03b8 | e : \u03c8i \u2190 Pi \u2208 \u03a0 \u2227 \u03b8 = mgu(e, e )} B, !e \u2212\u2192 B, \u0394 Event \u03c8i : Pi \u2208 \u0394 B |= \u03c8i B, \u0394 \u2212\u2192 B, Pi \u0394 \\ {\u03c8i : Pi} Select B, P1 \u2212\u2192 B, (P1 P2) \u2212\u2192 B, P2 fail B, P1 \u2212\u2192 B , P1 B, (P1; P2) \u2212\u2192 B , (P ; P2) Sequence B, P1 \u2212\u2192 B , P B, (P1 P2) \u2212\u2192 B , (P P2) Parallel1 B, P2 \u2212\u2192 B , P B, (P1 P2) \u2212\u2192 B , (P P1) Parallel2 Figure 1: Operational rules of CAN. where P1, . . . , Pn are themselves programs, act is a primitive action that is not further specified, and +b and \u2212b are operations to add and delete beliefs. The belief base contains ground belief atoms in the form of first-order relations but could be orthogonally extended to other logics. It is assumed that well-defined operations are provided to check whether a condition follows from a belief set (B |= c), to add a belief to a belief set (B \u222a {b}), and to delete a belief from a belief set (B \\ {b}). ?\u03c6 is a test for condition \u03c6, and !e5 is an event6 that is posted from within the program. The compound constructs are sequencing (P1; P2), parallel execution (P1 P2), and goals (Goal \u03c6s, P, \u03c6f ). The above defines the user language. In addition, a set of auxiliary compound forms are used internally when assigning semantics to constructs. nil is the basic (terminating) program. When an event matches a set of plan clauses these are collected into a set of guarded alternatives ( c1 : P1, . . . , cn : Pn ). The other auxiliary compound form, , is a choice operator dual to sequencing: P1 P2 executes P1 and then executes P2 only if P1 failed. A summary of the operational semantics for CAN in line with and following some of the simplifications of is as follows. A basic configuration S = B, G, \u0393 consists of the current belief base B of the agent, the current set of goals G being pursued (i.e., set of formulae), and the current program P being executed (i.e., the current intention). A transition S0 \u2212\u2192 S1 specifies that executing S0 for a single step yields configuration S1. S0 \u2212\u2192\u2217 Sn is the usual reflexive transitive closure of \u2212\u2192: Sn is the result of one or more singlestep transitions. A derivation rule S \u2212\u2192 Sr S \u2212\u2192 Sr consists of a (possibly empty) set of premises, which are transitions together with some auxiliary conditions (numerator), and a single transition conclusion derivable from these premises (denominator). Figure 1 gives some of the operational rules. The Event rule handles task events by collecting all relevant plan clauses for the event in question: for each plan clause e : \u03c8i \u2190 Pi, if there is a most general unifier, \u03b8 = mgu(e, e ) of e and the event in Where it is obvious that e is an event we will sometimes exclude the exclamation mark for readability. Typically an achievement goal. The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 11 B |= \u03c6s B, Goal \u03c6s, P, \u03c6f \u2212\u2192 B, true Gs B |= \u03c6f B, Goal \u03c6s, P, \u03c6f \u2212\u2192 B, fail Gf P = Goal \u03c6s, P , \u03c6f P = P1 \u00a3 P2 B |= \u03c6s \u2228 \u03c6f B, P \u2212\u2192 B, Goal \u03c6s, P \u00a3 P , \u03c6f \u00b4 GI P = P1 \u00a3 P2 B |= \u03c6s \u2228 \u03c6f B, P1 \u2212\u2192 B , P B, Goal \u03c6s, P, \u03c6f \u2212\u2192 B , Goal \u03c6s, P \u00a3 P2, \u03c6f \u00b4 GS P = P1 \u00a3 P2 B |= \u03c6s \u2228 \u03c6f P1 \u2208 {true, fail} B, Goal \u03c6s, P, \u03c6f \u2212\u2192 B, Goal \u03c6s, P2 \u00a3 P2, \u03c6f \u00b4 GR Figure 2: Rules for goals in CAN. question, then the rule constructs a guarded alternative \u03c8i\u03b8 : Pi\u03b8. The Select rule then selects one applicable plan body from a set of (remaining) relevant alternatives: program P \u0394 states that program P should be tried first, falling back to the remaining alternatives, \u0394 \\ P, if necessary. This rule and the fail rule together are used for failure handling: if the current program Pi from a plan clause for a task fails, rule fail is applied first, and then if possible, rule Select will choose another applicable alternative for the task if one exists. Rule Sequence handles sequencing of programs in the usual way. Rules Parallel1 and Parallel2 define the possible interleaving when executing two programs in parallel. Figure 2 gives simplified rules for dealing with goals, in line with those presented in . The first rule states that a goal succeeds when \u03c6s become true; the second rule states that a goal fails when \u03c6f become true. The third rule GI initializes the execution of a goal-program by updating the goal base and setting the program in the goal to P \u00a3 P; the first P is to be executed and the second P is used to keep track of the original program for the goal. The fourth rule GS executes a single step of the goal-program. The final rule GR restarts the original program (encoded as P2 of pair P1 \u00a3 P2) whenever the current program is finished but the desired and still possible goal has not yet been achieved. 4.2 Aborting Intentions and Handling Failure We next introduce the ability to specify handler programs, in the form of failure- and abort-methods, that deal with the clean-up required when a given program respectively fails or is aborted. We do not associate failure- and abort- methods with plan clauses or with tasks (events), but rather we introduce a new program construct that specifies failure- and abort- methods for an arbitrary program. The FAb(P, PF , PA) construct executes the program P. Should P fail, it executes the failure handling program PF ; should P need to be aborted, it executes the abort handling program PA. Thus to add failure- and abort- methods PF and PA to a plan clause e : c \u2190 P, we write e : c \u2190 FAb(P, PF , PA). With the introduction of the ability to abort programs, we modify the parallel construct to allow the failure of one branch to abort the other. We must take into consideration the possible existence of abort-methods in the aborted branch. Similarly, with the Goal construct we can no longer completely abandon the program the goal contains as soon as the success or failure condition holds; we must now take into consideration the existence of any abort-methods applicable to the program. We provide the semantics of an augmented agent language containing the FAb construct by defining a source transformation, similar to macro-expansion, that maps a plan library containing the FAb(P, PF , PA) construct into (almost) standard CAN. The one non-standard extension to CAN is a wait-until-condition construct. We explain this simple modification of the parallel construct below when we come to translation of the Goal construct. First we describe the general nature of the source transformation, which proves to be quite simple for most of the language constructs, and then we concentrate on the three more complex cases: the FAb, parallel, and Goal constructs. A key issue is that the FAb constructs may be nested, either directly or indirectly. Let us call each instantiation of the construct at execution time a possible abort point (pap). Where these constructs are nested, it is important that before the failure- or abort-method of a parent pap is executed, the failure- or abort-methods programs of the children paps are executed first, as described earlier in Section 3. The need to coordinate the execution of the abort-methods of nested paps requires that there be some way to identify the parents and children of a particular pap. We achieve this as part of the source transformation by explicitly keeping track of the context of execution as an extra parameter on the events and an extra variable within each plan body.7 The source transformation replaces each plan clause of the form e : c \u2190 P with a plan clause e(v) : c \u2190 \u03bcv(P) where v is a free variable, not previously present in the plan clause. This variable is used to keep track of the context of execution. The value of the context variable is a list of identifiers, where each new pap is represented by prepending a new identifier to the context. For example, if the identifiers are integers, the context of one pap may be represented by a list and the context introduced by a new pap may be represented by . We will refer to paps by the context rather than by the new identifier added, e.g., by not 51. This enables us to equate the ancestor relationship between paps with the list suffix relationship on the relevant contexts, i.e., v is an ancestor of v if and only if v is a suffix of v . For most CAN constructs, the context variable is unused or passed unchanged: \u03bcv(act) = act \u03bcv(+b) = +b \u03bcv(\u2212b) = \u2212b \u03bcv(nil) = nil \u03bcv(!e) = !e(v) \u03bcv(P1; P2) = \u03bcv(P1); \u03bcv(P2) \u03bcv(P1 P2) = \u03bcv(P1) \u03bcv(P2) \u03bcv( \u03c81 : P1, . . . , \u03c8n : Pn ) = \u03c81 : \u03bcv(P1), . . . , \u03c8n : \u03bcv(Pn) It remains to specify the transformation \u03bcv(\u00b7) in three cases: the FAb, parallel, and Goal constructs. These are more complex in that the transformed source needs to create a new pap identifier dynamically, for use as a new context within the construct, and to keep track of when the pap is active (i.e., currently in execution) by adding and removing beliefs about the context. Let us introduce the primitive action prependID(v, v ) that creates a new pap identifier and prepends it to list v giving list v . We also introduce the following predicates: \u2022 a(v) - the pap v is currently active. \u2022 abort(v) - the pap v should be aborted (after aborting all of its descendants). An alternative would be to use meta-level predicates that reflect the current state of the intention structure. 12 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) \u2022 f(v) - the program of pap v has failed. \u2022 ancestorof(v, v ) \u2261 v = v \u2228 ancestorof(v, tail(v ))the pap v is an ancestor of pap v . \u2022 nac(v) \u2261 \u00ac\u2203v .(a(v ) \u2227 ancestorof(v, v ) \u2227 v = v ) - v has no active children. \u2022 sa(v) \u2261 \u2203v .abort(v ) \u2227 ancestorof(v , v) - we should abort v, i.e., abort is true of v or some ancestor; however, we need to wait until no children of v are active. \u2022 san(v) \u2261 sa(v) \u2227 nac(v) - we should abort v now if we should abort v and v has no active children. First let us consider the case of the FAb construct. The idea is that, whenever a new pap occurs, the prependID(v, v ) action is used to create a new pap identifier list v from the existing list v. We then add the belief that v is the active context, i.e., +a(v ), and start processing the program within the pap using v instead of v as the context. We need to make sure that we retract the belief that v is active at the end, i.e., \u2212a(v ). We use the Goal construct to allow us to drop the execution of a program within a pap v when it is necessary to abort. While executing the program P, we know that we need to drop P and invoke its abort-method if some ancestor of P has been told to abort. This is represented by the predicate sa(v ) being true. However, we need to make sure that we do this only after every child pap has had the chance to invoke its abort-method and all these abort-methods have completed: if we drop the program too soon, then execution of the abort-methods of the children will also be dropped. Therefore, the condition we actually use in the Goal construct to test when to drop the program is san(v ). This condition relies on the fact that as the children paps complete, they remove the relevant a facts. Our use of the Goal construct is for its ability to drop the execution of a program when conditions are met. To leave aside the repeat execution until a condition is met aspect, we must ensure that the success or failure condition of the construct is satisfied once the execution of the program succeeds or fails. We make sure of this by retracting the fact a(v ) on success and asserting the fact f(v ) on failure, and by having the appropriate success and failure conditions on the Goal. Hence, if the Goal construct fails, then the program either was aborted or it failed. We invoke the relevant failure- or abort- method, retract the a(v ) fact, and then fail. Putting all this together, we formally define \u03bcv(FAb(P, PA, PF )) to be the following, where v is a new variable distinct from any other in the agent\"s plan library: prependID(v, v ); +a(v ); Goal ( \u00aca(v ), (\u03bcv (P); \u2212a(v ) +f(v )), san(v ) \u2228 f(v ) ) (((?sa(v ); \u03bcv(PA)) \u03bcv(PF )); \u2212a(v ); ?false) Second, we must transform the parallel operator to ensure that the failure of one branch safely aborts the other. Here we construct two new contexts, v and v , from the existing context v. If one branch fails, it must abort the other branch. At the end, if either branch was aborted, then we must fail. Let v and v be new variables distinct from any other in the agent\"s plan library. We define \u03bcv(P1 P2) to be: prependID(v, v ); prependID(v, v ); +a(v ); +a(v ); ( Goal (\u00aca(v ), (\u03bcv (P1); \u2212a(v ) +f(v )), san(v ) \u2228 f(v ) ) (+abort(v ); \u2212a(v )) Goal (\u00aca(v ), (\u03bcv (P2); \u2212a(v ) +f(v )), san(v ) \u2228 f(v ) ) (+abort(v ); \u2212a(v )) ); ?\u00acabort(v ) \u2227 \u00acabort(v ) Finally, we need to modify occurrences of the Goal construct in two ways: first, to make sure that the abort handling methods are not bypassed when the success or failure conditions are satisfied, and second, to trigger the aborting of the contained program when either the success or failure conditions are satisfied. To transform the Goal construct we need to extend standard CAN with a wait-until-condition construct. The construct \u03c6 : P does not execute P until \u03c6 becomes true. We augment the CAN language with the following rules for the guard operator \u2018:\": B |= \u03c6 B, G, (\u03c6 : P \u2212\u2192 B, G, P :true B |= \u03c6 B, G, (\u03c6 : P) \u2212\u2192 B, G, (\u03c6 : P) :false In order to specify \u03bcv(Goal \u03c6s, P, \u03c6f ), we generate a new pap and execute the program within the Goal construct in this new context. We must ensure that belief a(v ) is removed whether the Goal succeeds or fails. We shift the success and failure condition of the Goal construct into a parallel branch using the wait-until-condition construct, and modify the Goal to use the should abort now condition san(v ) as the success condition. The waiting branch will trigger the abort of the program should either the success or failure condition be met. To avoid any problems with terminating the wait condition, we also end the wait if the pap is no longer active. Let v be a new variable distinct from any other in the agent\"s plan library. We define \u03bcv(Goal \u03c6s, P, \u03c6f ) to be: prependID(v, v ); +a(v ); ( Goal ( san(v ), \u03bcv (P), false) ; \u2212a(v ); ?\u03c6s ) \u03c6s \u2228 \u03c6f \u2228 \u00aca(v ) : +abort(v ) ) The program P will be repeatedly executed until san(v ) becomes true. There are two ways this can occur. First, if either the success condition \u03c6s or the failure condition \u03c6f becomes true, then the second branch of the parallel construct executes. This causes abort(v ) to become true, and, after the descendant paps\" abortmethods are executed, san(v ) becomes true. In this case, P is now dropped, the a(v ) is removed, and the entire construct succeeds or fails based on \u03c6s. The second way for san(v ) to become true is if v or one of its ancestors is aborted. In this case, once the descendant paps\" abort-methods are executed, san(v ) becomes true, P is dropped, the a(v ) belief is removed (allowing the second parallel branch to execute, vacuously instructing v to abort), and the first parallel branch fails (assuming \u03c6s is false). 4.3 Worked Example Let us look at translation of the IJCAI submission example of Section 2. We will express tasks by events, for example, the task Allocate a Paper Number we express as the event APN. Let the output of the Apply For Clearance task be Y or N, indicating the approval or not of Alice\"s manager, respectively. Then we have (at least) the following two plan clauses in CAN, for the Support Meeting Submission and Apply For Clearance tasks, respectively: SMS(m) : isconf(m) \u2190 FAb(!APN; !TWA; (!AFC !TWP); !HPS, !CPN, !CPN) AFC : true \u2190 FAb(!SCR; !WFR(r); ?r = Y, nil, !CCR) Note that Support Meeting Submission has a parameter m, the meeting of interest (IJCAI, in our example), while Apply For Clearance has no parameters. The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 13 Let us look first at the translation of the second plan clause, for AFC, since it is the simpler of the two. Let v and v denote new variables. Then we have as the translated plan clause: AFC(v ) : true \u2190 prependID(v , v ); +a(v ); Goal ( \u00aca(v ), (!SCR(v ); !WFR(r, v ); ?r = Y; \u2212a(v ) +f(v )), san(v ) \u2228 f(v ) ) (((?sa(v ); !CCR(v )) nil); \u2212a(v ); ?false) We can see that an extra context parameter has been added to each task and that the old plan body now appears inside a Goal construct. Should the old plan body succeed, belief a(v ) is retracted, causing the Goal to succeed. If the old plan body fails, or if the task is to be aborted, the Goal construct fails. This is followed by the execution of CCR (in the case of an abort), the retraction of a(v ), and failure. The translation of the first plan clause, for SMS, is more complex, because of the parallel construct that introduces nested paps: SMS(m, v) : isconf(m) \u2190 prependID(v, v ); +a(v ); Goal ( \u00aca(v ), ((!APN(v ); !TWA(v ); prependID(v , v ); prependID(v , v ); +a(v ); +a(v ); ( Goal ( \u00aca(v ), (!AFC(v ); \u2212a(v ) +f(v )), san(v ) \u2228 f(v ) ) (+abort(v ); \u2212a(v )) Goal ( \u00aca(v ), (!TWP(v ); \u2212a(v ) +f(v )), san(v ) \u2228 f(v ) ) (+abort(v ); \u2212a(v )) ) ; ?\u00acabort(v ) \u2227 \u00acabort(v ); !HPS(v ); \u2212a(v )) +f(v )), san(v ) \u2228 f(v ) ) (((?sa(v ); !CPN(v)) !CPN(v)); \u2212a(v ); ?false) Here we can see that if the task !TWP(v ) fails then f(v ) will be asserted, failing the Goal construct that contains it, and leading to abort(v ) being asserted. If the !WFR(r, v ) task in the expansion of !AFC(v ) is still executing and has no active child paps, then sa(v ) and sa(v ) will be true; however, only san(v ) and not san(v ) will be true. This set of conditions will cause the Goal construct in the first plan clause to fail, dropping execution of !WFR(r, v ). The task !CCR(v ) will be executed. Once this task completes, belief a(v ) is retracted, causing san(v ) to become true, leading to the first Goal construct of the second plan clause to fail. While the translated plan clauses appear complicated, observe that the translation from the initial plan clauses is entirely automated, according to the rules set out in Section 4.2. The translated plan clauses, with the semantics of CAN augmented by our waituntil-condition construct, thus specify the operation of the agent to handle both failure and aborting for the example. 5. RELATED WORK Plan failure is handled in the extended version of AgentSpeak found in the Jason system . Failure clean-up plans are triggered from goal deletion events \u2212!g. Such plans, similar to our failure methods, are designed for the agent to effect state changes (act to undo its earlier actions) prior to possibly attempting another plan to achieve the failed goal g. Given Jason\"s constructs for dropping a goal with an indication of whether or not to try an alternate plan for it, H\u00a8ubner et al. provide an informal description of how a Jason agent modifies its intention structure when a goal failure event occurs. In a goal deletion plan, the programmer can specify any undo actions and whether to attempt the goal again. If no goal deletion plan is provided, Jason\"s default behaviour is to not reattempt the goal. Failure handling is applied only to plans triggered by addition of an achievement or test goal; in particular, goal deletion events are not posted for failure of a goal deletion plan. Further, the informal semantics of do not consider parallel sub-goals (i.e., the CAN construct), since such execution is not part of Jason\"s language. The implementation of H\u00a8ubner et al. requires Jason\"s internal actions. A requirement for implementing our approach is a reflective capability in the BDI agent implementation. Suitable implementations of the BDI formalism are JACK , Jadex , and SPARK . All three allow meta level methods that are cued by meta events such as goal adoption or plan failure, and offer introspective capabilities over goal and intention states. Such meta level facilities are also required by the approach of Unruh et al. , who define goal-based semantic compensation for an agent. Failure-handling goals are invoked according to failurehandling strategy rules, by a dedicated agent Failure Handling Component (FHC) that tracks task execution. These goals are specified by the agent programmer and attached to tasks, much like our FAb(P, PF , PA) construct associates failure and abort methods with a plan P. Note, however, that in contrast to both and our semantics, attach the failure-handling knowledge at the goal, not plan, level. Their failure-handling goals may consist of stabilization goals that perform localized, immediate clean-up to restore the agent\"s state to a known, stable state, and compensation goals that perform undo actions. Compensation goals are triggered on aborting a goal, and so not necessarily on goal failure (i.e., if the FHC directs the agent to retry the failed goal and the retry is successful). The FHC approach is defined at the goal level in order to facilitate abstract specification of failure-handling knowledge; the FHC decides when to address a failure and what to do (i.e., what failurehandling goals to invoke), separating this knowledge from the how of implementing corrective actions (i.e., what plan to execute to meet the adopted failure-handling goal). This contrasts with simplistic plan-level failure handling in which the what and how are intermingled in domain task knowledge. While our approach is defined at the plan level, our extended BDI semantics provides for the separation of execution and failure handling. Further, the FHC explicitly maintains data structures to track agent execution. We leverage the existing execution structures and self-reflective ability of a BDI agent to accomplish both aborting and failure handling without additional overhead. FHC\"s failure-handling strategy rules (e.g., whether to retry a failed goal) are replaced by instructions in our PF and PA plans, together with meta-level default failure handlers according to the agent\"s nature (e.g., blindly committed). The FHC approach is independent of the architecture of the agent itself, in contrast to our work that is dedicated to the BDI formalism (although not tied to any one agent system). Thus no formal semantics are developed in ; the FHC\"s operation is given as 14 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) a state-based protocol. This approach, together with state checkpointing, is used for multi-agent systems in . The resulting architecture embeds their failure handling approach within a pair processing architecture for agent crash recovery. Other work on multi-agent exception handling includes AOEX\"s distributed exception handling agents , and the similar sentinels of . In both cases, failure-handling logic and knowledge are decoupled from the agents; by contrast, while separating exception handling from domain-specific knowledge, Unruh et al.\"s FHC and our approach both retain failure-handling logic within an agent. 6. CONCLUSION AND FUTURE WORK The tasks and plans of an agent may not successfully reach completion, either by the choice of the agent to abort them (perhaps at the request of another agent to do so), or by unbidden factors that lead to failure. In this paper we have presented a procedure-based approach that incorporates aborting tasks and plans into the deliberation cycle of a BDI-style agent, thus providing a unified approach to failure and abort. Our primary contribution is an analysis of the requirements on the operation of the agent for aborting tasks and plans, and a corresponding operational semantics for aborting in the abstract agent language CAN. We are planning to implement an instance of our approach in the SPARK agent system ; in particular, the work of this paper will be the basis for SPARK\"s abort handling mechanism. We are also developing an analysis tool for our extended version of CAN as a basis for experimentation. An intelligent agent will not only gracefully handle unsuccessful tasks and plans, but also will deliberate over its cognitive attitudes to decide its next course of action. We have assumed the default behaviour of a BDI-style agent, according to its nature: for instance, to retry alternatives to a failed plan until one succeeds or until no alternative plans remain (in which case to fail the task). Future work is to place our approach in service of more dynamic agent reasoning, such as the introspection that an agent capable of reasoning over task interaction effects and resource requirements can accomplish . Related to this is determining the cost of aborting a task or plan, and using this as an input to the deliberation process. This would in particular influence the commitment the agent has towards a particular task: the higher the cost, the greater the commitment. Our assumption that abort-methods do not fail, as discussed above, is a pragmatic one. However, this is an issue worthy of further exploration, either to develop weaker assumptions that are also practical, or to analyze conditions under which our assumption is realistic. A further item of interest is extending our approach to failure and abort to maintenance goals . For such goals a different operational semantics for abort is necessary than for achievement goals, to match the difference in semantics of the goals themselves.", "body1": "Intelligent agents generally work in complex, dynamic environments, such as air traffic control or robot navigation, in which the success of any particular action or plan cannot be guaranteed . In agent architectures inspired by the Belief-Desire-Intention (BDI) model , these properties are often characterized by the interactions between beliefs, goals, and plans .1 In general, an agent that wishes to achieve a particular set of tasks will pursue a number of plans concurrently. Besides dealing with failure, an important capability of an intelligent agent is to be able to abort a particular task or plan. Aborting a task or plan is distinct from its failure. However, there has been relatively little work on the development of abort techniques beyond simple dropping of currently intended plans and tasks, which does not deal with the clean-up required. As one consequence, most agent systems are quite limited in their treatment of the situation where one branch of a parallel construct One can consider both tasks to be performed and goals to achieve a certain state of the world. We adopt the latter view and use task to also refer to goals. 978-81--7-5 (RPS) IFAAMAS fails (common approaches include either letting the other branch run to completion unhindered or dropping it completely). In this paper we discuss in detail the incorporation of abort cleanup methods into the agent execution cycle, providing a unified approach to failure and abort. This paper is organized as follows. Alice is a knowledge worker assisted by a learning, personal assistive agent such as CALO . 3. 4. 5. These steps must be performed in order, with the exception of steps 3 (AFC) and 4 (TWP), which may be performed in parallel. Similarly, CALO can perform the task Apply For Clearance (AFC) by a plan consisting of: 1. 2. Now suppose that a change in circumstances causes Alice to reconsider her travel plans while she is writing the paper. We note a number of important observations from the example. Second, once the decision is made to abort the attempt to submit a paper, there are some actions the agent should take, such as cancelling the clearance request. Third, there is a distinction between aborting a task and aborting a plan. As we have alluded to, failure and aborting are related concepts. Failure and aborting, however, differ in the way they arise. Each plan clause has a plan body, which is a program (i.e., combination of primitive actions, sub-tasks, etc.) The Sixth Intl. The agent\"s action upon plan failure depends on its nature: for example, the agent may declare the task to have failed if one plan has been tried and resulted in failure, or it may retry alternate plans and declare (indeed, must declare) task failure only if all possible alternate plans to perform the task have been tried and resulted in failure. Our approach associates an abort-method with each plan. In order to understand how the agent\"s abort processing should function, we consider three situations where it is sensible for an agent to consider aborting some of its tasks and plans: 1. 3. The operational semantics we provide in Section 4 for aborting tasks and plans captures the first two situations above. Abort Method Representation The intent of aborting a task or plan is that the task or plan and all its children cease to execute, and that appropriate clean-up methods are performed as required. In our example, the abort-method for the plan for Support Meeting Submission consists of invoking the sub-task Cancel Paper Number. The assumption here is that, like the failure-method, the programmer of the agent system has the opportunity to specify a sensible abort-method that takes into consideration the point in the plan at which the abort is to be executed. Abort Method Invocation We now informally lay out the agent\"s action upon aborting plans and tasks. 10 The Sixth Intl. The agent thus no longer pursues T. In order to prevent infinitely cascading clean-up efforts, we assume that abort-methods will never be aborted nor fail. We provide the semantics for the task and plan failure and aborting processes outlined above. 4.1 CAN Language CAN is a high-level agent language, in a spirit similar to that of AgentSpeak and Kinny\"s \u03a8 , both of which attempt to extract the essence of a class of implemented BDI agent systems. In practical systems, tasks are typically translated into events that trigger the execution of some plans. An agent\"s behavior is specified by a plan library, denoted by \u03a0, that consists of a collection of plan clauses of the form e : c \u2190 P, where e is an event, c is a context condition (a logical formula over the agent\"s beliefs that must be true in order for the plan to be applicable)4 and P is the plan body. where P1, . The above defines the user language. A basic configuration S = B, G, \u0393 consists of the current belief base B of the agent, the current set of goals G being pursued (i.e., set of formulae), and the current program P being executed (i.e., the current intention). A transition S0 \u2212\u2192 S1 specifies that executing S0 for a single step yields configuration S1. Typically an achievement goal. The Select rule then selects one applicable plan body from a set of (remaining) relevant alternatives: program P \u0394 states that program P should be tried first, falling back to the remaining alternatives, \u0394 \\ P, if necessary. Figure 2 gives simplified rules for dealing with goals, in line with those presented in . 4.2 Aborting Intentions and Handling Failure We next introduce the ability to specify handler programs, in the form of failure- and abort-methods, that deal with the clean-up required when a given program respectively fails or is aborted. With the introduction of the ability to abort programs, we modify the parallel construct to allow the failure of one branch to abort the other. We explain this simple modification of the parallel construct below when we come to translation of the Goal construct. A key issue is that the FAb constructs may be nested, either directly or indirectly. The value of the context variable is a list of identifiers, where each new pap is represented by prepending a new identifier to the context. For most CAN constructs, the context variable is unused or passed unchanged: \u03bcv(act) = act \u03bcv(+b) = +b \u03bcv(\u2212b) = \u2212b \u03bcv(nil) = nil \u03bcv(!e) = !e(v) \u03bcv(P1; P2) = \u03bcv(P1); \u03bcv(P2) \u03bcv(P1 P2) = \u03bcv(P1) \u03bcv(P2) \u03bcv( \u03c81 : P1, . \u2022 abort(v) - the pap v should be aborted (after aborting all of its descendants). An alternative would be to use meta-level predicates that reflect the current state of the intention structure. 12 The Sixth Intl. \u2022 nac(v) \u2261 \u00ac\u2203v . \u2022 sa(v) \u2261 \u2203v .abort(v ) \u2227 ancestorof(v , v) - we should abort v, i.e., abort is true of v or some ancestor; however, we need to wait until no children of v are active. \u2022 san(v) \u2261 sa(v) \u2227 nac(v) - we should abort v now if we should abort v and v has no active children. First let us consider the case of the FAb construct. We then add the belief that v is the active context, i.e., +a(v ), and start processing the program within the pap using v instead of v as the context. We use the Goal construct to allow us to drop the execution of a program within a pap v when it is necessary to abort. This is represented by the predicate sa(v ) being true. Our use of the Goal construct is for its ability to drop the execution of a program when conditions are met. Putting all this together, we formally define \u03bcv(FAb(P, PA, PF )) to be the following, where v is a new variable distinct from any other in the agent\"s plan library: prependID(v, v ); +a(v ); Goal ( \u00aca(v ), (\u03bcv (P); \u2212a(v ) +f(v )), san(v ) \u2228 f(v ) ) (((?sa(v ); \u03bcv(PA)) \u03bcv(PF )); \u2212a(v ); ?false) Second, we must transform the parallel operator to ensure that the failure of one branch safely aborts the other. To transform the Goal construct we need to extend standard CAN with a wait-until-condition construct. Let v be a new variable distinct from any other in the agent\"s plan library. 4.3 Worked Example Let us look at translation of the IJCAI submission example of Section 2. The translation of the first plan clause, for SMS, is more complex, because of the parallel construct that introduces nested paps: SMS(m, v) : isconf(m) \u2190 prependID(v, v ); +a(v ); Goal ( \u00aca(v ), ((!APN(v ); !TWA(v ); prependID(v , v ); prependID(v , v ); +a(v ); +a(v ); ( Goal ( \u00aca(v ), (!AFC(v ); \u2212a(v ) +f(v )), san(v ) \u2228 f(v ) ) (+abort(v ); \u2212a(v )) Goal ( \u00aca(v ), (!TWP(v ); \u2212a(v ) +f(v )), san(v ) \u2228 f(v ) ) (+abort(v ); \u2212a(v )) ) ; ?\u00acabort(v ) \u2227 \u00acabort(v ); !HPS(v ); \u2212a(v )) +f(v )), san(v ) \u2228 f(v ) ) (((?sa(v ); !CPN(v)) !CPN(v)); \u2212a(v ); ?false) Here we can see that if the task !TWP(v ) fails then f(v ) will be asserted, failing the Goal construct that contains it, and leading to abort(v ) being asserted. Plan failure is handled in the extended version of AgentSpeak found in the Jason system . Given Jason\"s constructs for dropping a goal with an indication of whether or not to try an alternate plan for it, H\u00a8ubner et al. The implementation of H\u00a8ubner et al. Such meta level facilities are also required by the approach of Unruh et al. The FHC approach is defined at the goal level in order to facilitate abstract specification of failure-handling knowledge; the FHC decides when to address a failure and what to do (i.e., what failurehandling goals to invoke), separating this knowledge from the how of implementing corrective actions (i.e., what plan to execute to meet the adopted failure-handling goal). The FHC approach is independent of the architecture of the agent itself, in contrast to our work that is dedicated to the BDI formalism (although not tied to any one agent system). Other work on multi-agent exception handling includes AOEX\"s distributed exception handling agents , and the similar sentinels of .", "body2": "Accordingly, dealing with failure is fundamental to agent programming, and is an important element of agent characteristics such as robustness, flexibility, and persistence . Given this need for deliberation about failed tasks or plans, failure deliberation is commonly built into the agent\"s execution cycle. This decision may be due to an internal deliberation (such as the agent believing the task can no longer be achieved, or that some conflicting task now has a higher priority) or due to an external factor (such as another agent altering a commitment, or a change in the environment). There has been a considerable amount of work on plan failures (such as detecting and resolving resource conflicts ) and most agent systems incorporate some notion of failure handling. However, there has been relatively little work on the development of abort techniques beyond simple dropping of currently intended plans and tasks, which does not deal with the clean-up required. A task can be considered a goal of achieving the state of the task having been performed, and a goal can be considered a task of bringing about that state of the world. We adopt the latter view and use task to also refer to goals. 978-81--7-5 (RPS) IFAAMAS fails (common approaches include either letting the other branch run to completion unhindered or dropping it completely). Our focus is on a single agent, complementary to related work that considers exception handling for single- and multiagent systems (e.g., ). Section 5 discusses related work, and in Section 6 we present our conclusions and future work. Track Writing Abstract (TWA): keep track of Alice\"s progress in preparing an abstract. Apply For Clearance (AFC) for publication from Alice\"s manager based on the abstract and conference details. Track Writing Paper (TWP): keep track of Alice\"s progress in writing the paper. Handle Paper Submission (HPS): follow company internal procedures for submitting a paper to a conference. These steps must be performed in order, with the exception of steps 3 (AFC) and 4 (TWP), which may be performed in parallel. Send Clearance Request (SCR) to Alice\"s manager. Wait For Response (WFR) from the manager. Confirm that the response was positive, and fail otherwise. Aborting the first plan requires CALO to notify the paper number registry that the allocated paper number is obsolete, which it can achieve by the Cancel Paper Number (CPN) task.2 Aborting the second plan requires CALO to notify Alice\"s manager that Alice no longer requires clearance for publication, which CALO can achieve by invoking the Cancel Clearance Request (CCR) task. Hence, our objective is to determine how to incorporate aborting mechanisms into the standard execution cycle rather than determine what should be aborted and when. Rather, cleaning up involves compensation via forward recovery actions . This is the source of one of the technical challenges that we address: determining the precise sequence of actions once a parent task or plan is aborted. They both cause the execution of existing plans to cease and, consequentially, the agent to reflect over its current tasks and intentions. In BDI-style systems such as JACK and SPARK, an agent\"s domain knowledge includes a pre-defined plan library of plan clauses. Plans may have sub-tasks that must succeed CALO needs only drop the TWA and TWP tasks to abort them: for the sake of simplicity we suppose no explicit clean up of its internal state is necessary. In such systems, a plan failure occurs if one of the actions or sub-tasks within the plan fails. Moreover, task failure can also arise separately from plan failure, if the agent decides to abort the task. We choose the former because: (1) action-level abort-methods would incur a greater overhead, (2) plans are meant to be designed as single cohesive units and are the unit of deliberation in BDI systems, and (3) the cleanup methods for failure in current systems are attached to plans. The failure of the Track Writing Paper task should cause the Apply For Clearance task being executed in parallel to be aborted. Given that the new task has greater importance, a rational agent will evaluate its best course of action and may decide to abortor at least suspend - the existing task of submitting a paper and intentions derived from it . Different forms of reasoning apply to other goal types such as maintenance goals , where the goal is satisfied by maintaining a state of the world for some period of time (e.g., maintain $100 in cash). To this, we add the abort-method, which is invoked if the plan is to be aborted. The abort-method need not explicitly abort Apply For Clearance, because the agent will invoke the abort-method for the subtask appropriately, as we outline below. Hence, aborting a task means aborting the current plan that is executed to perform that task, as we next describe. When there are no more active children, invoke the abort method of plan P. Indicate a plan failure to TP , the parent task of P. We note here that if the parent task TP is not to be aborted then the agent may choose another applicable plan to satisfy TP . When there are no more active child processes, drop the task. The agent thus no longer pursues T. Note here that when the current active plan for performing T is aborted, no other applicable plans to perform T should be tried as it is the task that is to be aborted. We also assume that failure- and abort-methods terminate in finite time. Following a presentation of the operational semantics of our approach in Section 4.2, in Section 4.3 we provide a worked example to clarify the semantics that we present. This equates to the default failure handling mechanism typically found in implemented BDI systems such as JACK . This is denoted by Goal \u03c6s, P, \u03c6f , where \u03c6s is the success condition that determines when the goal is considered achieved, \u03c6f is a fail condition under which it is considered the goal is no longer achievable or relevant, and P is a program for achieving the goal, which will be aborted once \u03c6s or \u03c6f become true. \u0394 = {\u03c8i\u03b8 : Pi\u03b8 | e : \u03c8i \u2190 Pi \u2208 \u03a0 \u2227 \u03b8 = mgu(e, e )} B, !e \u2212\u2192 B, \u0394 Event \u03c8i : Pi \u2208 \u0394 B |= \u03c8i B, \u0394 \u2212\u2192 B, Pi \u0394 \\ {\u03c8i : Pi} Select B, P1 \u2212\u2192 B, (P1 P2) \u2212\u2192 B, P2 fail B, P1 \u2212\u2192 B , P1 B, (P1; P2) \u2212\u2192 B , (P ; P2) Sequence B, P1 \u2212\u2192 B , P B, (P1 P2) \u2212\u2192 B , (P P2) Parallel1 B, P2 \u2212\u2192 B , P B, (P1 P2) \u2212\u2192 B , (P P1) Parallel2 Figure 1: Operational rules of CAN. The compound constructs are sequencing (P1; P2), parallel execution (P1 P2), and goals (Goal \u03c6s, P, \u03c6f ). A summary of the operational semantics for CAN in line with and following some of the simplifications of is as follows. A basic configuration S = B, G, \u0393 consists of the current belief base B of the agent, the current set of goals G being pursued (i.e., set of formulae), and the current program P being executed (i.e., the current intention). The Event rule handles task events by collecting all relevant plan clauses for the event in question: for each plan clause e : \u03c8i \u2190 Pi, if there is a most general unifier, \u03b8 = mgu(e, e ) of e and the event in Where it is obvious that e is an event we will sometimes exclude the exclamation mark for readability. Typically an achievement goal. question, then the rule constructs a guarded alternative \u03c8i\u03b8 : Pi\u03b8. Rules Parallel1 and Parallel2 define the possible interleaving when executing two programs in parallel. The final rule GR restarts the original program (encoded as P2 of pair P1 \u00a3 P2) whenever the current program is finished but the desired and still possible goal has not yet been achieved. Thus to add failure- and abort- methods PF and PA to a plan clause e : c \u2190 P, we write e : c \u2190 FAb(P, PF , PA). The one non-standard extension to CAN is a wait-until-condition construct. First we describe the general nature of the source transformation, which proves to be quite simple for most of the language constructs, and then we concentrate on the three more complex cases: the FAb, parallel, and Goal constructs. This variable is used to keep track of the context of execution. This enables us to equate the ancestor relationship between paps with the list suffix relationship on the relevant contexts, i.e., v is an ancestor of v if and only if v is a suffix of v . We also introduce the following predicates: \u2022 a(v) - the pap v is currently active. \u2022 abort(v) - the pap v should be aborted (after aborting all of its descendants). An alternative would be to use meta-level predicates that reflect the current state of the intention structure. \u2022 ancestorof(v, v ) \u2261 v = v \u2228 ancestorof(v, tail(v ))the pap v is an ancestor of pap v . (a(v ) \u2227 ancestorof(v, v ) \u2227 v = v ) - v has no active children. \u2022 sa(v) \u2261 \u2203v .abort(v ) \u2227 ancestorof(v , v) - we should abort v, i.e., abort is true of v or some ancestor; however, we need to wait until no children of v are active. \u2022 san(v) \u2261 sa(v) \u2227 nac(v) - we should abort v now if we should abort v and v has no active children. The idea is that, whenever a new pap occurs, the prependID(v, v ) action is used to create a new pap identifier list v from the existing list v. We need to make sure that we retract the belief that v is active at the end, i.e., \u2212a(v ). While executing the program P, we know that we need to drop P and invoke its abort-method if some ancestor of P has been told to abort. This condition relies on the fact that as the children paps complete, they remove the relevant a facts. We invoke the relevant failure- or abort- method, retract the a(v ) fact, and then fail. We define \u03bcv(P1 P2) to be: prependID(v, v ); prependID(v, v ); +a(v ); +a(v ); ( Goal (\u00aca(v ), (\u03bcv (P1); \u2212a(v ) +f(v )), san(v ) \u2228 f(v ) ) (+abort(v ); \u2212a(v )) Goal (\u00aca(v ), (\u03bcv (P2); \u2212a(v ) +f(v )), san(v ) \u2228 f(v ) ) (+abort(v ); \u2212a(v )) ); ?\u00acabort(v ) \u2227 \u00acabort(v ) Finally, we need to modify occurrences of the Goal construct in two ways: first, to make sure that the abort handling methods are not bypassed when the success or failure conditions are satisfied, and second, to trigger the aborting of the contained program when either the success or failure conditions are satisfied. To avoid any problems with terminating the wait condition, we also end the wait if the pap is no longer active. In this case, once the descendant paps\" abort-methods are executed, san(v ) becomes true, P is dropped, the a(v ) belief is removed (allowing the second parallel branch to execute, vacuously instructing v to abort), and the first parallel branch fails (assuming \u03c6s is false). Then we have (at least) the following two plan clauses in CAN, for the Support Meeting Submission and Apply For Clearance tasks, respectively: SMS(m) : isconf(m) \u2190 FAb(!APN; !TWA; (!AFC !TWP); !HPS, !CPN, !CPN) AFC : true \u2190 FAb(!SCR; !WFR(r); ?r = Y, nil, !CCR) Note that Support Meeting Submission has a parameter m, the meeting of interest (IJCAI, in our example), while Apply For Clearance has no parameters. This is followed by the execution of CCR (in the case of an abort), the retraction of a(v ), and failure. The translated plan clauses, with the semantics of CAN augmented by our waituntil-condition construct, thus specify the operation of the agent to handle both failure and aborting for the example. Such plans, similar to our failure methods, are designed for the agent to effect state changes (act to undo its earlier actions) prior to possibly attempting another plan to achieve the failed goal g. Further, the informal semantics of do not consider parallel sub-goals (i.e., the CAN construct), since such execution is not part of Jason\"s language. All three allow meta level methods that are cued by meta events such as goal adoption or plan failure, and offer introspective capabilities over goal and intention states. Compensation goals are triggered on aborting a goal, and so not necessarily on goal failure (i.e., if the FHC directs the agent to retry the failed goal and the retry is successful). FHC\"s failure-handling strategy rules (e.g., whether to retry a failed goal) are replaced by instructions in our PF and PA plans, together with meta-level default failure handlers according to the agent\"s nature (e.g., blindly committed). The resulting architecture embeds their failure handling approach within a pair processing architecture for agent crash recovery. \"s FHC and our approach both retain failure-handling logic within an agent.", "introduction": "Intelligent agents generally work in complex, dynamic environments, such as air traffic control or robot navigation, in which the success of any particular action or plan cannot be guaranteed . Accordingly, dealing with failure is fundamental to agent programming, and is an important element of agent characteristics such as robustness, flexibility, and persistence . In agent architectures inspired by the Belief-Desire-Intention (BDI) model , these properties are often characterized by the interactions between beliefs, goals, and plans .1 In general, an agent that wishes to achieve a particular set of tasks will pursue a number of plans concurrently. When failures occur, the choice of plans will be reviewed. This may involve seeking alternative plans for a particular task, re-scheduling tasks to better comply with resource constraints, dropping some tasks, or some other decision that will increase the likelihood of success . Failures can occur for a number of reasons, and it is often not possible to predict these in advance, either because of the complexity of the system or because changes in the environment invalidate some earlier decisions. Given this need for deliberation about failed tasks or plans, failure deliberation is commonly built into the agent\"s execution cycle. Besides dealing with failure, an important capability of an intelligent agent is to be able to abort a particular task or plan. This decision may be due to an internal deliberation (such as the agent believing the task can no longer be achieved, or that some conflicting task now has a higher priority) or due to an external factor (such as another agent altering a commitment, or a change in the environment). Aborting a task or plan is distinct from its failure. Failure reflects an inability to perform and does not negate the need to perform - for example, a reasonable response to failure may be to try again. In contrast, aborting says nothing about the ability to perform; it merely eliminates the need. Failure propagates from the bottom up, whereas aborting propagates from the top down. The potential for concurrently executing sub-plans introduces different complexities for aborting and failure. For aborting, it means that multiple concurrent sub-plans may need to be aborted as the abort is propagated down. For failure, it means that parallel-sibling plans may need to be aborted as the failure is propagated up. There has been a considerable amount of work on plan failures (such as detecting and resolving resource conflicts ) and most agent systems incorporate some notion of failure handling. However, there has been relatively little work on the development of abort techniques beyond simple dropping of currently intended plans and tasks, which does not deal with the clean-up required. As one consequence, most agent systems are quite limited in their treatment of the situation where one branch of a parallel construct One can consider both tasks to be performed and goals to achieve a certain state of the world. A task can be considered a goal of achieving the state of the task having been performed, and a goal can be considered a task of bringing about that state of the world. We adopt the latter view and use task to also refer to goals. 978-81--7-5 (RPS) IFAAMAS fails (common approaches include either letting the other branch run to completion unhindered or dropping it completely). In this paper we discuss in detail the incorporation of abort cleanup methods into the agent execution cycle, providing a unified approach to failure and abort. A key feature of our procedure-based approach is that we allow each plan to execute some particular code on a failure and on an abort. This allows a plan to attempt to ensure a stable, known state, and possibly to recover some resources and otherwise clean up before exiting. Accordingly, a central technical challenge is to manage the orderly execution of the appropriate clean-up code. We show how aborts can be smoothly introduced into a BDI-style architecture, and for the first time we give an operational semantics for aborting in the abstract agent language CAN . This allows us to specify an appropriate level of detail for the execution model, without focusing on the specific constructs of any one agent system such as JACK , Jadex , Jason , or SPARK . Our focus is on a single agent, complementary to related work that considers exception handling for single- and multiagent systems (e.g., ). This paper is organized as follows. In Section 2 we give an example of the consequences of aborting a task, and in Section 3 we discuss some circumstances under which aborts should occur, and the appropriate representation and invocation procedures. In Section 4 we show how we can use CAN to formally specify the behaviour of an aborted plan. Section 5 discusses related work, and in Section 6 we present our conclusions and future work.", "conclusion": "The tasks and plans of an agent may not successfully reach completion, either by the choice of the agent to abort them (perhaps at the request of another agent to do so), or by unbidden factors that lead to failure.. In this paper we have presented a procedure-based approach that incorporates aborting tasks and plans into the deliberation cycle of a BDI-style agent, thus providing a unified approach to failure and abort.. Our primary contribution is an analysis of the requirements on the operation of the agent for aborting tasks and plans, and a corresponding operational semantics for aborting in the abstract agent language CAN.. We are planning to implement an instance of our approach in the SPARK agent system ; in particular, the work of this paper will be the basis for SPARK\"s abort handling mechanism.. We are also developing an analysis tool for our extended version of CAN as a basis for experimentation.. An intelligent agent will not only gracefully handle unsuccessful tasks and plans, but also will deliberate over its cognitive attitudes to decide its next course of action.. We have assumed the default behaviour of a BDI-style agent, according to its nature: for instance, to retry alternatives to a failed plan until one succeeds or until no alternative plans remain (in which case to fail the task).. Future work is to place our approach in service of more dynamic agent reasoning, such as the introspection that an agent capable of reasoning over task interaction effects and resource requirements can accomplish .. Related to this is determining the cost of aborting a task or plan, and using this as an input to the deliberation process.. This would in particular influence the commitment the agent has towards a particular task: the higher the cost, the greater the commitment.. Our assumption that abort-methods do not fail, as discussed above, is a pragmatic one.. However, this is an issue worthy of further exploration, either to develop weaker assumptions that are also practical, or to analyze conditions under which our assumption is realistic.. A further item of interest is extending our approach to failure and abort to maintenance goals .. For such goals a different operational semantics for abort is necessary than for achievement goals, to match the difference in semantics of the goals themselves."}
{"id": "C-62", "keywords": ["innov", "commodit", "monitor", "contract", "incent"], "title": "Network Monitors and Contracting Systems: Competition and Innovation", "abstract": "Today's Internet industry suffers from several well-known pathologies, but none is as destructive in the long term as its resistance to evolution. Rather than introducing new services, ISPs are presently moving towards greater commoditization. It is apparent that the network's primitive system of contracts does not align incentives properly. In this study, we identify the network's lack of accountability as a fundamental obstacle to correcting this problem: Employing an economic model, we argue that optimal routes and innovation are impossible unless new monitoring capability is introduced and incorporated with the contracting system. Furthermore, we derive the minimum requirements a monitoring system must meet to support first-best routing and innovation characteristics. Our work does not constitute a new protocol; rather, we provide practical and specific guidance for the design of monitoring systems, as well as a theoretical framework to explore the factors that influence innovation.", "references": ["Using Repeated Games to Design Incentive-Based Routing Systems", "On the Benefits and Feasibility of Incentive Based Routing Infrastructure", "Providing Packet Obituaries", "The Design Philosophy of the DARPA Internet Protocols", "Tussle in cyberspace: Defining tomorrow's internet", "Interconnection Agreements: Strategic Behaviour and Property Rights", "A BGP-based Mechanism for Lowest-Cost Routing", "Interconnection, Peering, and Settlements", "On the Interaction Between Overlay Routing and Traffic Engineering", "Pricing the Internet", "Towards an Evolvable Internet Architecture", "Economics of Network Pricing with Multiple ISPs"], "full_text": "1. INTRODUCTION Many studies before us have noted the Internet\"s resistance to new services and evolution. In recent decades, numerous ideas have been developed in universities, implemented in code, and even written into the routers and end systems of the network, only to languish as network operators fail to turn them on on a large scale. The list includes Multicast, IPv6, IntServ, and DiffServ. Lacking the incentives just to activate services, there seems to be little hope of ISPs devoting adequate resources to developing new ideas. In the long term, this pathology stands out as a critical obstacle to the network\"s continued success (Ratnasamy, Shenker, and McCanne provide extensive discussion in ). On a smaller time scale, ISPs shun new services in favor of cost cutting measures. Thus, the network has characteristics of a commodity market. Although in theory, ISPs have a plethora of routing policies at their disposal, the prevailing strategy is to route in the cheapest way possible . On one hand, this leads directly to suboptimal routing. More importantly, commoditization in the short term is surely related to the lack of innovation in the long term. When the routing decisions of others ignore quality characteristics, ISPs are motivated only to lower costs. There is simply no reward for introducing new services or investing in quality improvements. In response to these pathologies and others, researchers have put forth various proposals for improving the situation. These can be divided according to three high-level strategies: The first attempts to improve the status quo by empowering end-users. Clark, et al., suggest that giving end-users control over routing would lead to greater service diversity, recognizing that some payment mechanism must also be provided . Ratnasamy, Shenker, and McCanne postulate a link between network evolution and user-directed routing . They propose a system of Anycast to give end-users the ability to tunnel their packets to an ISP that introduces a desirable protocol. The extra traffic to the ISP, the authors suggest, will motivate the initial investment. The second strategy suggests a revision of the contracting system. This is exemplified by MacKie-Mason and Varian, who propose a smart market to control access to network resources . Prices are set to the market-clearing level based on bids that users associate to their traffic. In another direction, Afergan and Wroclawski suggest that prices should be explicitly encoded in the routing protocols . They argue that such a move would improve stability and align incentives. The third high-level strategy calls for greater network accountability. In this vein, Argyraki, et al., propose a system of packet obituaries to provide feedback as to which ISPs drop packets . They argue that such feedback would help reveal which ISPs were adequately meeting their contractual obligations. Unlike the first two strategies, we are not aware of any previous studies that have connected accountability with the pathologies of commoditization or lack of innovation. It is clear that these three strategies are closely linked to each other (for example, , , and each argue that giving end-users routing control within the current contracting system is problematic). Until today, however, the relationship between them has been poorly understood. There is currently little theoretical foundation to compare the relative merits of each proposal, and a particular lack of evidence linking accountability with innovation and service differentiation. This paper will address both issues. We will begin by introducing an economic network model that relates accountability, contracts, competition, and innovation. Our model is highly stylized and may be considered preliminary: it is based on a single source sending data to a single destination. Nevertheless, the structure is rich enough to expose previously unseen features of network behavior. We will use our model for two main purposes: First, we will use our model to argue that the lack of accountability in today\"s network is a fundamental obstacle to overcoming the pathologies of commoditization and lack of innovation. In other words, unless new monitoring capabilities are introduced, and integrated with the system of contracts, the network cannot achieve optimal routing and innovation characteristics. This result provides motivation for the remainder of the paper, in which we explore how accountability can be leveraged to overcome these pathologies and create a sustainable industry. We will approach this problem from a clean-slate perspective, deriving the level of accountability needed to sustain an ideal competitive structure. When we say that today\"s Internet has poor accountability, we mean that it reveals little information about the behavior - or misbehavior - of ISPs. This well-known trait is largely rooted in the network\"s history. In describing the design philosophy behind the Internet protocols, Clark lists accountability as the least important among seven second level goals. Accordingly, accountability received little attention during the network\"s formative years. Clark relates this to the network\"s military context, and finds that had the network been designed for commercial development, accountability would have been a top priority. Argyraki, et al., conjecture that applying the principles of layering and transparency may have led to the network\"s lack of accountability . According to these principles, end hosts should be informed of network problems only to the extent that they are required to adapt. They notice when packet drops occur so that they can perform congestion control and retransmit packets. Details of where and why drops occur are deliberately concealed. The network\"s lack of accountability is highly relevant to a discussion of innovation because it constrains the system of contracts. This is because contracts depend upon external institutions to function - the judge in the language of incomplete contract theory, or simply the legal system. Ultimately, if a judge cannot verify that some condition holds, she cannot enforce a contract based on that condition. Of course, the vast majority of contracts never end up in court. Especially when a judge\"s ruling is easily predicted, the parties will typically comply with the contract terms on their own volition. This would not be possible, however, without the judge acting as a last resort. An institution to support contracts is typically complex, but we abstract it as follows: We imagine that a contract is an algorithm that outputs a payment transfer among a set of ISPs (the parties) at every time. This payment is a function of the past and present behaviors of the participants, but only those that are verifiable. Hence, we imagine that a contract only accepts proofs as inputs. We will call any process that generates these proofs a contractible monitor. Such a monitor includes metering or sensing devices on the physical network, but it is a more general concept. Constructing a proof of a particular behavior may require readings from various devices distributed among many ISPs. The contractible monitor includes whatever distributed algorithmic mechanism is used to motivate ISPs to share this private information. Figure 1 demonstrates how our model of contracts fits together. We make the assumption that all payments are mediated by contracts. This means that without contractible monitors that attest to, say, latency, payments cannot be conditioned on latency. Figure 1: Relationship between monitors and contracts With this model, we may conclude that the level of accountability in today\"s Internet only permits best effort contracts. Nodes cannot condition payments on either quality or path characteristics. Is there anything wrong with best-effort contracts? The reader might wonder why the Internet needs contracts at all. After all, in non-network industries, traditional firms invest in research and differentiate their products, all in the hopes of keeping their customers and securing new ones. One might believe that such market forces apply to ISPs as well. We may adopt this as our null hypothesis: Null hypothesis: Market forces are sufficient to maintain service diversity and innovation on a network, at least to the same extent as they do in traditional markets. There is a popular intuitive argument that supports this hypothesis, and it may be summarized as follows: Intuitive argument supporting null hypothesis: 1. Access providers try to increase their quality to get more consumers. 2. Access providers are themselves customers for second hop ISPs, and the second hops will therefore try to provide highquality service in order to secure traffic from access providers. Access providers try to select high quality transit because that increases their quality. 3. The process continues through the network, giving every ISP a competitive reason to increase quality. We are careful to model our network in continuous time, in order to capture the essence of this argument. We can, for example, specify equilibria in which nodes switch to a new next hop in the event of a quality drop. Moreover, our model allows us to explore any theoretically possible punishments against cheaters, including those that are costly for end-users to administer. By contrast, customers in the real world rarely respond collectively, and often simply seek the best deal currently offered. These constraints limit their ability to punish cheaters. Even with these liberal assumptions, however, we find that we must reject our null hypothesis. Our model will demonstrate that identifying a cheating ISP is difficult under low accountability, limiting the threat of market driven punishment. We will define an index of commoditization and show that it increases without bound as data paths grow long. Furthermore, we will demonstrate a framework in which an ISP\"s maximum research investment decreases hyperbolically with its distance from the end-user. Network Behavior Monitor Contract Proof Payments 184 To summarize, we argue that the Internet\"s lack of accountability must be addressed before the pathologies of commoditization and lack of innovation can be resolved. This leads us to our next topic: How can we leverage accountability to overcome these pathologies? We approach this question from a clean-slate perspective. Instead of focusing on incremental improvements, we try to imagine how an ideal industry would behave, then derive the level of accountability needed to meet that objective. According to this approach, we first craft a new equilibrium concept appropriate for network competition. Our concept includes the following requirements: First, we require that punishing ISPs that cheat is done without rerouting the path. Rerouting is likely to prompt end-users to switch providers, punishing access providers who administer punishments correctly. Next, we require that the equilibrium cannot be threatened by a coalition of ISPs that exchanges illicit side payments. Finally, we require that the punishment mechanism that enforces contracts does not punish innocent nodes that are not in the coalition. The last requirement is somewhat unconventional from an economic perspective, but we maintain that it is crucial for any reasonable solution. Although ISPs provide complementary services when they form a data path together, they are likely to be horizontal competitors as well. If innocent nodes may be punished, an ISP may decide to deliberately cheat and draw punishment onto itself and its neighbors. By cheating, the ISP may save resources, thereby ensuring that the punishment is more damaging to the other ISPs, which probably compete with the cheater directly for some customers. In the extreme case, the cheater may force the other ISPs out of business, thereby gaining a monopoly on some routes. Applying this equilibrium concept, we derive the monitors needed to maintain innovation and optimize routes. The solution is surprisingly simple: contractible monitors must report the quality of the rest of the path, from each ISP to the destination. It turns out that this is the correct minimum accountability requirement, as opposed to either end-to-end monitors or hop-by-hop monitors, as one might initially suspect. Rest of path monitors can be implemented in various ways. They may be purely local algorithms that listen for packet echoes. Alternately, they can be distributed in nature. We describe a way to construct a rest of path monitor out of monitors for individual ISP quality and for the data path. This requires a mechanism to motivate ISPs to share their monitor outputs with each other. The rest of path monitor then includes the component monitors and the distributed algorithmic mechanism that ensures that information is shared as required. This example shows that other types of monitors may be useful as building blocks, but must be combined to form rest of path monitors in order to achieve ideal innovation characteristics. Our study has several practical implications for future protocol design. We show that new monitors must be implemented and integrated with the contracting system before the pathologies of commoditization and lack of innovation can be overcome. Moreover, we derive exactly what monitors are needed to optimize routes and support innovation. In addition, our results provide useful input for clean-slate architectural design, and we use several novel techniques that we expect will be applicable to a variety of future research. The rest of this paper is organized as follows: In section 2, we lay out our basic network model. In section 3, we present a lowaccountability network, modeled after today\"s Internet. We demonstrate how poor monitoring causes commoditization and a lack of innovation. In section 4, we present verifiable monitors, and show that proofs, even without contracts, can improve the status quo. In section 5, we turn our attention to contractible monitors. We show that rest of path monitors can support competition games with optimal routing and innovation. We further show that rest of path monitors are required to support such competition games. We continue by discussing how such monitors may be constructed using other monitors as building blocks. In section 6, we conclude and present several directions for future research. 2. BASIC NETWORK MODEL A source, S, wants to send data to destination, D. S and D are nodes on a directed, acyclic graph, with a finite set of intermediate nodes, { }NV ,...2,1= , representing ISPs. All paths lead to D, and every node not connected to D has at least two choices for next hop. We will represent quality by a finite dimensional vector space, Q, called the quality space. Each dimension represents a distinct network characteristic that end-users care about. For example, latency, loss probability, jitter, and IP version can each be assigned to a dimension. To each node, i, we associate a vector in the quality space, Qqi \u2208 . This corresponds to the quality a user would experience if i were the only ISP on the data path. Let N Q\u2208q be the vector of all node qualities. Of course, when data passes through multiple nodes, their qualities combine in some way to yield a path quality. We represent this by an associative binary operation, *: QQQ \u2192\u00d7 . For path ( )nvvv ,...,, 21 , the quality is given by nvvv qqq \u2217\u2217\u2217 ...21 . The * operation reflects the characteristics of each dimension of quality. For example, * can act as an addition in the case of latency, multiplication in the case of loss probability, or a minimumargument function in the case of security. When data flows along a complete path from S to D, the source and destination, generally regarded as a single player, enjoy utility given by a function of the path quality, \u2192Qu : . Each node along the path, i, experiences some cost of transmission, ci. 2.1 Game Dynamics Ultimately, we are most interested in policies that promote innovation on the network. In this study, we will use innovation in a fairly general sense. Innovation describes any investment by an ISP that alters its quality vector so that at least one potential data path offers higher utility. This includes researching a new routing algorithm that decreases the amount of jitter users experience. It also includes deploying a new protocol that supports quality of service. Even more broadly, buying new equipment to decrease S D 185 latency may also be regarded as innovation. Innovation may be thought of as the micro-level process by which the network evolves. Our analysis is limited in one crucial respect: We focus on inventions that a single ISP can implement to improve the end-user experience. This excludes technologies that require adoption by all ISPs on the network to function. Because such technologies do not create a competitive advantage, rewarding them is difficult and may require intellectual property or some other market distortion. We defer this interesting topic to future work. At first, it may seem unclear how a large-scale distributed process such as innovation can be influenced by mechanical details like networks monitors. Our model must draw this connection in a realistic fashion. The rate of innovation depends on the profits that potential innovators expect in the future. The reward generated by an invention must exceed the total cost to develop it, or the inventor will not rationally invest. This reward, in turn, is governed by the competitive environment in which the firm operates, including the process by which firms select prices, and agree upon contracts with each other. Of course, these decisions depend on how routes are established, and how contracts determine actual monetary exchanges. Any model of network innovation must therefore relate at least three distinct processes: innovation, competition, and routing. We select a game dynamics that makes the relation between these processes as explicit as possible. This is represented schematically in Figure 2. The innovation stage occurs first, at time 2\u2212=t . In this stage, each agent decides whether or not to make research investments. If she chooses not to, her quality remains fixed. If she makes an investment, her quality may change in some way. It is not necessary for us to specify how such changes take place. The agents\" choices in this stage determine the vector of qualities, q, common knowledge for the rest of the game. Next, at time 1\u2212=t , agents participate in the competition stage, in which contracts are agreed upon. In today\"s industry, these contracts include prices for transit access, and peering agreements. Since access is provided on a best-effort basis, a transit agreement can simply be represented by its price. Other contracting systems we will explore will require more detail. Finally, beginning at 0=t , firms participate in the routing stage. Other research has already employed repeated games to study routing, for example , . Repetition reveals interesting effects not visible in a single stage game, such as informal collusion to elevate prices in . We use a game in continuous time in order to study such properties. For example, we will later ask whether a player will maintain higher quality than her contracts require, in the hope of keeping her customer base or attracting future customers. Our dynamics reflect the fact that ISPs make innovation decisions infrequently. Although real firms have multiple opportunities to innovate, each opportunity is followed by a substantial length of time in which qualities are fixed. The decision to invest focuses on how the firm\"s new quality will improve the contracts it can enter into. Hence, our model places innovation at the earliest stage, attempting to capture a single investment decision. Contracting decisions are made on an intermediate time scale, thus appearing next in the dynamics. Routing decisions are made very frequently, mainly to maximize immediate profit flows, so they appear in the last stage. Because of this ordering, our model does not allow firms to route strategically to affect future innovation or contracting decisions. In opposition, Afergan and Wroclawski argue that contracts are formed in response to current traffic patterns, in a feedback loop . Although we are sympathetic to their observation, such an addition would make our analysis intractable. Our model is most realistic when contracting decisions are infrequent. Throughout this paper, our solution concept will be a subgame perfect equilibrium (SPE). An SPE is a strategy point that is a Nash equilibrium when restricted to each subgame. Three important subgames have been labeled in Figure 2. The innovation game includes all three stages. The competition game includes only the competition stage and the routing stage. The routing game includes only the routing stage. An SPE guarantees that players are forward-looking. This means, for example, that in the competition stage, firms must act rationally, maximizing their expected profits in the routing stage. They cannot carry out threats they made in the innovation stage if it lowers their expected payoff. Our schematic already suggests that the routing game is crucial for promoting innovation. To support innovation, the competition game must somehow reward ISPs with high quality. But that means that the routing game must tend to route to nodes with high quality. If the routing game always selects the lowest-cost routes, for example, innovation will not be supported. We will support this observation with analysis later. 2.2 The Routing Game The routing game proceeds in continuous time, with all players discounting by a common factor, r. The outputs from previous stages, q and the set of contracts, are treated as exogenous parameters for this game. For each time 0\u2265t , each node must select a next hop to route data to. Data flows across the resultant path, causing utility flow to S and D, and a flow cost to the nodes on the path, as described above. Payment flows are also created, based on the contracts in place. Relating our game to the familiar repeated prisoners\" dilemma, imagine that we are trying to impose a high quality, but costly path. As we argued loosely above, such paths must be sustainable in order to support innovation. Each ISP on the path tries to maximize her own payment, net of costs, so she may not want to cooperate with our plan. Rather, if she can find a way to save on costs, at the expense of the high quality we desire, she will be tempted to do so. Innovation Game Competition Game Routing Game Innovation stage Competition stage Routing stageQualities (q) Contracts (prices) Profits t = -2 t = -1 t \u2208 [ 0 , ) Figure 2: Game Dynamics 186 Analogously to the prisoners\" dilemma, we will call such a decision cheating. A little more formally, Cheating refers to any action that an ISP can take, contrary to some target strategy point that we are trying to impose, that enhances her immediate payoff, but compromises the quality of the data path. One type of cheating relates to the data path. Each node on the path has to pay the next node to deliver its traffic. If the next node offers high quality transit, we may expect that a lower quality node will offer a lower price. Each node on the path will be tempted to route to a cheaper next hop, increasing her immediate profits, but lowering the path quality. We will call this type of action cheating in route. Another possibility we can model, is that a node finds a way to save on its internal forwarding costs, at the expense of its own quality. We will call this cheating internally to distinguish it from cheating in route. For example, a node might drop packets beyond the rate required for congestion control, in order to throttle back TCP flows and thus save on forwarding costs . Alternately, a node employing quality of service could give high priority packets a lower class of service, thus saving on resources and perhaps allowing itself to sell more high priority service. If either cheating in route or cheating internally is profitable, the specified path will not be an equilibrium. We assume that cheating can never be caught instantaneously. Rather, a cheater can always enjoy the payoff from cheating for some positive time, which we label 0t . This includes the time for other players to detect and react to the cheating. If the cheater has a contract which includes a customer lock-in period, 0t also includes the time until customers are allowed to switch to a new ISP. As we will see later, it is socially beneficial to decrease 0t , so such lock-in is detrimental to welfare. 3. PATHOLOGIES OF A LOWACCOUNTABILITY NETWORK In order to motivate an exploration of monitoring systems, we begin in this section by considering a network with a poor degree of accountability, modeled after today\"s Internet. We will show how the lack of monitoring necessarily leads to poor routing and diminishes the rate of innovation. Thus, the network\"s lack of accountability is a fundamental obstacle to resolving these pathologies. 3.1 Accountability in the Current Internet First, we reflect on what accountability characteristics the present Internet has. Argyraki, et al., point out that end hosts are given minimal information about packet drops . Users know when drops occur, but not where they occur, nor why. Dropped packets may represent the innocent signaling of congestion, or, as we mentioned above, they may be a form of cheating internally. The problem is similar for other dimensions of quality, or in fact more acute. Finding an ISP that gives high priority packets a lower class of service, for example, is further complicated by the lack of even basic diagnostic tools. In fact, it is similarly difficult to identify an ISP that cheats in route. Huston notes that Internet traffic flows do not always correspond to routing information . An ISP may hand a packet off to a neighbor regardless of what routes that neighbor has advertised. Furthermore, blocks of addresses are summarized together for distant hosts, so a destination may not even be resolvable until packets are forwarded closer. One might argue that diagnostic tools like ping and traceroute can identify cheaters. Unfortunately, Argyraki, et al., explain that these tools only reveal whether probe packets are echoed, not the fate of past packets . Thus, for example, they are ineffective in detecting low-frequency packet drops. Even more fundamentally, a sophisticated cheater can always spot diagnostic packets and give them special treatment. As a further complication, a cheater may assume different aliases for diagnostic packets arriving over different routes. As we will see below, this gives the cheater a significant advantage in escaping punishment for bad behavior, even if the data path is otherwise observable. 3.2 Modeling Low-Accountability As the above evidence suggests, the current industry allows for very little insight into the behavior of the network. In this section, we attempt to capture this lack of accountability in our model. We begin by defining a monitor, our model of the way that players receive external information about network behavior, A monitor is any distributed algorithmic mechanism that runs on the network graph, and outputs, to specific nodes, informational statements about current or past network behavior. We assume that all external information about network behavior is mediated in this way. The accountability properties of the Internet can be represented by the following monitors: E2E (End to End): A monitor that informs S/D about what the total path quality is at any time (this is the quality they experience). ROP (Rest of Path): A monitor that informs each node along the data path what the quality is for the rest of the path to the destination. PRc (Packets Received): A monitor that tells nodes how much data they accept from each other, so that they can charge by volume. It is important to note, however, that this information is aggregated over many source-destination pairs. Hence, for the sake of realism, it cannot be used to monitor what the data path is. Players cannot measure the qualities of other, single nodes, just the rest of the path. Nodes cannot see the path past the next hop. This last assumption is stricter than needed for our results. The critical ingredient is that nodes cannot verify that the path avoids a specific hop. This holds, for example, if the path is generally visible, except nodes can use different aliases for different parents. Similar results also hold if alternate paths always converge after some integer number, m, of hops. It is important to stress that E2E and ROP are not the contractible monitors we described in the introduction - they do not generate proofs. Thus, even though a player observes certain information, she generally cannot credibly share it with another player. For example, if a node after the first hop starts cheating, the first hop will detect the sudden drop in quality for the rest of the path, but the first hop cannot make the source believe this observation - the 187 source will suspect that the first hop was the cheater, and fabricated the claim against the rest of the path. Typically, E2E and ROP are envisioned as algorithms that run on a single node, and listen for packet echoes. This is not the only way that they could be implemented, however; an alternate strategy is to aggregate quality measurements from multiple points in the network. These measurements can originate in other monitors, located at various ISPs. The monitor then includes the component monitors as well as whatever mechanisms are in place to motivate nodes to share information honestly as needed. For example, if the source has monitors that reveal the qualities of individual nodes, they could be combined with path information to create an ROP monitor. Since we know that contracts only accept proofs as input, we can infer that payments in this environment can only depend on the number of packets exchanged between players. In other words, contracts are best-effort. For the remainder of this section, we will assume that contracts are also linear - there is a constant payment flow so long as a node accepts data, and all conditions of the contract are met. Other, more complicated tariffs are also possible, and are typically used to generate lock-in. We believe that our parameter t0 is sufficient to describe lock-in effects, and we believe that the insights in this section apply equally to any tariffs that are bounded so that the routing game remains continuous at infinity. Restricting attention to linear contracts allows us to represent some node i\"s contract by its price, pi. Because we further know that nodes cannot observe the path after the next hop, we can infer that contracts exist only between neighboring nodes on the graph. We will call this arrangement of contracts bilateral. When a competition game exclusively uses bilateral contracts, we will call it a bilateral contract competition game. We first focus on the routing game and ask whether a high quality route can be maintained, even when a low quality route is cheaper. Recall that this is a requirement in order for nodes to have any incentive to innovate. If nodes tend to route to low price next hops, regardless of quality, we say that the network is commoditized. To measure this tendency, we define an index of commoditization as follows: For a node on the data path, i, define its quality premium, minppd ji \u2212= , where pj is the flow payment to the next hop in equilibrium, and pmin is the price of the lowest cost next hop. Definition: The index of commoditization, CI , is the average, over each node on the data path, i, of i\"s flow profit as a fraction of i\"s quality premium, ( ) ijii dpcp /\u2212\u2212 . CI ranges from 0, when each node spends all of its potential profit on its quality premium, to infinite, when a node absorbs positive profit, but uses the lowest price next hop. A high value for CI implies that nodes are spending little of their money inflow on purchasing high quality for the rest of the path. As the next claim shows, this is exactly what happens as the path grows long: Claim 1. If the only monitors are E2E, ROP, and PRc, \u221e\u2192CI as \u221e\u2192n , where n is the number of nodes on the data path. To show that this is true, we first need the following lemma, which will establish the difficulty of punishing nodes in the network. First a bit of notation: Recall that a cheater can benefit from its actions for 00 >t before other players can react. When a node cheats, it can expect a higher profit flow, at least until it is caught and other players react, perhaps by diverting traffic. Let node i\"s normal profit flow be i\u03c0 , and her profit flow during cheating be some greater value, yi. We will call the ratio, iiy \u03c0/ , the temptation to cheat. Lemma 1. If the only monitors are E2E, ROP, and PRc, the discounted time, \u2212nt rt , needed to punish a cheater increases at least as fast as the product of the temptations to cheat along the data path, \u220f \u2212\u2212 pathdataon rt i i rt (1) Corollary. If nodes share a minimum temptation to cheat, \u03c0/y , the discounted time needed to punish cheating increases at least exponentially in the length of the data path, n, \u2212\u2212 00 rt nt rt (2) Since it is the discounted time that increases exponentially, the actual time increases faster than exponentially. If n is so large that tn is undefined, the given path cannot be maintained in equilibrium. Proof. The proof proceeds by induction on the number of nodes on the equilibrium data path, n. For 1=n , there is a single node, say i. By cheating, the node earns extra profit ( ) \u2212 rt ii ey \u03c0 . If node i is then punished until time 1t , the extra profit must be cancelled out by the lost profit between time 0t and 1t , \u22121 rt i e\u03c0 . A little manipulation gives \u2212\u2212 01 00 rt rt , as required. For 1>n , assume for induction that the claim holds for 1\u2212n . The source does not know whether the cheater is the first hop, or after the first hop. Because the source does not know the data path after the first hop, it is unable to punish nodes beyond it. If it chooses a new first hop, it might not affect the rest of the data path. Because of this, the source must rely on the first hop to punish cheating nodes farther along the path. The first hop needs discounted time, \u220f \u22120 hopfirstafter rt i i i e , to accomplish this by assumption. So the source must give the first hop this much discounted time in order to punish defectors further down the line (and the source will expect poor quality during this period). Next, the source must be protected against a first hop that cheats, and pretends that the problem is later in the path. The first hop can 188 do this for the full discounted time, \u220f \u22120 hopfirstafter rt i i i e , so the source must punish the first hop long enough to remove the extra profit it can make. Following the same argument as for 1=n , we can show that the full discounted time is \u220f \u22120 pathdataon rt i i i e which completes the proof. The above lemma and its corollary show that punishing cheaters becomes more and more difficult as the data path grows long, until doing so is impossible. To capture some intuition behind this result, imagine that you are an end user, and you notice a sudden drop in service quality. If your data only travels through your access provider, you know it is that provider\"s fault. You can therefore take your business elsewhere, at least for some time. This threat should motivate your provider to maintain high quality. Suppose, on the other hand, that your data traverses two providers. When you complain to your ISP, he responds, yes, we know your quality went down, but it\"s not our fault, it\"s the next ISP. Give us some time to punish them and then normal quality will resume. If your access provider is telling the truth, you will want to listen, since switching access providers may not even route around the actual offender. Thus, you will have to accept lower quality service for some longer time. On the other hand, you may want to punish your access provider as well, in case he is lying. This means you have to wait longer to resume normal service. As more ISPs are added to the path, the time increases in a recursive fashion. With this lemma in hand, we can return to prove Claim 1. Proof of Claim 1. Fix an equilibrium data path of length n. Label the path nodes 1,2,\u2026,n. For each node i, let i\"s quality premium be '11 ++ \u2212= iii ppd . Then we have, ++ \u2212=\u2212 \u2212\u2212 \u2212\u2212 \u2212\u2212 \u2212\u2212 i iii iii i iii ii i i iii npcp pcp pcp pp nd pcp 11 '1 '11 , (3) where gi is node i\"s temptation to cheat by routing to the lowest price next hop. Lemma 1 tells us that Tg i <\u220f =1 , where ( )01 rt eT \u2212 \u2212= . It requires a bit of calculus to show that IC is minimized by setting each gi equal to n T /1 . However, as \u221e\u2192n , we have 1/1 \u2192n T , which shows that \u221e\u2192CI . According to the claim, as the data path grows long, it increasingly resembles a lowest-price path. Since lowest-price routing does not support innovation, we may speculate that innovation degrades with the length of the data path. Though we suspect stronger claims are possible, we can demonstrate one such result by including an extra assumption: Available Bargain Path: A competitive market exists for lowcost transit, such that every node can route to the destination for no more than flow payment, lp . Claim 2. Under the available bargain path assumption, if node i , a distance n from S, can invest to alter its quality, and the source will spend no more than sP for a route including node i\"s new quality, then the payment to node i, p, decreases hyperbolically with n, ( ) ( ) s l P pp 1/1 +\u2264 , (4) where ( )01 rt eT \u2212 \u2212= is the bound on the product of temptations from the previous claim. Thus, i will spend no more than ( ) ( )\u2212 l P r 1 1 1/1 on this quality improvement, which approaches the bargain path\"s payment, pl , as \u221e\u2192n . The proof is given in the appendix. As a node gets farther from the source, its maximum payment approaches the bargain price, pl. Hence, the reward for innovation is bounded by the same amount. Large innovations, meaning substantially more expensive than rpl / , will not be pursued deep into the network. Claim 2 can alternately be viewed as a lower bound on how much it costs to elicit innovation in a network. If the source S wants node i to innovate, it needs to get a motivating payment, p, to i during the routing stage. However, it must also pay the nodes on the way to i a premium in order to motivate them to route properly. The claim shows that this premium increases with the distance to i, until it dwarfs the original payment, p. Our claims stand in sharp contrast to our null hypothesis from the introduction. Comparing the intuitive argument that supported our hypothesis with these claims, we can see that we implicitly used an oversimplified model of market pressure (as either present or not). As is now clear, market pressure relies on the decisions of customers, but these are limited by the lack of information. Hence, competitive forces degrade as the network deepens. 4. VERIFIABLE MONITORS In this section, we begin to introduce more accountability into the network. Recall that in the previous section, we assumed that players couldn\"t convince each other of their private information. What would happen if they could? If a monitor\"s informational signal can be credibly conveyed to others, we will call it a verifiable monitor. The monitor\"s output in this case can be thought of as a statement accompanied by a proof, a string that can be processed by any player to determine that the statement is true. A verifiable monitor is a distributed algorithmic mechanism that runs on the network graph, and outputs, to specific nodes, proofs about current or past network behavior. Along these lines, we can imagine verifiable counterparts to E2E and ROP. We will label these E2Ev and ROPv. With these monitors, each node observes the quality of the rest of the path and can also convince other players of these observations by giving them a proof. 189 By adding verifiability to our monitors, identifying a single cheater is straightforward. The cheater is the node that cannot produce proof that the rest of path quality decreased. This means that the negative results of the previous section no longer hold. For example, the following lemma stands in contrast to Lemma 1. Lemma 2. With monitors E2Ev, ROPv, and PRc, and provided that the node before each potential cheater has an alternate next hop that isn\"t more expensive, it is possible to enforce any data path in SPE so long as the maximum temptation is less than what can be deterred in finite time, max rt er (5) Proof. This lemma follows because nodes can share proofs to identify who the cheater is. Only that node must be punished in equilibrium, and the preceding node does not lose any payoff in administering the punishment. With this lemma in mind, it is easy to construct counterexamples to Claim 1 and Claim 2 in this new environment. Unfortunately, there are at least four reasons not to be satisfied with this improved monitoring system. The first, and weakest reason is that the maximum temptation remains finite, causing some distortion in routes or payments. Each node along a route must extract some positive profit unless the next hop is also the cheapest. Of course, if t0 is small, this effect is minimal. The second, and more serious reason is that we have always given our source the ability to commit to any punishment. Real world users are less likely to act collectively, and may simply search for the best service currently offered. Since punishment phases are generally characterized by a drop in quality, real world end-users may take this opportunity to shop for a new access provider. This will make nodes less motivated to administer punishments. The third reason is that Lemma 2 does not apply to cheating by coalitions. A coalition node may pretend to punish its successor, but instead enjoy a secret payment from the cheating node. Alternately, a node may bribe its successor to cheat, if the punishment phase is profitable, and so forth. The required discounted time for punishment may increase exponentially in the number of coalition members, just as in the previous section! The final reason not to accept this monitoring system is that when a cheater is punished, the path will often be routed around not just the offender, but around other nodes as well. Effectively, innocent nodes will be punished along with the guilty. In our abstract model, this doesn\"t cause trouble since the punishment falls off the equilibrium path. The effects are not so benign in the real world. When ISPs lie in sequence along a data path, they contribute complementary services, and their relationship is vertical. From the perspective of other source-destination pairs, however, these same firms are likely to be horizontal competitors. Because of this, a node might deliberately cheat, in order to trigger punishment for itself and its neighbors. By cheating, the node will save money to some extent, so the cheater is likely to emerge from the punishment phase better off than the innocent nodes. This may give the cheater a strategic advantage against its competitors. In the extreme, the cheater may use such a strategy to drive neighbors out of business, and thereby gain a monopoly on some routes. 5. CONTRACTIBLE MONITORS At the end of the last section, we identified several drawbacks that persist in an environment with E2Ev, ROPv, and PRc. In this section, we will show how all of these drawbacks can be overcome. To do this, we will require our third and final category of monitor: A contractible monitor is simply a verifiable monitor that generates proofs that can serve as input to a contract. Thus, contractible is jointly a property of the monitor and the institutions that must verify its proofs. Contractibility requires that a court, 1. Can verify the monitor\"s proofs. 2. Can understand what the proofs and contracts represent to the extent required to police illegal activity. 3. Can enforce payments among contracting parties. Understanding the agreements between companies has traditionally been a matter of reading contracts on paper. This may prove to be a harder task in a future network setting. Contracts may plausibly be negotiated by machine, be numerous, even per-flow, and be further complicated by the many dimensions of quality. When a monitor (together with institutional infrastructure) meets these criteria, we will label it with a subscript c, for contractible. The reader may recall that this is how we labeled the packets received monitor, PRc, which allows ISPs to form contracts with per-packet payments. Similarly, E2Ec and ROPc are contractible versions of the monitors we are now familiar with. At the end of the previous section, we argued for some desirable properties that we\"d like our solution to have. Briefly, we would like to enforce optimal data paths with an equilibrium concept that doesn\"t rely on re-routing for punishment, is coalition proof, and doesn\"t punish innocent nodes when a coalition cheats. We will call such an equilibrium a fixed-route coalition-proof protect-theinnocent equilibrium. As the next claim shows, ROPc allows us to create a system of linear (price, quality) contracts under just such an equilibrium. Claim 3. With ROPc, for any feasible and consistent assignment of rest of path qualities to nodes, and any corresponding payment schedule that yields non-negative payoffs, these qualities can be maintained with bilateral contracts in a fixed-route coalition-proof protect-the-innocent equilibrium. Proof: Fix any data path consistent with the given rest of path qualities. Select some monetary punishment, P, large enough to prevent any cheating for time t0 (the discounted total payment from the source will work). Let each node on the path enter into a contract with its parent, which fixes an arbitrary payment schedule so long as the rest of path quality is as prescribed. When the parent node, which has ROPc, submits a proof that the rest of path quality is less than expected, the contract awards her an instantaneous transfer, P, from the downstream node. Such proofs can be submitted every 0t for the previous interval. Suppose now that a coalition, C, decides to cheat. The source measures a decrease in quality, and according to her contract, is awarded P from the first hop. This means that there is a net outflow of P from the ISPs as a whole. Suppose that node i is not in C. In order for the parent node to claim P from i, it must submit proof that the quality of the path starting at i is not as prescribed. This means 190 that there is a cheater after i. Hence, i would also have detected a change in quality, so i can claim P from the next node on the path. Thus, innocent nodes are not punished. The sequence of payments must end by the destination, so the net outflow of P must come from the nodes in C. This establishes all necessary conditions of the equilibrium. Essentially, ROPc allows for an implementation of (price, quality) contracts. Building upon this result, we can construct competition games in which nodes offer various qualities to each other at specified prices, and can credibly commit to meet these performance targets, even allowing for coalitions and a desire to damage other ISPs. Example 1. Define a Stackelberg price-quality competition game as follows: Extend the partial order of nodes induced by the graph to any complete ordering, such that downstream nodes appear before their parents. In this order, each node selects a contract to offer to its parents, consisting of a rest of path quality, and a linear price. In the routing game, each node selects a next hop at every time, consistent with its advertised rest of path quality. The Stackelberg price-quality competition game can be implemented in our model with ROPc monitors, by using the strategy in the proof, above. It has the following useful property: Claim 4. The Stackelberg price-quality competition game yields optimal routes in SPE. The proof is given in the appendix. This property is favorable from an innovation perspective, since firms that invest in high quality will tend to fall on the optimal path, gaining positive payoff. In general, however, investments may be over or under rewarded. Extra conditions may be given under which innovation decisions approach perfect efficiency for large innovations. We omit the full analysis here. Example 2. Alternately, we can imagine that players report their private information to a central authority, which then assigns all contracts. For example, contracts could be computed to implement the cost-minimizing VCG mechanism proposed by Feigenbaum, et al. in . With ROPc monitors, we can adapt this mechanism to maximize welfare. For node, i, on the optimal path, L, the net payment must equal, essentially, its contribution to the welfare of S, D, and the other nodes. If L\" is an optimal path in the graph with i removed, the profit flow to i is, ( ) ( ) \u2208\u2260\u2208 +\u2212\u2212 ', Lj ijLj jLL ccququ , (6) where Lq and 'Lq are the qualities of the two paths. Here, (price, quality) contracts ensure that nodes report their qualities honestly. The incentive structure of the VCG mechanism is what motivates nodes to report their costs accurately. A nice feature of this game is that individual innovation decisions are efficient, meaning that a node will invest in an innovation whenever the investment cost is less than the increased welfare of the optimal data path. Unfortunately, the source may end up paying more than the utility of the path. Notice that with just E2Ec, a weaker version of Claim 3 holds. Bilateral (price, quality) contracts can be maintained in an equilibrium that is fixed-route and coalition-proof, but not protectthe-innocent. This is done by writing contracts to punish everyone on the path when the end to end quality drops. If the path length is n, the first hop pays nP to the source, the second hop pays ( )Pn 1\u2212 to the first, and so forth. This ensures that every node is punished sufficiently to make cheating unprofitable. For the reasons we gave previously, we believe that this solution concept is less than ideal, since it allows for malicious nodes to deliberately trigger punishments for potential competitors. Up to this point, we have adopted fixed-route coalition-proof protect-the-innocent equilibrium as our desired solution concept, and shown that ROPc monitors are sufficient to create some competition games that are desirable in terms of service diversity and innovation. As the next claim will show, rest of path monitoring is also necessary to construct such games under our solution concept. Before we proceed, what does it mean for a game to be desirable from the perspective of service diversity and innovation? We will use a very weak assumption, essentially, that the game is not fully commoditized for any node. The claim will hold for this entire class of games. Definition: A competition game is nowhere-commoditized if for each node, i, not adjacent to D, there is some assignment of qualities and marginal costs to nodes, such that the optimal data path includes i, and i has a positive temptation to cheat. In the case of linear contracts, it is sufficient to require that \u221e<CI , and that every node make positive profit under some assignment of qualities and marginal costs. Strictly speaking, ROPc monitors are not the only way to construct these desirable games. To prove the next claim, we must broaden our notion of rest of path monitoring to include the similar ROPc\" monitor, which attests to the quality starting at its own node, through the end of the path. Compare the two monitors below: ROPc: gives a node proof that the path quality from the next node to the destination is not correct. ROPc\": gives a node proof that the path quality from that node to the destination is correct. We present a simplified version of this claim, by including an assumption that only one node on the path can cheat at a time (though conspirators can still exchange side payments). We will discuss the full version after the proof. Claim 5. Assume a set of monitors, and a nowhere-commoditized bilateral contract competition game that always maintains the optimal quality in fixed-route coalition-proof protect-the-innocent equilibrium, with only one node allowed to cheat at a time. Then for each node, i, not adjacent to D, either i has an ROPc monitor, or i\"s children each have an ROPc\" monitor. Proof: First, because of the fixed-route assumption, punishments must be purely monetary. Next, when cheating occurs, if the payment does not go to the source or destination, it may go to another coalition member, rendering it ineffective. Thus, the source must accept some monetary compensation, net of its normal flow payment, when cheating occurs. Since the source only contracts with the first hop, it must accept this money from the first hop. The source\"s contract must therefore distinguish when the path quality is normal from when it is lowered by cheating. To do so, it can either accept proofs 191 from the source, that the quality is lower than required, or it can accept proofs from the first hop, that the quality is correct. These nodes will not rationally offer the opposing type of proof. By definition, any monitor that gives the source proof that the path quality is wrong is an ROPc monitor. Any monitor that gives the first hop proof that the quality is correct is a ROPc\" monitor. Thus, at least one of these monitors must exist. By the protect-the-innocent assumption, if cheating occurs, but the first hop is not a cheater, she must be able to claim the same size reward from the next ISP on the path, and thus pass on the punishment. The first hop\"s contract with the second must then distinguish when cheating occurs after the first hop. By argument similar to that for the source, either the first hop has a ROPc monitor, or the second has a ROPc\" monitor. This argument can be iterated along the entire path to the penultimate node before D. Since the marginal costs and qualities can be arranged to make any path the optimal path, these statements must hold for all nodes and their children, which completes the proof. The two possibilities for monitor correspond to which node has the burden of proof. In one case, the prior node must prove the suboptimal quality to claim its reward. In the other, the subsequent node must prove that the quality was correct to avoid penalty. Because the two monitors are similar, it seems likely that they require comparable costs to implement. If submitting the proofs is costly, it seems natural that nodes would prefer to use the ROPc monitor, placing the burden of proof on the upstream node. Finally, we note that it is straightforward to derive the full version of the claim, which allows for multiple cheaters. The only complication is that cheaters can exchange side payments, which makes any money transfers between them redundant. Because of this, we have to further generalize our rest of path monitors, so they are less constrained in the case that there are cheaters on either side. 5.1 Implementing Monitors Claim 5 should not be interpreted as a statement that each node must compute the rest of path quality locally, without input from other nodes. Other monitors, besides ROPc and ROPc\" can still be used, loosely speaking, as building blocks. For instance, network tomography is concerned with measuring properties of the network interior with tools located at the edge. Using such techniques, our source might learn both individual node qualities and the data path. This is represented by the following two monitors: SHOPc : (source-based hop quality) A monitor that gives the source proof of what the quality of node i is. SPATHc: (source-based path) A monitor that gives the source proof of what the data path is at any time, at least as far as it matches the equilibrium path. With these monitors, a punishment mechanism can be designed to fulfill the conditions of Claim 5. It involves the source sharing the proofs it generates with nodes further down the path, which use them to determine bilateral payments. Ultimately however, the proof of Claim 5 shows us that each node i\"s bilateral contracts require proof of the rest of path quality. This means that node i (or possibly its children) will have to combine the proofs that they receive to generate a proof of the rest of path quality. Thus, the combined process is itself a rest of path monitor. What we have done, all in all, is constructed a rest of path monitor using SPATHc and SHOPc as building blocks. Our new monitor includes both the component monitors and whatever distributed algorithmic mechanism exists to make sure nodes share their proofs correctly. This mechanism can potentially involve external institutions. For a concrete example, suppose that when node i suspects it is getting poor rest of path quality from its successor, it takes the downstream node to court. During the discovery process, the court subpoenas proofs of the path and of node qualities from the source (ultimately, there must be some threat to ensure the source complies). Finally, for the court to issue a judgment, one party or the other must compile a proof of what the rest of path quality was. Hence, the entire discovery process acts as a rest of path monitor, albeit a rather costly monitor in this case. Of course, mechanisms can be designed to combine these monitors at much lower cost. Typically, such mechanisms would call for automatic sharing of proofs, with court intervention only as a last resort. We defer these interesting mechanisms to future work. As an aside, intuition might dictate that SHOPc generates more information than ROPc; after all, inferring individual node qualities seems a much harder problem. Yet, without path information, SHOPc is not sufficient for our first-best innovation result. The proof of this demonstrates a useful technique: Claim 6. With monitors E2E, ROP, SHOPc and PRc, and a nowhere-commoditized bilateral contract competition game, the optimal quality cannot be maintained for all assignments of quality and marginal cost, in fixed-route coalition-proof protect-theinnocent equilibrium. Proof: Because nodes cannot verify the data path, they cannot form a proof of what the rest of path quality is. Hence, ROPc monitors do not exist, and therefore the requirements of Claim 5 cannot hold. 6. CONCLUSIONS AND FUTURE WORK It is our hope that this study will have a positive impact in at least three different ways. The first is practical: we believe our analysis has implications for the design of future monitoring protocols and for public policy. For protocol designers, we first provide fresh motivation to create monitoring systems. We have argued that the poor accountability of the Internet is a fundamental obstacle to alleviating the pathologies of commoditization and lack of innovation. Unless accountability improves, these pathologies are guaranteed to remain. Secondly, we suggest directions for future advances in monitoring. We have shown that adding verifiability to monitors allows for some improvements in the characteristics of competition. At the same time, this does not present a fully satisfying solution. This paper has suggested a novel standard for monitors to aspire to - one of supporting optimal routes in innovative competition games under fixed-route coalition-proof protect-the-innocent equilibrium. We have shown that under bilateral contracts, this specifically requires contractible rest of path monitors. This is not to say that other types of monitors are unimportant. We included an example in which individual hop quality monitors and a path monitor can also meet our standard for sustaining competition. However, in order for this to happen, a mechanism must be included 192 to combine proofs from these monitors to form a proof of rest of path quality. In other words, the monitors must ultimately be combined to form contractible rest of path monitors. To support service differentiation and innovation, it may be easier to design rest of path monitors directly, thereby avoiding the task of designing mechanisms for combining component monitors. As far as policy implications, our analysis points to the need for legal institutions to enforce contracts based on quality. These institutions must be equipped to verify proofs of quality, and police illegal contracting behavior. As quality-based contracts become numerous and complicated, and possibly negotiated by machine, this may become a challenging task, and new standards and regulations may have to emerge in response. This remains an interesting and unexplored area for research. The second area we hope our study will benefit is that of clean-slate architectural design. Traditionally, clean-slate design tends to focus on creating effective and elegant networks for a static set of requirements. Thus, the approach is often one of engineering, which tends to neglect competitive effects. We agree with Ratnasamy, Shenker, and McCanne, that designing for evolution should be a top priority . We have demonstrated that the network\"s monitoring ability is critical to supporting innovation, as are the institutions that support contracting. These elements should feature prominently in new designs. Our analysis specifically suggests that architectures based on bilateral contracts should include contractible rest of path monitoring. From a clean-slate perspective, these monitors can be transparently and fully integrated with the routing and contracting systems. Finally, the last contribution our study makes is methodological. We believe that the mathematical formalization we present is applicable to a variety of future research questions. While a significant literature addresses innovation in the presence of network effects, to the best of our knowledge, ours is the first model of innovation in a network industry that successfully incorporates the actual topological structure as input. This allows the discovery of new properties, such as the weakening of market forces with the number of ISPs on a data path that we observe with lowaccountability. Our method also stands in contrast to the typical approach of distributed algorithmic mechanism design. Because this field is based on a principle-agent framework, contracts are usually proposed by the source, who is allowed to make a take it or leave it offer to network nodes. Our technique allows contracts to emerge from a competitive framework, so the source is limited to selecting the most desirable contract. We believe this is a closer reflection of the industry. Based on the insights in this study, the possible directions for future research are numerous and exciting. To some degree, contracting based on quality opens a Pandora\"s Box of pressing questions: Do quality-based contracts stand counter to the principle of network neutrality? Should ISPs be allowed to offer a choice of contracts at different quality levels? What anti-competitive behaviors are enabled by quality-based contracts? Can a contracting system support optimal multicast trees? In this study, we have focused on bilateral contracts. This system has seemed natural, especially since it is the prevalent system on the current network. Perhaps its most important benefit is that each contract is local in nature, so both parties share a common, familiar legal jurisdiction. There is no need to worry about who will enforce a punishment against another ISP on the opposite side of the planet, nor is there a dispute over whose legal rules to apply in interpreting a contract. Although this benefit is compelling, it is worth considering other systems. The clearest alternative is to form a contract between the source and every node on the path. We may call these source contracts. Source contracting may present surprising advantages. For instance, since ISPs do not exchange money with each other, an ISP cannot save money by selecting a cheaper next hop. Additionally, if the source only has contracts with nodes on the intended path, other nodes won\"t even be willing to accept packets from this source since they won\"t receive compensation for carrying them. This combination seems to eliminate all temptation for a single cheater to cheat in route. Because of this and other encouraging features, we believe source contracts are a fertile topic for further study. Another important research task is to relax our assumption that quality can be measured fully and precisely. One possibility is to assume that monitoring is only probabilistic or suffers from noise. Even more relevant is the possibility that quality monitors are fundamentally incomplete. A quality monitor can never anticipate every dimension of quality that future applications will care about, nor can it anticipate a new and valuable protocol that an ISP introduces. We may define a monitor space as a subspace of the quality space that a monitor can measure, QM \u2282 , and a corresponding monitoring function that simply projects the full range of qualities onto the monitor space, MQm \u2192: . Clearly, innovations that leave quality invariant under m are not easy to support - they are invisible to the monitoring system. In this environment, we expect that path monitoring becomes more important, since it is the only way to ensure data reaches certain innovator ISPs. Further research is needed to understand this process.", "body1": "Many studies before us have noted the Internet\"s resistance to new services and evolution. The list includes Multicast, IPv6, IntServ, and DiffServ. On a smaller time scale, ISPs shun new services in favor of cost cutting measures. In response to these pathologies and others, researchers have put forth various proposals for improving the situation. This is exemplified by MacKie-Mason and Varian, who propose a smart market to control access to network resources . The third high-level strategy calls for greater network accountability. It is clear that these three strategies are closely linked to each other (for example, , , and each argue that giving end-users routing control within the current contracting system is problematic). Nevertheless, the structure is rich enough to expose previously unseen features of network behavior. When we say that today\"s Internet has poor accountability, we mean that it reveals little information about the behavior - or misbehavior - of ISPs. Argyraki, et al., conjecture that applying the principles of layering and transparency may have led to the network\"s lack of accountability . The network\"s lack of accountability is highly relevant to a discussion of innovation because it constrains the system of contracts. Hence, we imagine that a contract only accepts proofs as inputs. We will call any process that generates these proofs a contractible monitor. This means that without contractible monitors that attest to, say, latency, payments cannot be conditioned on latency. Figure 1: Relationship between monitors and contracts With this model, we may conclude that the level of accountability in today\"s Internet only permits best effort contracts. Is there anything wrong with best-effort contracts? 2. We are careful to model our network in continuous time, in order to capture the essence of this argument. Moreover, our model allows us to explore any theoretically possible punishments against cheaters, including those that are costly for end-users to administer. Even with these liberal assumptions, however, we find that we must reject our null hypothesis. Network Behavior Monitor Contract Proof Payments 184 To summarize, we argue that the Internet\"s lack of accountability must be addressed before the pathologies of commoditization and lack of innovation can be resolved. The last requirement is somewhat unconventional from an economic perspective, but we maintain that it is crucial for any reasonable solution. Applying this equilibrium concept, we derive the monitors needed to maintain innovation and optimize routes. Alternately, they can be distributed in nature. Moreover, we derive exactly what monitors are needed to optimize routes and support innovation. The rest of this paper is organized as follows: In section 2, we lay out our basic network model. We show that rest of path monitors can support competition games with optimal routing and innovation. A source, S, wants to send data to destination, D. S and D are nodes on a directed, acyclic graph, with a finite set of intermediate nodes, { }NV ,...2,1= , representing ISPs. We will represent quality by a finite dimensional vector space, Q, called the quality space. This corresponds to the quality a user would experience if i were the only ISP on the data path. Of course, when data passes through multiple nodes, their qualities combine in some way to yield a path quality. When data flows along a complete path from S to D, the source and destination, generally regarded as a single player, enjoy utility given by a function of the path quality, \u2192Qu : . 2.1 Game Dynamics Ultimately, we are most interested in policies that promote innovation on the network. Because such technologies do not create a competitive advantage, rewarding them is difficult and may require intellectual property or some other market distortion. At first, it may seem unclear how a large-scale distributed process such as innovation can be influenced by mechanical details like networks monitors. The rate of innovation depends on the profits that potential innovators expect in the future. Any model of network innovation must therefore relate at least three distinct processes: innovation, competition, and routing. The innovation stage occurs first, at time 2\u2212=t . Since access is provided on a best-effort basis, a transit agreement can simply be represented by its price. Finally, beginning at 0=t , firms participate in the routing stage. Other research has already employed repeated games to study routing, for example , . Our dynamics reflect the fact that ISPs make innovation decisions infrequently. Although we are sympathetic to their observation, such an addition would make our analysis intractable. Throughout this paper, our solution concept will be a subgame perfect equilibrium (SPE). An SPE guarantees that players are forward-looking. Our schematic already suggests that the routing game is crucial for promoting innovation. 2.2 The Routing Game The routing game proceeds in continuous time, with all players discounting by a common factor, r. The outputs from previous stages, q and the set of contracts, are treated as exogenous parameters for this game. As we argued loosely above, such paths must be sustainable in order to support innovation. One type of cheating relates to the data path. We will call this cheating internally to distinguish it from cheating in route. If either cheating in route or cheating internally is profitable, the specified path will not be an equilibrium. In order to motivate an exploration of monitoring systems, we begin in this section by considering a network with a poor degree of accountability, modeled after today\"s Internet. 3.1 Accountability in the Current Internet First, we reflect on what accountability characteristics the present Internet has. Huston notes that Internet traffic flows do not always correspond to routing information . Furthermore, blocks of addresses are summarized together for distant hosts, so a destination may not even be resolvable until packets are forwarded closer. One might argue that diagnostic tools like ping and traceroute can identify cheaters. 3.2 Modeling Low-Accountability As the above evidence suggests, the current industry allows for very little insight into the behavior of the network. ROP (Rest of Path): A monitor that informs each node along the data path what the quality is for the rest of the path to the destination. PRc (Packets Received): A monitor that tells nodes how much data they accept from each other, so that they can charge by volume. Players cannot measure the qualities of other, single nodes, just the rest of the path. It is important to stress that E2E and ROP are not the contractible monitors we described in the introduction - they do not generate proofs. Typically, E2E and ROP are envisioned as algorithms that run on a single node, and listen for packet echoes. Since we know that contracts only accept proofs as input, we can infer that payments in this environment can only depend on the number of packets exchanged between players. Because we further know that nodes cannot observe the path after the next hop, we can infer that contracts exist only between neighboring nodes on the graph. Recall that this is a requirement in order for nodes to have any incentive to innovate. CI ranges from 0, when each node spends all of its potential profit on its quality premium, to infinite, when a node absorbs positive profit, but uses the lowest price next hop. First a bit of notation: Recall that a cheater can benefit from its actions for 00 >t before other players can react. Lemma 1. By cheating, the node earns extra profit ( ) \u2212 rt ii ey \u03c0 . For 1>n , assume for induction that the claim holds for 1\u2212n . Next, the source must be protected against a first hop that cheats, and pretends that the problem is later in the path. The above lemma and its corollary show that punishing cheaters becomes more and more difficult as the data path grows long, until doing so is impossible. When you complain to your ISP, he responds, yes, we know your quality went down, but it\"s not our fault, it\"s the next ISP. Proof of Claim 1. According to the claim, as the data path grows long, it increasingly resembles a lowest-price path. Claim 2. Hence, the reward for innovation is bounded by the same amount. Large innovations, meaning substantially more expensive than rpl / , will not be pursued deep into the network. Claim 2 can alternately be viewed as a lower bound on how much it costs to elicit innovation in a network. As is now clear, market pressure relies on the decisions of customers, but these are limited by the lack of information. In this section, we begin to introduce more accountability into the network. What would happen if they could? Along these lines, we can imagine verifiable counterparts to E2E and ROP. 189 By adding verifiability to our monitors, identifying a single cheater is straightforward. Lemma 2. Unfortunately, there are at least four reasons not to be satisfied with this improved monitoring system. The second, and more serious reason is that we have always given our source the ability to commit to any punishment. Alternately, a node may bribe its successor to cheat, if the punishment phase is profitable, and so forth. When ISPs lie in sequence along a data path, they contribute complementary services, and their relationship is vertical. At the end of the last section, we identified several drawbacks that persist in an environment with E2Ev, ROPv, and PRc. To do this, we will require our third and final category of monitor: A contractible monitor is simply a verifiable monitor that generates proofs that can serve as input to a contract. 3. Understanding the agreements between companies has traditionally been a matter of reading contracts on paper. The reader may recall that this is how we labeled the packets received monitor, PRc, which allows ISPs to form contracts with per-packet payments. At the end of the previous section, we argued for some desirable properties that we\"d like our solution to have. Claim 3. Proof: Fix any data path consistent with the given rest of path qualities. Suppose now that a coalition, C, decides to cheat. Essentially, ROPc allows for an implementation of (price, quality) contracts. Example 1. The proof is given in the appendix. Example 2. A nice feature of this game is that individual innovation decisions are efficient, meaning that a node will invest in an innovation whenever the investment cost is less than the increased welfare of the optimal data path. Notice that with just E2Ec, a weaker version of Claim 3 holds. Bilateral (price, quality) contracts can be maintained in an equilibrium that is fixed-route and coalition-proof, but not protectthe-innocent. Before we proceed, what does it mean for a game to be desirable from the perspective of service diversity and innovation? In the case of linear contracts, it is sufficient to require that \u221e<CI , and that every node make positive profit under some assignment of qualities and marginal costs. Strictly speaking, ROPc monitors are not the only way to construct these desirable games. We present a simplified version of this claim, by including an assumption that only one node on the path can cheat at a time (though conspirators can still exchange side payments). Claim 5. Next, when cheating occurs, if the payment does not go to the source or destination, it may go to another coalition member, rendering it ineffective. By definition, any monitor that gives the source proof that the path quality is wrong is an ROPc monitor. By the protect-the-innocent assumption, if cheating occurs, but the first hop is not a cheater, she must be able to claim the same size reward from the next ISP on the path, and thus pass on the punishment. The two possibilities for monitor correspond to which node has the burden of proof. Finally, we note that it is straightforward to derive the full version of the claim, which allows for multiple cheaters. 5.1 Implementing Monitors Claim 5 should not be interpreted as a statement that each node must compute the rest of path quality locally, without input from other nodes. SPATHc: (source-based path) A monitor that gives the source proof of what the data path is at any time, at least as far as it matches the equilibrium path. With these monitors, a punishment mechanism can be designed to fulfill the conditions of Claim 5. This mechanism can potentially involve external institutions. Of course, mechanisms can be designed to combine these monitors at much lower cost. As an aside, intuition might dictate that SHOPc generates more information than ROPc; after all, inferring individual node qualities seems a much harder problem.", "body2": "In recent decades, numerous ideas have been developed in universities, implemented in code, and even written into the routers and end systems of the network, only to languish as network operators fail to turn them on on a large scale. In the long term, this pathology stands out as a critical obstacle to the network\"s continued success (Ratnasamy, Shenker, and McCanne provide extensive discussion in ). There is simply no reward for introducing new services or investing in quality improvements. The second strategy suggests a revision of the contracting system. They argue that such a move would improve stability and align incentives. Unlike the first two strategies, we are not aware of any previous studies that have connected accountability with the pathologies of commoditization or lack of innovation. Our model is highly stylized and may be considered preliminary: it is based on a single source sending data to a single destination. We will approach this problem from a clean-slate perspective, deriving the level of accountability needed to sustain an ideal competitive structure. Clark relates this to the network\"s military context, and finds that had the network been designed for commercial development, accountability would have been a top priority. Details of where and why drops occur are deliberately concealed. This payment is a function of the past and present behaviors of the participants, but only those that are verifiable. Hence, we imagine that a contract only accepts proofs as inputs. We make the assumption that all payments are mediated by contracts. This means that without contractible monitors that attest to, say, latency, payments cannot be conditioned on latency. Nodes cannot condition payments on either quality or path characteristics. Access providers try to increase their quality to get more consumers. The process continues through the network, giving every ISP a competitive reason to increase quality. We can, for example, specify equilibria in which nodes switch to a new next hop in the event of a quality drop. These constraints limit their ability to punish cheaters. Furthermore, we will demonstrate a framework in which an ISP\"s maximum research investment decreases hyperbolically with its distance from the end-user. Finally, we require that the punishment mechanism that enforces contracts does not punish innocent nodes that are not in the coalition. In the extreme case, the cheater may force the other ISPs out of business, thereby gaining a monopoly on some routes. They may be purely local algorithms that listen for packet echoes. We show that new monitors must be implemented and integrated with the contracting system before the pathologies of commoditization and lack of innovation can be overcome. In addition, our results provide useful input for clean-slate architectural design, and we use several novel techniques that we expect will be applicable to a variety of future research. In section 5, we turn our attention to contractible monitors. In section 6, we conclude and present several directions for future research. All paths lead to D, and every node not connected to D has at least two choices for next hop. To each node, i, we associate a vector in the quality space, Qqi \u2208 . Let N Q\u2208q be the vector of all node qualities. For example, * can act as an addition in the case of latency, multiplication in the case of loss probability, or a minimumargument function in the case of security. Each node along the path, i, experiences some cost of transmission, ci. This excludes technologies that require adoption by all ISPs on the network to function. We defer this interesting topic to future work. Our model must draw this connection in a realistic fashion. Of course, these decisions depend on how routes are established, and how contracts determine actual monetary exchanges. This is represented schematically in Figure 2. In today\"s industry, these contracts include prices for transit access, and peering agreements. Other contracting systems we will explore will require more detail. Finally, beginning at 0=t , firms participate in the routing stage. For example, we will later ask whether a player will maintain higher quality than her contracts require, in the hope of keeping her customer base or attracting future customers. In opposition, Afergan and Wroclawski argue that contracts are formed in response to current traffic patterns, in a feedback loop . Our model is most realistic when contracting decisions are infrequent. The routing game includes only the routing stage. They cannot carry out threats they made in the innovation stage if it lowers their expected payoff. We will support this observation with analysis later. Relating our game to the familiar repeated prisoners\" dilemma, imagine that we are trying to impose a high quality, but costly path. A little more formally, Cheating refers to any action that an ISP can take, contrary to some target strategy point that we are trying to impose, that enhances her immediate payoff, but compromises the quality of the data path. Another possibility we can model, is that a node finds a way to save on its internal forwarding costs, at the expense of its own quality. Alternately, a node employing quality of service could give high priority packets a lower class of service, thus saving on resources and perhaps allowing itself to sell more high priority service. As we will see later, it is socially beneficial to decrease 0t , so such lock-in is detrimental to welfare. Thus, the network\"s lack of accountability is a fundamental obstacle to resolving these pathologies. In fact, it is similarly difficult to identify an ISP that cheats in route. An ISP may hand a packet off to a neighbor regardless of what routes that neighbor has advertised. Furthermore, blocks of addresses are summarized together for distant hosts, so a destination may not even be resolvable until packets are forwarded closer. As we will see below, this gives the cheater a significant advantage in escaping punishment for bad behavior, even if the data path is otherwise observable. The accountability properties of the Internet can be represented by the following monitors: E2E (End to End): A monitor that informs S/D about what the total path quality is at any time (this is the quality they experience). ROP (Rest of Path): A monitor that informs each node along the data path what the quality is for the rest of the path to the destination. Hence, for the sake of realism, it cannot be used to monitor what the data path is. Similar results also hold if alternate paths always converge after some integer number, m, of hops. For example, if a node after the first hop starts cheating, the first hop will detect the sudden drop in quality for the rest of the path, but the first hop cannot make the source believe this observation - the 187 source will suspect that the first hop was the cheater, and fabricated the claim against the rest of the path. For example, if the source has monitors that reveal the qualities of individual nodes, they could be combined with path information to create an ROP monitor. Restricting attention to linear contracts allows us to represent some node i\"s contract by its price, pi. We first focus on the routing game and ask whether a high quality route can be maintained, even when a low quality route is cheaper. Definition: The index of commoditization, CI , is the average, over each node on the data path, i, of i\"s flow profit as a fraction of i\"s quality premium, ( ) ijii dpcp /\u2212\u2212 . To show that this is true, we first need the following lemma, which will establish the difficulty of punishing nodes in the network. We will call the ratio, iiy \u03c0/ , the temptation to cheat. The proof proceeds by induction on the number of nodes on the equilibrium data path, n. For 1=n , there is a single node, say i. A little manipulation gives \u2212\u2212 01 00 rt rt , as required. So the source must give the first hop this much discounted time in order to punish defectors further down the line (and the source will expect poor quality during this period). Following the same argument as for 1=n , we can show that the full discounted time is \u220f \u22120 pathdataon rt i i i e which completes the proof. Suppose, on the other hand, that your data traverses two providers. With this lemma in hand, we can return to prove Claim 1. However, as \u221e\u2192n , we have 1/1 \u2192n T , which shows that \u221e\u2192CI . Though we suspect stronger claims are possible, we can demonstrate one such result by including an extra assumption: Available Bargain Path: A competitive market exists for lowcost transit, such that every node can route to the destination for no more than flow payment, lp . As a node gets farther from the source, its maximum payment approaches the bargain price, pl. Hence, the reward for innovation is bounded by the same amount. Large innovations, meaning substantially more expensive than rpl / , will not be pursued deep into the network. Comparing the intuitive argument that supported our hypothesis with these claims, we can see that we implicitly used an oversimplified model of market pressure (as either present or not). Hence, competitive forces degrade as the network deepens. Recall that in the previous section, we assumed that players couldn\"t convince each other of their private information. A verifiable monitor is a distributed algorithmic mechanism that runs on the network graph, and outputs, to specific nodes, proofs about current or past network behavior. With these monitors, each node observes the quality of the rest of the path and can also convince other players of these observations by giving them a proof. For example, the following lemma stands in contrast to Lemma 1. With this lemma in mind, it is easy to construct counterexamples to Claim 1 and Claim 2 in this new environment. Of course, if t0 is small, this effect is minimal. A coalition node may pretend to punish its successor, but instead enjoy a secret payment from the cheating node. The effects are not so benign in the real world. In the extreme, the cheater may use such a strategy to drive neighbors out of business, and thereby gain a monopoly on some routes. In this section, we will show how all of these drawbacks can be overcome. Can understand what the proofs and contracts represent to the extent required to police illegal activity. Can enforce payments among contracting parties. When a monitor (together with institutional infrastructure) meets these criteria, we will label it with a subscript c, for contractible. Similarly, E2Ec and ROPc are contractible versions of the monitors we are now familiar with. As the next claim shows, ROPc allows us to create a system of linear (price, quality) contracts under just such an equilibrium. With ROPc, for any feasible and consistent assignment of rest of path qualities to nodes, and any corresponding payment schedule that yields non-negative payoffs, these qualities can be maintained with bilateral contracts in a fixed-route coalition-proof protect-the-innocent equilibrium. Such proofs can be submitted every 0t for the previous interval. The sequence of payments must end by the destination, so the net outflow of P must come from the nodes in C. This establishes all necessary conditions of the equilibrium. Building upon this result, we can construct competition games in which nodes offer various qualities to each other at specified prices, and can credibly commit to meet these performance targets, even allowing for coalitions and a desire to damage other ISPs. The Stackelberg price-quality competition game yields optimal routes in SPE. We omit the full analysis here. The incentive structure of the VCG mechanism is what motivates nodes to report their costs accurately. Unfortunately, the source may end up paying more than the utility of the path. Notice that with just E2Ec, a weaker version of Claim 3 holds. As the next claim will show, rest of path monitoring is also necessary to construct such games under our solution concept. Definition: A competition game is nowhere-commoditized if for each node, i, not adjacent to D, there is some assignment of qualities and marginal costs to nodes, such that the optimal data path includes i, and i has a positive temptation to cheat. In the case of linear contracts, it is sufficient to require that \u221e<CI , and that every node make positive profit under some assignment of qualities and marginal costs. ROPc\": gives a node proof that the path quality from that node to the destination is correct. We will discuss the full version after the proof. Proof: First, because of the fixed-route assumption, punishments must be purely monetary. These nodes will not rationally offer the opposing type of proof. Thus, at least one of these monitors must exist. This argument can be iterated along the entire path to the penultimate node before D. Since the marginal costs and qualities can be arranged to make any path the optimal path, these statements must hold for all nodes and their children, which completes the proof. If submitting the proofs is costly, it seems natural that nodes would prefer to use the ROPc monitor, placing the burden of proof on the upstream node. Because of this, we have to further generalize our rest of path monitors, so they are less constrained in the case that there are cheaters on either side. This is represented by the following two monitors: SHOPc : (source-based hop quality) A monitor that gives the source proof of what the quality of node i is. SPATHc: (source-based path) A monitor that gives the source proof of what the data path is at any time, at least as far as it matches the equilibrium path. Our new monitor includes both the component monitors and whatever distributed algorithmic mechanism exists to make sure nodes share their proofs correctly. Hence, the entire discovery process acts as a rest of path monitor, albeit a rather costly monitor in this case. We defer these interesting mechanisms to future work. Hence, ROPc monitors do not exist, and therefore the requirements of Claim 5 cannot hold.", "introduction": "Many studies before us have noted the Internet\"s resistance to new services and evolution. In recent decades, numerous ideas have been developed in universities, implemented in code, and even written into the routers and end systems of the network, only to languish as network operators fail to turn them on on a large scale. The list includes Multicast, IPv6, IntServ, and DiffServ. Lacking the incentives just to activate services, there seems to be little hope of ISPs devoting adequate resources to developing new ideas. In the long term, this pathology stands out as a critical obstacle to the network\"s continued success (Ratnasamy, Shenker, and McCanne provide extensive discussion in ). On a smaller time scale, ISPs shun new services in favor of cost cutting measures. Thus, the network has characteristics of a commodity market. Although in theory, ISPs have a plethora of routing policies at their disposal, the prevailing strategy is to route in the cheapest way possible . On one hand, this leads directly to suboptimal routing. More importantly, commoditization in the short term is surely related to the lack of innovation in the long term. When the routing decisions of others ignore quality characteristics, ISPs are motivated only to lower costs. There is simply no reward for introducing new services or investing in quality improvements. In response to these pathologies and others, researchers have put forth various proposals for improving the situation. These can be divided according to three high-level strategies: The first attempts to improve the status quo by empowering end-users. Clark, et al., suggest that giving end-users control over routing would lead to greater service diversity, recognizing that some payment mechanism must also be provided . Ratnasamy, Shenker, and McCanne postulate a link between network evolution and user-directed routing . They propose a system of Anycast to give end-users the ability to tunnel their packets to an ISP that introduces a desirable protocol. The extra traffic to the ISP, the authors suggest, will motivate the initial investment. The second strategy suggests a revision of the contracting system. This is exemplified by MacKie-Mason and Varian, who propose a smart market to control access to network resources . Prices are set to the market-clearing level based on bids that users associate to their traffic. In another direction, Afergan and Wroclawski suggest that prices should be explicitly encoded in the routing protocols . They argue that such a move would improve stability and align incentives. The third high-level strategy calls for greater network accountability. In this vein, Argyraki, et al., propose a system of packet obituaries to provide feedback as to which ISPs drop packets . They argue that such feedback would help reveal which ISPs were adequately meeting their contractual obligations. Unlike the first two strategies, we are not aware of any previous studies that have connected accountability with the pathologies of commoditization or lack of innovation. It is clear that these three strategies are closely linked to each other (for example, , , and each argue that giving end-users routing control within the current contracting system is problematic). Until today, however, the relationship between them has been poorly understood. There is currently little theoretical foundation to compare the relative merits of each proposal, and a particular lack of evidence linking accountability with innovation and service differentiation. This paper will address both issues. We will begin by introducing an economic network model that relates accountability, contracts, competition, and innovation. Our model is highly stylized and may be considered preliminary: it is based on a single source sending data to a single destination. Nevertheless, the structure is rich enough to expose previously unseen features of network behavior. We will use our model for two main purposes: First, we will use our model to argue that the lack of accountability in today\"s network is a fundamental obstacle to overcoming the pathologies of commoditization and lack of innovation. In other words, unless new monitoring capabilities are introduced, and integrated with the system of contracts, the network cannot achieve optimal routing and innovation characteristics. This result provides motivation for the remainder of the paper, in which we explore how accountability can be leveraged to overcome these pathologies and create a sustainable industry. We will approach this problem from a clean-slate perspective, deriving the level of accountability needed to sustain an ideal competitive structure. When we say that today\"s Internet has poor accountability, we mean that it reveals little information about the behavior - or misbehavior - of ISPs. This well-known trait is largely rooted in the network\"s history. In describing the design philosophy behind the Internet protocols, Clark lists accountability as the least important among seven second level goals. Accordingly, accountability received little attention during the network\"s formative years. Clark relates this to the network\"s military context, and finds that had the network been designed for commercial development, accountability would have been a top priority. Argyraki, et al., conjecture that applying the principles of layering and transparency may have led to the network\"s lack of accountability . According to these principles, end hosts should be informed of network problems only to the extent that they are required to adapt. They notice when packet drops occur so that they can perform congestion control and retransmit packets. Details of where and why drops occur are deliberately concealed. The network\"s lack of accountability is highly relevant to a discussion of innovation because it constrains the system of contracts. This is because contracts depend upon external institutions to function - the judge in the language of incomplete contract theory, or simply the legal system. Ultimately, if a judge cannot verify that some condition holds, she cannot enforce a contract based on that condition. Of course, the vast majority of contracts never end up in court. Especially when a judge\"s ruling is easily predicted, the parties will typically comply with the contract terms on their own volition. This would not be possible, however, without the judge acting as a last resort. An institution to support contracts is typically complex, but we abstract it as follows: We imagine that a contract is an algorithm that outputs a payment transfer among a set of ISPs (the parties) at every time. This payment is a function of the past and present behaviors of the participants, but only those that are verifiable. Hence, we imagine that a contract only accepts proofs as inputs. We will call any process that generates these proofs a contractible monitor. Such a monitor includes metering or sensing devices on the physical network, but it is a more general concept. Constructing a proof of a particular behavior may require readings from various devices distributed among many ISPs. The contractible monitor includes whatever distributed algorithmic mechanism is used to motivate ISPs to share this private information. Figure 1 demonstrates how our model of contracts fits together. We make the assumption that all payments are mediated by contracts. This means that without contractible monitors that attest to, say, latency, payments cannot be conditioned on latency. Figure 1: Relationship between monitors and contracts With this model, we may conclude that the level of accountability in today\"s Internet only permits best effort contracts. Nodes cannot condition payments on either quality or path characteristics. Is there anything wrong with best-effort contracts? The reader might wonder why the Internet needs contracts at all. After all, in non-network industries, traditional firms invest in research and differentiate their products, all in the hopes of keeping their customers and securing new ones. One might believe that such market forces apply to ISPs as well. We may adopt this as our null hypothesis: Null hypothesis: Market forces are sufficient to maintain service diversity and innovation on a network, at least to the same extent as they do in traditional markets. There is a popular intuitive argument that supports this hypothesis, and it may be summarized as follows: Intuitive argument supporting null hypothesis: 1. Access providers try to increase their quality to get more consumers. Access providers are themselves customers for second hop ISPs, and the second hops will therefore try to provide highquality service in order to secure traffic from access providers. Access providers try to select high quality transit because that increases their quality. The process continues through the network, giving every ISP a competitive reason to increase quality. We are careful to model our network in continuous time, in order to capture the essence of this argument. We can, for example, specify equilibria in which nodes switch to a new next hop in the event of a quality drop. Moreover, our model allows us to explore any theoretically possible punishments against cheaters, including those that are costly for end-users to administer. By contrast, customers in the real world rarely respond collectively, and often simply seek the best deal currently offered. These constraints limit their ability to punish cheaters. Even with these liberal assumptions, however, we find that we must reject our null hypothesis. Our model will demonstrate that identifying a cheating ISP is difficult under low accountability, limiting the threat of market driven punishment. We will define an index of commoditization and show that it increases without bound as data paths grow long. Furthermore, we will demonstrate a framework in which an ISP\"s maximum research investment decreases hyperbolically with its distance from the end-user. Network Behavior Monitor Contract Proof Payments 184 To summarize, we argue that the Internet\"s lack of accountability must be addressed before the pathologies of commoditization and lack of innovation can be resolved. This leads us to our next topic: How can we leverage accountability to overcome these pathologies? We approach this question from a clean-slate perspective. Instead of focusing on incremental improvements, we try to imagine how an ideal industry would behave, then derive the level of accountability needed to meet that objective. According to this approach, we first craft a new equilibrium concept appropriate for network competition. Our concept includes the following requirements: First, we require that punishing ISPs that cheat is done without rerouting the path. Rerouting is likely to prompt end-users to switch providers, punishing access providers who administer punishments correctly. Next, we require that the equilibrium cannot be threatened by a coalition of ISPs that exchanges illicit side payments. Finally, we require that the punishment mechanism that enforces contracts does not punish innocent nodes that are not in the coalition. The last requirement is somewhat unconventional from an economic perspective, but we maintain that it is crucial for any reasonable solution. Although ISPs provide complementary services when they form a data path together, they are likely to be horizontal competitors as well. If innocent nodes may be punished, an ISP may decide to deliberately cheat and draw punishment onto itself and its neighbors. By cheating, the ISP may save resources, thereby ensuring that the punishment is more damaging to the other ISPs, which probably compete with the cheater directly for some customers. In the extreme case, the cheater may force the other ISPs out of business, thereby gaining a monopoly on some routes. Applying this equilibrium concept, we derive the monitors needed to maintain innovation and optimize routes. The solution is surprisingly simple: contractible monitors must report the quality of the rest of the path, from each ISP to the destination. It turns out that this is the correct minimum accountability requirement, as opposed to either end-to-end monitors or hop-by-hop monitors, as one might initially suspect. Rest of path monitors can be implemented in various ways. They may be purely local algorithms that listen for packet echoes. Alternately, they can be distributed in nature. We describe a way to construct a rest of path monitor out of monitors for individual ISP quality and for the data path. This requires a mechanism to motivate ISPs to share their monitor outputs with each other. The rest of path monitor then includes the component monitors and the distributed algorithmic mechanism that ensures that information is shared as required. This example shows that other types of monitors may be useful as building blocks, but must be combined to form rest of path monitors in order to achieve ideal innovation characteristics. Our study has several practical implications for future protocol design. We show that new monitors must be implemented and integrated with the contracting system before the pathologies of commoditization and lack of innovation can be overcome. Moreover, we derive exactly what monitors are needed to optimize routes and support innovation. In addition, our results provide useful input for clean-slate architectural design, and we use several novel techniques that we expect will be applicable to a variety of future research. The rest of this paper is organized as follows: In section 2, we lay out our basic network model. In section 3, we present a lowaccountability network, modeled after today\"s Internet. We demonstrate how poor monitoring causes commoditization and a lack of innovation. In section 4, we present verifiable monitors, and show that proofs, even without contracts, can improve the status quo. In section 5, we turn our attention to contractible monitors. We show that rest of path monitors can support competition games with optimal routing and innovation. We further show that rest of path monitors are required to support such competition games. We continue by discussing how such monitors may be constructed using other monitors as building blocks. In section 6, we conclude and present several directions for future research.", "conclusion": "It is our hope that this study will have a positive impact in at least three different ways.. The first is practical: we believe our analysis has implications for the design of future monitoring protocols and for public policy.. For protocol designers, we first provide fresh motivation to create monitoring systems.. We have argued that the poor accountability of the Internet is a fundamental obstacle to alleviating the pathologies of commoditization and lack of innovation.. Unless accountability improves, these pathologies are guaranteed to remain.. Secondly, we suggest directions for future advances in monitoring.. We have shown that adding verifiability to monitors allows for some improvements in the characteristics of competition.. At the same time, this does not present a fully satisfying solution.. This paper has suggested a novel standard for monitors to aspire to - one of supporting optimal routes in innovative competition games under fixed-route coalition-proof protect-the-innocent equilibrium.. We have shown that under bilateral contracts, this specifically requires contractible rest of path monitors.. This is not to say that other types of monitors are unimportant.. We included an example in which individual hop quality monitors and a path monitor can also meet our standard for sustaining competition.. However, in order for this to happen, a mechanism must be included 192 to combine proofs from these monitors to form a proof of rest of path quality.. In other words, the monitors must ultimately be combined to form contractible rest of path monitors.. To support service differentiation and innovation, it may be easier to design rest of path monitors directly, thereby avoiding the task of designing mechanisms for combining component monitors.. As far as policy implications, our analysis points to the need for legal institutions to enforce contracts based on quality.. These institutions must be equipped to verify proofs of quality, and police illegal contracting behavior.. As quality-based contracts become numerous and complicated, and possibly negotiated by machine, this may become a challenging task, and new standards and regulations may have to emerge in response.. This remains an interesting and unexplored area for research.. The second area we hope our study will benefit is that of clean-slate architectural design.. Traditionally, clean-slate design tends to focus on creating effective and elegant networks for a static set of requirements.. Thus, the approach is often one of engineering, which tends to neglect competitive effects.. We agree with Ratnasamy, Shenker, and McCanne, that designing for evolution should be a top priority .. We have demonstrated that the network\"s monitoring ability is critical to supporting innovation, as are the institutions that support contracting.. These elements should feature prominently in new designs.. Our analysis specifically suggests that architectures based on bilateral contracts should include contractible rest of path monitoring.. From a clean-slate perspective, these monitors can be transparently and fully integrated with the routing and contracting systems.. Finally, the last contribution our study makes is methodological.. We believe that the mathematical formalization we present is applicable to a variety of future research questions.. While a significant literature addresses innovation in the presence of network effects, to the best of our knowledge, ours is the first model of innovation in a network industry that successfully incorporates the actual topological structure as input.. This allows the discovery of new properties, such as the weakening of market forces with the number of ISPs on a data path that we observe with lowaccountability.. Our method also stands in contrast to the typical approach of distributed algorithmic mechanism design.. Because this field is based on a principle-agent framework, contracts are usually proposed by the source, who is allowed to make a take it or leave it offer to network nodes.. Our technique allows contracts to emerge from a competitive framework, so the source is limited to selecting the most desirable contract.. We believe this is a closer reflection of the industry.. Based on the insights in this study, the possible directions for future research are numerous and exciting.. To some degree, contracting based on quality opens a Pandora\"s Box of pressing questions: Do quality-based contracts stand counter to the principle of network neutrality?. Should ISPs be allowed to offer a choice of contracts at different quality levels?. What anti-competitive behaviors are enabled by quality-based contracts?. Can a contracting system support optimal multicast trees?. In this study, we have focused on bilateral contracts.. This system has seemed natural, especially since it is the prevalent system on the current network.. Perhaps its most important benefit is that each contract is local in nature, so both parties share a common, familiar legal jurisdiction.. There is no need to worry about who will enforce a punishment against another ISP on the opposite side of the planet, nor is there a dispute over whose legal rules to apply in interpreting a contract.. Although this benefit is compelling, it is worth considering other systems.. The clearest alternative is to form a contract between the source and every node on the path.. We may call these source contracts.. Source contracting may present surprising advantages.. For instance, since ISPs do not exchange money with each other, an ISP cannot save money by selecting a cheaper next hop.. Additionally, if the source only has contracts with nodes on the intended path, other nodes won\"t even be willing to accept packets from this source since they won\"t receive compensation for carrying them.. This combination seems to eliminate all temptation for a single cheater to cheat in route.. Because of this and other encouraging features, we believe source contracts are a fertile topic for further study.. Another important research task is to relax our assumption that quality can be measured fully and precisely.. One possibility is to assume that monitoring is only probabilistic or suffers from noise.. Even more relevant is the possibility that quality monitors are fundamentally incomplete.. A quality monitor can never anticipate every dimension of quality that future applications will care about, nor can it anticipate a new and valuable protocol that an ISP introduces.. We may define a monitor space as a subspace of the quality space that a monitor can measure, QM \u2282 , and a corresponding monitoring function that simply projects the full range of qualities onto the monitor space, MQm \u2192: .. Clearly, innovations that leave quality invariant under m are not easy to support - they are invisible to the monitoring system.. In this environment, we expect that path monitoring becomes more important, since it is the only way to ensure data reaches certain innovator ISPs.. Further research is needed to understand this process."}
{"id": "H-35", "keywords": ["inform retriev", "learn to rank", "boost"], "title": "AdaRank: A Boosting Algorithm for Information Retrieval", "abstract": "In this paper we address the issue of learning to rank for document retrieval. In the task, a model is automatically created with some training data and then is utilized for ranking of documents. The goodness of a model is usually evaluated with performance measures such as MAP (Mean Average Precision) and NDCG (Normalized Discounted Cumulative Gain). Ideally a learning algorithm would train a ranking model that could directly optimize the performance measures with respect to the training data. Existing methods, however, are only able to train ranking models by minimizing loss functions loosely related to the performance measures. For example, Ranking SVM and RankBoost train ranking models by minimizing classification errors on instance pairs. To deal with the problem, we propose a novel learning algorithm within the framework of boosting, which can minimize a loss function directly defined on the performance measures. Our algorithm, referred to as AdaRank, repeatedly constructs 'weak rankers' on the basis of reweighted training data and finally linearly combines the weak rankers for making ranking predictions. We prove that the training process of AdaRank is exactly that of enhancing the performance measure used. Experimental results on four benchmark datasets show that AdaRank significantly outperforms the baseline methods of BM25, Ranking SVM, and RankBoost.", "references": ["Modern Information Retrieval", "Learning to rank with nonsmooth cost functions", "Learning to rank using gradient descent", "Adapting ranking SVM to document retrieval", "Subset ranking using regression", "Overview of the TREC 2003 web track", "Boosting methods for regression", "An efficient boosting algorithm for combining preferences", "A decision-theoretic generalization of on-line learning and an application to boosting", "Additive logistic regression: A statistical view of boosting", "Learning rankings via convex hull separation", "The Elements of Statistical Learning", "Large Margin rank boundaries for ordinal regression", "Ohsumed: an interactive retrieval evaluation and new large test collection for research", "IR evaluation methods for retrieving highly relevant documents", "Optimizing search engines using clickthrough data", "A support vector method for multivariate performance measures", "Document language models, query models, and risk minimization for information retrieval", "Direct maximization of rank-based metrics for information retrieval", "Discriminative models for information retrieval", "The pagerank citation ranking: Bringing order to the web", "A language modeling approach to information retrieval", "A study of relevance propagation for web search", "The TREC-9 filtering track final report", "Boosting the margin: A new explanation for the effectiveness of voting methods", "Improved boosting algorithms using confidence-rated predictions", "Microsoft Research Asia at web track and terabyte track of TREC 2004", "Learning to rank", "Cost-sensitive learning of SVM for ranking", "Exploiting the hierarchical structure for link analysis", "SVM selective sampling for ranking with application to data retrieval"], "full_text": "1. INTRODUCTION Recently \u2018learning to rank\" has gained increasing attention in both the fields of information retrieval and machine learning. When applied to document retrieval, learning to rank becomes a task as follows. In training, a ranking model is constructed with data consisting of queries, their corresponding retrieved documents, and relevance levels given by humans. In ranking, given a new query, the corresponding retrieved documents are sorted by using the trained ranking model. In document retrieval, usually ranking results are evaluated in terms of performance measures such as MAP (Mean Average Precision) and NDCG (Normalized Discounted Cumulative Gain) . Ideally, the ranking function is created so that the accuracy of ranking in terms of one of the measures with respect to the training data is maximized. Several methods for learning to rank have been developed and applied to document retrieval. For example, Herbrich et al. propose a learning algorithm for ranking on the basis of Support Vector Machines, called Ranking SVM. Freund et al. take a similar approach and perform the learning by using boosting, referred to as RankBoost. All the existing methods used for document retrieval are designed to optimize loss functions loosely related to the IR performance measures, not loss functions directly based on the measures. For example, Ranking SVM and RankBoost train ranking models by minimizing classification errors on instance pairs. In this paper, we aim to develop a new learning algorithm that can directly optimize any performance measure used in document retrieval. Inspired by the work of AdaBoost for classification , we propose to develop a boosting algorithm for information retrieval, referred to as AdaRank. AdaRank utilizes a linear combination of \u2018weak rankers\" as its model. In learning, it repeats the process of re-weighting the training sample, creating a weak ranker, and calculating a weight for the ranker. We show that AdaRank algorithm can iteratively optimize an exponential loss function based on any of IR performance measures. A lower bound of the performance on training data is given, which indicates that the ranking accuracy in terms of the performance measure can be continuously improved during the training process. AdaRank offers several advantages: ease in implementation, theoretical soundness, efficiency in training, and high accuracy in ranking. Experimental results indicate that AdaRank can outperform the baseline methods of BM25, Ranking SVM, and RankBoost, on four benchmark datasets including OHSUMED, WSJ, AP, and .Gov. Tuning ranking models using certain training data and a performance measure is a common practice in IR . As the number of features in the ranking model gets larger and the amount of training data gets larger, the tuning becomes harder. From the viewpoint of IR, AdaRank can be viewed as a machine learning method for ranking model tuning. Recently, direct optimization of performance measures in learning has become a hot research topic. Several methods for classification and ranking have been proposed. AdaRank can be viewed as a machine learning method for direct optimization of performance measures, based on a different approach. The rest of the paper is organized as follows. After a summary of related work in Section 2, we describe the proposed AdaRank algorithm in details in Section 3. Experimental results and discussions are given in Section 4. Section 5 concludes this paper and gives future work. 2. RELATED WORK 2.1 Information Retrieval The key problem for document retrieval is ranking, specifically, how to create the ranking model (function) that can sort documents based on their relevance to the given query. It is a common practice in IR to tune the parameters of a ranking model using some labeled data and one performance measure . For example, the state-ofthe-art methods of BM25 and LMIR (Language Models for Information Retrieval) all have parameters to tune. As the ranking models become more sophisticated (more features are used) and more labeled data become available, how to tune or train ranking models turns out to be a challenging issue. Recently methods of \u2018learning to rank\" have been applied to ranking model construction and some promising results have been obtained. For example, Joachims applies Ranking SVM to document retrieval. He utilizes click-through data to deduce training data for the model creation. Cao et al. adapt Ranking SVM to document retrieval by modifying the Hinge Loss function to better meet the requirements of IR. Specifically, they introduce a Hinge Loss function that heavily penalizes errors on the tops of ranking lists and errors from queries with fewer retrieved documents. Burges et al. employ Relative Entropy as a loss function and Gradient Descent as an algorithm to train a Neural Network model for ranking in document retrieval. The method is referred to as \u2018RankNet\". 2.2 Machine Learning There are three topics in machine learning which are related to our current work. They are \u2018learning to rank\", boosting, and direct optimization of performance measures. Learning to rank is to automatically create a ranking function that assigns scores to instances and then rank the instances by using the scores. Several approaches have been proposed to tackle the problem. One major approach to learning to rank is that of transforming it into binary classification on instance pairs. This \u2018pair-wise\" approach fits well with information retrieval and thus is widely used in IR. Typical methods of the approach include Ranking SVM , RankBoost , and RankNet . For other approaches to learning to rank, refer to . In the pair-wise approach to ranking, the learning task is formalized as a problem of classifying instance pairs into two categories (correctly ranked and incorrectly ranked). Actually, it is known that reducing classification errors on instance pairs is equivalent to maximizing a lower bound of MAP . In that sense, the existing methods of Ranking SVM, RankBoost, and RankNet are only able to minimize loss functions that are loosely related to the IR performance measures. Boosting is a general technique for improving the accuracies of machine learning algorithms. The basic idea of boosting is to repeatedly construct \u2018weak learners\" by re-weighting training data and form an ensemble of weak learners such that the total performance of the ensemble is \u2018boosted\". Freund and Schapire have proposed the first well-known boosting algorithm called AdaBoost (Adaptive Boosting) , which is designed for binary classification (0-1 prediction). Later, Schapire & Singer have introduced a generalized version of AdaBoost in which weak learners can give confidence scores in their predictions rather than make 0-1 decisions . Extensions have been made to deal with the problems of multi-class classification , regression , and ranking . In fact, AdaBoost is an algorithm that ingeniously constructs a linear model by minimizing the \u2018exponential loss function\" with respect to the training data . Our work in this paper can be viewed as a boosting method developed for ranking, particularly for ranking in IR. Recently, a number of authors have proposed conducting direct optimization of multivariate performance measures in learning. For instance, Joachims presents an SVM method to directly optimize nonlinear multivariate performance measures like the F1 measure for classification. Cossock & Zhang find a way to approximately optimize the ranking performance measure DCG . Metzler et al. also propose a method of directly maximizing rank-based metrics for ranking on the basis of manifold learning. AdaRank is also one that tries to directly optimize multivariate performance measures, but is based on a different approach. AdaRank is unique in that it employs an exponential loss function based on IR performance measures and a boosting technique. 3. OUR METHOD: ADARANK 3.1 General Framework We first describe the general framework of learning to rank for document retrieval. In retrieval (testing), given a query the system returns a ranking list of documents in descending order of the relevance scores. The relevance scores are calculated with a ranking function (model). In learning (training), a number of queries and their corresponding retrieved documents are given. Furthermore, the relevance levels of the documents with respect to the queries are also provided. The relevance levels are represented as ranks (i.e., categories in a total order). The objective of learning is to construct a ranking function which achieves the best results in ranking of the training data in the sense of minimization of a loss function. Ideally the loss function is defined on the basis of the performance measure used in testing. Suppose that Y = {r1, r2, \u00b7 \u00b7 \u00b7 , r } is a set of ranks, where denotes the number of ranks. There exists a total order between the ranks r r \u22121 \u00b7 \u00b7 \u00b7 r1, where \u2018 \" denotes a preference relationship. In training, a set of queries Q = {q1, q2, \u00b7 \u00b7 \u00b7 , qm} is given. Each query qi is associated with a list of retrieved documents di = {di1, di2, \u00b7 \u00b7 \u00b7 , di,n(qi)} and a list of labels yi = {yi1, yi2, \u00b7 \u00b7 \u00b7 , yi,n(qi)}, where n(qi) denotes the sizes of lists di and yi, dij denotes the jth document in di, and yij \u2208 Y denotes the rank of document di j. A feature vector xij = \u03a8(qi, di j) \u2208 X is created from each query-document pair (qi, di j), i = 1, 2, \u00b7 \u00b7 \u00b7 , m; j = 1, 2, \u00b7 \u00b7 \u00b7 , n(qi). Thus, the training set can be represented as S = {(qi, di, yi)}m i=1. The objective of learning is to create a ranking function f : X \u2192 , such that for each query the elements in its corresponding document list can be assigned relevance scores using the function and then be ranked according to the scores. Specifically, we create a permutation of integers \u03c0(qi, di, f) for query qi, the corresponding list of documents di, and the ranking function f. Let di = {di1, di2, \u00b7 \u00b7 \u00b7 , di,n(qi)} be identified by the list of integers {1, 2, \u00b7 \u00b7 \u00b7 , n(qi)}, then permutation \u03c0(qi, di, f) is defined as a bijection from {1, 2, \u00b7 \u00b7 \u00b7 , n(qi)} to itself. We use \u03c0( j) to denote the position of item j (i.e., di j). The learning process turns out to be that of minimizing the loss function which represents the disagreement between the permutation \u03c0(qi, di, f) and the list of ranks yi, for all of the queries. Table 1: Notations and explanations. Notations Explanations qi \u2208 Q ith query di = {di1, di2, \u00b7 \u00b7 \u00b7 , di,n(qi)} List of documents for qi yi j \u2208 {r1, r2, \u00b7 \u00b7 \u00b7 , r } Rank of di j w.r.t. qi yi = {yi1, yi2, \u00b7 \u00b7 \u00b7 , yi,n(qi)} List of ranks for qi S = {(qi, di, yi)}m i=1 Training set xij = \u03a8(qi, dij) \u2208 X Feature vector for (qi, di j) f(xij) \u2208 Ranking model \u03c0(qi, di, f) Permutation for qi, di, and f ht(xi j) \u2208 tth weak ranker E(\u03c0(qi, di, f), yi) \u2208 [\u22121, +1] Performance measure function In the paper, we define the rank model as a linear combination of weak rankers: f(x) = T t=1 \u03b1tht(x), where ht(x) is a weak ranker, \u03b1t is its weight, and T is the number of weak rankers. In information retrieval, query-based performance measures are used to evaluate the \u2018goodness\" of a ranking function. By query based measure, we mean a measure defined over a ranking list of documents with respect to a query. These measures include MAP, NDCG, MRR (Mean Reciprocal Rank), WTA (Winners Take ALL), and Precision@n . We utilize a general function E(\u03c0(qi, di, f), yi) \u2208 [\u22121, +1] to represent the performance measures. The first argument of E is the permutation \u03c0 created using the ranking function f on di. The second argument is the list of ranks yi given by humans. E measures the agreement between \u03c0 and yi. Table 1 gives a summary of notations described above. Next, as examples of performance measures, we present the definitions of MAP and NDCG. Given a query qi, the corresponding list of ranks yi, and a permutation \u03c0i on di, average precision for qi is defined as: AvgPi = n(qi) j=1 Pi( j) \u00b7 yij n(qi) j=1 yij , (1) where yij takes on 1 and 0 as values, representing being relevant or irrelevant and Pi( j) is defined as precision at the position of dij: Pi( j) = k:\u03c0i(k)\u2264\u03c0i(j) yik \u03c0i(j) , (2) where \u03c0i( j) denotes the position of di j. Given a query qi, the list of ranks yi, and a permutation \u03c0i on di, NDCG at position m for qi is defined as: Ni = ni \u00b7 j:\u03c0i(j)\u2264m 2yi j \u2212 1 log(1 + \u03c0i( j)) , (3) where yij takes on ranks as values and ni is a normalization constant. ni is chosen so that a perfect ranking \u03c0\u2217 i \"s NDCG score at position m is 1. 3.2 Algorithm Inspired by the AdaBoost algorithm for classification, we have devised a novel algorithm which can optimize a loss function based on the IR performance measures. The algorithm is referred to as \u2018AdaRank\" and is shown in Figure 1. AdaRank takes a training set S = {(qi, di, yi)}m i=1 as input and takes the performance measure function E and the number of iterations T as parameters. AdaRank runs T rounds and at each round it creates a weak ranker ht(t = 1, \u00b7 \u00b7 \u00b7 , T). Finally, it outputs a ranking model f by linearly combining the weak rankers. At each round, AdaRank maintains a distribution of weights over the queries in the training data. We denote the distribution of weights Input: S = {(qi, di, yi)}m i=1, and parameters E and T Initialize P1(i) = 1/m. For t = 1, \u00b7 \u00b7 \u00b7 , T \u2022 Create weak ranker ht with weighted distribution Pt on training data S . \u2022 Choose \u03b1t \u03b1t = \u00b7 ln i=1 Pt(i){1 + E(\u03c0(qi, di, ht), yi)} i=1 Pt(i){1 \u2212 E(\u03c0(qi, di, ht), yi)} \u2022 Create ft ft(x) = k=1 \u03b1khk(x). \u2022 Update Pt+1 Pt+1(i) = exp{\u2212E(\u03c0(qi, di, ft), yi)} j=1 exp{\u2212E(\u03c0(qj, dj, ft), yj)} End For Output ranking model: f(x) = fT (x). Figure 1: The AdaRank algorithm. at round t as Pt and the weight on the ith training query qi at round t as Pt(i). Initially, AdaRank sets equal weights to the queries. At each round, it increases the weights of those queries that are not ranked well by ft, the model created so far. As a result, the learning at the next round will be focused on the creation of a weak ranker that can work on the ranking of those \u2018hard\" queries. At each round, a weak ranker ht is constructed based on training data with weight distribution Pt. The goodness of a weak ranker is measured by the performance measure E weighted by Pt: i=1 Pt(i)E(\u03c0(qi, di, ht), yi). Several methods for weak ranker construction can be considered. For example, a weak ranker can be created by using a subset of queries (together with their document list and label list) sampled according to the distribution Pt. In this paper, we use single features as weak rankers, as will be explained in Section 3.6. Once a weak ranker ht is built, AdaRank chooses a weight \u03b1t > 0 for the weak ranker. Intuitively, \u03b1t measures the importance of ht. A ranking model ft is created at each round by linearly combining the weak rankers constructed so far h1, \u00b7 \u00b7 \u00b7 , ht with weights \u03b11, \u00b7 \u00b7 \u00b7 , \u03b1t. ft is then used for updating the distribution Pt+1. 3.3 Theoretical Analysis The existing learning algorithms for ranking attempt to minimize a loss function based on instance pairs (document pairs). In contrast, AdaRank tries to optimize a loss function based on queries. Furthermore, the loss function in AdaRank is defined on the basis of general IR performance measures. The measures can be MAP, NDCG, WTA, MRR, or any other measures whose range is within [\u22121, +1]. We next explain why this is the case. Ideally we want to maximize the ranking accuracy in terms of a performance measure on the training data: max f\u2208F i=1 E(\u03c0(qi, di, f), yi), (4) where F is the set of possible ranking functions. This is equivalent to minimizing the loss on the training data min f\u2208F i=1 (1 \u2212 E(\u03c0(qi, di, f), yi)). (5) It is difficult to directly optimize the loss, because E is a noncontinuous function and thus may be difficult to handle. We instead attempt to minimize an upper bound of the loss in (5) min f\u2208F i=1 exp{\u2212E(\u03c0(qi, di, f), yi)}, (6) because e\u2212x \u2265 1 \u2212 x holds for any x \u2208 . We consider the use of a linear combination of weak rankers as our ranking model: f(x) = t=1 \u03b1tht(x). (7) The minimization in (6) then turns out to be min ht\u2208H,\u03b1t\u2208 + L(ht, \u03b1t) = i=1 exp{\u2212E(\u03c0(qi, di, ft\u22121 + \u03b1tht), yi)}, (8) where H is the set of possible weak rankers, \u03b1t is a positive weight, and ( ft\u22121 + \u03b1tht)(x) = ft\u22121(x) + \u03b1tht(x). Several ways of computing coefficients \u03b1t and weak rankers ht may be considered. Following the idea of AdaBoost, in AdaRank we take the approach of \u2018forward stage-wise additive modeling\" and get the algorithm in Figure 1. It can be proved that there exists a lower bound on the ranking accuracy for AdaRank on training data, as presented in Theorem 1. T\uf768\uf765\uf76f\uf772\uf765\uf76d 1. The following bound holds on the ranking accuracy of the AdaRank algorithm on training data: i=1 E(\u03c0(qi, di, fT ), yi) \u2265 1 \u2212 t=1 e\u2212\u03b4t min 1 \u2212 \u03d5(t)2, where \u03d5(t) = m i=1 Pt(i)E(\u03c0(qi, di, ht), yi), \u03b4t min = mini=1,\u00b7\u00b7\u00b7 ,m \u03b4t i, and \u03b4t i = E(\u03c0(qi, di, ft\u22121 + \u03b1tht), yi) \u2212 E(\u03c0(qi, di, ft\u22121), yi) \u2212\u03b1tE(\u03c0(qi, di, ht), yi), for all i = 1, 2, \u00b7 \u00b7 \u00b7 , m and t = 1, 2, \u00b7 \u00b7 \u00b7 , T. A proof of the theorem can be found in appendix. The theorem implies that the ranking accuracy in terms of the performance measure can be continuously improved, as long as e\u2212\u03b4t min 1 \u2212 \u03d5(t)2 < 1 holds. 3.4 Advantages AdaRank is a simple yet powerful method. More importantly, it is a method that can be justified from the theoretical viewpoint, as discussed above. In addition AdaRank has several other advantages when compared with the existing learning to rank methods such as Ranking SVM, RankBoost, and RankNet. First, AdaRank can incorporate any performance measure, provided that the measure is query based and in the range of [\u22121, +1]. Notice that the major IR measures meet this requirement. In contrast the existing methods only minimize loss functions that are loosely related to the IR measures . Second, the learning process of AdaRank is more efficient than those of the existing learning algorithms. The time complexity of AdaRank is of order O((k+T)\u00b7m\u00b7n log n), where k denotes the number of features, T the number of rounds, m the number of queries in training data, and n is the maximum number of documents for queries in training data. The time complexity of RankBoost, for example, is of order O(T \u00b7 m \u00b7 n2 ) . Third, AdaRank employs a more reasonable framework for performing the ranking task than the existing methods. Specifically in AdaRank the instances correspond to queries, while in the existing methods the instances correspond to document pairs. As a result, AdaRank does not have the following shortcomings that plague the existing methods. (a) The existing methods have to make a strong assumption that the document pairs from the same query are independently distributed. In reality, this is clearly not the case and this problem does not exist for AdaRank. (b) Ranking the most relevant documents on the tops of document lists is crucial for document retrieval. The existing methods cannot focus on the training on the tops, as indicated in . Several methods for rectifying the problem have been proposed (e.g., ), however, they do not seem to fundamentally solve the problem. In contrast, AdaRank can naturally focus on training on the tops of document lists, because the performance measures used favor rankings for which relevant documents are on the tops. (c) In the existing methods, the numbers of document pairs vary from query to query, resulting in creating models biased toward queries with more document pairs, as pointed out in . AdaRank does not have this drawback, because it treats queries rather than document pairs as basic units in learning. 3.5 Differences from AdaBoost AdaRank is a boosting algorithm. In that sense, it is similar to AdaBoost, but it also has several striking differences from AdaBoost. First, the types of instances are different. AdaRank makes use of queries and their corresponding document lists as instances. The labels in training data are lists of ranks (relevance levels). AdaBoost makes use of feature vectors as instances. The labels in training data are simply +1 and \u22121. Second, the performance measures are different. In AdaRank, the performance measure is a generic measure, defined on the document list and the rank list of a query. In AdaBoost the corresponding performance measure is a specific measure for binary classification, also referred to as \u2018margin\" . Third, the ways of updating weights are also different. In AdaBoost, the distribution of weights on training instances is calculated according to the current distribution and the performance of the current weak learner. In AdaRank, in contrast, it is calculated according to the performance of the ranking model created so far, as shown in Figure 1. Note that AdaBoost can also adopt the weight updating method used in AdaRank. For AdaBoost they are equivalent (cf., page 305). However, this is not true for AdaRank. 3.6 Construction of Weak Ranker We consider an efficient implementation for weak ranker construction, which is also used in our experiments. In the implementation, as weak ranker we choose the feature that has the optimal weighted performance among all of the features: max i=1 Pt(i)E(\u03c0(qi, di, xk), yi). Creating weak rankers in this way, the learning process turns out to be that of repeatedly selecting features and linearly combining the selected features. Note that features which are not selected in the training phase will have a weight of zero. 4. EXPERIMENTAL RESULTS We conducted experiments to test the performances of AdaRank using four benchmark datasets: OHSUMED, WSJ, AP, and .Gov. Table 2: Features used in the experiments on OHSUMED, WSJ, and AP datasets. C(w, d) represents frequency of word w in document d; C represents the entire collection; n denotes number of terms in query; | \u00b7 | denotes the size function; and id f(\u00b7) denotes inverse document frequency. 1 wi\u2208q d ln(c(wi, d) + 1) 2 wi\u2208q d ln( |C| c(wi,C) + 1) 3 wi\u2208q d ln(id f(wi)) 4 wi\u2208q d ln(c(wi,d) |d| + 1) 5 wi\u2208q d ln(c(wi,d) |d| \u00b7 id f(wi) + 1) 6 wi\u2208q d ln(c(wi,d)\u00b7|C| |d|\u00b7c(wi,C) + 1) 7 ln(BM25 score) 0.2 0.3 0.4 0.5 0.6 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RarnkBoost AdaRank.MAP AdaRank.NDCG Figure 2: Ranking accuracies on OHSUMED data. 4.1 Experiment Setting Ranking SVM and RankBoost were selected as baselines in the experiments, because they are the state-of-the-art learning to rank methods. Furthermore, BM25 was used as a baseline, representing the state-of-the-arts IR method (we actually used the tool Lemur1 ). For AdaRank, the parameter T was determined automatically during each experiment. Specifically, when there is no improvement in ranking accuracy in terms of the performance measure, the iteration stops (and T is determined). As the measure E, MAP and NDCG@5 were utilized. The results for AdaRank using MAP and NDCG@5 as measures in training are represented as AdaRank.MAP and AdaRank.NDCG, respectively. 4.2 Experiment with OHSUMED Data In this experiment, we made use of the OHSUMED dataset to test the performances of AdaRank. The OHSUMED dataset consists of 348,566 documents and 106 queries. There are in total 16,140 query-document pairs upon which relevance judgments are made. The relevance judgments are either \u2018d\" (definitely relevant), \u2018p\" (possibly relevant), or \u2018n\"(not relevant). The data have been used in many experiments in IR, for example . As features, we adopted those used in document retrieval . Table 2 shows the features. For example, tf (term frequency), idf (inverse document frequency), dl (document length), and combinations of them are defined as features. BM25 score itself is also a feature. Stop words were removed and stemming was conducted in the data. We randomly divided queries into four even subsets and conducted 4-fold cross-validation experiments. We tuned the parameters for BM25 during one of the trials and applied them to the other trials. The results reported in Figure 2 are those averaged over four trials. In MAP calculation, we define the rank \u2018d\" as relevant and Table 3: Statistics on WSJ and AP datasets. Dataset # queries # retrieved docs # docs per query AP 116 24,727 213.16 WSJ 126 40,230 319.29 0.40 0.45 0.50 0.55 0.60 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 3: Ranking accuracies on WSJ dataset. the other two ranks as irrelevant. From Figure 2, we see that both AdaRank.MAP and AdaRank.NDCG outperform BM25, Ranking SVM, and RankBoost in terms of all measures. We conducted significant tests (t-test) on the improvements of AdaRank.MAP over BM25, Ranking SVM, and RankBoost in terms of MAP. The results indicate that all the improvements are statistically significant (p-value < 0.05). We also conducted t-test on the improvements of AdaRank.NDCG over BM25, Ranking SVM, and RankBoost in terms of NDCG@5. The improvements are also statistically significant. 4.3 Experiment with WSJ and AP Data In this experiment, we made use of the WSJ and AP datasets from the TREC ad-hoc retrieval track, to test the performances of AdaRank. WSJ contains 74, and AP contains 158,. 200 queries are selected from the TREC topics (No.101 \u223c No.300). Each query has a number of documents associated and they are labeled as \u2018relevant\" or \u2018irrelevant\" (to the query). Following the practice in , the queries that have less than 10 relevant documents were discarded. Table 3 shows the statistics on the two datasets. In the same way as in section 4.2, we adopted the features listed in Table 2 for ranking. We also conducted 4-fold cross-validation experiments. The results reported in Figure 3 and 4 are those averaged over four trials on WSJ and AP datasets, respectively. From Figure 3 and 4, we can see that AdaRank.MAP and AdaRank.NDCG outperform BM25, Ranking SVM, and RankBoost in terms of all measures on both WSJ and AP. We conducted t-tests on the improvements of AdaRank.MAP and AdaRank.NDCG over BM25, Ranking SVM, and RankBoost on WSJ and AP. The results indicate that all the improvements in terms of MAP are statistically significant (p-value < 0.05). However only some of the improvements in terms of NDCG@5 are statistically significant, although overall the improvements on NDCG scores are quite high (1-2 points). 4.4 Experiment with .Gov Data In this experiment, we further made use of the TREC .Gov data to test the performance of AdaRank for the task of web retrieval. The corpus is a crawl from the .. There are a total 0.40 0.45 0.50 0.55 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 4: Ranking accuracies on AP dataset. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 5: Ranking accuracies on .Gov dataset. Table 4: Features used in the experiments on .Gov dataset. 3 PageRank 4 HostRank 5 Relevance Propagation (10 features) of 1,053,110 web pages with 11,164,829 hyperlinks in the data. were used. The ground truths for the queries are provided by the TREC committee with binary judgment: relevant or irrelevant. The number of relevant pages vary from query to query (from 1 to 86). We extracted 14 features from each query-document pair. Table 4 gives a list of the features. They are the outputs of some well-known algorithms (systems). These features are different from those in Table 2, because the task is different. Again, we conducted 4-fold cross-validation experiments. The results averaged over four trials are reported in Figure 5. From the results, we can see that AdaRank.MAP and AdaRank.NDCG outperform all the baselines in terms of all measures. We conducted ttests on the improvements of AdaRank.MAP and AdaRank.NDCG over BM25, Ranking SVM, and RankBoost. Some of the improvements are not statistically significant. This is because we have only 50 queries used in the experiments, and the number of queries is too small. 4.5 Discussions We investigated the reasons that AdaRank outperforms the baseline methods, using the results of the OHSUMED dataset as examples. First, we examined the reason that AdaRank has higher performances than Ranking SVM and RankBoost. Specifically we com0.58 0.60 0.62 0.64 0.66 0.68 d-n d-p p-n accuracy pair type Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 6: Accuracy on ranking document pairs with OHSUMED dataset. 10 12 numberofqueries number of document pairs per query Figure 7: Distribution of queries with different number of document pairs in training data of trial 1. pared the error rates between different rank pairs made by Ranking SVM, RankBoost, AdaRank.MAP, and AdaRank.NDCG on the test data. The results averaged over four trials in the 4-fold cross validation are shown in Figure 6. We use \u2018d-n\" to stand for the pairs between \u2018definitely relevant\" and \u2018not relevant\", \u2018d-p\" the pairs between \u2018definitely relevant\" and \u2018partially relevant\", and \u2018p-n\" the pairs between \u2018partially relevant\" and \u2018not relevant\". From Figure 6, we can see that AdaRank.MAP and AdaRank.NDCG make fewer errors for \u2018d-n\" and \u2018d-p\", which are related to the tops of rankings and are important. This is because AdaRank.MAP and AdaRank.NDCG can naturally focus upon the training on the tops by optimizing MAP and NDCG@5, respectively. We also made statistics on the number of document pairs per query in the training data (for trial 1). The queries are clustered into different groups based on the the number of their associated document pairs. Figure 7 shows the distribution of the query groups. In the figure, for example, \u20180-1k\" is the group of queries whose number of document pairs are between 0 and 999. We can see that the numbers of document pairs really vary from query to query. Next we evaluated the accuracies of AdaRank.MAP and RankBoost in terms of MAP for each of the query group. The results are reported in Figure 8. We found that the average MAP of AdaRank.MAP over the groups is two points higher than RankBoost. Furthermore, it is interesting to see that AdaRank.MAP performs particularly better than RankBoost for queries with small numbers of document pairs (e.g., \u20180-1k\", \u20181k-2k\", and \u20182k-3k\"). The results indicate that AdaRank.MAP can effectively avoid creating a model biased towards queries with more document pairs. For AdaRank.NDCG, similar results can be observed. 0.2 0.3 0.4 0.5 MAP query group RankBoost AdaRank.MAP Figure 8: Differences in MAP for different query groups. 0.30 0.31 0.32 0.33 0.34 trial 1 trial 2 trial 3 trial 4 MAP AdaRank.MAP AdaRank.NDCG Figure 9: MAP on training set when model is trained with MAP or NDCG@5. We further conducted an experiment to see whether AdaRank has the ability to improve the ranking accuracy in terms of a measure by using the measure in training. Specifically, we trained ranking models using AdaRank.MAP and AdaRank.NDCG and evaluated their accuracies on the training dataset in terms of both MAP and NDCG@5. The experiment was conducted for each trial. Figure 9 and Figure 10 show the results in terms of MAP and NDCG@5, respectively. We can see that, AdaRank.MAP trained with MAP performs better in terms of MAP while AdaRank.NDCG trained with NDCG@5 performs better in terms of NDCG@5. The results indicate that AdaRank can indeed enhance ranking performance in terms of a measure by using the measure in training. Finally, we tried to verify the correctness of Theorem 1. That is, the ranking accuracy in terms of the performance measure can be continuously improved, as long as e\u2212\u03b4t min 1 \u2212 \u03d5(t)2 < 1 holds. As an example, Figure 11 shows the learning curve of AdaRank.MAP in terms of MAP during the training phase in one trial of the cross validation. From the figure, we can see that the ranking accuracy of AdaRank.MAP steadily improves, as the training goes on, until it reaches to the peak. The result agrees well with Theorem 1. 5. CONCLUSION AND FUTURE WORK In this paper we have proposed a novel algorithm for learning ranking models in document retrieval, referred to as AdaRank. In contrast to existing methods, AdaRank optimizes a loss function that is directly defined on the performance measures. It employs a boosting technique in ranking model learning. AdaRank offers several advantages: ease of implementation, theoretical soundness, efficiency in training, and high accuracy in ranking. Experimental results based on four benchmark datasets show that AdaRank can significantly outperform the baseline methods of BM25, Ranking SVM, and RankBoost. 0.49 0.50 0.51 0.52 0.53 trial 1 trial 2 trial 3 trial 4 NDCG@5 AdaRank.MAP AdaRank.NDCG Figure 10: NDCG@5 on training set when model is trained with MAP or NDCG@5. 0.29 0.30 0.31 0.32 0 50 100 150 200 250 300 350 MAP number of rounds Figure 11: Learning curve of AdaRank. Future work includes theoretical analysis on the generalization error and other properties of the AdaRank algorithm, and further empirical evaluations of the algorithm including comparisons with other algorithms that can directly optimize performance measures. 6. ACKNOWLEDGMENTS We thank Harry Shum, Wei-Ying Ma, Tie-Yan Liu, Gu Xu, Bin Gao, Robert Schapire, and Andrew Arnold for their valuable comments and suggestions to this paper.", "body1": "Recently \u2018learning to rank\" has gained increasing attention in both the fields of information retrieval and machine learning. Several methods for learning to rank have been developed and applied to document retrieval. In this paper, we aim to develop a new learning algorithm that can directly optimize any performance measure used in document retrieval. A lower bound of the performance on training data is given, which indicates that the ranking accuracy in terms of the performance measure can be continuously improved during the training process. AdaRank offers several advantages: ease in implementation, theoretical soundness, efficiency in training, and high accuracy in ranking. Experimental results indicate that AdaRank can outperform the baseline methods of BM25, Ranking SVM, and RankBoost, on four benchmark datasets including OHSUMED, WSJ, AP, and .Gov. Tuning ranking models using certain training data and a performance measure is a common practice in IR . Recently, direct optimization of performance measures in learning has become a hot research topic. The rest of the paper is organized as follows. 2.1 Information Retrieval The key problem for document retrieval is ranking, specifically, how to create the ranking model (function) that can sort documents based on their relevance to the given query. Recently methods of \u2018learning to rank\" have been applied to ranking model construction and some promising results have been obtained. Learning to rank is to automatically create a ranking function that assigns scores to instances and then rank the instances by using the scores. In the pair-wise approach to ranking, the learning task is formalized as a problem of classifying instance pairs into two categories (correctly ranked and incorrectly ranked). Boosting is a general technique for improving the accuracies of machine learning algorithms. Recently, a number of authors have proposed conducting direct optimization of multivariate performance measures in learning. AdaRank is also one that tries to directly optimize multivariate performance measures, but is based on a different approach. 3.1 General Framework We first describe the general framework of learning to rank for document retrieval. In training, a set of queries Q = {q1, q2, \u00b7 \u00b7 \u00b7 , qm} is given. The objective of learning is to create a ranking function f : X \u2192 , such that for each query the elements in its corresponding document list can be assigned relevance scores using the function and then be ranked according to the scores. Notations Explanations qi \u2208 Q ith query di = {di1, di2, \u00b7 \u00b7 \u00b7 , di,n(qi)} List of documents for qi yi j \u2208 {r1, r2, \u00b7 \u00b7 \u00b7 , r } Rank of di j w.r.t. In information retrieval, query-based performance measures are used to evaluate the \u2018goodness\" of a ranking function. Given a query qi, the list of ranks yi, and a permutation \u03c0i on di, NDCG at position m for qi is defined as: Ni = ni \u00b7 j:\u03c0i(j)\u2264m 2yi j \u2212 1 log(1 + \u03c0i( j)) , (3) where yij takes on ranks as values and ni is a normalization constant. 3.2 Algorithm Inspired by the AdaBoost algorithm for classification, we have devised a novel algorithm which can optimize a loss function based on the IR performance measures. AdaRank takes a training set S = {(qi, di, yi)}m i=1 as input and takes the performance measure function E and the number of iterations T as parameters. For t = 1, \u00b7 \u00b7 \u00b7 , T \u2022 Create weak ranker ht with weighted distribution Pt on training data S . \u2022 Choose \u03b1t \u03b1t = \u00b7 ln i=1 Pt(i){1 + E(\u03c0(qi, di, ht), yi)} i=1 Pt(i){1 \u2212 E(\u03c0(qi, di, ht), yi)} \u2022 Create ft ft(x) = k=1 \u03b1khk(x). \u2022 Update Pt+1 Pt+1(i) = exp{\u2212E(\u03c0(qi, di, ft), yi)} j=1 exp{\u2212E(\u03c0(qj, dj, ft), yj)} End For Output ranking model: f(x) = fT (x). Figure 1: The AdaRank algorithm. at round t as Pt and the weight on the ith training query qi at round t as Pt(i). Several methods for weak ranker construction can be considered. For example, a weak ranker can be created by using a subset of queries (together with their document list and label list) sampled according to the distribution Pt. Once a weak ranker ht is built, AdaRank chooses a weight \u03b1t > 0 for the weak ranker. A ranking model ft is created at each round by linearly combining the weak rankers constructed so far h1, \u00b7 \u00b7 \u00b7 , ht with weights \u03b11, \u00b7 \u00b7 \u00b7 , \u03b1t. 3.3 Theoretical Analysis The existing learning algorithms for ranking attempt to minimize a loss function based on instance pairs (document pairs). Furthermore, the loss function in AdaRank is defined on the basis of general IR performance measures. Ideally we want to maximize the ranking accuracy in terms of a performance measure on the training data: max f\u2208F i=1 E(\u03c0(qi, di, f), yi), (4) where F is the set of possible ranking functions. A proof of the theorem can be found in appendix. 3.4 Advantages AdaRank is a simple yet powerful method. Notice that the major IR measures meet this requirement. Second, the learning process of AdaRank is more efficient than those of the existing learning algorithms. Third, AdaRank employs a more reasonable framework for performing the ranking task than the existing methods. First, the types of instances are different. Second, the performance measures are different. Third, the ways of updating weights are also different. Creating weak rankers in this way, the learning process turns out to be that of repeatedly selecting features and linearly combining the selected features. We conducted experiments to test the performances of AdaRank using four benchmark datasets: OHSUMED, WSJ, AP, and .Gov. 1 wi\u2208q d ln(c(wi, d) + 1) 2 wi\u2208q d ln( |C| c(wi,C) + 1) 3 wi\u2208q d ln(id f(wi)) 4 wi\u2208q d ln(c(wi,d) |d| + 1) 5 wi\u2208q d ln(c(wi,d) |d| \u00b7 id f(wi) + 1) 6 wi\u2208q d ln(c(wi,d)\u00b7|C| |d|\u00b7c(wi,C) + 1) 7 ln(BM25 score) 0.2 0.3 0.4 0.5 0.6 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RarnkBoost AdaRank.MAP AdaRank.NDCG Figure 2: Ranking accuracies on OHSUMED data. 4.1 Experiment Setting Ranking SVM and RankBoost were selected as baselines in the experiments, because they are the state-of-the-art learning to rank methods. For AdaRank, the parameter T was determined automatically during each experiment. 4.2 Experiment with OHSUMED Data In this experiment, we made use of the OHSUMED dataset to test the performances of AdaRank. Table 2 shows the features. We randomly divided queries into four even subsets and conducted 4-fold cross-validation experiments. the other two ranks as irrelevant. 4.3 Experiment with WSJ and AP Data In this experiment, we made use of the WSJ and AP datasets from the TREC ad-hoc retrieval track, to test the performances of AdaRank. In the same way as in section 4.2, we adopted the features listed in Table 2 for ranking. The corpus is a crawl from the .. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 5: Ranking accuracies on .Gov dataset. Table 4: Features used in the experiments on .Gov dataset. 3 PageRank 4 HostRank 5 Relevance Propagation (10 features) of 1,053,110 web pages with 11,164,829 hyperlinks in the data. were used. Table 4 gives a list of the features. Again, we conducted 4-fold cross-validation experiments. First, we examined the reason that AdaRank has higher performances than Ranking SVM and RankBoost. 10 12 numberofqueries number of document pairs per query Figure 7: Distribution of queries with different number of document pairs in training data of trial 1. pared the error rates between different rank pairs made by Ranking SVM, RankBoost, AdaRank.MAP, and AdaRank.NDCG on the test data. We also made statistics on the number of document pairs per query in the training data (for trial 1). 0.30 0.31 0.32 0.33 0.34 trial 1 trial 2 trial 3 trial 4 MAP AdaRank.MAP AdaRank.NDCG Figure 9: MAP on training set when model is trained with MAP or NDCG@5. We further conducted an experiment to see whether AdaRank has the ability to improve the ranking accuracy in terms of a measure by using the measure in training. Finally, we tried to verify the correctness of Theorem 1.", "body2": "Ideally, the ranking function is created so that the accuracy of ranking in terms of one of the measures with respect to the training data is maximized. For example, Ranking SVM and RankBoost train ranking models by minimizing classification errors on instance pairs. We show that AdaRank algorithm can iteratively optimize an exponential loss function based on any of IR performance measures. A lower bound of the performance on training data is given, which indicates that the ranking accuracy in terms of the performance measure can be continuously improved during the training process. AdaRank offers several advantages: ease in implementation, theoretical soundness, efficiency in training, and high accuracy in ranking. Experimental results indicate that AdaRank can outperform the baseline methods of BM25, Ranking SVM, and RankBoost, on four benchmark datasets including OHSUMED, WSJ, AP, and .Gov. From the viewpoint of IR, AdaRank can be viewed as a machine learning method for ranking model tuning. AdaRank can be viewed as a machine learning method for direct optimization of performance measures, based on a different approach. Section 5 concludes this paper and gives future work. As the ranking models become more sophisticated (more features are used) and more labeled data become available, how to tune or train ranking models turns out to be a challenging issue. They are \u2018learning to rank\", boosting, and direct optimization of performance measures. For other approaches to learning to rank, refer to . In that sense, the existing methods of Ranking SVM, RankBoost, and RankNet are only able to minimize loss functions that are loosely related to the IR performance measures. Our work in this paper can be viewed as a boosting method developed for ranking, particularly for ranking in IR. also propose a method of directly maximizing rank-based metrics for ranking on the basis of manifold learning. AdaRank is unique in that it employs an exponential loss function based on IR performance measures and a boosting technique. There exists a total order between the ranks r r \u22121 \u00b7 \u00b7 \u00b7 r1, where \u2018 \" denotes a preference relationship. Thus, the training set can be represented as S = {(qi, di, yi)}m i=1. Table 1: Notations and explanations. qi yi = {yi1, yi2, \u00b7 \u00b7 \u00b7 , yi,n(qi)} List of ranks for qi S = {(qi, di, yi)}m i=1 Training set xij = \u03a8(qi, dij) \u2208 X Feature vector for (qi, di j) f(xij) \u2208 Ranking model \u03c0(qi, di, f) Permutation for qi, di, and f ht(xi j) \u2208 tth weak ranker E(\u03c0(qi, di, f), yi) \u2208 [\u22121, +1] Performance measure function In the paper, we define the rank model as a linear combination of weak rankers: f(x) = T t=1 \u03b1tht(x), where ht(x) is a weak ranker, \u03b1t is its weight, and T is the number of weak rankers. Given a query qi, the corresponding list of ranks yi, and a permutation \u03c0i on di, average precision for qi is defined as: AvgPi = n(qi) j=1 Pi( j) \u00b7 yij n(qi) j=1 yij , (1) where yij takes on 1 and 0 as values, representing being relevant or irrelevant and Pi( j) is defined as precision at the position of dij: Pi( j) = k:\u03c0i(k)\u2264\u03c0i(j) yik \u03c0i(j) , (2) where \u03c0i( j) denotes the position of di j. ni is chosen so that a perfect ranking \u03c0\u2217 i \"s NDCG score at position m is 1. The algorithm is referred to as \u2018AdaRank\" and is shown in Figure 1. We denote the distribution of weights Input: S = {(qi, di, yi)}m i=1, and parameters E and T Initialize P1(i) = 1/m. For t = 1, \u00b7 \u00b7 \u00b7 , T \u2022 Create weak ranker ht with weighted distribution Pt on training data S . \u2022 Choose \u03b1t \u03b1t = \u00b7 ln i=1 Pt(i){1 + E(\u03c0(qi, di, ht), yi)} i=1 Pt(i){1 \u2212 E(\u03c0(qi, di, ht), yi)} \u2022 Create ft ft(x) = k=1 \u03b1khk(x). \u2022 Update Pt+1 Pt+1(i) = exp{\u2212E(\u03c0(qi, di, ft), yi)} j=1 exp{\u2212E(\u03c0(qj, dj, ft), yj)} End For Output ranking model: f(x) = fT (x). Figure 1: The AdaRank algorithm. The goodness of a weak ranker is measured by the performance measure E weighted by Pt: i=1 Pt(i)E(\u03c0(qi, di, ht), yi). Several methods for weak ranker construction can be considered. In this paper, we use single features as weak rankers, as will be explained in Section 3.6. Intuitively, \u03b1t measures the importance of ht. ft is then used for updating the distribution Pt+1. In contrast, AdaRank tries to optimize a loss function based on queries. We next explain why this is the case. The following bound holds on the ranking accuracy of the AdaRank algorithm on training data: i=1 E(\u03c0(qi, di, fT ), yi) \u2265 1 \u2212 t=1 e\u2212\u03b4t min 1 \u2212 \u03d5(t)2, where \u03d5(t) = m i=1 Pt(i)E(\u03c0(qi, di, ht), yi), \u03b4t min = mini=1,\u00b7\u00b7\u00b7 ,m \u03b4t i, and \u03b4t i = E(\u03c0(qi, di, ft\u22121 + \u03b1tht), yi) \u2212 E(\u03c0(qi, di, ft\u22121), yi) \u2212\u03b1tE(\u03c0(qi, di, ht), yi), for all i = 1, 2, \u00b7 \u00b7 \u00b7 , m and t = 1, 2, \u00b7 \u00b7 \u00b7 , T. The theorem implies that the ranking accuracy in terms of the performance measure can be continuously improved, as long as e\u2212\u03b4t min 1 \u2212 \u03d5(t)2 < 1 holds. First, AdaRank can incorporate any performance measure, provided that the measure is query based and in the range of [\u22121, +1]. In contrast the existing methods only minimize loss functions that are loosely related to the IR measures . The time complexity of RankBoost, for example, is of order O(T \u00b7 m \u00b7 n2 ) . In that sense, it is similar to AdaBoost, but it also has several striking differences from AdaBoost. The labels in training data are simply +1 and \u22121. In AdaBoost the corresponding performance measure is a specific measure for binary classification, also referred to as \u2018margin\" . In the implementation, as weak ranker we choose the feature that has the optimal weighted performance among all of the features: max i=1 Pt(i)E(\u03c0(qi, di, xk), yi). Note that features which are not selected in the training phase will have a weight of zero. C(w, d) represents frequency of word w in document d; C represents the entire collection; n denotes number of terms in query; | \u00b7 | denotes the size function; and id f(\u00b7) denotes inverse document frequency. 1 wi\u2208q d ln(c(wi, d) + 1) 2 wi\u2208q d ln( |C| c(wi,C) + 1) 3 wi\u2208q d ln(id f(wi)) 4 wi\u2208q d ln(c(wi,d) |d| + 1) 5 wi\u2208q d ln(c(wi,d) |d| \u00b7 id f(wi) + 1) 6 wi\u2208q d ln(c(wi,d)\u00b7|C| |d|\u00b7c(wi,C) + 1) 7 ln(BM25 score) 0.2 0.3 0.4 0.5 0.6 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RarnkBoost AdaRank.MAP AdaRank.NDCG Figure 2: Ranking accuracies on OHSUMED data. Furthermore, BM25 was used as a baseline, representing the state-of-the-arts IR method (we actually used the tool Lemur1 ). The results for AdaRank using MAP and NDCG@5 as measures in training are represented as AdaRank.MAP and AdaRank.NDCG, respectively. As features, we adopted those used in document retrieval . Stop words were removed and stemming was conducted in the data. Dataset # queries # retrieved docs # docs per query AP 116 24,727 213.16 WSJ 126 40,230 319.29 0.40 0.45 0.50 0.55 0.60 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 3: Ranking accuracies on WSJ dataset. The improvements are also statistically significant. Table 3 shows the statistics on the two datasets. 4.4 Experiment with .Gov Data In this experiment, we further made use of the TREC .Gov data to test the performance of AdaRank for the task of web retrieval. There are a total 0.40 0.45 0.50 0.55 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 4: Ranking accuracies on AP dataset. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 MAP NDCG@1 NDCG@3 NDCG@5 NDCG@10 BM25 Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 5: Ranking accuracies on .Gov dataset. Table 4: Features used in the experiments on .Gov dataset. 3 PageRank 4 HostRank 5 Relevance Propagation (10 features) of 1,053,110 web pages with 11,164,829 hyperlinks in the data. We extracted 14 features from each query-document pair. These features are different from those in Table 2, because the task is different. 4.5 Discussions We investigated the reasons that AdaRank outperforms the baseline methods, using the results of the OHSUMED dataset as examples. Specifically we com0.58 0.60 0.62 0.64 0.66 0.68 d-n d-p p-n accuracy pair type Ranking SVM RankBoost AdaRank.MAP AdaRank.NDCG Figure 6: Accuracy on ranking document pairs with OHSUMED dataset. 10 12 numberofqueries number of document pairs per query Figure 7: Distribution of queries with different number of document pairs in training data of trial 1. This is because AdaRank.MAP and AdaRank.NDCG can naturally focus upon the training on the tops by optimizing MAP and NDCG@5, respectively. 0.2 0.3 0.4 0.5 MAP query group RankBoost AdaRank.MAP Figure 8: Differences in MAP for different query groups. 0.30 0.31 0.32 0.33 0.34 trial 1 trial 2 trial 3 trial 4 MAP AdaRank.MAP AdaRank.NDCG Figure 9: MAP on training set when model is trained with MAP or NDCG@5. The results indicate that AdaRank can indeed enhance ranking performance in terms of a measure by using the measure in training. The result agrees well with Theorem 1.", "introduction": "Recently \u2018learning to rank\" has gained increasing attention in both the fields of information retrieval and machine learning. When applied to document retrieval, learning to rank becomes a task as follows. In training, a ranking model is constructed with data consisting of queries, their corresponding retrieved documents, and relevance levels given by humans. In ranking, given a new query, the corresponding retrieved documents are sorted by using the trained ranking model. In document retrieval, usually ranking results are evaluated in terms of performance measures such as MAP (Mean Average Precision) and NDCG (Normalized Discounted Cumulative Gain) . Ideally, the ranking function is created so that the accuracy of ranking in terms of one of the measures with respect to the training data is maximized. Several methods for learning to rank have been developed and applied to document retrieval. propose a learning algorithm for ranking on the basis of Support Vector Machines, called Ranking SVM. take a similar approach and perform the learning by using boosting, referred to as RankBoost. All the existing methods used for document retrieval are designed to optimize loss functions loosely related to the IR performance measures, not loss functions directly based on the measures. For example, Ranking SVM and RankBoost train ranking models by minimizing classification errors on instance pairs. In this paper, we aim to develop a new learning algorithm that can directly optimize any performance measure used in document retrieval. Inspired by the work of AdaBoost for classification , we propose to develop a boosting algorithm for information retrieval, referred to as AdaRank. AdaRank utilizes a linear combination of \u2018weak rankers\" as its model. In learning, it repeats the process of re-weighting the training sample, creating a weak ranker, and calculating a weight for the ranker. We show that AdaRank algorithm can iteratively optimize an exponential loss function based on any of IR performance measures. A lower bound of the performance on training data is given, which indicates that the ranking accuracy in terms of the performance measure can be continuously improved during the training process. AdaRank offers several advantages: ease in implementation, theoretical soundness, efficiency in training, and high accuracy in ranking. Experimental results indicate that AdaRank can outperform the baseline methods of BM25, Ranking SVM, and RankBoost, on four benchmark datasets including OHSUMED, WSJ, AP, and .Gov. Tuning ranking models using certain training data and a performance measure is a common practice in IR . As the number of features in the ranking model gets larger and the amount of training data gets larger, the tuning becomes harder. From the viewpoint of IR, AdaRank can be viewed as a machine learning method for ranking model tuning. Recently, direct optimization of performance measures in learning has become a hot research topic. Several methods for classification and ranking have been proposed. AdaRank can be viewed as a machine learning method for direct optimization of performance measures, based on a different approach. The rest of the paper is organized as follows. After a summary of related work in Section 2, we describe the proposed AdaRank algorithm in details in Section 3. Experimental results and discussions are given in Section 4. Section 5 concludes this paper and gives future work.", "conclusion": "In this paper we have proposed a novel algorithm for learning ranking models in document retrieval, referred to as AdaRank.. In contrast to existing methods, AdaRank optimizes a loss function that is directly defined on the performance measures.. It employs a boosting technique in ranking model learning.. AdaRank offers several advantages: ease of implementation, theoretical soundness, efficiency in training, and high accuracy in ranking.. Experimental results based on four benchmark datasets show that AdaRank can significantly outperform the baseline methods of BM25, Ranking SVM, and RankBoost.. 0.49 0.50 0.51 0.52 0.53 trial 1 trial 2 trial 3 trial 4 NDCG@5 AdaRank.MAP AdaRank.NDCG Figure 10: NDCG@5 on training set when model is trained with MAP or NDCG@5.. 0.29 0.30 0.31 0.32 0 50 100 150 200 250 300 350 MAP number of rounds Figure 11: Learning curve of AdaRank.. Future work includes theoretical analysis on the generalization error and other properties of the AdaRank algorithm, and further empirical evaluations of the algorithm including comparisons with other algorithms that can directly optimize performance measures.. ACKNOWLEDGMENTS We thank Harry Shum, Wei-Ying Ma, Tie-Yan Liu, Gu Xu, Bin Gao, Robert Schapire, and Andrew Arnold for their valuable comments and suggestions to this paper."}
{"id": "I-47", "keywords": ["social interact", "softwar connector", "oper semant"], "title": "Operational Semantics of Multiagent Interactions", "abstract": "The social stance advocated by institutional frameworks and most multiagent system methodologies has resulted in a wide spectrum of organizational and communicative abstractions which have found currency in several programming frameworks and software platforms. Still, these tools and frameworks are designed to support a limited range of interaction capabilities that constrain developers to a fixed set of particular, pre-defined abstractions. The main hypothesis motivating this paper is that the variety of multiagent interaction mechanisms -- both, organizational and communicative, share a common semantic core. In the realm of software architectures, the paper proposes a connector-based model of multiagent interactions which attempts to identify the essential structure underlying multiagent interactions. Furthermore, the paper also provides this model with a formal execution semantics which describes the dynamics of social interactions. The proposed model is intended as the abstract machine of an organizational programming language which allows programmers to accommodate an open set of interaction mechanisms.", "references": ["A Formal Basis for Architectural Connection", "Engineering open environments with electronic institutions", "Role-based semantics for agent communication: embedding of the 'mental attitudes' and 'social commitments' semantics", "A survey of programming languages and platforms for multiagent systems", "Jason and the golden fleece of agent-oriented programming", "Specifying and analysing agent-based social institutions using answer set programming", "Omni: Introducing social structure, norms and ontologies into agent organizations", "ISLANDER: an electronic institutions editor", "AMELI: An agent-based middleware for electronic institutions", "From agents to organizations: An organizational view of multiagent systems", "Foundation for Intelligent Physical Agents", "Norm-oriented programming of electronic institutions", "The MADKIT agent platform architecture", "The JADE project home page", "Agent Technology: Computing as Interaction -- A Roadmap for Agent-Based Computing", "A formal framework for inter-agent dialogues", "Towards a taxonomy of software connectors", "Agent oriented software engineering with ingenias", "Types and Programming Languages", "Voting in multiagent systems", "A structural approach to operational semantics", "Speech Acts", "A computational theory of normative positions", "Agent-based abstractions for software development", "Agentcities / opennet testbed", "Developing multiagent systems: The Gaia methodology"], "full_text": "1. INTRODUCTION The suitability of agent-based computing to manage the complex patterns of interactions naturally occurring in the development of large scale, open systems, has become one of its major assets over the last few years . Particularly, the organizational or social stance advocated by institutional frameworks and most multi-agent system (MAS) methodologies , provides an excellent basis to deal with the complexity and dynamism of the interactions among system components. This approach has resulted in a wide spectrum of organizational and communicative abstractions, such as institutions, normative positions, power relationships, organizations, groups, scenes, dialogue games, communicative actions (CAs), etc., to effectively model the interaction space of MAS. This wealth of computational abstractions has found currency in several programming frameworks and software platforms (AMELI , MadKit , INGENIAS toolkit , etc.), which leverage multi-agent middlewares built upon raw ACL-based interaction mechanism , and minimize the gap between organizational metamodels and target implementation languages. Still, these tools and frameworks are designed to support a limited range of interaction capabilities that constrain developers to a fixed set of particular, pre-defined abstractions. The main hypothesis motivating this paper is that the variety of multi-agent interaction mechanisms - both, organizational and communicative, share a common semantic core. This paper thus focuses on the fundamental building blocks of multi-agent interactions: those which may be composed, extended or refined in order to define more complex organizational or communicative types of interactions. Its first goal is to carry out a principled analysis of multiagent interactions, departing from general features commonly ascribed to agent-based computing: autonomy, situatedness and sociality . To approach this issue, we draw on the notion of connector, put forward within the field of software architectures . The outcome of this analysis will be a connector-based model of multi-agent interactions between autonomous social and situated components, i.e. agents, attempting to identify their essential structure. Furthermore, the paper also provides this model with a formal execution semantics which describes the dynamics of multi-agent (or social) interactions. Structural Operational Semantics (SOS), a common technique to specify the operational semantics of programming languages, is used for this purpose. The paper is structured as follows: first, the major entities and relationships which constitute the structure of social interactions are introduced. Next, the dynamics of social interactions will show how these entities and relationships evolve. Last, relevant work in the literature is discussed 889 978-81--7-5 (RPS) IFAAMAS with respect to the proposal, limitations are addressed, and current and future work is described. 2. SOCIAL INTERACTION STRUCTURE From an architectural point of view, interactions between software components are embodied in software connectors: first-class entities defined on the basis of the different roles played by software components and the protocols that regulate their behaviour . The roles of a connector represent its participants, such as the caller and callee roles of an RPC connector, or the sender and receiver roles in a message passing connector. The attachment operation binds a component to the role of a given connector. The analysis of social interactions introduced in this section gives rise to a new kind of social connector. It refines the generic model in several respects, attending to the features commonly ascribed to agent-based computing: \u2022 According to the autonomy feature, we may distinguish a first kind of participant (i.e. role) in a social interaction, so-called agents. Basically, agents are those software components which will be regarded as autonomous within the scope of the interaction1 \u2022 A second group of participants, so-called environmental resources, may be identified from the situatedness feature. Unlike agents, resources represent those nonautonomous components whose state may be externally controlled by other components (agents or resources) within the interaction. Moreover, the participation of resources in an interaction is not mandatory. \u2022 Last, according to the sociality of agents, the specification of social connector protocols - the glue linking agents among themselves and with resources, will rely on normative concepts such as permissions, obligations and empowerments . Besides agents, resources and social protocols, two other kinds of entities are of major relevance in our analysis of social interactions: actions, which represent the way in which agents alter the environmental and social state of the interaction; and events, which represent the changes in the interaction resulting from the performance of actions or the activity of environmental resources. In the following, we describe the basic entities involved in social interactions. Each kind of entity T will be specified as a record type T l1 : T1, . . . ln : Tn , possibly followed by a number of invariants, definitions, and the actions affecting their state. Instances or values v of a record type T will be represented as v = v1, . . . , vn : T. The type SetT represents a collection of values drawn from type T. The type QueueT represents a queue of values v : T waiting to be processed. The value v in the expression [v| ] : Queue[T] represents the head of the queue. The type Enum {v1, . . . , vn} Note that we think of the autonomy feature in a relative, rather than absolute, perspective. Basically, this means that software components counting as agents in a social interaction may behave non-autonomously in other contexts, e.g. in their interactions through human-user interfaces. This conceptualization of agenthood resembles the way in which objects are understood in CORBA: as any kind of software component (C, Prolog, Cobol, etc.) attached to an ORB. represents an enumeration type whose values are v1, . . . , vn. Given some value v : T, the term vl refers to the value of the field l of a record type T. Given some labels l1, l2, . . . , the expression vl1,l2,... is syntactic sugar for ((vl1 )l2 ) . . .. The special term nil will be used to represent the absence of proper value for an optional field, so that vl = nil will be true in those cases and false otherwise. The formal model will be illustrated with several examples drawn from the design of a virtual organization to aid in the management of university courses. 2.1 Social Interactions Social interactions shall be considered as composite connectors , structured in terms of a tree of nested subinteractions. Let\"s consider an interaction representing a university course (e.g. on data structures). On the one hand, this interaction is actually a complex one, made up of lower-level interactions. For instance, within the scope of the course agents will participate in programming assignment groups, lectures, tutoring meetings, examinations and so on. Assignment groups, in turn, may hold a number of assignment submissions and test requests interactions. A test request may also be regarded as a complex interaction, ultimately decomposed in the atomic, or bottom-level interactions represented by communicative actions (e.g. request, agree, refuse, . . . ). On the other hand, courses are run within the scope of a particular degree (e.g. computer science), a higher-level interaction. Traversing upwards from a degree to its ancestors, we find its faculty, the university and, finally, the multi-agent community or agent society. The community is thus the top-level interaction which subsumes any other kind of multi-agent interaction2 The organizational and communicative interaction types identified above clearly differ in many ways. However, we may identify four major components in all of them: the participating agents, the resources that agents manipulate, the protocol regulating the agent activities and the subinteraction space. Accordingly, we may specify the type I of social interactions, ranged over by the meta-variable i, as follows: I state : SI, ini : A, mem : Set A, env : Set R, sub : Set I, prot : P, ch : CH def. : (1) icontext = i1 \u21d4 i \u2208 isub inv. : (2) iini = nil \u21d4 icontext = nil act. : setUp, join, create, destroy where the member and environment fields represent the agents (A) and local resources (R) participating in the interaction; the sub-interaction field, its set of inner interactions; and the protocol field the rules that govern the interaction (P). The event channel, to be described in the next section, allows the dispatching of local events to external interactions. The context of some interaction is defined as its super-interaction (def. 1), so that the context of the toplevel interaction is nil. The type SI Enum {open, closing, closed} represents the possible execution states of the interaction. Any interaction, but the top-level one, is set up within the context of another interaction by an initiator agent. The initiator is In the context of this application, a one-to-one mapping between human users and software components attached to the community as agents would be a right choice. 890 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) thus a mandatory feature for any interaction different to the community (inv. 2). The life-cycle of the interaction begins in the open state. Its sets of agent and resource participants, initially empty, vary as agents join and leave the interaction, and as they create and destroy resources from its local environment. Eventually, the interaction may come to an end (according to the protocol\"s rules), or be explicitly closed by some agent, thus prematurely disabling the activity of its participants. The transient closing state will be described in the next section. 2.2 Agents Components attach themselves as agents in social interactions with the purpose of achieving something. The purpose declared by some agent when it joins an interaction shall be regarded as the institutional goal that it purports to satisfy within that context3 . The types of agents participating in a given interaction are primarily identified from their purposes. For instance, students are those agents participating in a course who purport to obtain a certificate in the course\"s subject. Other members of the course include lecturers and teaching assistants. The type A of agents, ranged over by meta-variable a, is defined as follows: A state : SA, player : A, purp : F, att : Queue ACT , ev : Queue E, obl : Set O def. : (3) acontext = i \u21d4 a \u2208 imem (4) a1 \u2208 aroles \u21d4 aplayer 1 = a (5) i \u2208 apartIn \u21d4 a1 \u2208 imem \u2227 a1 \u2208 aroles act. : see where the purpose is represented as a well-formed boolean formula, of a generic type F, which evaluates to true if the purpose is satisfied and false otherwise. The context of some agent is defined as the interaction in which it participates (def. 3). The type SA Enum {playing, leaving, succ, unsuc} represents the execution state of the agent. Its life-cycle begins in the playing state when its player agent joins the interaction, or some software component is attached as an agent to the multi-agent system (in this latter case, the player value is nil). The derived roles and partIn features represent the roles played by the agent and the contexts in which these roles are played (def. 4, 5)4 . An agent may play roles at interactions within or outside the scope of its context. For instance, students of a course are played by student agents belonging to the (undergraduate) degree, whereas lecturers may be played by teachers of a given department and the assistant role may be played by students of a Ph.D degree (both, the department and the Ph.D. degrees, are modelled as sub-interactions of the faculty). Components will normally attempt to perform different actions (e.g. to set up sub-interactions) in order to satisfy their purposes within some interaction. Moreover, components need to be aware of the current state of the interaction, so that they will also be capable of observing certain events from the interaction. Both, the visibility of the interaction Thus, it may or may not correspond to actual internal goals or intentions of the component. Free variables in the antecedents/consequents of implications shall be understood as universally/existentially quantified. and the attempts of members, are subject to the rules governing the interaction. The attempts and events fields of the agent structure represent the queues of attempts to execute some actions (ACT ), and the events (E) received by the agent which have not been observed yet. An agent may update its event queue by seeing the state of some entity of the community. The last field of the structure represents the obligations (O) of agents, to be described later. Eventually, the participation of some agent in the interaction will be over. This may either happen when certain conditions are met (specified by the protocol rules), or when the agent takes the explicit decision of leaving the interaction. In either case, the final state of the agent will be successful if its purpose was satisfied; unsuccessful otherwise. The transient leaving state will be described in the next section. 2.3 Resources Resources are software components which may represent different types of non-autonomous informational or computational entities. For instance, objectives, topics, assignments, grades and exams are different kinds of informational resources created by lecturers and assistants in the context of the course interaction. Students may also create programs to satisfy the requirements of some assignment. Other types of computational resources put at the disposal of students by teachers include compilers and interpreters. The type R of resources, ranged over by meta-variable r, can be specified by the following record type: R cr : A, owners : Set A, op : Set OP def. : (6) rcontext = i \u21d4 r \u2208 ienv act. : take, share, give, invoke Essentially, resources can be regarded as objects deployed in a social setting. This means that resources are created, accessed and manipulated by agents in a social interaction context (def. 6), according to the rules specified by its protocol. The mandatory feature creator represents the agent who created this resource. Moreover, resources may have owners. The ownership relationship between members and resources is considered as a normative device aimed at the simplification of the protocol\"s rules that govern the interaction of agents and the environment. Members may gain ownership of some resource by taking it, and grant ownership to other agents by giving or sharing their own properties. For instance, the ownership of programs may be shared by several students if the assignment can be performed by groups of two or more students. The last operations feature represents the interface of the resource, consisting of a set of operations. A resource is structured around several public operations that participants may invoke, in accordance to the rules specified by the interaction\"s protocol. The set of operations of a resource makes up its interface. 2.4 Protocols The protocol of any interaction is made up of the rules which govern its overall state and dynamics. The present specification abstracts away the particular formalism used to specify these rules, and focuses instead on several requirements concerning the structure and interface of protocols. Accordingly, the type P of protocols, ranged over by metaThe Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 891 variable p, is defined as follows5 P emp : A \u00d7 ACT \u2192 Boolean, perm : A \u00d7 ACT \u2192 Boolean, obl :\u2192 Set (A \u00d7 Set O \u00d7 Set E), monitor : E \u2192 Set A, finish :\u2192 Boolean, over : A \u2192 Boolean def. : (7) pcontext = i \u21d4 p = iprot inv. : (8) pfinish \u2227 s \u2208 pcontext,sub \u21d2 sprot,finish (9) pfinish \u2227 a \u2208 pcontext,mem \u21d2 pover(a) (10) pover(a) \u2227 ai \u2208 aroles \u21d2 acontext,prot,over i (ai) (11) \u03b1add \u222a {a} \u2286 pmonitor( a, \u03b1, ) act. : Close, Leave We demand from protocols four major kinds of functions. Firstly, protocols shall include rules to identify the empowerments and permissions of any agent attempting to alter the state of the interaction (e.g. its members, the environment, etc.) through the execution of some action (e.g. join, create, etc.). Empowerments shall be regarded as the institutional capabilities which some agent possesses in order to satisfy its purpose. Corresponding rules, encapsulated by the empowered function field, shall allow to determine whether some agent is capable to perform a given action over the interaction. Empowerments may only be exercised under certain circumstances - that permissions specify. Permission rules shall allow to determine whether the attempt of an empowered agent to perform some particular action is satisfied or not (cf. permitted field). For instance, the course\"s protocol specifies that the agents empowered to join the interaction as students are those students of the degree who have payed the fee established for the course\"s subject, and own the certificates corresponding to its prerequisite subjects. Permission rules, in turn, specify that those students may only join the course in the admission stage. Hence, even if some student has paid the fee, the attempt to join the course will fail if the course has not entered the corresponding stage6 Secondly, protocols shall allow to determine the obligations of agents towards the interaction. Obligations represent a normative device of social enforcement, fully compatible with the autonomy of agents, used to bias their behaviour in a certain direction. These kinds of rules shall allow to determine whether some agent must perform an action of a given type, as well as if some obligation was fulfilled, violated or needs to be revoked. The function obligations of the protocol structure thus identifies the agents whose obligation set must be updated. Moreover, it returns for each agent a collection of events representing the changes in the obligation set. For instance, the course\"s protocol establishes that members of departments must join the course as teachers whenever they are assigned to the course\"s subject. Thirdly, the protocol shall allow to specify monitoring rules for the different events originating within the interaction. Corresponding rules shall establish the set of agents that must be awared of some event. For instance, this func5 The formalization assumes that protocol\"s functions implicitly recieve as input the interaction being regulated. The hasPaidFee relationship between (degree) students and subject resources is represented by an additional, application-dependent field of the agent structure for this kind of roles. Similarly, the admission stage is an additional boolean field of the structure for school interactions. The generic types I, A, R and P are thus extendable. tionality is exploited by teachers in order to monitor the enrollment of students to the course. Last, the protocol shall allow to control the state of the interaction as well as the states of its members. Corresponding rules identify the conditions under which some interaction will be automatically finished, and whether the participation of some member agent will be automatically over. Thus, the function field finish returns true if the regulated interaction must finish its execution. If so happens, a well-defined set of protocols must ensure that its sub-interactions and members are finished as well (inv. 8,9). Similarly, the function over returns true if the participation of the specified member must be over. Well-formed protocols must ensure the consistency between these functions across playing roles (inv. 10)7 . For instance, the course\"s protocol establishes that the participation of students is over when they gain ownership of the course\"s certificate or the chances to get it are exhausted. It also establishes that the course must be finished when the admission stage has passed and all the students finished their participation. 3. SOCIAL INTERACTION DYNAMICS The dynamics of the multi-agent community is influenced by the external actions executed by software components and the protocols governing their interactions. This section focuses on the dynamics resulting from a particular kind of external action: the attempt of some component, attached to the community as an agent, to execute a given (internal) action. The description of other external actions concerning agents (e.g. observe the events from its event queue, enter or exit from the community) and resources (e.g. a timer resource may signal the pass of time) will be skipped. The processing of some attempt may give rise to changes in the scope of the target interaction, such as the instantiation of new participants (agents or resources) or the setting up of new sub-interactions. These resulting events may cause further changes in the state of other interactions (the target one included), namely, in its execution state as well as in the execution state, obligations and visibility of their members. This section will also describe the way in which these events are processed. The resulting dynamics described bellow allows for actions and events corresponding to different agents and interactions to be processed simultaneously. Due to lack of space, we only include some of the operational rules that formalise the execution semantics. 3.1 Attempt processing An attempt is defined by the structure AT T perf : A, act : ACT , where the performer represents the agent in charge of executing the specified action. This action is intended to alter the state of some target interaction (possibly, the performer\"s context itself), and notify a collection of addressees of the changes resulting from a successful execution. Accordingly, the type ACT of actions, ranged over by meta-variable \u03b1, is specified as follows: ACT state : SACT , target : I, add : Set A def. : (12) \u03b1perf = a \u21d4 \u03b1 \u2208 aatt The close and leave actions update the finish and over function fields as explained in the next section. Additional actions, such as permit, forbid, empower, etc., to update other protocol\"s fields are yet to be identified in future work. 892 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) where: the performer is formally defined as the agent who stores the action in its queue of attempts, and the state field represents the current phase of processing. This process goes through four major phases, as specified by the enumeration type SACT Enum {emp, perm, exec} : empowerment checking, permission checking and action execution, described in the sequel. 3.1.1 Empowerment checking The post-condition of an attempt consists of inserting the action in the queue of attempts of the specified performer. As rule 1 specifies8 , this will only be possible if the performer is empowered to execute that action according to the rules that govern the state of the target interaction. If this condition is not met, the attempt will simply be ignored. Moreover, the performer agent must be in the playing state (this pre-condition is also required for any rule concerning the processing of attempts). If these pre-conditions are satisfied the rule is fired and the processing of the action continues in the permission checking stage. For instance, when the software component attached as a student in a degree attempts to join as a student the course in which some subject is teached, the empowerment rules of the course interaction are checked. If the (degree) student has passed the course\"s prerequisite subjects the join action will be inserted in its queue of attempts and considered for execution. \u03b1target,prot,emp(a, \u03b1) a = playing, , , qACT , , a,\u03b1 :AT T \u2212\u2192 playing, , , qACT , , (1) W here : (\u03b1 )state = perm (qACT ) = insert(\u03b1 , qACT ) 3.1.2 Permissions checking The processing of the action resumes when the possible preceding actions in the performer\"s queue of attempts are fully processed and removed from the queue. Moreover, there should be no pending events to be processed in the interaction, for these events may cause the member or the interaction to be finished (as will be shortly explained in the next sub-section). If these conditions are met the permissions to execute the given action (and notify the specified addressees) are checked (e.g. it will be checked whether the student paid the fee for the course\"s subject). If the protocol of the target interaction grants permission, the processing of the attempt moves to the action execution stage (rule 2). Otherwise, the action is discharged and removed from the queue. Unlike unempowered attempts, a forbidden one will cause an event to be generated and transfered to the event channel for further processing. \u03b1state = perm \u2227 acontext,ch,in,ev = \u2205 \u2227 \u03b1target,prot,perm(a, \u03b1) a = playing, , , [\u03b1| ], , \u2212\u2192 playing, , , [\u03b1 | ], , (2) W here : (\u03b1 )state = exec Labels of record instances are omitted to allow for more compact specifications. Moreover, note that record updates in where clauses only affect the specified fields. 3.1.3 Action execution The transitions fired in this stage are classified according to the different types of actions to be executed. The intended effects of some actions may directly be achieved in a single step, while others will required an indirect approach and possibly several execution steps. Actions of the first kind are constructive ones such as set up and join. The second group of actions include those, such as close and leave, whose effects are indirectly achieved by updating the interaction protocol. As an example of constructive action, let\"s consider the execution of a set up action, whose type is defined as follows9 SetUp ACT \u00b7 new : I inv. : (13) \u03b1new,mem = \u03b1new,res = \u03b1new,sub = \u2205 (14) \u03b1new,state = open where the new field represents the new interaction to be initiated. Its sets of participants (agents and resources) and sub-interactions must be empty (inv. 13) and its state must be open (inv. 14). The setting up of the new interaction may thus affect its protocol and possible application-dependent fields (e.g. the subject of a course interaction). According to rule 3, the outcome of the execution is threefold: firstly, the performer\"s attempt queue is updated so that the executing action is removed; secondly, the new interaction is added to the target\"s set of sub-interactions (moreover, its initiator field is set to the performer agent); last, the event representing this change (which includes a description of the change, the agent that caused it and the action performed) is inserted in the output port of the target\"s event channel. \u03b1state = exec \u2227 \u03b1 : SetUp \u2227 \u03b1new = i a = playing, , , [\u03b1|qACT ], , \u2212\u2192 playing, , , qACT , , \u03b1target = open, , , , , sI , c \u2212\u2192 open, , , , , sI \u222a i , c (3) W here : (i )ini = a (c )out,ev = insert( a, \u03b1, sub(\u03b1target , i ) , cout,ev Let\"s consider now the case of a close action. This action represents an attempt by the performer to force some interaction to finish, thus bypassing its current protocol rules (those concerning the finish function). The way to achieve this effect is to cause an update on the protocol so that the finish function returns true afterwards10 . Accordingly, we may specify this type of action as follows: Close ACT \u00b7 upd : (\u2192 Bool) \u2192 (\u2192 Bool) inv. : (15) \u03b1target,state = open (16) \u03b1target,context = nil (17) \u03b1upd(\u03b1target,prot,finish) where the inherited target field represents the interaction to be closed (which must be open and different to the topinteraction, according to invariants 15 and 16) and the new The resulting type consists of the fields of the ACT record extended with an additional new field. 10 This strategy is also followed in the definition of leave and may also be used in the definition of other types of actions such as fire, permit, forbid, etc. The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 893 update field represents a proper higher-order function to update the target\"s protocol (inv. 17). The transition which models the execution of this action, specified by rule 4, defines two effects in the target interaction: its protocol is updated and the event representing this change is inserted in its output port. This event will actually trigger the closing process of the interaction as described in the next subsection. \u03b1state = exec \u2227 \u03b1 : Close a = playing, , , [\u03b1|qACT ], , \u2212\u2192 playing, , , qACT , , \u03b1target = open, , , , , p, c \u2212\u2192 open, , , , , p , c (4) W here : (p )finish = \u03b1upd (pfinish (c )out,ev = insert( a, \u03b1, finish(\u03b1target ) , cout,ev 3.2 Event Processing The processing of events is encapsulated in the event channels of interactions. Channels, ranged over by meta-variable c, are defined by two input and output ports, according to the following definition: CH out : OutP, in : InP inv. : (18) ccontext \u2208 cout,disp( , , finish(ccontext) ) (19) ccontext \u2208 cout,disp( , , over(a) ) (20) ccontext,sub \u2286 cout,disp(closing(ccontext)) (21) apartsIn \u2286 cout,disp(leaving(a)) (22) ccontext \u2208 cout,disp(closed(i)) (23) {ccontext, aplayer,context} \u2286 cout,disp(left(a)) OutP ev : Queue E, disp : E \u2192 Set I, int : Set I, ag : Set A InP ev : Queue E, stage : Enum {int, mem, obl}, ag : Set A The output port stores and processes the events originated within the scope of the channel\"s interaction. Its first purpose is to dispatch the local events to the agents identified by the protocol\"s monitoring function. Moreover, since these events may influence the results of the finishing, over and obligation functions of certain protocols, they will also be dispatched to the input ports of the interactions identified through a dispatching function - whose invariants will be explained later on. Thus, input ports serve as a coordination mechanism which activate the re-evaluation of the above functios whenever some event is received11 . Accordingly, the processing of some event goes through four major stages: event dispatching, interaction state update, member state update and obligations update. The first one takes place in the output port of the interaction in which the event originated, whereas the other ones execute in separate control threads associated to the input ports of the interactions to which the event was dispatched. 3.2.1 Event dispatching The processing of some event stored in the output port is triggered when all its preceding events have been dispatched. As a first step, the auxiliary int and ag fields are initialised 11 Alternatively, we may have assumed that interactions are fully aware of any change in the multi-agent community. In this scenario, interactions would trigger themselves without requiring any explicit notification. On the contrary, we adhere to the more realistic assumption of limited awareness. with the returned values of the dispatching and protocol\"s monitoring functions, respectively (rule 5). Then, additional rules simply iterate over these collections until all agents and interactions have been notified (i.e., both sets are empty). Last, the event is removed from the queue and the auxiliary fields are re-set to nil. The dispatching function shall identify the set of interactions (possibly, empty) that may be affected by the event (which may include the channel\"s interaction itself)12 . For instance, according to the finishing rule of university courses mentioned in the last section, the event representing the end of the admission stage, originated within the scope of the school interaction, will be dispatched to every course of the school\"s degrees. Concerning the monitoring function, according to invariant 11 of protocols, if the event is generated as the result of an action performance, the agents to be notified will include the performer and addressees of that action. Thus, according to the monitoring rule of university courses, if a student of some degree joins a certain course and specifies a colleague as addressee of that action, the course\"s teachers and itself will also be notified of the successful execution. ccontext,state s = open \u2227 ccontext,prot,monitor s = mon cs = [e| ], d, nil, nil , \u2212\u2192 [e| ], , d(e), mon(e) , (5) 3.2.2 Interaction state update Input port activity is triggered when a new event is received. Irrespective of the kind of incoming event, the first processing action is to check whether the channel\"s interaction must be finished. Thus, the dispatching of the finish event resulting from a close action (inv. 18) serves as a trigger of the closing procedure. If the interaction has not to be finished, the input port stage field is set to the member state update stage and the auxiliary ag field is initialised to the interaction members. Otherwise, we can consider two possible scenarios. In the first one, the interaction has no members and no sub-interactions. In this case, the interaction can be inmediately closed down. As rule 6 shows, the interaction is closed, removed from the context\"s set of sub-interactions and a closed event is inserted in its output channel. According to invariant 22, this event will be later inserted to its input channel to allow for further treatment. cin,ev 1 = \u2205 \u2227 cin,stage 1 = int \u2227 pfinish , , , , {i} \u222a sI , , c \u2212\u2192 , , , , sI , , c i = , , \u2205, , \u2205, p, c1 \u2212\u2192 closed, , , , , , (6) W here : (c )out,ev = insert(closed(i), cout,ev In the second scenario, the interaction has some member or sub-interaction. In this case, clean-up is required prior to the disposal of the interaction (e.g. if the admission period ends and no student has matriculated for the course, teachers has to be finished before finishing the course itself). As rule 7 shows, the interaction is moved to the transient closing state and a corresponding event is inserted in the output port. According to invariant 20, the closing event will be dispatched to every sub-interaction in order to activate its closing procedure (guaranteed by invariant 8). Moreover, 12 This is essentially determined by the protocol rules of these interactions. The way in which the dispatching function is initialised and updated is out of the scope of this paper. 894 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) the stage and ag fields are properly initialised so that the process goes on in the next member state update stage. This stage will further initiate the leaving process of the members (according to invariant 9). cin,ev = \u2205 \u2227 cin,stage = int \u2227 pfinish \u2227 (sA = \u2205 \u2228 sI = \u2205) i = open, , sA, , sI , p, c \u2212\u2192 closing, , sA, , sI , p, c (7) W here : (c )out,ev = insert(closing(i), cout,ev (c )in,stage = mem (c )in,ag = sA Eventually, every member will leave the interaction and every sub-interaction will be closed. Corresponding events will be received by the interaction (according to invariants 23 and 22) so that the conditions of the first scenario will hold. 3.2.3 Member state update This stage simply iterates over the members of the interaction to check whether they must be finished according to the protocol\"s over function. When all members have been checked, the stage field will be set to the next obligation update stage and the auxiliary ag field will be initalised with the agents identified by the protocol\"s obligation update function. If some member has to end its participation in the interaction and it is not playing any role, it will be inmediately abandoned (successfully or unsuccessfully, according to the satisfaction of its purpose). The corresponding event will be forwarded to its interaction and to the interaction of its player agent to account for further changes (inv. 23). Otherwise, the member enters the transient leaving state, thus preventing any action performance. Then, it waits for the completion of the leaving procedures of its played roles, triggered by proper dispatching of the leaving event (inv. 21). 3.2.4 Obligations update In this stage, the obligations of agents (not necessaryly members of the interaction) towards the interaction are updated accordingly. When all the identified agents have been updated, the event is removed from the input queue and the stage field is set back to the interaction state update. For instance, when a course interaction receives an event representing the assignment of some department member to its subject, an obligation to join the course as a teacher is created for that member. Moreover, the event representing this change is added to the output channel of the department interaction. 4. DISCUSSION This paper has attempted to expose a possible semantic core underlying the wide spectrum of interaction types between autonomous, social and situated software components. In the realm of software architectures, this core has been formalised as an operational model of social connectors, intended to describe both the basic structure and dynamics of multi-agent interactions, from the largest (the agent society itself) down to the smallest ones (communicative actions). Thus, top-level interactions may represent the kind of agent-web pursued by large-scale initiatives such as the Agentcities/openNet one . Large-scale interactions, modelling complex aggregates of agent interactions such as those represented by e-institutions or virtual organizations , are also amenable to be conceptualised as particular kinds of first-level social interactions. The last levels of the interaction tree may represent small-scale multiagent interactions such as those represented by interaction protocols , dialogue games , or scenes . Finally, bottom-level interactions may represent communicative actions. From this perspective, the member types of a CA include the speaker and possibly many listeners. The purpose of the speaker coincides with the illocutionary purpose of the CA , whereas the purpose of any listener is to declare that it (actually, the software component) successfully processed the meaning of the CA. The analysis of social interactions put forward in this paper draws upon current proposals of the literature in several general respects, such as the institutional and organizational character of multi-agent systems and the normative perspective on multi-agent protocols . These proposals as well as others focusing in relevant abstractions such as power relationships, contracts, trust and reputation mechanisms in organizational settings, etc., could be further exploited in order to characterize more accurately the organizational character of some multi-agent interactions. Similarly, the conceptualization of communicative actions as atomic interactions may similarly benefit from public semantics of communicative actions such as the one introduced in . Last, the abstract model of protocols may be refined taking into account existing operational models of norms . These analyses shall result in new organizational and communicative abstractions obtained through a refinement and/or extension of the general model of social interactions. Thus, the proposed model is not intended to capture every organizational or communicative feature of multi-agent interactions, but to reveal their roots in basic interaction mechanisms. In turn, this would allow for the exploitation of common formalisms, particularly concerning protocols. Unlike the development of individual agents, which has greatly benefited from the design of several agent programming languages , societal features of multi-agent systems are mostly implemented in terms of visual modelling and a fixed set of interaction abstractions. We argue that the current field of multi-agent system programming may greatly benefit from multi-agent programming languages that allow programmers to accommodate an open set of interaction mechanisms. The model of social interactions put forward in this paper is intended as the abstract machine of a language of this type. This abstract machine would be independent of particular agent architectures and languages (i.e. software components may be programmed in a BDI language such as Jason or in a non-agent oriented language). On top of the presented execution semantics, current and future work aims at the specification of the type system which allows to program the abstract machine, the specification of the corresponding surface syntaxes (both textual and visual) and the design and implementation of a virtual machine over existing middleware technologies such as FIPA platforms or Web services. We also plan to study particular refinements and limitations to the proposed model, particularly with respect to the dispatching of events, semantics The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 895 of obligations, dynamic updates of protocols and rule formalisms. In this latter aspect, we plan to investigate the use of Answer Set Programming to specify the rules of protocols, attending to the role that incompleteness (rules may only specify either necessary or sufficient conditions, for instance), explicit negation (e.g. prohibitions) and defaults play in this domain. 5. ACKNOWLEDGMENTS The authors thank anonymous reviewers for their comments and suggestions. Research sponsored by the Spanish Ministry of Science and Education (MEC),-C03-03.", "body1": "The suitability of agent-based computing to manage the complex patterns of interactions naturally occurring in the development of large scale, open systems, has become one of its major assets over the last few years . Particularly, the organizational or social stance advocated by institutional frameworks and most multi-agent system (MAS) methodologies , provides an excellent basis to deal with the complexity and dynamism of the interactions among system components. The main hypothesis motivating this paper is that the variety of multi-agent interaction mechanisms - both, organizational and communicative, share a common semantic core. This paper thus focuses on the fundamental building blocks of multi-agent interactions: those which may be composed, extended or refined in order to define more complex organizational or communicative types of interactions. Its first goal is to carry out a principled analysis of multiagent interactions, departing from general features commonly ascribed to agent-based computing: autonomy, situatedness and sociality . The paper is structured as follows: first, the major entities and relationships which constitute the structure of social interactions are introduced. From an architectural point of view, interactions between software components are embodied in software connectors: first-class entities defined on the basis of the different roles played by software components and the protocols that regulate their behaviour . The analysis of social interactions introduced in this section gives rise to a new kind of social connector. Besides agents, resources and social protocols, two other kinds of entities are of major relevance in our analysis of social interactions: actions, which represent the way in which agents alter the environmental and social state of the interaction; and events, which represent the changes in the interaction resulting from the performance of actions or the activity of environmental resources. In the following, we describe the basic entities involved in social interactions. in their interactions through human-user interfaces. represents an enumeration type whose values are v1, . is syntactic sugar for ((vl1 )l2 ) . 2.1 Social Interactions Social interactions shall be considered as composite connectors , structured in terms of a tree of nested subinteractions. request, agree, refuse, . The community is thus the top-level interaction which subsumes any other kind of multi-agent interaction2 The organizational and communicative interaction types identified above clearly differ in many ways. The type SI Enum {open, closing, closed} represents the possible execution states of the interaction. 890 The Sixth Intl. 2.2 Agents Components attach themselves as agents in social interactions with the purpose of achieving something. The type A of agents, ranged over by meta-variable a, is defined as follows: A state : SA, player : A, purp : F, att : Queue ACT , ev : Queue E, obl : Set O def. The type SA Enum {playing, leaving, succ, unsuc} represents the execution state of the agent. Components will normally attempt to perform different actions (e.g. and the attempts of members, are subject to the rules governing the interaction. Eventually, the participation of some agent in the interaction will be over. 2.3 Resources Resources are software components which may represent different types of non-autonomous informational or computational entities. The type R of resources, ranged over by meta-variable r, can be specified by the following record type: R cr : A, owners : Set A, op : Set OP def. The last operations feature represents the interface of the resource, consisting of a set of operations. Accordingly, the type P of protocols, ranged over by metaThe Sixth Intl. Firstly, protocols shall include rules to identify the empowerments and permissions of any agent attempting to alter the state of the interaction (e.g. Permission rules shall allow to determine whether the attempt of an empowered agent to perform some particular action is satisfied or not (cf. Hence, even if some student has paid the fee, the attempt to join the course will fail if the course has not entered the corresponding stage6 Secondly, protocols shall allow to determine the obligations of agents towards the interaction. Thirdly, the protocol shall allow to specify monitoring rules for the different events originating within the interaction. The hasPaidFee relationship between (degree) students and subject resources is represented by an additional, application-dependent field of the agent structure for this kind of roles. Last, the protocol shall allow to control the state of the interaction as well as the states of its members. The dynamics of the multi-agent community is influenced by the external actions executed by software components and the protocols governing their interactions. The processing of some attempt may give rise to changes in the scope of the target interaction, such as the instantiation of new participants (agents or resources) or the setting up of new sub-interactions. 3.1 Attempt processing An attempt is defined by the structure AT T perf : A, act : ACT , where the performer represents the agent in charge of executing the specified action. 892 The Sixth Intl. As rule 1 specifies8 , this will only be possible if the performer is empowered to execute that action according to the rules that govern the state of the target interaction. Moreover, the performer agent must be in the playing state (this pre-condition is also required for any rule concerning the processing of attempts). \u03b1target,prot,emp(a, \u03b1) a = playing, , , qACT , , a,\u03b1 :AT T \u2212\u2192 playing, , , qACT , , (1) W here : (\u03b1 )state = perm (qACT ) = insert(\u03b1 , qACT ) 3.1.2 Permissions checking The processing of the action resumes when the possible preceding actions in the performer\"s queue of attempts are fully processed and removed from the queue. \u03b1state = perm \u2227 acontext,ch,in,ev = \u2205 \u2227 \u03b1target,prot,perm(a, \u03b1) a = playing, , , [\u03b1| ], , \u2212\u2192 playing, , , [\u03b1 | ], , (2) W here : (\u03b1 )state = exec Labels of record instances are omitted to allow for more compact specifications. 3.1.3 Action execution The transitions fired in this stage are classified according to the different types of actions to be executed. As an example of constructive action, let\"s consider the execution of a set up action, whose type is defined as follows9 SetUp ACT \u00b7 new : I inv. \u03b1state = exec \u2227 \u03b1 : SetUp \u2227 \u03b1new = i a = playing, , , [\u03b1|qACT ], , \u2212\u2192 playing, , , qACT , , \u03b1target = open, , , , , sI , c \u2212\u2192 open, , , , , sI \u222a i , c (3) W here : (i )ini = a (c )out,ev = insert( a, \u03b1, sub(\u03b1target , i ) , cout,ev Let\"s consider now the case of a close action. The Sixth Intl. \u03b1state = exec \u2227 \u03b1 : Close a = playing, , , [\u03b1|qACT ], , \u2212\u2192 playing, , , qACT , , \u03b1target = open, , , , , p, c \u2212\u2192 open, , , , , p , c (4) W here : (p )finish = \u03b1upd (pfinish (c )out,ev = insert( a, \u03b1, finish(\u03b1target ) , cout,ev 3.2 Event Processing The processing of events is encapsulated in the event channels of interactions. 3.2.1 Event dispatching The processing of some event stored in the output port is triggered when all its preceding events have been dispatched. As a first step, the auxiliary int and ag fields are initialised 11 Alternatively, we may have assumed that interactions are fully aware of any change in the multi-agent community. Last, the event is removed from the queue and the auxiliary fields are re-set to nil. The dispatching function shall identify the set of interactions (possibly, empty) that may be affected by the event (which may include the channel\"s interaction itself)12 . ccontext,state s = open \u2227 ccontext,prot,monitor s = mon cs = [e| ], d, nil, nil , \u2212\u2192 [e| ], , d(e), mon(e) , (5) 3.2.2 Interaction state update Input port activity is triggered when a new event is received. cin,ev 1 = \u2205 \u2227 cin,stage 1 = int \u2227 pfinish , , , , {i} \u222a sI , , c \u2212\u2192 , , , , sI , , c i = , , \u2205, , \u2205, p, c1 \u2212\u2192 closed, , , , , , (6) W here : (c )out,ev = insert(closed(i), cout,ev In the second scenario, the interaction has some member or sub-interaction. 894 The Sixth Intl. 3.2.3 Member state update This stage simply iterates over the members of the interaction to check whether they must be finished according to the protocol\"s over function. If some member has to end its participation in the interaction and it is not playing any role, it will be inmediately abandoned (successfully or unsuccessfully, according to the satisfaction of its purpose). Otherwise, the member enters the transient leaving state, thus preventing any action performance. For instance, when a course interaction receives an event representing the assignment of some department member to its subject, an obligation to join the course as a teacher is created for that member.", "body2": "The suitability of agent-based computing to manage the complex patterns of interactions naturally occurring in the development of large scale, open systems, has become one of its major assets over the last few years . Still, these tools and frameworks are designed to support a limited range of interaction capabilities that constrain developers to a fixed set of particular, pre-defined abstractions. The main hypothesis motivating this paper is that the variety of multi-agent interaction mechanisms - both, organizational and communicative, share a common semantic core. This paper thus focuses on the fundamental building blocks of multi-agent interactions: those which may be composed, extended or refined in order to define more complex organizational or communicative types of interactions. Structural Operational Semantics (SOS), a common technique to specify the operational semantics of programming languages, is used for this purpose. Last, relevant work in the literature is discussed 889 978-81--7-5 (RPS) IFAAMAS with respect to the proposal, limitations are addressed, and current and future work is described. The attachment operation binds a component to the role of a given connector. \u2022 Last, according to the sociality of agents, the specification of social connector protocols - the glue linking agents among themselves and with resources, will rely on normative concepts such as permissions, obligations and empowerments . Besides agents, resources and social protocols, two other kinds of entities are of major relevance in our analysis of social interactions: actions, which represent the way in which agents alter the environmental and social state of the interaction; and events, which represent the changes in the interaction resulting from the performance of actions or the activity of environmental resources. Basically, this means that software components counting as agents in a social interaction may behave non-autonomously in other contexts, e.g. attached to an ORB. , the expression vl1,l2,... The formal model will be illustrated with several examples drawn from the design of a virtual organization to aid in the management of university courses. A test request may also be regarded as a complex interaction, ultimately decomposed in the atomic, or bottom-level interactions represented by communicative actions (e.g. Traversing upwards from a degree to its ancestors, we find its faculty, the university and, finally, the multi-agent community or agent society. 1), so that the context of the toplevel interaction is nil. The initiator is In the context of this application, a one-to-one mapping between human users and software components attached to the community as agents would be a right choice. The transient closing state will be described in the next section. Other members of the course include lecturers and teaching assistants. 3). For instance, students of a course are played by student agents belonging to the (undergraduate) degree, whereas lecturers may be played by teachers of a given department and the assistant role may be played by students of a Ph.D degree (both, the department and the Ph.D. degrees, are modelled as sub-interactions of the faculty). Free variables in the antecedents/consequents of implications shall be understood as universally/existentially quantified. The last field of the structure represents the obligations (O) of agents, to be described later. The transient leaving state will be described in the next section. Other types of computational resources put at the disposal of students by teachers include compilers and interpreters. For instance, the ownership of programs may be shared by several students if the assignment can be performed by groups of two or more students. The present specification abstracts away the particular formalism used to specify these rules, and focuses instead on several requirements concerning the structure and interface of protocols. : Close, Leave We demand from protocols four major kinds of functions. Empowerments may only be exercised under certain circumstances - that permissions specify. Permission rules, in turn, specify that those students may only join the course in the admission stage. For instance, the course\"s protocol establishes that members of departments must join the course as teachers whenever they are assigned to the course\"s subject. For instance, this func5 The formalization assumes that protocol\"s functions implicitly recieve as input the interaction being regulated. tionality is exploited by teachers in order to monitor the enrollment of students to the course. It also establishes that the course must be finished when the admission stage has passed and all the students finished their participation. a timer resource may signal the pass of time) will be skipped. Due to lack of space, we only include some of the operational rules that formalise the execution semantics. Additional actions, such as permit, forbid, empower, etc., to update other protocol\"s fields are yet to be identified in future work. 3.1.1 Empowerment checking The post-condition of an attempt consists of inserting the action in the queue of attempts of the specified performer. If this condition is not met, the attempt will simply be ignored. If the (degree) student has passed the course\"s prerequisite subjects the join action will be inserted in its queue of attempts and considered for execution. Unlike unempowered attempts, a forbidden one will cause an event to be generated and transfered to the event channel for further processing. Moreover, note that record updates in where clauses only affect the specified fields. The second group of actions include those, such as close and leave, whose effects are indirectly achieved by updating the interaction protocol. According to rule 3, the outcome of the execution is threefold: firstly, the performer\"s attempt queue is updated so that the executing action is removed; secondly, the new interaction is added to the target\"s set of sub-interactions (moreover, its initiator field is set to the performer agent); last, the event representing this change (which includes a description of the change, the agent that caused it and the action performed) is inserted in the output port of the target\"s event channel. 10 This strategy is also followed in the definition of leave and may also be used in the definition of other types of actions such as fire, permit, forbid, etc. This event will actually trigger the closing process of the interaction as described in the next subsection. The first one takes place in the output port of the interaction in which the event originated, whereas the other ones execute in separate control threads associated to the input ports of the interactions to which the event was dispatched. 3.2.1 Event dispatching The processing of some event stored in the output port is triggered when all its preceding events have been dispatched. Then, additional rules simply iterate over these collections until all agents and interactions have been notified (i.e., both sets are empty). Last, the event is removed from the queue and the auxiliary fields are re-set to nil. Thus, according to the monitoring rule of university courses, if a student of some degree joins a certain course and specifies a colleague as addressee of that action, the course\"s teachers and itself will also be notified of the successful execution. According to invariant 22, this event will be later inserted to its input channel to allow for further treatment. The way in which the dispatching function is initialised and updated is out of the scope of this paper. Corresponding events will be received by the interaction (according to invariants 23 and 22) so that the conditions of the first scenario will hold. When all members have been checked, the stage field will be set to the next obligation update stage and the auxiliary ag field will be initalised with the agents identified by the protocol\"s obligation update function. 23). When all the identified agents have been updated, the event is removed from the input queue and the stage field is set back to the interaction state update. Moreover, the event representing this change is added to the output channel of the department interaction.", "introduction": "The suitability of agent-based computing to manage the complex patterns of interactions naturally occurring in the development of large scale, open systems, has become one of its major assets over the last few years . Particularly, the organizational or social stance advocated by institutional frameworks and most multi-agent system (MAS) methodologies , provides an excellent basis to deal with the complexity and dynamism of the interactions among system components. This approach has resulted in a wide spectrum of organizational and communicative abstractions, such as institutions, normative positions, power relationships, organizations, groups, scenes, dialogue games, communicative actions (CAs), etc., to effectively model the interaction space of MAS. This wealth of computational abstractions has found currency in several programming frameworks and software platforms (AMELI , MadKit , INGENIAS toolkit , etc. ), which leverage multi-agent middlewares built upon raw ACL-based interaction mechanism , and minimize the gap between organizational metamodels and target implementation languages. Still, these tools and frameworks are designed to support a limited range of interaction capabilities that constrain developers to a fixed set of particular, pre-defined abstractions. The main hypothesis motivating this paper is that the variety of multi-agent interaction mechanisms - both, organizational and communicative, share a common semantic core. This paper thus focuses on the fundamental building blocks of multi-agent interactions: those which may be composed, extended or refined in order to define more complex organizational or communicative types of interactions. Its first goal is to carry out a principled analysis of multiagent interactions, departing from general features commonly ascribed to agent-based computing: autonomy, situatedness and sociality . To approach this issue, we draw on the notion of connector, put forward within the field of software architectures . The outcome of this analysis will be a connector-based model of multi-agent interactions between autonomous social and situated components, i.e. agents, attempting to identify their essential structure. Furthermore, the paper also provides this model with a formal execution semantics which describes the dynamics of multi-agent (or social) interactions. Structural Operational Semantics (SOS), a common technique to specify the operational semantics of programming languages, is used for this purpose. The paper is structured as follows: first, the major entities and relationships which constitute the structure of social interactions are introduced. Next, the dynamics of social interactions will show how these entities and relationships evolve. Last, relevant work in the literature is discussed 889 978-81--7-5 (RPS) IFAAMAS with respect to the proposal, limitations are addressed, and current and future work is described.", "conclusion": "This paper has attempted to expose a possible semantic core underlying the wide spectrum of interaction types between autonomous, social and situated software components.. In the realm of software architectures, this core has been formalised as an operational model of social connectors, intended to describe both the basic structure and dynamics of multi-agent interactions, from the largest (the agent society itself) down to the smallest ones (communicative actions).. Thus, top-level interactions may represent the kind of agent-web pursued by large-scale initiatives such as the Agentcities/openNet one .. Large-scale interactions, modelling complex aggregates of agent interactions such as those represented by e-institutions or virtual organizations , are also amenable to be conceptualised as particular kinds of first-level social interactions.. The last levels of the interaction tree may represent small-scale multiagent interactions such as those represented by interaction protocols , dialogue games , or scenes .. Finally, bottom-level interactions may represent communicative actions.. From this perspective, the member types of a CA include the speaker and possibly many listeners.. The purpose of the speaker coincides with the illocutionary purpose of the CA , whereas the purpose of any listener is to declare that it (actually, the software component) successfully processed the meaning of the CA.. The analysis of social interactions put forward in this paper draws upon current proposals of the literature in several general respects, such as the institutional and organizational character of multi-agent systems and the normative perspective on multi-agent protocols .. These proposals as well as others focusing in relevant abstractions such as power relationships, contracts, trust and reputation mechanisms in organizational settings, etc., could be further exploited in order to characterize more accurately the organizational character of some multi-agent interactions.. Similarly, the conceptualization of communicative actions as atomic interactions may similarly benefit from public semantics of communicative actions such as the one introduced in .. Last, the abstract model of protocols may be refined taking into account existing operational models of norms .. These analyses shall result in new organizational and communicative abstractions obtained through a refinement and/or extension of the general model of social interactions.. Thus, the proposed model is not intended to capture every organizational or communicative feature of multi-agent interactions, but to reveal their roots in basic interaction mechanisms.. In turn, this would allow for the exploitation of common formalisms, particularly concerning protocols.. Unlike the development of individual agents, which has greatly benefited from the design of several agent programming languages , societal features of multi-agent systems are mostly implemented in terms of visual modelling and a fixed set of interaction abstractions.. We argue that the current field of multi-agent system programming may greatly benefit from multi-agent programming languages that allow programmers to accommodate an open set of interaction mechanisms.. The model of social interactions put forward in this paper is intended as the abstract machine of a language of this type.. This abstract machine would be independent of particular agent architectures and languages (i.e.. software components may be programmed in a BDI language such as Jason or in a non-agent oriented language).. On top of the presented execution semantics, current and future work aims at the specification of the type system which allows to program the abstract machine, the specification of the corresponding surface syntaxes (both textual and visual) and the design and implementation of a virtual machine over existing middleware technologies such as FIPA platforms or Web services.. We also plan to study particular refinements and limitations to the proposed model, particularly with respect to the dispatching of events, semantics The Sixth Intl.. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 895 of obligations, dynamic updates of protocols and rule formalisms.. In this latter aspect, we plan to investigate the use of Answer Set Programming to specify the rules of protocols, attending to the role that incompleteness (rules may only specify either necessary or sufficient conditions, for instance), explicit negation (e.g.. prohibitions) and defaults play in this domain.. ACKNOWLEDGMENTS The authors thank anonymous reviewers for their comments and suggestions.. Research sponsored by the Spanish Ministry of Science and Education (MEC),-C03-03."}
{"id": "I-55", "keywords": ["autom negoti", "integr negoti", "multi-criterion decis make"], "title": "Searching for Joint Gains in Automated Negotiations Based on Multi-criteria Decision Making Theory", "abstract": "It is well established by conflict theorists and others that successful negotiation should incorporate \"creating value\" as well as \"claiming value.\" Joint improvements that bring benefits to all parties can be realised by (i) identifying attributes that are not of direct conflict between the parties, (ii) tradeoffs on attributes that are valued differently by different parties, and (iii) searching for values within attributes that could bring more gains to one party while not incurring too much loss on the other party. In this paper we propose an approach for maximising joint gains in automated negotiations by formulating the negotiation problem as a multi-criteria decision making problem and taking advantage of several optimisation techniques introduced by operations researchers and conflict theorists. We use a mediator to protect the negotiating parties from unnecessary disclosure of information to their opponent, while also allowing an objective calculation of maximum joint gains. We separate out attributes that take a finite set of values (simple attributes) from those with continuous values, and we show that for simple attributes, the mediator can determine the Pareto-optimal values. In addition we show that if none of the simple attributes strongly dominates the other simple attributes, then truth telling is an equilibrium strategy for negotiators during the optimisation of simple attributes. We also describe an approach for improving joint gains on non-simple attributes, by moving the parties in a series of steps, towards the Pareto-optimal frontier.", "references": ["A demonstration of methods for studying negotiations between physicians and health care managers", "Multicriteria Optimization", "Generating pareto solutions in a two-party setting: Constraint proposal methods", "Searching for joint gains in multi-party negotiations", "Automated Service Negotiation Between Autonomous Computational Agents", "Minimizing negotiation process losses with computerized negotiation support systems", "An agent architecture for multi-attribute negotiation using incomplete preference information", "Decisions with Multiple Objectives: Preferences and Value Trade-Offs", "Rational agents, contract curves, and non-efficient compromises", "Protocols for negotiating complex contracts", "Multiagent negotiation under time constraints", "Efficient multi-attribute negotiation with incomplete information", "The manager as negotiator: The negotiator's dilemma: Creating and claiming value", "A classification scheme for negotiation in electronic commerce", "Agents that buy and sell", "Two-person cooperative games", "The Art and Science of Negotiation", "Negotiation Analysis: The Science and Art of Collaborative Decision Making", "Agents in electronic commerce: Component technologies for automated negotiation and coalition formation", "Negotiation analysis: A characterization and review", "Knowledge matters: The effect of tactical descriptions on negotiation behavior and outcome", "Integrative negotiation among agents situated in organizations"], "full_text": "1. INTRODUCTION Given that negotiation is perhaps one of the oldest activities in the history of human communication, it\"s perhaps surprising that conducted experiments on negotiations have shown that negotiators more often than not reach inefficient compromises . Raiffa and Sebenius provide analyses on the negotiators\" failure to achieve efficient agreements in practice and their unwillingness to disclose private information due to strategic reasons. According to conflict theorists Lax and Sebenius , most negotiation actually involves both integrative and distributive bargaining which they refer to as creating value and claiming value. They argue that negotiation necessarily includes both cooperative and competitive elements, and that these elements exist in tension. Negotiators face a dilemma in deciding whether to pursue a cooperative or a competitive strategy at a particular time during a negotiation. They refer to this problem as the Negotiator\"s Dilemma. We argue that the Negotiator\"s Dilemma is essentially informationbased, due to the private information held by the agents. Such private information contains both the information that implies the agent\"s bottom lines (or, her walk-away positions) and the information that enforces her bargaining strength. For instance, when bargaining to sell a house to a potential buyer, the seller would try to hide her actual reserve price as much as possible for she hopes to reach an agreement at a much higher price than her reserve price. On the other hand, the outside options available to her (e.g. other buyers who have expressed genuine interest with fairly good offers) consist in the information that improves her bargaining strength about which she would like to convey to her opponent. But at the same time, her opponent is well aware of the fact that it is her incentive to boost her bargaining strength and thus will not accept every information she sends out unless it is substantiated by evidence. Coming back to the Negotiator\"s Dilemma, it\"s not always possible to separate the integrative bargaining process from the distributive bargaining process. In fact, more often than not, the two processes interplay with each other making information manipulation become part of the integrative bargaining process. This is because a negotiator could use the information about his opponent\"s interests against her during the distributive negotiation process. That is, a negotiator may refuse to concede on an important conflicting issue by claiming that he has made a major concession (on another issue) to meet his opponent\"s interests even though the concession he made could be insignificant to him. For instance, few buyers would start a bargaining with a dealer over a deal for a notebook computer by declaring that he is most interested in an extended warranty for the item and therefore prepared to pay a high price to get such an extended warranty. Negotiation Support Systems (NSSs) and negotiating software 508 978-81--7-5 (RPS) IFAAMAS agents (NSAs) have been introduced either to assist humans in making decisions or to enable automated negotiation to allow computer processes to engage in meaningful negotiation to reach agreements (see, for instance, ). However, because of the Negotiator\"s Dilemma and given even bargaining power and incomplete information, the following two undesirable situations often arise: (i) negotiators reach inefficient compromises, or (ii) negotiators engage in a deadlock situation in which both negotiators refuse to act upon with incomplete information and at the same time do not want to disclose more information. In this paper, we argue for the role of a mediator to resolve the above two issues. The mediator thus plays two roles in a negotiation: (i) to encourage cooperative behaviour among the negotiators, and (ii) to absorb the information disclosure by the negotiators to prevent negotiators from using uncertainty and private information as a strategic device. To take advantage of existing results in negotiation analysis and operations research (OR) literatures , we employ multi-criteria decision making (MCDM) theory to allow the negotiation problem to be represented and analysed. Section 2 provides background on MCDM theory and the negotiation framework. Section 3 formulates the problem. In Section 4, we discuss our approach to integrative negotiation. Section 5 discusses the future work with some concluding remarks. 2. BACKGROUND 2.1 Multi-criteria decision making theory Let A denote the set of feasible alternatives available to a decision maker M. As an act, or decision, a in A may involve multiple aspects, we usually describe the alternatives a with a set of attributes j; (j = 1, . . . , m). (Attributes are also referred to as issues, or decision variables.) A typical decision maker also has several objectives X1, . . . , Xk. We assume that Xi, (i = 1, . . . , k), maps the alternatives to real numbers. Thus, a tuple (x1, . . . , xk) = (X1(a), . . . , Xk(a)) denotes the consequence of the act a to the decision maker M. By definition, objectives are statements that delineate the desires of a decision maker. Thus, M wishes to maximise his objectives. However, as discussed thoroughly by Keeney and Raiffa , it is quite likely that a decision maker\"s objectives will conflict with each other in that the improved achievement with one objective can only be accomplished at the expense of another. For instance, most businesses and public services have objectives like minimise cost and maximise the quality of services. Since better services can often only be attained for a price, these objectives conflict. Due to the conflicting nature of a decision maker\"s objectives, M usually has to settle at a compromise solution. That is, he may have to choose an act a \u2208 A that does not optimise every objective. This is the topic of the multi-criteria decision making theory. Part of the solution to this problem is that M has to try to identify the Pareto frontier in the consequence space {(X1(a), . . . , Xk(a))}a\u2208A. DEFINITION 1. (Dominant) Let x = (x1, . . . , xk) and x = (x1, . . . , xk) be two consequences. x dominates x iff xi > xi for all i, and the inequality is strict for at least one i. The Pareto frontier in a consequence space then consists of all consequences that are not dominated by any other consequence. This is illustrated in Fig. 1 in which an alternative consists of two attributes d1 and d2 and the decision maker tries to maximise the two objectives X1 and X2. A decision a \u2208 A whose consequence does not lie on the Pareto frontier is inefficient. While the Pareto 1x d2 a (X (a),X (a)) d1 x2 Alternative spaceA Pareto frontier Consequence space optimal consequenc Figure 1: The Pareto frontier frontier allows M to avoid taking inefficient decisions, M still has to decide which of the efficient consequences on the Pareto frontier is most preferred by him. MCDM theorists introduce a mechanism to allow the objective components of consequences to be normalised to the payoff valuations for the objectives. Consequences can then be ordered: if the gains in satisfaction brought about by C1 (in comparison to C2) equals to the losses in satisfaction brought about by C1 (in comparison to C2), then the two consequences C1 and C2 are considered indifferent. M can now construct the set of indifference curves1 in the consequence space (the dashed curves in Fig. 1). The most preferred indifference curve that intersects with the Pareto frontier is in focus: its intersection with the Pareto frontier is the sought after consequence (i.e., the optimal consequence in Fig. 1). 2.2 A negotiation framework A multi-agent negotiation framework consists of: 1. A set of two negotiating agents N = {1, 2}. 2. A set of attributes Att = {\u03b11, . . . , \u03b1m} characterising the issues the agents are negotiating over. Each attribute \u03b1 can take a value from the set V al\u03b1; 3. A set of alternative outcomes O. An outcome o \u2208 O is represented by an assignment of values to the corresponding attributes in Att. 4. Agents\" utility: Based on the theory of multiple-criteria decision making , we define the agents\" utility as follows: \u2022 Objectives: Agent i has a set of ni objectives, or interests; denoted by j (j = 1, . . . , ni). To measure how much an outcome o fulfills an objective j to an agent i, we use objective functions: for each agent i, we define i\"s interests using the objective vector function fi = [fij ] : O \u2192 Rni \u2022 Value functions: Instead of directly evaluating an outcome o, agent i looks at how much his objectives are fulfilled and will make a valuation based on these more basic criteria. Thus, for each agent i, there is a value function \u03c3i : Rni \u2192 R. In particular, Raiffa shows how to systematically construct an additive value function to each party involved in a negotiation. \u2022 Utility: Now, given an outcome o \u2208 O, an agent i is able to determine its value, i.e., \u03c3i(fi(o)). However, a negotiation infrastructure is usually required to facilitate negotiation. This might involve other mechanisms and factors/parties, e.g., a mediator, a legal institution, participation fees, etc. The standard way to implement such a thing is to allow money In fact, given the k-dimensional space, these should be called indifference surfaces. However, we will not bog down to that level of details. The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 509 and side-payments. In this paper, we ignore those side-effects and assume that agent i\"s utility function ui is normalised so that ui : O \u2192 . EXAMPLE 1. There are two agents, A and B. Agent A has a task T that needs to be done and also 100 units of a resource R. Agent B has the capacity to perform task T and would like to obtain at least 10 and at most 20 units of the resource R. Agent B is indifferent on any amount between 10 and 20 units of the resource R. The objective functions for both agents A and B are cost and revenue. And they both aim at minimising costs while maximising revenues. Having T done generates for A a revenue rA,T while doing T incurs a cost cB,T to B. Agent B obtains a revenue rB,R for each unit of the resource R while providing each unit of the resource R costs agent A cA,R. Assuming that money transfer between agents is possible, the set Att then contains three attributes: \u2022 T, taking values from the set {0, 1}, indicates whether the task T is assigned to agent B; \u2022 R, taking values from the set of non-negative integer, indicates the amount of resource R being allocated to agent B; and \u2022 MT, taking values from R, indicates the payment p to be transferred from A to B. Consider the outcome o = [T = 1, R = k, MT = p], i.e., the task T is assigned to B, and A allocates to B with k units of the resource R, and A transfers p dollars to B. Then, costA(o) = k.cA,R + p and revA(o) = rA,T ; and costB(o) = cB,T and revA(o) = k.rB,R + p if 10 \u2264 k \u2264 20 p otherwise. And, \u03c3i(costi(o), revi(o)) = revi(o) \u2212 costi(o), (i = A, B). 3. PROBLEM FORMALISATION Consider Example 1, assume that rA,T = $150 and cB,T = $100 and rB,R = $10 and cA,R = $7. That is, the revenues generated for A exceeds the costs incurred to B to do task T, and B values resource R more highly than the cost for A to provide it. The optimal solution to this problem scenario is to assign task T to agent B and to allocate 20 units of resource R (i.e., the maximal amount of resource R required by agent B) from agent A to agent B. This outcome regarding the resource and task allocation problems leaves payoffs of $10 to agent A and $100 to agent B.2 Any other outcome would leave at least one of the agents worse off. In other words, the presented outcome is Pareto-efficient and should be part of the solution outcome for this problem scenario. However, as the agents still have to bargain over the amount of money transfer p, neither agent would be willing to disclose their respective costs and revenues regarding the task T and the resource R. As a consequence, agents often do not achieve the optimal outcome presented above in practice. To address this issue, we introduce a mediator to help the agents discover better agreements than the ones they might try to settle on. Note that this problem is essentially the problem of searching for joint gains in a multilateral negotiation in which the involved parties hold strategic information, i.e., the integrative part in a negotiation. In order to help facilitate this process, we introduce the role of a neutral mediator. Before formalising the decision problems faced by the mediator and the Certainly, without money transfer to compensate agent A, this outcome is not a fair one. negotiating agents, we discuss the properties of the solution outcomes to be achieved by the mediator. In a negotiation setting, the two typical design goals would be: \u2022 Efficiency: Avoid the agents from settling on an outcome that is not Pareto-optimal; and \u2022 Fairness: Avoid agreements that give the most of the gains to a subset of agents while leaving the rest with too little. The above goals are axiomatised in Nash\"s seminal work on cooperative negotiation games. Essentially, Nash advocates for the following properties to be satisfied by solution to the bilateral negotiation problem: (i) it produces only Pareto-optimal outcomes; (ii) it is invariant to affine transformation (to the consequence space); (iii) it is symmetric; and (iv) it is independent from irrelevant alternatives. A solution satisfying Nash\"s axioms is called a Nash bargaining solution. It then turns out that, by taking the negotiators\" utilities as its objectives the mediator itself faces a multi-criteria decision making problem. The issues faced by the mediator are: (i) the mediator requires access to the negotiators\" utility functions, and (ii) making (fair) tradeoffs between different agents\" utilities. Our methods allow the agents to repeatedly interact with the mediator so that a Nash solution outcome could be found by the parties. Informally, the problem faced by both the mediator and the negotiators is construction of the indifference curves. Why are the indifference curves so important? \u2022 To the negotiators, knowing the options available along indifference curves opens up opportunities to reach more efficient outcomes. For instance, consider an agent A who is presenting his opponent with an offer \u03b8A which she refuses to accept. Rather than having to concede, A could look at his indifference curve going through \u03b8A and choose another proposal \u03b8A. To him, \u03b8A and \u03b8A are indifferent but \u03b8A could give some gains to B and thus will be more acceptable to B. In other words, the outcome \u03b8A is more efficient than \u03b8A to these two negotiators. \u2022 To the mediator, constructing indifference curves requires a measure of fairness between the negotiators. The mediator needs to determine how much utility it needs to take away from the other negotiators to give a particular negotiator a specific gain G (in utility). In order to search for integrative solutions within the outcome space O, we characterise the relationship between the agents over the set of attributes Att. As the agents hold different objectives and have different capacities, it may be the case that changing between two values of a specific attribute implies different shifts in utility of the agents. However, the problem of finding the exact Paretooptimal set3 is NP-hard . Our approach is thus to solve this optimisation problem in two steps. In the first steps, the more manageable attributes will be solved. These are attributes that take a finite set of values. The result of this step would be a subset of outcomes that contains the Pareto-optimal set. In the second step, we employ an iterative procedure that allows the mediator to interact with the negotiators to find joint improvements that move towards a Pareto-optimal outcome. This approach will not work unless the attributes from Att The Pareto-optimal set is the set of outcomes whose consequences (in the consequence space) correspond to the Pareto frontier. 510 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) are independent. Most works on multi-attribute, or multi-issue, negotiation (e.g. ) assume that the attributes or the issues are independent, resulting in an additive value function for each agent.4 ASSUMPTION 1. Let i \u2208 N and S \u2286 Att. Denote by \u00afS the set Att \\ S. Assume that vS and vS are two assignments of values to the attributes of S and v1 \u00afS, v2 \u00afS are two arbitrary value assignments to the attributes of \u00afS, then (ui([vS, v1 \u00afS]) \u2212 ui([vS, v2 \u00afS])) = (ui([vS, v1 \u00afS])\u2212ui([vS, v2 \u00afS])). That is, the utility function of agent i will be defined on the attributes from S independently of any value assignment to other attributes. 4. MEDIATOR-BASED BILATERAL NEGOTIATIONS As discussed by Lax and Sebenius , under incomplete information the tension between creating and claiming values is the primary cause of inefficient outcomes. This can be seen most easily in negotiations involving two negotiators; during the distributive phase of the negotiation, the two negotiators\"s objectives are directly opposing each other. We will now formally characterise this relationship between negotiators by defining the opposition between two negotiating parties. The following exposition will be mainly reproduced from . Assuming for the moment that all attributes from Att take values from the set of real numbers R, i.e., V alj \u2286 R for all j \u2208 Att. We further assume that the set O = \u00d7j\u2208AttV alj of feasible outcomes is defined by constraints that all parties must obey and O is convex. Now, an outcome o \u2208 O is just a point in the m-dimensional space of real numbers. Then, the questions are: (i) from the point of view of an agent i, is o already the best outcome for i? (ii) if o is not the best outcome for i then is there another outcome o such that o gives i a better utility than o and o does not cause a utility loss to the other agent j in comparison to o? The above questions can be answered by looking at the directions of improvement of the negotiating parties at o, i.e., the directions in the outcome space O into which their utilities increase at point o. Under the assumption that the parties\" utility functions ui are differentiable concave, the set of all directions of improvement for a party at a point o can be defined in terms of his most preferred, or gradient, direction at that point. When the gradient direction \u2207ui(o) of agent i at point o is outright opposing to the gradient direction \u2207uj (o) of agent j at point o then the two parties strongly disagree at o and no joint improvements can be achieved for i and j in the locality surrounding o. Since opposition between the two parties can vary considerably over the outcome space (with one pair of outcomes considered highly antagonistic and another pair being highly cooperative), we need to describe the local properties of the relationship. We begin with the opposition at any point of the outcome space Rm . The following definition is reproduced from : DEFINITION 2. 1. The parties are in local strict opposition at a point x \u2208 Rm iff for all points x \u2208 Rm that are sufficiently close to x (i.e., for some > 0 such that \u2200x x \u2212x < ), an increase of one utility can be achieved only at the expense of a decrease of the other utility. 2. The parties are in local non-strict opposition at a point x \u2208 Rm iff they are not in local strict opposition at x, i.e., iff it is possible for both parties to raise their utilities by moving an infinitesimal distance from x. Klein et al. explore several implications of complex contracts in which attributes are possibly inter-dependent. 3. The parties are in local weak opposition at a point x \u2208 Rm iff \u2207u1(x).\u2207u2(x) \u2265 0, i.e., iff the gradients at x of the two utility functions form an acute or right angle. 4. The parties are in local strong opposition at a point x \u2208 Rm iff \u2207u1(x).\u2207u2(x) < 0, i.e., iff the gradients at x form an obtuse angle. 5. The parties are in global strict (nonstrict, weak, strong) opposition iff for every x \u2208 Rm they are in local strict (nonstrict, weak, strong) opposition. Global strict and nonstrict oppositions are complementary cases. Essentially, under global strict opposition the whole outcome space O becomes the Pareto-optimal set as at no point in O can the negotiating parties make a joint improvement, i.e., every point in O is a Pareto-efficient outcome. In other words, under global strict opposition the outcome space O can be flattened out into a single line such that for each pair of outcomes x, y \u2208 O, u1(x) < u1(y) iff u2(x) > u2(y), i.e., at every point in O, the gradient of the two utility functions point to two different ends of the line. Intuitively, global strict opposition implies that there is no way to obtain joint improvements for both agents. As a consequence, the negotiation degenerates to a distributive negotiation, i.e., the negotiating parties should try to claim as much shares from the negotiation issues as possible while the mediator should aim for the fairness of the division. On the other hand, global nonstrict opposition allows room for joint improvements and all parties might be better off trying to realise the potential gains by reaching Pareto-efficient agreements. Weak and strong oppositions indicate different levels of opposition. The weaker the opposition, the more potential gains can be realised making cooperation the better strategy to employ during negotiation. On the other hand, stronger opposition suggests that the negotiating parties tend to behave strategically leading to misrepresentation of their respective objectives and utility functions and making joint gains more difficult to realise. We have been temporarily making the assumption that the outcome space O is the subset of Rm . In many real-world negotiations, this assumption would be too restrictive. We will continue our exposition by lifting this restriction and allowing discrete attributes. However, as most negotiations involve only discrete issues with a bounded number of options, we will assume that each attribute takes values either from a finite set or from the set of real numbers R. In the rest of the paper, we will refer to attributes whose values are from finite sets as simple attributes and attributes whose values are from R as continuous attributes. The notions of local oppositions, i.e., strict, nonstrict, weak and strong, are not applicable to outcome spaces that contain simple attributes and nor are the notions of global weak and strong oppositions. However, the notions of global strict and nonstrict oppositions can be generalised for outcome spaces that contain simple attributes. DEFINITION 3. Given an outcome space O, the parties are in global strict opposition iff \u2200x, y \u2208 O, u1(x) < u1(y) iff u2(x) > u2(y). The parties are in global nonstrict opposition if they are not in global strict opposition. 4.1 Optimisation on simple attributes In order to extract the optimal values for a subset of attributes, in the first step of this optimisation process the mediator requests the negotiators to submit their respective utility functions over the set of simple attributes. Let Simp \u2286 Att denote the set of all simple attributes from Att. Note that, due to Assumption 1, agent i\"s The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 511 utility function can be characterised as follows: ui([vSimp, vSimp]) = wi 1 \u2217 ui,1([vSimp]) + wi 2 \u2217 ui,2([vSimp]), where Simp = Att \\ Simp, and ui,1 and ui,2 are the utility components of ui over the sets of attributes Simp and Simp, respectively, and 0 < wi 1, wi 2 < 1 and wi 1 + wi 2 = 1. As attributes are independent of each other regarding the agents\" utility functions, the optimisation problem over the attributes from Simp can be carried out by fixing ui([vSimp]) to a constant C, and then search for the optimal values within the set of attributes Simp. Now, how does the mediator determine the optimal values for the attributes in Simp? Several well-known optimisation strategies could be applicable here: \u2022 The utilitarian solution: The sum of the agents\" utilities are maximised. Thus, the optimal values are the solution of the following optimisation problem: arg max v\u2208V alSimp i\u2208N ui(v) \u2022 The Nash solution: The product of the agents\" utilities are maximised. Thus, the optimal values are the solution of the following optimisation problem: arg max v\u2208V alSimp i\u2208N ui(v) \u2022 The egalitarian solution (aka. the maximin solution): The utility of the agent with minimum utility is maximised. Thus, the optimal values are the solution of the following optimisation problem: arg max v\u2208V alSimp min i\u2208N ui(v) The question now is of course whether a negotiator has the incentive to misrepresent his utility function. First of all, recall that the agents\" utility functions are bounded, i.e., \u2200o \u2208 O.0 \u2264 ui(o) \u2264 1. Thus, the agents have no incentive to overstate their utility regarding an outcome o: If o is the most preferred outcome to an agent i then he already assigns the maximal utility to o. On the other hand, if o is not the most preferred outcome to i then by overstating the utility he assigns to o, the agent i runs the risk of having to settle on an agreement which would give him less payoffs than he is supposed to receive. However, agents do have an incentive to understate their utility if the final settlement will be based on the above solutions alone. Essentially, the mechanism to avoid an agent to understate his utility regarding particular outcomes is to guarantee a certain measure of fairness for the final settlement. That is, the agents lose the incentive to be dishonest to obtain gains from taking advantage of the known solutions to determine the settlement outcome for they would be offset by the fairness maintenance mechanism. Firsts, we state an easy lemma. LEMMA 1. When Simp contains one single attributes, the agents have the incentive to understate their utility functions regarding outcomes that are not attractive to them. By way of illustration, consider the set Simp containing only one attribute that could take values from the finite set {A, B, C, D}. Assume that negotiator 1 assigns utilities of 0.4, 0.7, 0.9, and 1 to A, B, C, and D, respectively. Assume also that negotiator 2 assigns utilities of 1, 0.9, 0.7, and 0.4 to A, B, C, and D, respectively. If agent 1 misrepresents his utility function to the mediator by reporting utility 0 for all values A, B and C and utility 1 for value D then the agent 2 who plays honestly in his report to the mediator will obtain the worst outcome D given any of the above solutions. Note that agent 1 doesn\"t need to know agent 2\"s utility function, nor does he need to know the strategy employed by agent 2. As long as he knows that the mediator is going to employ one of the above three solutions, then the above misrepresentation is the dominant strategy for this game. However, when the set Simp contains more than one attribute and none of the attributes strongly dominate the other attributes then the above problem disminishes by itself thanks to the integrative solution. We of course have to define clearly what it means for an attribute to strongly dominate other attributes. Intuitively, if most of an agent\"s utility concentrates on one of the attributes then this attribute strongly dominates other attributes. We again appeal to the Assumption 1 on additivity of utility functions to achieve a measure of fairness within this negotiation setting. Due to Assumption 1, we can characterise agent i\"s utility component over the set of attributes Simp by the following equation: ui,1([vSimp]) = j\u2208Simp wi j \u2217 ui,j([vj]) (1) where j\u2208Simp wj = 1. Then, an attribute \u2208 Simp strongly dominates the rest of the attributes in Simp (for agent i) iff wi j\u2208(Simp\u2212 ) wi j . Attribute is said to be strongly dominant (for agent i) wrt. the set of simple attributes Simp. The following theorem shows that if the set of attributes Simp does not contain a strongly dominant attribute then the negotiators have no incentive to be dishonest. THEOREM 1. Given a negotiation framework, if for every agent the set of simple attributes doesn\"t contain a strongly dominant attribute, then truth-telling is an equilibrium strategy for the negotiators during the optimisation of simple attributes. So far, we have been concentrating on the efficiency issue while leaving the fairness issue aside. A fair framework does not only support a more satisfactory distribution of utility among the agents, but also often a good measure to prevent misrepresentation of private information by the agents. Of the three solutions presented above, the utilitarian solution does not support fairness. On the other hand, Nash proves that the Nash solution satisfies the above four axioms for the cooperative bargaining games and is considered a fair solution. The egalitarian solution is another mechanism to achieve fairness by essentially helping the worst off. The problem with these solutions, as discussed earlier, is that they are vulnerable to strategic behaviours when one of the attributes strongly dominates the rest of attributes. However, there is yet another solution that aims to guarantee fairness, the minimax solution. That is, the utility of the agent with maximum utility is minimised. It\"s obvious that the minimax solution produces inefficient outcomes. However, to get around this problem (given that the Pareto-optimal set can be tractably computed), we can apply this solution over the Pareto-optimal set only. Let POSet \u2286 V alSimp be the Pareto-optimal subset of the simple outcomes, the minimax solution is defined to be the solution of the following optimisation problem. arg min v\u2208P OSet max i\u2208N ui(v) While overall efficiency often suffers under a minimax solution, i.e., the sum of all agents\" utilities are often lower than under other solutions, it can be shown that the minimax solution is less vulnerable to manipulation. 512 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) THEOREM 2. Given a negotiation framework, under the minimax solution, if the negotiators are uncertain about their opponents\" preferences then truth-telling is an equilibrium strategy for the negotiators during the optimisation of simple attributes. That is, even when there is only one single simple attribute, if an agent is uncertain whether the other agent\"s most preferred resolution is also his own most preferred resolution then he should opt for truth-telling as the optimal strategy. 4.2 Optimisation on continuous attributes When the attributes take values from infinite sets, we assume that they are continuous. This is similar to the common practice in operations research in which linear programming solutions/techniques are applied to integer programming problems. We denote the number of continuous attributes by k, i.e., Att = Simp \u222a Simp and |Simp| = k. Then, the outcome space O can be represented as follows: O = ( j\u2208Simp V alj) \u00d7 ( l\u2208Simp V all), where l\u2208Simp V all \u2286 Rk is the continuous component of O. Let Oc denote the set l\u2208Simp V all. We\"ll refer to Oc as the feasible set and assume that Oc is closed and convex. After carrying out the optimisation over the set of simple attributes, we are able to assign the optimal values to the simple attributes from Simp. Thus, we reduce the original problem to the problem of searching for optimal (and fair) outcomes within the feasible set Oc . Recall that, by Assumption 1, we can characterise agent i\"s utility function as follows: ui([v\u2217 Simp, vSimp]) = C + wi 2 \u2217 ui,2([vSimp]), where C is the constant wi 1 \u2217 ui,1([v\u2217 Simp]) and v\u2217 Simp denotes the optimal values of the simple attributes in Simp. Hence, without loss of generality (albeit with a blatant abuse of notation), we can take the agent i\"s utility function as ui : Rk \u2192 . Accordingly we will also take the set of outcomes under consideration by the agents to be the feasible set Oc . We now state another assumption to be used in this section: ASSUMPTION 2. The negotiators\" utility functions can be described by continuously differentiable and concave functions ui : Rk \u2192 , (i = 1, 2). It should be emphasised that we do not assume that agents explicitly know their utility functions. For the method to be described in the following to work, we only assume that the agents know the relevant information, e.g. at certain point within the feasible set Oc the gradient direction of their own utility functions and some section of their respective indifference curves. Assume that a tentative agreement (which is a point x \u2208 Rk ) is currently on the table, the process for the agents to jointly improve this agreement in order to reach a Pareto-optimal agreement can be described as follows. The mediator asks the negotiators to discretely submit their respective gradient directions at x, i.e., \u2207u1(x) and \u2207u2(x). Note that the goal of the process to be described here is to search for agreements that are more efficient than the tentative agreement currently on the table. That is, we are searching for points x within the feasible set Oc such that moving to x from the current tentative agreement x brings more gains to at least one of the agents while not hurting any of the agents. Due to the assumption made above, i.e. the feasible set Oc is bounded, the conditions for an alternative x \u2208 Oc to be efficient vary depending on the position of x. The following results are proved in : Let B(x) = 0 denote the equation of the boundary of Oc , defining x \u2208 Oc iff B(x) \u2265 0. An alternative x\u2217 \u2208 Oc is efficient iff, either A. x\u2217 is in the interior of Oc and the parties are in local strict opposition at x\u2217 , i.e., \u2207u1(x\u2217 ) = \u2212\u03b3\u2207u2(x\u2217 ) (2) where \u03b3 > 0; or B. x\u2217 is on the boundary of Oc , and for some \u03b1, \u03b2 \u2265 0: \u03b1\u2207u1(x\u2217 ) + \u03b2\u2207u2(x\u2217 ) = \u2207B(x\u2217 ) (3) We are now interested in answering the following questions: (i) What is the initial tentative agreement x0? (ii) How to find the more efficient agreement xh+1, given the current tentative agreement xh? 4.2.1 Determining a fair initial tentative agreement It should be emphasised that the choice of the initial tentative agreement affects the fairness of the final agreement to be reached by the presented method. For instance, if the initial tentative agreement x0 is chosen to be the most preferred alternative to one of the agents then it is also a Pareto-optimal outcome, making it impossible to find any joint improvement from x0. However, if x0 will then be chosen to be the final settlement and if x0 turns out to be the worst alternative to the other agent then this outcome is a very unfair one. Thus, it\"s important that the choice of the initial tentative agreement be sensibly made. Ehtamo et al present several methods to choose the initial tentative agreement (called reference point in their paper). However, their goal is to approximate the Pareto-optimal set by systematically choosing a set of reference points. Once an (approximate) Pareto-optimal set is generated, it is left to the negotiators to decide which of the generated Pareto-optimal outcomes to be chosen as the final settlement. That is, distributive negotiation will then be required to settle the issue. We, on the other hand, are interested in a fair initial tentative agreement which is not necessarily efficient. Improving a given tentative agreement to yield a Pareto-optimal agreement is considered in the next section. For each attribute j \u2208 Simp, an agent i will be asked to discretely submit three values (from the set V alj): the most preferred value, denoted by pvi,j, the least preferred value, denoted by wvi,j, and a value that gives i an approximately average payoff, denoted by avi,j. (Note that this is possible because the set V alj is bounded.) If pv1,j and pv2,j are sufficiently close, i.e., |pv1,j \u2212 pv2,j| < \u0394 for some pre-defined \u0394 > 0, then pv1,j and pv2,j are chosen to be the two core values, denoted by cv1 and cv2. Otherwise, between the two values pv1,j and av1,j, we eliminate the one that is closer to wv2,j, the remaining value is denoted by cv1. Similarly, we obtain cv2 from the two values pv2,j and av2,j. If cv1 = cv2 then cv1 is selected as the initial value for the attribute j as part of the initial tentative agreement. Otherwise, without loss of generality, we assume that cv1 < cv2. The mediator selects randomly p values mv1, . . . , mvp from the open interval (cv1, cv2), where p \u2265 1. The mediator then asks the agents to submit their valuations over the set of values {cv1, cv2, mv1, . . . , mvp}. The value whose the two valuations of two agents are closest is selected as the initial value for the attribute j as part of the initial tentative agreement. The above procedure guarantees that the agents do not gain by behaving strategically. By performing the above procedure on every attribute j \u2208 Simp, we are able to identify the initial tentative agreement x0 such that x0 \u2208 Oc . The next step is to compute a new tentative agreement from an existing tentative agreement so that the new one would be more efficient than the existing one. The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 513 4.2.2 Computing new tentative agreement Our procedure is a combination of the method of jointly improving direction introduced by Ehtamo et al and a method we propose in the coming section. Basically, the idea is to see how strong the opposition the parties are in. If the two parties are in (local) weak opposition at the current tentative agreement xh, i.e., their improving directions at xh are close to each other, then the compromise direction proposed by Ehtamo et al is likely to point to a better agreement for both agents. However, if the two parties are in local strong opposition at the current point xh then it\"s unclear whether the compromise direction would really not hurt one of the agents whilst bringing some benefit to the other. We will first review the method proposed by Ehtamo et al to compute the compromise direction for a group of negotiators at a given point x \u2208 Oc . Ehtamo et al define a a function T(x) that describes the mediator\"s choice for a compromise direction at x. For the case of two-party negotiations, the following bisecting function, denoted by T BS , can be defined over the interior set of Oc Note that the closed set Oc contains two disjoint subsets: Oc Oc 0 \u222aOc B , where Oc 0 denotes the set of interior points of Oc and Oc denotes the boundary of Oc . The bisecting compromise is defined by a function T BS : Oc 0 \u2192 R2 T BS (x) = \u2207u1(x) \u2207u1(x) \u2207u2(x) \u2207u2(x) , x \u2208 Oc 0. (4) Given the current tentative agreement xh (h \u2265 0), the mediator has to choose a point xh+1 along d = T(xh) so that all parties gain. Ehtamo et al then define a mechanism to generate a sequence of points and prove that when the generated sequence is bounded and when all generated points (from the sequence) belong to the interior set Oc 0 then the sequence converges to a weakly Paretooptimal agreement [4, pp. 59-60].5 As the above mechanism does not work at the boundary points of Oc , we will introduce a procedure that works everywhere in an alternative space Oc . Let x \u2208 Oc and let \u03b8(x) denote the angle between the gradients \u2207u1(x) and \u2207u2(x) at x. That is, \u03b8(x) = arccos( \u2207u1(x).\u2207u2(x) \u2207u1(x) . \u2207u2(x) From Definition 2, it is obvious that the two parties are in local strict opposition (at x) iff \u03b8(x) = \u03c0, and they are in local strong opposition iff \u03c0 \u2265 \u03b8(x) > \u03c0/2, and they are in local weak opposition iff \u03c0/2 \u2265 \u03b8(x) \u2265 0. Note also that the two vectors \u2207u1(x) and \u2207u2(x) define a hyperplane, denoted by h\u2207(x), in the kdimensional space Rk . Furthermore, there are two indifference curves of agents 1 and 2 going through point x, denoted by IC1(x) and IC2(x), respectively. Let hT1(x) and hT2(x) denote the tangent hyperplanes to the indifference curves IC1(x) and IC2(x), respectively, at point x. The planes hT1(x) and hT2(x) intersect h\u2207(x) in the lines IS1(x) and IS2(x), respectively. Note that given a line L(x) going through the point x, there are two (unit) vectors from x along L(x) pointing to two opposite directions, denoted by L+ (x) and L\u2212 (x). We can now informally explain our solution to the problem of searching for joint gains. When it isn\"t possible to obtain a compromise direction for joint improvements at a point x \u2208 Oc either because the compromise vector points to the space outside of the feasible set Oc or because the two parties are in local strong opposition at x, we will consider to move along the indifference curve of one party while trying to improve the utility of the other party. As Let S be the set of alternatives, x\u2217 is weakly Pareto optimal if there is no x \u2208 S such that ui(x) > ui(x\u2217 ) for all agents i. the mediator does not know the indifference curves of the parties, he has to use the tangent hyperplanes to the indifference curves of the parties at point x. Note that the tangent hyperplane to a curve is a useful approximation of the curve in the immediate vicinity of the point of tangency, x. We are now describing an iteration step to reach the next tentative agreement xh+1 from the current tentative agreement xh \u2208 Oc . A vector v whose tail is xh is said to be bounded in Oc if \u2203\u03bb > 0 such that xh +\u03bbv \u2208 Oc . To start, the mediator asks the negotiators for their gradients \u2207u1(xh) and \u2207u2(xh), respectively, at xh. 1. If xh is a Pareto-optimal outcome according to equation 2 or equation 3, then the process is terminated. 2. If 1 \u2265 \u2207u1(xh).\u2207u2(xh) > 0 and the vector T BS (xh) is bounded in Oc then the mediator chooses the compromise improving direction d = T BS (xh) and apply the method described by Ehtamo et al to generate the next tentative agreement xh+1. 3. Otherwise, among the four vectors IS\u03c3 i (xh), i = 1, 2 and \u03c3 = +/\u2212, the mediator chooses the vector that (i) is bounded in Oc , and (ii) is closest to the gradient of the other agent, \u2207uj (xh)(j = i). Denote this vector by T G(xh). That is, we will be searching for a point on the indifference curve of agent i, ICi(xh), while trying to improve the utility of agent j. Note that when xh is an interior point of Oc then the situation is symmetric for the two agents 1 and 2, and the mediator has the choice of either finding a point on IC1(xh) to improve the utility of agent 2, or finding a point on IC2(xh) to improve the utility of agent 1. To decide on which choice to make, the mediator has to compute the distribution of gains throughout the whole process to avoid giving more gains to one agent than to the other. Now, the point xh+1 to be generated lies somewhere on the intersection of ICi(xh) and the hyperplane defined by \u2207ui(xh) and T G(xh). This intersection is approximated by T G(xh). Thus, the sought after point xh+1 can be generated by first finding a point yh along the direction of T G(xh) and then move from yh to the same direction of \u2207ui(xh) until we intersect with ICi(xh). Mathematically, let \u03b6 and \u03be denote the vectors T G(xh) and \u2207ui(xh), respectively, xh+1 is the solution to the following optimisation problem: max \u03bb1,\u03bb2\u2208L uj(xh + \u03bb1\u03b6 + \u03bb2\u03be) s.t. xh+\u03bb1\u03b6+\u03bb2\u03be \u2208 Oc , and ui(xh+\u03bb1\u03b6+\u03bb2\u03be) = ui(xh), where L is a suitable interval of positive real numbers; e.g., L = {\u03bb|\u03bb > 0}, or L = {\u03bb|a < \u03bb \u2264 b}, 0 \u2264 a < b. Given an initial tentative agreement x0, the method described above allows a sequence of tentative agreements x1, x2, . . . to be iteratively generated. The iteration stops whenever a weakly Pareto optimal agreement is reached. THEOREM 3. If the sequence of agreements generated by the above method is bounded then the method converges to a point x\u2217 \u2208 Oc that is weakly Pareto optimal. 5. CONCLUSION AND FUTURE WORK In this paper we have established a framework for negotiation that is based on MCDM theory for representing the agents\" objectives and utilities. The focus of the paper is on integrative negotiation in which agents aim to maximise joint gains, or create value. 514 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) We have introduced a mediator into the negotiation in order to allow negotiators to disclose information about their utilities, without providing this information to their opponents. Furthermore, the mediator also works toward the goal of achieving fairness of the negotiation outcome. That is, the approach that we describe aims for both efficiency, in the sense that it produces Pareto optimal outcomes (i.e. no aspect can be improved for one of the parties without worsening the outcome for another party), and also for fairness, which chooses optimal solutions which distribute gains amongst the agents in some appropriate manner. We have developed a two step process for addressing the NP-hard problem of finding a solution for a set of integrative attributes, which is within the Pareto-optimal set for those attributes. For simple attributes (i.e. those which have a finite set of values) we use known optimisation techniques to find a Paretooptimal solution. In order to discourage agents from misrepresenting their utilities to gain an advantage, we look for solutions that are least vulnerable to manipulation. We have shown that as long as one of the simple attributes does not strongly dominate the others, then truth telling is an equilibrium strategy for the negotiators during the stage of optimising simple attributes. For non-simple attributes we propose a mechanism that provides stepwise improvements to move the proposed solution in the direction of a Paretooptimal solution. The approach presented in this paper is similar to the ideas behind negotiation analysis . Ehtamo et al presents an approach to searching for joint gains in multi-party negotiations. The relation of their approach to our approach is discussed in the preceding section. Lai et al provide an alternative approach to integrative negotiation. While their approach was clearly described for the case of two-issue negotiations, the generalisation to negotiations with more than two issues is not entirely clear. Zhang et at discuss the use of integrative negotiation in agent organisations. They assume that agents are honest. Their main result is an experiment showing that in some situations, agents\" cooperativeness may not bring the most benefits to the organisation as a whole, while giving no explanation. Jonker et al consider an approach to multi-attribute negotiation without the use of a mediator. Thus, their approach can be considered a complement of ours. Their experimental results show that agents can reach Paretooptimal outcomes using their approach. The details of the approach have currently been shown only for bilateral negotiation, and while we believe they are generalisable to multiple negotiators, this work remains to be done. There is also future work to be done in more fully characterising the outcomes of the determination of values for the non-simple attributes. In order to provide a complete framework we are also working on the distributive phase using the mediator.", "body1": "Given that negotiation is perhaps one of the oldest activities in the history of human communication, it\"s perhaps surprising that conducted experiments on negotiations have shown that negotiators more often than not reach inefficient compromises . We argue that the Negotiator\"s Dilemma is essentially informationbased, due to the private information held by the agents. Coming back to the Negotiator\"s Dilemma, it\"s not always possible to separate the integrative bargaining process from the distributive bargaining process. In this paper, we argue for the role of a mediator to resolve the above two issues. 2.1 Multi-criteria decision making theory Let A denote the set of feasible alternatives available to a decision maker M. As an act, or decision, a in A may involve multiple aspects, we usually describe the alternatives a with a set of attributes j; (j = 1, . Due to the conflicting nature of a decision maker\"s objectives, M usually has to settle at a compromise solution. DEFINITION 1. This is illustrated in Fig. MCDM theorists introduce a mechanism to allow the objective components of consequences to be normalised to the payoff valuations for the objectives. 2. 4. \u2022 Utility: Now, given an outcome o \u2208 O, an agent i is able to determine its value, i.e., \u03c3i(fi(o)). This might involve other mechanisms and factors/parties, e.g., a mediator, a legal institution, participation fees, etc. The Sixth Intl. EXAMPLE 1. Consider the outcome o = [T = 1, R = k, MT = p], i.e., the task T is assigned to B, and A allocates to B with k units of the resource R, and A transfers p dollars to B. And, \u03c3i(costi(o), revi(o)) = revi(o) \u2212 costi(o), (i = A, B). Consider Example 1, assume that rA,T = $150 and cB,T = $100 and rB,R = $10 and cA,R = $7. The optimal solution to this problem scenario is to assign task T to agent B and to allocate 20 units of resource R (i.e., the maximal amount of resource R required by agent B) from agent A to agent B. However, as the agents still have to bargain over the amount of money transfer p, neither agent would be willing to disclose their respective costs and revenues regarding the task T and the resource R. As a consequence, agents often do not achieve the optimal outcome presented above in practice. The above goals are axiomatised in Nash\"s seminal work on cooperative negotiation games. It then turns out that, by taking the negotiators\" utilities as its objectives the mediator itself faces a multi-criteria decision making problem. Informally, the problem faced by both the mediator and the negotiators is construction of the indifference curves. \u2022 To the mediator, constructing indifference curves requires a measure of fairness between the negotiators. In order to search for integrative solutions within the outcome space O, we characterise the relationship between the agents over the set of attributes Att. Our approach is thus to solve this optimisation problem in two steps. 510 The Sixth Intl. NEGOTIATIONS As discussed by Lax and Sebenius , under incomplete information the tension between creating and claiming values is the primary cause of inefficient outcomes. Now, an outcome o \u2208 O is just a point in the m-dimensional space of real numbers. Since opposition between the two parties can vary considerably over the outcome space (with one pair of outcomes considered highly antagonistic and another pair being highly cooperative), we need to describe the local properties of the relationship. Klein et al. 3. 4. 5. Global strict and nonstrict oppositions are complementary cases. Essentially, under global strict opposition the whole outcome space O becomes the Pareto-optimal set as at no point in O can the negotiating parties make a joint improvement, i.e., every point in O is a Pareto-efficient outcome. Intuitively, global strict opposition implies that there is no way to obtain joint improvements for both agents. We have been temporarily making the assumption that the outcome space O is the subset of Rm . The parties are in global nonstrict opposition if they are not in global strict opposition. 4.1 Optimisation on simple attributes In order to extract the optimal values for a subset of attributes, in the first step of this optimisation process the mediator requests the negotiators to submit their respective utility functions over the set of simple attributes. As attributes are independent of each other regarding the agents\" utility functions, the optimisation problem over the attributes from Simp can be carried out by fixing ui([vSimp]) to a constant C, and then search for the optimal values within the set of attributes Simp. Thus, the agents have no incentive to overstate their utility regarding an outcome o: If o is the most preferred outcome to an agent i then he already assigns the maximal utility to o. By way of illustration, consider the set Simp containing only one attribute that could take values from the finite set {A, B, C, D}. Assume that negotiator 1 assigns utilities of 0.4, 0.7, 0.9, and 1 to A, B, C, and D, respectively. However, when the set Simp contains more than one attribute and none of the attributes strongly dominate the other attributes then the above problem disminishes by itself thanks to the integrative solution. Then, an attribute \u2208 Simp strongly dominates the rest of the attributes in Simp (for agent i) iff wi j\u2208(Simp\u2212 ) wi j . THEOREM 1. So far, we have been concentrating on the efficiency issue while leaving the fairness issue aside. However, there is yet another solution that aims to guarantee fairness, the minimax solution. arg min v\u2208P OSet max i\u2208N ui(v) While overall efficiency often suffers under a minimax solution, i.e., the sum of all agents\" utilities are often lower than under other solutions, it can be shown that the minimax solution is less vulnerable to manipulation. 512 The Sixth Intl. 4.2 Optimisation on continuous attributes When the attributes take values from infinite sets, we assume that they are continuous. We denote the number of continuous attributes by k, i.e., Att = Simp \u222a Simp and |Simp| = k. Then, the outcome space O can be represented as follows: O = ( j\u2208Simp V alj) \u00d7 ( l\u2208Simp V all), where l\u2208Simp V all \u2286 Rk is the continuous component of O. It should be emphasised that we do not assume that agents explicitly know their utility functions. Note that the goal of the process to be described here is to search for agreements that are more efficient than the tentative agreement currently on the table. Ehtamo et al present several methods to choose the initial tentative agreement (called reference point in their paper). We, on the other hand, are interested in a fair initial tentative agreement which is not necessarily efficient. The above procedure guarantees that the agents do not gain by behaving strategically. We will first review the method proposed by Ehtamo et al to compute the compromise direction for a group of negotiators at a given point x \u2208 Oc . We can now informally explain our solution to the problem of searching for joint gains. We are now describing an iteration step to reach the next tentative agreement xh+1 from the current tentative agreement xh \u2208 Oc . 2. 3. Given an initial tentative agreement x0, the method described above allows a sequence of tentative agreements x1, x2, .", "body2": "They refer to this problem as the Negotiator\"s Dilemma. But at the same time, her opponent is well aware of the fact that it is her incentive to boost her bargaining strength and thus will not accept every information she sends out unless it is substantiated by evidence. However, because of the Negotiator\"s Dilemma and given even bargaining power and incomplete information, the following two undesirable situations often arise: (i) negotiators reach inefficient compromises, or (ii) negotiators engage in a deadlock situation in which both negotiators refuse to act upon with incomplete information and at the same time do not want to disclose more information. Section 5 discusses the future work with some concluding remarks. Since better services can often only be attained for a price, these objectives conflict. , Xk(a))}a\u2208A. The Pareto frontier in a consequence space then consists of all consequences that are not dominated by any other consequence. While the Pareto 1x d2 a (X (a),X (a)) d1 x2 Alternative spaceA Pareto frontier Consequence space optimal consequenc Figure 1: The Pareto frontier frontier allows M to avoid taking inefficient decisions, M still has to decide which of the efficient consequences on the Pareto frontier is most preferred by him. A set of two negotiating agents N = {1, 2}. An outcome o \u2208 O is represented by an assignment of values to the corresponding attributes in Att. Thus, for each agent i, there is a value function \u03c3i : Rni \u2192 R. In particular, Raiffa shows how to systematically construct an additive value function to each party involved in a negotiation. However, a negotiation infrastructure is usually required to facilitate negotiation. However, we will not bog down to that level of details. In this paper, we ignore those side-effects and assume that agent i\"s utility function ui is normalised so that ui : O \u2192 . Assuming that money transfer between agents is possible, the set Att then contains three attributes: \u2022 T, taking values from the set {0, 1}, indicates whether the task T is assigned to agent B; \u2022 R, taking values from the set of non-negative integer, indicates the amount of resource R being allocated to agent B; and \u2022 MT, taking values from R, indicates the payment p to be transferred from A to B. Then, costA(o) = k.cA,R + p and revA(o) = rA,T ; and costB(o) = cB,T and revA(o) = k.rB,R + p if 10 \u2264 k \u2264 20 p otherwise. And, \u03c3i(costi(o), revi(o)) = revi(o) \u2212 costi(o), (i = A, B). That is, the revenues generated for A exceeds the costs incurred to B to do task T, and B values resource R more highly than the cost for A to provide it. In other words, the presented outcome is Pareto-efficient and should be part of the solution outcome for this problem scenario. In a negotiation setting, the two typical design goals would be: \u2022 Efficiency: Avoid the agents from settling on an outcome that is not Pareto-optimal; and \u2022 Fairness: Avoid agreements that give the most of the gains to a subset of agents while leaving the rest with too little. A solution satisfying Nash\"s axioms is called a Nash bargaining solution. Our methods allow the agents to repeatedly interact with the mediator so that a Nash solution outcome could be found by the parties. In other words, the outcome \u03b8A is more efficient than \u03b8A to these two negotiators. The mediator needs to determine how much utility it needs to take away from the other negotiators to give a particular negotiator a specific gain G (in utility). However, the problem of finding the exact Paretooptimal set3 is NP-hard . This approach will not work unless the attributes from Att The Pareto-optimal set is the set of outcomes whose consequences (in the consequence space) correspond to the Pareto frontier. That is, the utility function of agent i will be defined on the attributes from S independently of any value assignment to other attributes. We further assume that the set O = \u00d7j\u2208AttV alj of feasible outcomes is defined by constraints that all parties must obey and O is convex. When the gradient direction \u2207ui(o) of agent i at point o is outright opposing to the gradient direction \u2207uj (o) of agent j at point o then the two parties strongly disagree at o and no joint improvements can be achieved for i and j in the locality surrounding o. The parties are in local non-strict opposition at a point x \u2208 Rm iff they are not in local strict opposition at x, i.e., iff it is possible for both parties to raise their utilities by moving an infinitesimal distance from x. explore several implications of complex contracts in which attributes are possibly inter-dependent. The parties are in local weak opposition at a point x \u2208 Rm iff \u2207u1(x).\u2207u2(x) \u2265 0, i.e., iff the gradients at x of the two utility functions form an acute or right angle. The parties are in local strong opposition at a point x \u2208 Rm iff \u2207u1(x).\u2207u2(x) < 0, i.e., iff the gradients at x form an obtuse angle. The parties are in global strict (nonstrict, weak, strong) opposition iff for every x \u2208 Rm they are in local strict (nonstrict, weak, strong) opposition. Global strict and nonstrict oppositions are complementary cases. In other words, under global strict opposition the outcome space O can be flattened out into a single line such that for each pair of outcomes x, y \u2208 O, u1(x) < u1(y) iff u2(x) > u2(y), i.e., at every point in O, the gradient of the two utility functions point to two different ends of the line. On the other hand, stronger opposition suggests that the negotiating parties tend to behave strategically leading to misrepresentation of their respective objectives and utility functions and making joint gains more difficult to realise. Given an outcome space O, the parties are in global strict opposition iff \u2200x, y \u2208 O, u1(x) < u1(y) iff u2(x) > u2(y). The parties are in global nonstrict opposition if they are not in global strict opposition. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 511 utility function can be characterised as follows: ui([vSimp, vSimp]) = wi 1 \u2217 ui,1([vSimp]) + wi 2 \u2217 ui,2([vSimp]), where Simp = Att \\ Simp, and ui,1 and ui,2 are the utility components of ui over the sets of attributes Simp and Simp, respectively, and 0 < wi 1, wi 2 < 1 and wi 1 + wi 2 = 1. First of all, recall that the agents\" utility functions are bounded, i.e., \u2200o \u2208 O.0 \u2264 ui(o) \u2264 1. When Simp contains one single attributes, the agents have the incentive to understate their utility functions regarding outcomes that are not attractive to them. By way of illustration, consider the set Simp containing only one attribute that could take values from the finite set {A, B, C, D}. As long as he knows that the mediator is going to employ one of the above three solutions, then the above misrepresentation is the dominant strategy for this game. Due to Assumption 1, we can characterise agent i\"s utility component over the set of attributes Simp by the following equation: ui,1([vSimp]) = j\u2208Simp wi j \u2217 ui,j([vj]) (1) where j\u2208Simp wj = 1. The following theorem shows that if the set of attributes Simp does not contain a strongly dominant attribute then the negotiators have no incentive to be dishonest. Given a negotiation framework, if for every agent the set of simple attributes doesn\"t contain a strongly dominant attribute, then truth-telling is an equilibrium strategy for the negotiators during the optimisation of simple attributes. The problem with these solutions, as discussed earlier, is that they are vulnerable to strategic behaviours when one of the attributes strongly dominates the rest of attributes. Let POSet \u2286 V alSimp be the Pareto-optimal subset of the simple outcomes, the minimax solution is defined to be the solution of the following optimisation problem. arg min v\u2208P OSet max i\u2208N ui(v) While overall efficiency often suffers under a minimax solution, i.e., the sum of all agents\" utilities are often lower than under other solutions, it can be shown that the minimax solution is less vulnerable to manipulation. That is, even when there is only one single simple attribute, if an agent is uncertain whether the other agent\"s most preferred resolution is also his own most preferred resolution then he should opt for truth-telling as the optimal strategy. This is similar to the common practice in operations research in which linear programming solutions/techniques are applied to integer programming problems. The negotiators\" utility functions can be described by continuously differentiable and concave functions ui : Rk \u2192 , (i = 1, 2). The mediator asks the negotiators to discretely submit their respective gradient directions at x, i.e., \u2207u1(x) and \u2207u2(x). Thus, it\"s important that the choice of the initial tentative agreement be sensibly made. That is, distributive negotiation will then be required to settle the issue. The value whose the two valuations of two agents are closest is selected as the initial value for the attribute j as part of the initial tentative agreement. The next step is to compute a new tentative agreement from an existing tentative agreement so that the new one would be more efficient than the existing one. However, if the two parties are in local strong opposition at the current point xh then it\"s unclear whether the compromise direction would really not hurt one of the agents whilst bringing some benefit to the other. Note that given a line L(x) going through the point x, there are two (unit) vectors from x along L(x) pointing to two opposite directions, denoted by L+ (x) and L\u2212 (x). Note that the tangent hyperplane to a curve is a useful approximation of the curve in the immediate vicinity of the point of tangency, x. If xh is a Pareto-optimal outcome according to equation 2 or equation 3, then the process is terminated. If 1 \u2265 \u2207u1(xh).\u2207u2(xh) > 0 and the vector T BS (xh) is bounded in Oc then the mediator chooses the compromise improving direction d = T BS (xh) and apply the method described by Ehtamo et al to generate the next tentative agreement xh+1. xh+\u03bb1\u03b6+\u03bb2\u03be \u2208 Oc , and ui(xh+\u03bb1\u03b6+\u03bb2\u03be) = ui(xh), where L is a suitable interval of positive real numbers; e.g., L = {\u03bb|\u03bb > 0}, or L = {\u03bb|a < \u03bb \u2264 b}, 0 \u2264 a < b. If the sequence of agreements generated by the above method is bounded then the method converges to a point x\u2217 \u2208 Oc that is weakly Pareto optimal.", "introduction": "Given that negotiation is perhaps one of the oldest activities in the history of human communication, it\"s perhaps surprising that conducted experiments on negotiations have shown that negotiators more often than not reach inefficient compromises . Raiffa and Sebenius provide analyses on the negotiators\" failure to achieve efficient agreements in practice and their unwillingness to disclose private information due to strategic reasons. According to conflict theorists Lax and Sebenius , most negotiation actually involves both integrative and distributive bargaining which they refer to as creating value and claiming value. They argue that negotiation necessarily includes both cooperative and competitive elements, and that these elements exist in tension. Negotiators face a dilemma in deciding whether to pursue a cooperative or a competitive strategy at a particular time during a negotiation. They refer to this problem as the Negotiator\"s Dilemma. We argue that the Negotiator\"s Dilemma is essentially informationbased, due to the private information held by the agents. Such private information contains both the information that implies the agent\"s bottom lines (or, her walk-away positions) and the information that enforces her bargaining strength. For instance, when bargaining to sell a house to a potential buyer, the seller would try to hide her actual reserve price as much as possible for she hopes to reach an agreement at a much higher price than her reserve price. On the other hand, the outside options available to her (e.g. other buyers who have expressed genuine interest with fairly good offers) consist in the information that improves her bargaining strength about which she would like to convey to her opponent. But at the same time, her opponent is well aware of the fact that it is her incentive to boost her bargaining strength and thus will not accept every information she sends out unless it is substantiated by evidence. Coming back to the Negotiator\"s Dilemma, it\"s not always possible to separate the integrative bargaining process from the distributive bargaining process. In fact, more often than not, the two processes interplay with each other making information manipulation become part of the integrative bargaining process. This is because a negotiator could use the information about his opponent\"s interests against her during the distributive negotiation process. That is, a negotiator may refuse to concede on an important conflicting issue by claiming that he has made a major concession (on another issue) to meet his opponent\"s interests even though the concession he made could be insignificant to him. For instance, few buyers would start a bargaining with a dealer over a deal for a notebook computer by declaring that he is most interested in an extended warranty for the item and therefore prepared to pay a high price to get such an extended warranty. Negotiation Support Systems (NSSs) and negotiating software 508 978-81--7-5 (RPS) IFAAMAS agents (NSAs) have been introduced either to assist humans in making decisions or to enable automated negotiation to allow computer processes to engage in meaningful negotiation to reach agreements (see, for instance, ). However, because of the Negotiator\"s Dilemma and given even bargaining power and incomplete information, the following two undesirable situations often arise: (i) negotiators reach inefficient compromises, or (ii) negotiators engage in a deadlock situation in which both negotiators refuse to act upon with incomplete information and at the same time do not want to disclose more information. In this paper, we argue for the role of a mediator to resolve the above two issues. The mediator thus plays two roles in a negotiation: (i) to encourage cooperative behaviour among the negotiators, and (ii) to absorb the information disclosure by the negotiators to prevent negotiators from using uncertainty and private information as a strategic device. To take advantage of existing results in negotiation analysis and operations research (OR) literatures , we employ multi-criteria decision making (MCDM) theory to allow the negotiation problem to be represented and analysed. Section 2 provides background on MCDM theory and the negotiation framework. In Section 4, we discuss our approach to integrative negotiation. Section 5 discusses the future work with some concluding remarks.", "conclusion": "In this paper we have established a framework for negotiation that is based on MCDM theory for representing the agents\" objectives and utilities.. The focus of the paper is on integrative negotiation in which agents aim to maximise joint gains, or create value.. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) We have introduced a mediator into the negotiation in order to allow negotiators to disclose information about their utilities, without providing this information to their opponents.. Furthermore, the mediator also works toward the goal of achieving fairness of the negotiation outcome.. That is, the approach that we describe aims for both efficiency, in the sense that it produces Pareto optimal outcomes (i.e.. no aspect can be improved for one of the parties without worsening the outcome for another party), and also for fairness, which chooses optimal solutions which distribute gains amongst the agents in some appropriate manner.. We have developed a two step process for addressing the NP-hard problem of finding a solution for a set of integrative attributes, which is within the Pareto-optimal set for those attributes.. those which have a finite set of values) we use known optimisation techniques to find a Paretooptimal solution.. In order to discourage agents from misrepresenting their utilities to gain an advantage, we look for solutions that are least vulnerable to manipulation.. We have shown that as long as one of the simple attributes does not strongly dominate the others, then truth telling is an equilibrium strategy for the negotiators during the stage of optimising simple attributes.. For non-simple attributes we propose a mechanism that provides stepwise improvements to move the proposed solution in the direction of a Paretooptimal solution.. The approach presented in this paper is similar to the ideas behind negotiation analysis .. Ehtamo et al presents an approach to searching for joint gains in multi-party negotiations.. The relation of their approach to our approach is discussed in the preceding section.. Lai et al provide an alternative approach to integrative negotiation.. While their approach was clearly described for the case of two-issue negotiations, the generalisation to negotiations with more than two issues is not entirely clear.. Zhang et at discuss the use of integrative negotiation in agent organisations.. They assume that agents are honest.. Their main result is an experiment showing that in some situations, agents\" cooperativeness may not bring the most benefits to the organisation as a whole, while giving no explanation.. Jonker et al consider an approach to multi-attribute negotiation without the use of a mediator.. Thus, their approach can be considered a complement of ours.. Their experimental results show that agents can reach Paretooptimal outcomes using their approach.. The details of the approach have currently been shown only for bilateral negotiation, and while we believe they are generalisable to multiple negotiators, this work remains to be done.. There is also future work to be done in more fully characterising the outcomes of the determination of values for the non-simple attributes.. In order to provide a complete framework we are also working on the distributive phase using the mediator."}
{"id": "H-21", "keywords": ["queri classif", "web search", "blind relev feedback"], "title": "Robust Classification of Rare Queries Using Web Knowledge", "abstract": "We propose a methodology for building a practical robust query classiflcation system that can identify thousands of query classes with reasonable accuracy, while dealing in real- time with the query volume of a commercial web search en- gine. We use a blind feedback technique: given a query, we determine its topic by classifying the web search results retrieved by the query. Motivated by the needs of search ad- vertising, we primarily focus on rare queries, which are the hardest from the point of view of machine learning, yet in ag- gregation account for a considerable fraction of search engine tra-c. Empirical evaluation conflrms that our methodology yields a considerably higher classiflcation accuracy than pre- viously reported. We believe that the proposed methodology will lead to better matching of online ads to rare queries and overall to a better user experience.", "references": ["Automatic web query classification using labeled and unlabeled training data", "Improving automatic query classification via semi-supervised learning", "Pattern Classification and Scene Analysis", "UCLA-Okapi at TREC-2: Query expansion experiments", "Feature generation for text categorization using world knowledge", "Categorizing web queries according to geographical locality", "Centroid-based document classification: Analysis and experimental results", "IR evaluation methods for retrieving highly relevant documents", "The ferrety algorithm for the KDD Cup 2005 problem", "Analyzing the effect of query class on document retrieval performance", "Facing a great challenge", "Improving automatic query expansion", "Search Engine Marketing", "Okapi at TREC-3", "Relevance feedback in information retrieval", "Improving retrieval performance by relevance feedback", "The Statistical Analysis of Discrete Data", "Our winning solution to query classification in KDDCUP 2005", "Query enrichment for web-query classification", "Building bridges for web query classification", "Classifying search engine queries using the web as background knowledge", "Query expansion using lexical-semantic relations", "Improving the effectiveness of information retrieval with local context analysis"], "full_text": "1. INTRODUCTION In its 12 year lifetime, web search had grown tremendously: it has simultaneously become a factor in the daily life of maybe a billion people and at the same time an eight billion dollar industry fueled by web advertising. One thing, however, has remained constant: people use very short queries. Various studies estimate the average length of a search query between 2.4 and 2.7 words, which by all accounts can carry only a small amount of information. Commercial search engines do a remarkably good job in interpreting these short strings, but they are not (yet!) omniscient. Therefore, using additional external knowledge to augment the queries can go a long way in improving the search results and the user experience. At the same time, better understanding of query meaning has the potential of boosting the economic underpinning of Web search, namely, online advertising, via the sponsored search mechanism that places relevant advertisements alongside search results. For instance, is about laptops can obviously lead to more focused advertisements even if no advertiser has specifically bidden on these particular queries. In this study we present a methodology for query classification, nodes. Given such classifications, one can directly use them to provide better search results as well as more focused ads. The problem of query classification is extremely difficult owing to the brevity of queries. Observe, however, that in many cases a human looking at a search query and the search query results does remarkably well in making sense of it. Of course, the sheer volume of search queries does not lend itself to human supervision, and therefore we need alternate sources of knowledge about the world. For instance, in the example above, SD450 brings pages about Canon cameras, brings pages about Compaq laptops, hence to a human the intent is quite clear. Search engines index colossal amounts of information, and as such can be viewed as very comprehensive repositories of knowledge. Following the heuristic described above, we propose to use the search results themselves to gain additional insights for query interpretation. To this end, we employ the pseudo relevance feedback paradigm, and assume the top search results to be relevant to the query. Certainly, not all results are equally relevant, and thus we use elaborate voting schemes in order to obtain reliable knowledge about the query. For the purpose of this study we first dispatch the given query to a general web search engine, and collect a number of the highest-scoring URLs. We crawl the Web pages pointed by these URLs, and classify these pages. Finally, we use these result-page classifications to classify the original query. Our empirical evaluation confirms that using Web search results in this manner yields substantial improvements in the accuracy of query classification. Note that in a practical implementation of our methodology within a commercial search engine, all indexed pages can be pre-classified using the normal text-processing and indexing pipeline. Thus, at run-time we only need to run the voting procedure, without doing any crawling or classification. This additional overhead is minimal, and therefore the use of search results to improve query classification is entirely feasible in run-time. Another important aspect of our work lies in the choice of queries. The volume of queries in today\"s search engines follows the familiar power law, where a few queries appear very often while most queries appear only a few times. While individual queries in this long tail are rare, together they account for a considerable mass of all searches. Furthermore, the aggregate volume of such queries provides a substantial opportunity for income through on-line advertising.1 Searching and advertising platforms can be trained to yield good results for frequent queries, including auxiliary data such as maps, shortcuts to related structured information, successful ads, and so on. However, the tail queries simply do not have enough occurrences to allow statistical learning on a per-query basis. Therefore, we need to aggregate such queries in some way, and to reason at the level of aggregated query clusters. A natural choice for such aggregation is to classify the queries into a topical taxonomy. Knowing which taxonomy nodes are most relevant to the given query will aid us to provide the same type of support for rare queries as for frequent queries. Consequently, in this work we focus on the classification of rare queries, whose correct classification is likely to be particularly beneficial. Early studies in query interpretation focused on query augmentation through external dictionaries . More recent studies also attempted to gather some additional knowledge from the Web. However, these studies had a number of shortcomings, which we overcome in this paper. Specifically, earlier works in the field used very small query classification taxonomies of only a few dozens of nodes, which do not allow ample specificity for online advertising . They also used a separate ancillary taxonomy for Web documents, so that an extra level of indirection had to be employed to establish the correspondence between the ancillary and the main taxonomies . The main contributions of this paper are as follows. First, we build the query classifier directly for the target taxonomy, instead of using a secondary auxiliary structure; this greatly simplifies taxonomy maintenance and development. The taxonomy used in this work is two orders of magnitude larger than that used in prior studies. The empirical evaluation demonstrates that our methodology for using external knowledge achieves greater improvements than those previously reported. Since our taxonomy is considerably larger, the classification problem we face is much more difficult, making the improvements we achieve particularly notable. We also report the results of a thorough empirical study of different voting schemes and different depths of knowledge (e.g., using search summaries vs. entire crawled pages). We found that crawling the search results yields deeper knowledge and leads to greater improvements than mere summaries. This result is in contrast with prior findings in query classification , but is supported by research in mainstream text classification . 2. METHODOLOGY Our methodology has two main phases. In the first phase, In the above examples, represent fairly old gadget models, and hence there are advertisers placing ads on these queries. However, in this paper we mainly deal with rare queries which are extremely difficult to match to relevant ads. we construct a document classifier for classifying search results into the same taxonomy into which queries are to be classified. In the second phase, we develop a query classifier that invokes the document classifier on search results, and uses the latter to perform query classification. 2. nodes used in a major US search engine (see Section 3.1). Human editors populated the taxonomy nodes with labeled examples that we used as training instances to learn a document classifier in phase 1. Given a taxonomy of this size, the computational efficiency of classification is a major issue. Few machine learning algorithms can efficiently handle so many different classes, each having hundreds of training examples. Suitable candidates include the nearest neighbor and the Naive Bayes classifier , as well as prototype formation methods such as Rocchio or centroid-based classifiers. A recent study showed centroid-based classifiers to be both effective and efficient for large-scale taxonomies and consequently, we used a centroid classifier in this work. 2.2 Query classification by search Having developed a document classifier for the query taxonomy, we now turn to the problem of obtaining a classification for a given query based on the initial search results it yields. Let\"s assume that there is a set of documents D = d1 . . . dm indexed by a search engine. The search engine can then be represented by a function f = similarity(q, d) that quantifies the affinity between a query q and a document d. Examples of such affinity scores used in this paper are rank-the rank of the document in the ordered list of search results; static score-the score of the goodness of the page regardless of the query (e.g., PageRank); and dynamic score-the closeness of the query and the document. Query classification is determined by first evaluating conditional probabilities of all possible classes P(Cj|q), and then selecting the alternative with the highest probability Cmax = arg maxCj \u2208C P(Cj|q). Our goal is to estimate the conditional probability of each possible class using the search results initially returned by the query. We use the following formula that incorporates classifications of individual search results: P(Cj|q) = d\u2208D P(Cj|q, d)\u00b7 P(d|q) = d\u2208D P(q|Cj, d) P(q|d) \u00b7 P(Cj|d)\u00b7 P(d|q). We assume that P(q|Cj, d) \u2248 P(q|d), that is, a probability of a query given a document can be determined without knowing the class of the query. This is the case for the majority of queries that are unambiguous. Counter examples are queries like \"jaguar\" (animal and car brand) or \"apple\" (fruit and computer manufacturer), but such ambiguous queries can not be classified by definition, and usually consists of common words. In this work we concentrate on rare queries, that tend to contain rare words, be longer, and match fewer documents; consequently in our setting this assumption mostly holds. Using this assumption, we can write P(Cj|q) = d\u2208D P(Cj|d)\u00b7 P(d|q). The conditional probability of a classification for a given document P(Cj|d) is estimated using the output of the document classifier (section 2.1). While P(d|q) is harder to compute, we consider the underlying relevance model for ranking documents given a query. This issue is further explored in the next section. 2.3 Classification-based relevance model In order to describe a formal relationship of classification and ad placement (or search), we consider a model for using classification to determine ads (or search) relevance. Let a be an ad and q be a query, we denote by R(a, q) the relevance of a to q. This number indicates how relevant the ad a is to query q, and can be used to rank ads a for a given query q. In this paper, we consider the following approximation of relevance function: R(a, q) \u2248 RC (a, q) = Cj \u2208C w(Cj)s(Cj, a)s(Cj, q). (1) The right hand-side expresses how we use the classification scheme C to rank ads, where s(c, a) is a scoring function that specifies how likely a is in class c, and s(c, q) is a scoring function that specifies how likely q is in class c. The value w(c) is a weighting term for category c, indicating the importance of category c in the relevance formula. This relevance function is an adaptation of the traditional word-based retrieval rules. For example, we may let categories be the words in the vocabulary. We take s(Cj, a) as the word counts of Cj in a, s(Cj, q) as the word counts of Cj in q, and w(Cj) as the IDF term weighting for word Cj. With such choices, the method given by (1) becomes the standard TFIDF retrieval rule. If we take s(Cj, a) = P(Cj|a), s(Cj, q) = P(Cj|q), and w(Cj) = 1/P(Cj), and assume that q and a are independently generated given a hidden concept C, then we have RC (a, q) = Cj \u2208C P(Cj|a)P(Cj|q)/P(Cj) Cj \u2208C P(Cj|a)P(q|Cj)/P(q) = P(q|a)/P(q). That is, the ads are ranked according to P(q|a). This relevance model has been employed in various statistical language modeling techniques for information retrieval. The intuition can be described as follows. We assume that a person searches an ad a by constructing a query q: the person first picks a concept Cj according to the weights P(Cj|a), and then constructs a query q with probability P(q|Cj) based on the concept Cj. For this query generation process, the ads can be ranked based on how likely the observed query is generated from each ad. It should be mentioned that in our case, each query and ad can have multiple categories. For simplicity, we denote by Cj a random variable indicating whether q belongs to category Cj. We use P(Cj|q) to denote the probability of q belonging to category Cj. Here the sum Cj \u2208C P(Cj|q) may not equal to one. We then consider the following ranking formula: RC (a, q) = Cj \u2208C P(Cj|a)P(Cj|q). (2) We assume the estimation of P(Cj|a) is based on an existing text-categorization system (which is known). Thus, we only need to obtain estimates of P(Cj|q) for each query q. Equation (2) is the ad relevance model that we consider in this paper, with unknown parameters P(Cj|q) for each query q. In order to obtain their estimates, we use search results from major US search engines, where we assume that the ranking formula in (2) gives good ranking for search. That is, top results ranked by search engines should also be ranked high by this formula. Therefore given a query q, and top K result pages d1(q), . . . , dK (q) from a major search engine, we fit parameters P(Cj|q) so that RC (di(q), q) have high scores for i = 1, . . . , K. It is worth mentioning that using this method we can only compute relative strength of P(Cj|q), but not the scale, because scale does not affect ranking. Moreover, it is possible that the parameters estimated may be of the form g(P(Cj|q)) for some monotone function g(\u00b7) of the actually conditional probability g(P(Cj|q)). Although this may change the meaning of the unknown parameters that we estimate, it does not affect the quality of using the formula to rank ads. Nor does it affect query classification with appropriately chosen thresholds. In what follows, we consider two methods to compute the classification information P(Cj|q). 2.4 The voting method We would like to compute P(Cj|q) so that RC (di(q), q) are high for i = 1, . . . , K and RC (d, q) are low for a random document d. Assume that the vector [P(Cj|d)]Cj \u2208C is random for an average document, then the condition that Cj \u2208C P(Cj|q)2 is small implies that RC (d, q) is also small averaged over d. Thus, a natural method is to maximize i=1 wiRC (di(q), q) subject to Cj \u2208C P(Cj|q)2 being small, where wi are weights associated with each rank i: max [P (\u00b7|q)] \uf8f0 1 i=1 wi Cj \u2208C P(Cj|di(q))P(Cj|q) \u2212 \u03bb Cj \u2208C P(Cj|q)2 \uf8fb , where we assume K i=1 wi = 1, and \u03bb > 0 is a tuning regularization parameter. The optimal solution is P(Cj|q) = 2\u03bb i=1 wiP(Cj|di(q)). Since both P(Cj|di(q)) and P(Cj|q) belong to , we may just take \u03bb = 0.5 to align the scale. In the experiment, we will simply take uniform weights wi. A more complex strategy is to let w depend on d as well: P(Cj|q) = w(d, q)g(P(Cj|d)), where g(x) is a certain transformation of x. In this general formulation, w(d, q) may depend on factors other than the rank of d in the search engine results for q. For example, it may be a function of r(d, q) where r(d, q) is the relevance score returned by the underlying search engine. Moreover, if we are given a set of hand-labeled training category/query pairs (C, q), then both the weights w(d, q) and the transformation g(\u00b7) can be learned using standard classification techniques. 2.5 Discriminative classification We can treat the problem of estimating P(Cj|q) as a classification problem, where for each q, we label di(q) for i = 1, . . . , K as positive data, and the remaining documents as negative data. That is, we assign label yi(q) = 1 for di(q) when i \u2264 K, and label yi(q) = \u22121 for di(q) when i > K. In this setting, the classification scoring rule for a document di(q) is linear. Let xi(q) = [P(Cj|di(q))], and w = [P(Cj|q)], then Cj \u2208C P(Cj|q)P(Cj|di(q)) = w\u00b7xi(q). The values P(Cj|d) are the features for the linear classifier, and [P(Cj|d)] is the weight vector, which can be computed using any linear classification method. In this paper, we consider estimating w using logistic regression as follows: P(\u00b7|q) = arg minw i ln(1 + e\u2212w\u00b7xi(q)yi(q) ). 0 1 2 3 4 5 6 7 8 9 10 Numberofcategories Taxonomy level Figure 1: Number of categories by level 3. EVALUATION In this section, we evaluate our methodology that uses Web search results for improving query classification. 3.1 Taxonomy Our choice of taxonomy was guided by a Web advertising application. Since we want the classes to be useful for matching ads to queries, the taxonomy needs to be elaborate enough to facilitate ample classification specificity. For example, classifying all medical queries into one node will likely result in poor ad matching, as both sore foot and flu queries will end up in the same node. The ads appropriate for these two queries are, however, very different. To avoid such situations, the taxonomy needs to provide sufficient discrimination between common commercial topics. Therefore, nodes, arranged in a hierarchy with median depth 5 and maximum depth 9. Figure 1 shows the distribution of categories by taxonomy levels. Human editors populated the taxonomy with labeled queries (approx. 150 queries per node), which were used as a training set; a small fraction of queries have been assigned to more than one category. 3.2 Digression: the basics of sponsored search To discuss our set of evaluation queries, we need a brief introduction to some basic concepts of Web advertising. Sponsored search (or paid search) advertising is placing textual ads on the result pages of web search engines, with ads being driven by the originating query. All major search engines (Google, Yahoo!, and MSN) support such ads and act simultaneously as a search engine and an ad agency. These textual ads are characterized by one or more bid phrases representing those queries where the advertisers would like to have their ad displayed. (The name bid phrase comes from the fact that advertisers bid various amounts to secure their position in the tower of ads associated to a query. A discussion of bidding and placement mechanisms is beyond the scope of this paper . However, many searches do not explicitly use phrases that someone bids on. Consequently, advertisers also buy broad matches, that is, they pay to place their advertisements on queries that constitute some modification of the desired bid phrase. In broad match, several syntactic modifications can be applied to the query to match it to the bid phrase, e.g., dropping or adding words, synonym substitution, etc. These transformations are based on rules and dictionaries. As advertisers tend to cover high-volume and high-revenue queries, broad-match queries fall into the tail of the distribution with respect to both volume and revenue. 3. queries. Both sets contain queries that cannot be directly matched to advertisements, that is, none of the queries contains a bid phrase (this means we eliminated practically all popular queries). The first set of queries can be matched to at least one ad using broad match as described above. Queries in the second set cannot be matched even by broad match, and therefore the search engine used in our study does not currently display any advertising for them. In a sense, these are even more rare queries and further away from common queries. As a measure of query rarity, we estimated their frequency in a month worth of query logs for a major US search engine; the median frequency was 1 for queries in Set 1 and 0 for queries in Set 2. The queries in the two sets differ in their classification difficulty. In fact, queries in Set 2 are difficult to interpret even for human evaluators. Queries in Set 1 have on average 3.50 words, with the longest one having 11 words; queries in Set 2 have on average 4.39 words, with the longest query of 81 words. Recent studies estimate the average length of web queries to be just under 3 words2 , which is lower than in our test sets. As another measure of query difficulty, we measured the fraction of queries that contain quotation marks, as the latter assist query interpretation by meaningfully grouping the words. Only 8% queries in Set 1 and 14% in Set 2 contained quotation marks. 3.4 Methodology and evaluation metrics The two sets of queries were classified into the target taxonomy using the techniques presented in section 2. Based on the confidence values assigned, the top 3 classes for each query were presented to human evaluators. These evaluators were trained editorial staff who possessed knowledge about the taxonomy. The editors considered every queryclass pair, and rated them on the scale 1 to 4, with 1 meaning the classification is highly relevant and 4 meaning it is irrelevant for the query. About 2.4% queries in Set 1 and 5.4% queries in Set 2 were judged to be unclassifiable (e.g., random strings of characters), and were consequently excluded from evaluation. To compute evaluation metrics, we treated classifications with ratings 1 and 2 to be correct, and those with ratings 3 and 4 to be incorrect. We used standard evaluation metrics: precision, recall and F1. In what follows, we plot precision-recall graphs for all the experiments. For comparison with other published studies, we also report precision and F1 values corresponding to complete recall (R = 1). Owing to the lack of space, we only show graphs for query Set 1; however, we show the numerical results for both sets in the tables. 3.5 Results We compared our method to a baseline query classifier that does not use any external knowledge. Our baseline classifier expanded queries using standard query expansion techniques, grouped their terms using a phrase recognizer, boosted certain phrases in the query based on their statistical properties, and performed classification using the 0.4 0.5 0.6 0.7 0.8 0.9 1.00.90.80.70.60.50.40.30.20.1 Precision Recall Baseline Engine A full-page Engine A summary Engine B full-page Engine B summary Figure 2: The effect of external knowledge nearest-neighbor approach. This baseline classifier is actually a production version of the query classifier running in a major US search engine. In our experiments, we varied values of pertinent parameters that characterize the exact way of using search results. In what follows, we start with the general assessment of the effect of using Web search results. We then proceed to exploring more refined techniques, such as using only search summaries versus actually crawling the returned URLs. We also experimented with using different numbers of search results per query, as well as with varying the number of classifications considered for each search result. For lack of space, we only show graphs for Set 1 queries and omit the graphs for Set 2 queries, which exhibit similar phenomena. 3.5.1 The effect of external knowledge Queries by themselves are very short and difficult to classify. We use top search engine results for collecting background knowledge for queries. We employed two major US search engines, and used their results in two ways, either only summaries or the full text of crawled result pages. Figure 2 and Table 1 show that such extra knowledge considerably improves classification accuracy. Interestingly, we found that search engine A performs consistently better with full-page text, while search engine B performs better when summaries are used. Engine Context Prec. F1 Prec. F1 Set 1 Set 1 Set 2 Set 2 A full-page 0.72 0.84 0.509 0.721 B full-page 0.706 0.827 0.497 0.665 A summary 0.586 0.744 0.396 0.572 B summary 0.645 0.788 0.467 0.638 Baseline 0.534 0.696 0.365 0.536 Table 1: The effect of using external knowledge 3.5.2 Aggregation techniques There are two major ways to use search results as additional knowledge. First, individual results can be classified separately, with subsequent voting among individual classifications. Alternatively, individual search results can be bundled together as one meta-document and classified as such using the document classifier. Figure 3 presents the results of these two approaches When full-text pages are used, the technique using individual classifications of search results evidently outperforms the bundling approach by a wide margin. However, in the case of summaries, bundling together is found to be consistently better than individual classification. This is because summaries by themselves are too short to be classified correctly individually, but when bundled together they are much more stable. 0.4 0.5 0.6 0.7 0.8 0.9 1.00.90.80.70.60.50.40.30.20.1 Precision Recall Baseline Bundled full-page Voting full-page Bundled summary Voting summary Figure 3: Voting vs. Bundling 3.5.3 Full page text vs. summary To summarize the two preceding sections, background knowledge for each query is obtained by using either the full-page text or only the summaries of the top search results. Full page text was found to be more in conjunction with voted classification, while summaries were found to be useful when bundled together. The best results overall were obtained with full-page results classified individually, with subsequent voting used to determine the final query classification. This observation differs from findings by Shen et al. , who found summaries to be more useful. We attribute this distinction to the fact that the queries we used in this study are tail ones, which are rare and difficult to classify. 3.5.4 Varying the number of classes per search result We also varied the number of classifications per search result, i.e., each result was permitted to have either 1, 3, or 5 classes. Figure 4 shows the corresponding precision-recall graphs for both full-page and summary-only settings. As can be readily seen, all three variants produce very similar results. However, the precision-recall curve for the 1-class experiment has higher fluctuations. Using 3 classes per search result yields a more stable curve, while with 5 classes per result the precision-recall curve is very smooth. Thus, as we increase the number of classes per result, we observe higher stability in query classification. 3.5.5 Varying the number of search results obtained We also experimented with different numbers of search results per query. Figure 5 and Table 2 present the results of this experiment. In line with our intuition, we observed that classification accuracy steadily rises as we increase the number of search results used from 10 to 40, with a slight drop as we continue to use even more results (50). This is because using too few search results provides too little external knowledge, while using too many results introduces extra noise. Using paired t-test, we assessed the statistical significance 0.4 0.5 0.6 0.7 0.8 0.9 1.00.90.80.70.60.50.40.30.20.1 Precision Recall Baseline 1 class full-page 3 classes full-page 5 classes full-page 1 class summary 3 classes summary 5 classes summary Figure 4: Varying the number of classes per page 0.4 0.5 0.6 0.7 0.8 0.9 1.00.90.80.70.60.50.40.30.20.1 Precision Recall 10 20 30 40 50 Baseline Figure 5: Varying the number of results per query of the improvements due to our methodology versus the baseline. We found the results to be highly significant (p <), thus confirming the value of external knowledge for query classification. 3.6 Voting versus alternative methods As explained in Section 2.2, one may use several methods to classify queries from search engine results based on our relevance model. As we have seen, the voting method works quite well. In this section, we compare the performance of voting top-ten search results to the following two methods: \u2022 A: Discriminative learning of query-classification based on logistic regression, described in Section 2.5. \u2022 B: Learning weights based on quality score returned by a search engine. We discretize the quality score s(d, q) of a query/document pair into {high, medium, low}, and learn the three weights w on a set of training queries, and test the performance on holdout queries. The classification formula, as explained at the end of Section 2.4, is P(Cj|q) = d w(s(d, q))P(Cj|d). Method B requires a training/testing split. Neither voting nor method A requires such a split; however, for consistency, we randomly draw 50-50 training/testing splits for ten times, and report the mean performance \u00b1 standard deviation on the test-split for all three methods. For this experiment, instead of precision and recall, we use DCG-k (k = 1, 5), popular in search engine evaluation. The DCG (discounted cumulated gain) metric, described in , is a ranking measure where the system is asked to rank a set of candidates (in Number of results Precision F1 baseline 0.534 0.696 10 0.706 0.827 20 0.751 0.857 30 0.796 0.886 40 0.807 0.893 50 0.798 0.887 Table 2: Varying the number of search results our case, judged categories for each query), and computes for each query q: DCGk(q) = k i=1 g(Ci(q))/ log2(i + 1), where Ci(q) is the i-th category for query q ranked by the system, and g(Ci) is the grade of Ci: we assign grade of 10, 5, 1, 0 to the 4-point judgment scale described earlier to compute DCG. The decaying choice of log2(i + 1) is conventional, which does not have particular importance. The overall DCG of a system is the averaged DCG over queries. We use this metric instead of precision/recall in this experiment because it can directly handle multi-grade output. Therefore as a single metric, it is convenient for comparing the methods. Note that precision/recall curves used in the earlier sections yield some additional insights not immediately apparent from the DCG numbers. Set 1 Method DCG-1 DCG-5 Oracle 7.58 \u00b1 0.19 14.52 \u00b1 0.40 Voting 5.28 \u00b1 0.15 11.80 \u00b1 0.31 Method A 5.48 \u00b1 0.16 12.22 \u00b1 0.34 Method B 5.36 \u00b1 0.18 12.15 \u00b1 0.35 Set 2 Method DCG-1 DCG-5 Oracle 5.69 \u00b1 0.18 9.94 \u00b1 0.32 Voting 3.50 \u00b1 0.17 7.80 \u00b1 0.28 Method A 3.63 \u00b1 0.23 8.11 \u00b1 0.33 Method B 3.55 \u00b1 0.18 7.99 \u00b1 0.31 Table 3: Voting and alternative methods Results from our experiments are given in Table 3. The oracle method is the best ranking of categories for each query after seeing human judgments. It cannot be achieved by any realistic algorithm, but is included here as an absolute upper bound on DCG performance. The simple voting method performs very well in our experiments. The more complicated methods may lead to moderate performance gain (especially method A, which uses discriminative training in Section 2.5). However, both methods are computationally more costly, and the potential gain is minor enough to be neglected. This means that as a simple method, voting is quite effective. We can observe that method B, which uses quality score returned by a search engine to adjust importance weights of returned pages for a query, does not yield appreciable improvement. This implies that putting equal weights (voting) performs similarly as putting higher weights to higher quality documents and lower weights to lower quality documents (method B), at least for the top search results. It may be possible to improve this method by including other page-features that can differentiate top-ranked search results. However, the effectiveness will require further investigation which we did not test. We may also observe that the performance on Set 2 is lower than that on Set 1, which means queries in Set 2 are harder than those in Set 1. 3.7 Failure analysis We scrutinized the cases when external knowledge did not improve query classification, and identified three main causes for such lack of improvement. (1)Queries containing random strings, such as telephone numbers - these queries do not yield coherent search results, and so the latter cannot help classification (around 5% of queries were of this kind). (2) Queries that yield no search results at all; there were 8% such queries in Set 1 and 15% in Set 2. (3) Queries corresponding to recent events, for which the search engine did not yet have ample coverage (around 5% of queries). One notable example of such queries are entire names of news articles-if the exact article has not yet been indexed by the search engine, search results are likely to be of little use. 4. RELATED WORK Even though the average length of search queries is steadily increasing over time, a typical query is still shorter than 3 words. Consequently, many researchers studied possible ways to enhance queries with additional information. One important direction in enhancing queries is through query expansion. This can be done either using electronic dictionaries and thesauri , or via relevance feedback techniques that make use of a few top-scoring search results. Early work in information retrieval concentrated on manually reviewing the returned results . However, the sheer volume of queries nowadays does not lend itself to manual supervision, and hence subsequent works focused on blind relevance feedback, which basically assumes top returned results to be relevant . More recently, studies in query augmentation focused on classification of queries, assuming such classifications to be beneficial for more focused query interpretation. Indeed, Kowalczyk et al. found that using query classes improved the performance of document retrieval. Studies in the field pursue different approaches for obtaining additional information about the queries. Beitzel et al. used semi-supervised learning as well as unlabeled data . Gravano et al. classified queries with respect to geographic locality in order to determine whether their intent is local or global. KDD Cup on web query classification inspired yet another line of research, which focused on enriching queries using Web search engines and directories . The KDD task specification provided a small taxonomy (67 nodes) along with a set of labeled queries, and posed a challenge to use this training data to build a query classifier. Several teams used the Web to enrich the queries and provide more context for classification. The main research questions of this approach the are (1) how to build a document classifier, (2) how to translate its classifications into the target taxonomy, and (3) how to determine the query class based on document classifications. The winning solution of the KDD Cup proposed using an ensemble of classifiers in conjunction with searching multiple search engines. To address issue (1) above, their solution used the Open Directory Project (ODP) to produce an ODP-based document classifier. The ODP hierarchy was then mapped into the target taxonomy using word matches at individual nodes. A document classifier was built for the target taxonomy by using the pages in the ODP taxonomy that appear in the nodes mapped to the particular target node. Thus, Web documents were first classified with respect to the ODP hierarchy, and their classifications were subsequently mapped to the target taxonomy for query classification. Compared to this approach, we solved the problem of document classification directly in the target taxonomy by using the queries to produce document classifier as described in Section 2. This simplifies the process and removes the need for mapping between taxonomies. This also streamlines taxonomy maintenance and development. Using this approach, we were able to achieve good performance in a very large scale taxonomy. We also evaluated a few alternatives how to combine individual document classifications when actually classifying the query. In a follow-up paper , Shen et al. proposed a framework for query classification based on bridging between two taxonomies. In this approach, the problem of not having a document classifier for web results is solved by using a training set available for documents with a different taxonomy. For this, an intermediate taxonomy with a training set (ODP) is used. Then several schemes are tried that establish a correspondence between the taxonomies or allow for mapping of the training set from the intermediate taxonomy to the target taxonomy. As opposed to this, we built a document classifier for the target taxonomy directly, without using documents from an intermediate taxonomy. While we were not able to directly compare the results due to the use of different taxonomies (we used a much larger taxonomy), our precision and recall results are consistently higher even over the hardest query set. 5. CONCLUSIONS Query classification is an important information retrieval task. Accurate classification of search queries is likely to benefit a number of higher-level tasks such as Web search and ad matching. Since search queries are usually short, by themselves they usually carry insufficient information for adequate classification accuracy. To address this problem, we proposed a methodology for using search results as a source of external knowledge. To this end, we send the query to a search engine, and assume that a plurality of the highestranking search results are relevant to the query. Classifying these results then allows us to classify the original query with substantially higher accuracy. The results of our empirical evaluation definitively confirmed that using the Web as a repository of world knowledge contributes valuable information about the query, and aids in its correct classification. Notably, our method exhibits significantly higher accuracy than methods described in prior studies3 Compared to prior studies, our approach does not require any auxiliary taxonomy, and we produce a query classifier directly for the target taxonomy. Furthermore, the taxonomy used in this study is approximately 2 orders of magnitude larger than that used in prior works. We also experimented with different values of parameters that characterize our method. When using search results, one can either use only summaries of the results provided by Since the field of query classification does not yet have established and agreed upon benchmarks, direct comparison of results is admittedly tricky. the search engine, or actually crawl the results pages for even deeper knowledge. Overall, query classification performance was the best when using the full crawled pages (Table 1). These results are consistent with prior studies , which found that using full crawled pages is superior for document classification than using only brief summaries. Our findings, however, are different from those reported by Shen et al. , who found summaries to yield better results. We attribute our observations to using a more elaborate voting scheme among the classifications of individual search results, as well as to using a more difficult set of rare queries. In this study we used two major search engines, A and B. Interestingly, we found notable distinctions in the quality of their output. Notably, for engine A the overall results were better when using the full crawled pages of the search results, while for engine B it seems to be more beneficial to use the summaries of results. This implies that while the quality of search results returned by engine A is apparently better, engine B does a better work in summarizing the pages. We also found that the best results were obtained by using full crawled pages and performing voting among their individual classifications. For a classifier that is external to the search engine, retrieving full pages may be prohibitively costly, in which case one might prefer to use summaries to gain computational efficiency. On the other hand, for the owners of a search engine, full page classification is much more efficient, since it is easy to preprocess all indexed pages by classifying them once onto the (fixed) taxonomy. Then, page classifications are obtained as part of the meta-data associated with each search result, and query classification can be nearly instantaneous. When using summaries it appears that better results are obtained by first concatenating individual summaries into a meta-document, and then using its classification as a whole. We believe the reason for this observation is that summaries are short and inherently noisier, and hence their aggregation helps to correctly identify the main theme. Consistent with our intuition, using too few search results yields useful but insufficient knowledge, and using too many search results leads to inclusion of marginally relevant Web pages. The best results were obtained when using 40 top search hits. In this work, we first classify search results, and then use their classifications directly to classify the original query. Alternatively, one can use the classifications of search results as features in order to learn a second-level classifier. In Section 3.6, we did some preliminary experiments in this direction, and found that learning such a secondary classifier did not yield considerably advantages. We plan to further investigate this direction in our future work. It is also essential to note that implementing our methodology incurs little overhead. If the search engine classifies crawled pages during indexing, then at query time we only need to fetch these classifications and do the voting. To conclude, we believe our methodology for using Web search results holds considerable promise for substantially improving the accuracy of Web search queries. This is particularly important for rare queries, for which little perquery learning can be done, and in this study we proved that such scarceness of information could be addressed by leveraging the knowledge found on the Web. We believe our findings will have immediate applications to improving the handling of rare queries, both for improving the search results as well as yielding better matched advertisements. In our further research we also plan to make use of session information in order to leverage knowledge about previous queries to better classify subsequent ones.", "body1": "In its 12 year lifetime, web search had grown tremendously: it has simultaneously become a factor in the daily life of maybe a billion people and at the same time an eight billion dollar industry fueled by web advertising. Therefore, using additional external knowledge to augment the queries can go a long way in improving the search results and the user experience. At the same time, better understanding of query meaning has the potential of boosting the economic underpinning of Web search, namely, online advertising, via the sponsored search mechanism that places relevant advertisements alongside search results. In this study we present a methodology for query classification, nodes. Given such classifications, one can directly use them to provide better search results as well as more focused ads. Search engines index colossal amounts of information, and as such can be viewed as very comprehensive repositories of knowledge. Note that in a practical implementation of our methodology within a commercial search engine, all indexed pages can be pre-classified using the normal text-processing and indexing pipeline. Another important aspect of our work lies in the choice of queries. Early studies in query interpretation focused on query augmentation through external dictionaries . The taxonomy used in this work is two orders of magnitude larger than that used in prior studies. Our methodology has two main phases. 2. nodes used in a major US search engine (see Section 3.1). Given a taxonomy of this size, the computational efficiency of classification is a major issue. 2.2 Query classification by search Having developed a document classifier for the query taxonomy, we now turn to the problem of obtaining a classification for a given query based on the initial search results it yields. Query classification is determined by first evaluating conditional probabilities of all possible classes P(Cj|q), and then selecting the alternative with the highest probability Cmax = arg maxCj \u2208C P(Cj|q). We assume that P(q|Cj, d) \u2248 P(q|d), that is, a probability of a query given a document can be determined without knowing the class of the query. 2.3 Classification-based relevance model In order to describe a formal relationship of classification and ad placement (or search), we consider a model for using classification to determine ads (or search) relevance. This relevance function is an adaptation of the traditional word-based retrieval rules. If we take s(Cj, a) = P(Cj|a), s(Cj, q) = P(Cj|q), and w(Cj) = 1/P(Cj), and assume that q and a are independently generated given a hidden concept C, then we have RC (a, q) = Cj \u2208C P(Cj|a)P(Cj|q)/P(Cj) Cj \u2208C P(Cj|a)P(q|Cj)/P(q) = P(q|a)/P(q). That is, the ads are ranked according to P(q|a). It should be mentioned that in our case, each query and ad can have multiple categories. Equation (2) is the ad relevance model that we consider in this paper, with unknown parameters P(Cj|q) for each query q. Moreover, it is possible that the parameters estimated may be of the form g(P(Cj|q)) for some monotone function g(\u00b7) of the actually conditional probability g(P(Cj|q)). 2.4 The voting method We would like to compute P(Cj|q) so that RC (di(q), q) are high for i = 1, . Since both P(Cj|di(q)) and P(Cj|q) belong to , we may just take \u03bb = 0.5 to align the scale. For example, it may be a function of r(d, q) where r(d, q) is the relevance score returned by the underlying search engine. 2.5 Discriminative classification We can treat the problem of estimating P(Cj|q) as a classification problem, where for each q, we label di(q) for i = 1, . In this setting, the classification scoring rule for a document di(q) is linear. In this section, we evaluate our methodology that uses Web search results for improving query classification. 3.1 Taxonomy Our choice of taxonomy was guided by a Web advertising application. Therefore, nodes, arranged in a hierarchy with median depth 5 and maximum depth 9. 3.2 Digression: the basics of sponsored search To discuss our set of evaluation queries, we need a brief introduction to some basic concepts of Web advertising. Sponsored search (or paid search) advertising is placing textual ads on the result pages of web search engines, with ads being driven by the originating query. However, many searches do not explicitly use phrases that someone bids on. As advertisers tend to cover high-volume and high-revenue queries, broad-match queries fall into the tail of the distribution with respect to both volume and revenue. 3. queries. The first set of queries can be matched to at least one ad using broad match as described above. The queries in the two sets differ in their classification difficulty. 3.4 Methodology and evaluation metrics The two sets of queries were classified into the target taxonomy using the techniques presented in section 2. We used standard evaluation metrics: precision, recall and F1. 3.5 Results We compared our method to a baseline query classifier that does not use any external knowledge. In what follows, we start with the general assessment of the effect of using Web search results. 3.5.1 The effect of external knowledge Queries by themselves are very short and difficult to classify. Engine Context Prec. 0.4 0.5 0.6 0.7 0.8 0.9 1.00.90.80.70.60.50.40.30.20.1 Precision Recall Baseline Bundled full-page Voting full-page Bundled summary Voting summary Figure 3: Voting vs. Bundling 3.5.3 Full page text vs. summary To summarize the two preceding sections, background knowledge for each query is obtained by using either the full-page text or only the summaries of the top search results. 3.5.4 Varying the number of classes per search result We also varied the number of classifications per search result, i.e., each result was permitted to have either 1, 3, or 5 classes. 3.5.5 Varying the number of search results obtained We also experimented with different numbers of search results per query. 3.6 Voting versus alternative methods As explained in Section 2.2, one may use several methods to classify queries from search engine results based on our relevance model. The classification formula, as explained at the end of Section 2.4, is P(Cj|q) = d w(s(d, q))P(Cj|d). Method B requires a training/testing split. Therefore as a single metric, it is convenient for comparing the methods. Set 1 Method DCG-1 DCG-5 Oracle 7.58 \u00b1 0.19 14.52 \u00b1 0.40 Voting 5.28 \u00b1 0.15 11.80 \u00b1 0.31 Method A 5.48 \u00b1 0.16 12.22 \u00b1 0.34 Method B 5.36 \u00b1 0.18 12.15 \u00b1 0.35 Set 2 Method DCG-1 DCG-5 Oracle 5.69 \u00b1 0.18 9.94 \u00b1 0.32 Voting 3.50 \u00b1 0.17 7.80 \u00b1 0.28 Method A 3.63 \u00b1 0.23 8.11 \u00b1 0.33 Method B 3.55 \u00b1 0.18 7.99 \u00b1 0.31 Table 3: Voting and alternative methods Results from our experiments are given in Table 3. We can observe that method B, which uses quality score returned by a search engine to adjust importance weights of returned pages for a query, does not yield appreciable improvement. (2) Queries that yield no search results at all; there were 8% such queries in Set 1 and 15% in Set 2. Even though the average length of search queries is steadily increasing over time, a typical query is still shorter than 3 words. Early work in information retrieval concentrated on manually reviewing the returned results . More recently, studies in query augmentation focused on classification of queries, assuming such classifications to be beneficial for more focused query interpretation. Studies in the field pursue different approaches for obtaining additional information about the queries. KDD Cup on web query classification inspired yet another line of research, which focused on enriching queries using Web search engines and directories . The winning solution of the KDD Cup proposed using an ensemble of classifiers in conjunction with searching multiple search engines. Compared to this approach, we solved the problem of document classification directly in the target taxonomy by using the queries to produce document classifier as described in Section 2. In a follow-up paper , Shen et al.", "body2": "omniscient. Therefore, using additional external knowledge to augment the queries can go a long way in improving the search results and the user experience. For instance, is about laptops can obviously lead to more focused advertisements even if no advertiser has specifically bidden on these particular queries. In this study we present a methodology for query classification, nodes. For instance, in the example above, SD450 brings pages about Canon cameras, brings pages about Compaq laptops, hence to a human the intent is quite clear. Our empirical evaluation confirms that using Web search results in this manner yields substantial improvements in the accuracy of query classification. This additional overhead is minimal, and therefore the use of search results to improve query classification is entirely feasible in run-time. Consequently, in this work we focus on the classification of rare queries, whose correct classification is likely to be particularly beneficial. First, we build the query classifier directly for the target taxonomy, instead of using a secondary auxiliary structure; this greatly simplifies taxonomy maintenance and development. This result is in contrast with prior findings in query classification , but is supported by research in mainstream text classification . In the second phase, we develop a query classifier that invokes the document classifier on search results, and uses the latter to perform query classification. Human editors populated the taxonomy nodes with labeled examples that we used as training instances to learn a document classifier in phase 1. A recent study showed centroid-based classifiers to be both effective and efficient for large-scale taxonomies and consequently, we used a centroid classifier in this work. The search engine can then be represented by a function f = similarity(q, d) that quantifies the affinity between a query q and a document d. Examples of such affinity scores used in this paper are rank-the rank of the document in the ordered list of search results; static score-the score of the goodness of the page regardless of the query (e.g., PageRank); and dynamic score-the closeness of the query and the document. We use the following formula that incorporates classifications of individual search results: P(Cj|q) = d\u2208D P(Cj|q, d)\u00b7 P(d|q) = d\u2208D P(q|Cj, d) P(q|d) \u00b7 P(Cj|d)\u00b7 P(d|q). This issue is further explored in the next section. (1) The right hand-side expresses how we use the classification scheme C to rank ads, where s(c, a) is a scoring function that specifies how likely a is in class c, and s(c, q) is a scoring function that specifies how likely q is in class c. The value w(c) is a weighting term for category c, indicating the importance of category c in the relevance formula. With such choices, the method given by (1) becomes the standard TFIDF retrieval rule. If we take s(Cj, a) = P(Cj|a), s(Cj, q) = P(Cj|q), and w(Cj) = 1/P(Cj), and assume that q and a are independently generated given a hidden concept C, then we have RC (a, q) = Cj \u2208C P(Cj|a)P(Cj|q)/P(Cj) Cj \u2208C P(Cj|a)P(q|Cj)/P(q) = P(q|a)/P(q). For this query generation process, the ads can be ranked based on how likely the observed query is generated from each ad. Thus, we only need to obtain estimates of P(Cj|q) for each query q. , K. It is worth mentioning that using this method we can only compute relative strength of P(Cj|q), but not the scale, because scale does not affect ranking. In what follows, we consider two methods to compute the classification information P(Cj|q). The optimal solution is P(Cj|q) = 2\u03bb i=1 wiP(Cj|di(q)). In this general formulation, w(d, q) may depend on factors other than the rank of d in the search engine results for q. Moreover, if we are given a set of hand-labeled training category/query pairs (C, q), then both the weights w(d, q) and the transformation g(\u00b7) can be learned using standard classification techniques. That is, we assign label yi(q) = 1 for di(q) when i \u2264 K, and label yi(q) = \u22121 for di(q) when i > K. In this paper, we consider estimating w using logistic regression as follows: P(\u00b7|q) = arg minw i ln(1 + e\u2212w\u00b7xi(q)yi(q) ). In this section, we evaluate our methodology that uses Web search results for improving query classification. To avoid such situations, the taxonomy needs to provide sufficient discrimination between common commercial topics. 150 queries per node), which were used as a training set; a small fraction of queries have been assigned to more than one category. 3.2 Digression: the basics of sponsored search To discuss our set of evaluation queries, we need a brief introduction to some basic concepts of Web advertising. A discussion of bidding and placement mechanisms is beyond the scope of this paper . These transformations are based on rules and dictionaries. As advertisers tend to cover high-volume and high-revenue queries, broad-match queries fall into the tail of the distribution with respect to both volume and revenue. Both sets contain queries that cannot be directly matched to advertisements, that is, none of the queries contains a bid phrase (this means we eliminated practically all popular queries). As a measure of query rarity, we estimated their frequency in a month worth of query logs for a major US search engine; the median frequency was 1 for queries in Set 1 and 0 for queries in Set 2. Only 8% queries in Set 1 and 14% in Set 2 contained quotation marks. To compute evaluation metrics, we treated classifications with ratings 1 and 2 to be correct, and those with ratings 3 and 4 to be incorrect. Owing to the lack of space, we only show graphs for query Set 1; however, we show the numerical results for both sets in the tables. In our experiments, we varied values of pertinent parameters that characterize the exact way of using search results. For lack of space, we only show graphs for Set 1 queries and omit the graphs for Set 2 queries, which exhibit similar phenomena. Interestingly, we found that search engine A performs consistently better with full-page text, while search engine B performs better when summaries are used. This is because summaries by themselves are too short to be classified correctly individually, but when bundled together they are much more stable. We attribute this distinction to the fact that the queries we used in this study are tail ones, which are rare and difficult to classify. Thus, as we increase the number of classes per result, we observe higher stability in query classification. We found the results to be highly significant (p <), thus confirming the value of external knowledge for query classification. We discretize the quality score s(d, q) of a query/document pair into {high, medium, low}, and learn the three weights w on a set of training queries, and test the performance on holdout queries. The classification formula, as explained at the end of Section 2.4, is P(Cj|q) = d w(s(d, q))P(Cj|d). We use this metric instead of precision/recall in this experiment because it can directly handle multi-grade output. Note that precision/recall curves used in the earlier sections yield some additional insights not immediately apparent from the DCG numbers. This means that as a simple method, voting is quite effective. (1)Queries containing random strings, such as telephone numbers - these queries do not yield coherent search results, and so the latter cannot help classification (around 5% of queries were of this kind). One notable example of such queries are entire names of news articles-if the exact article has not yet been indexed by the search engine, search results are likely to be of little use. This can be done either using electronic dictionaries and thesauri , or via relevance feedback techniques that make use of a few top-scoring search results. However, the sheer volume of queries nowadays does not lend itself to manual supervision, and hence subsequent works focused on blind relevance feedback, which basically assumes top returned results to be relevant . found that using query classes improved the performance of document retrieval. classified queries with respect to geographic locality in order to determine whether their intent is local or global. The main research questions of this approach the are (1) how to build a document classifier, (2) how to translate its classifications into the target taxonomy, and (3) how to determine the query class based on document classifications. Thus, Web documents were first classified with respect to the ODP hierarchy, and their classifications were subsequently mapped to the target taxonomy for query classification. We also evaluated a few alternatives how to combine individual document classifications when actually classifying the query. While we were not able to directly compare the results due to the use of different taxonomies (we used a much larger taxonomy), our precision and recall results are consistently higher even over the hardest query set.", "introduction": "In its 12 year lifetime, web search had grown tremendously: it has simultaneously become a factor in the daily life of maybe a billion people and at the same time an eight billion dollar industry fueled by web advertising. One thing, however, has remained constant: people use very short queries. Various studies estimate the average length of a search query between 2.4 and 2.7 words, which by all accounts can carry only a small amount of information. Commercial search engines do a remarkably good job in interpreting these short strings, but they are not (yet!) Therefore, using additional external knowledge to augment the queries can go a long way in improving the search results and the user experience. At the same time, better understanding of query meaning has the potential of boosting the economic underpinning of Web search, namely, online advertising, via the sponsored search mechanism that places relevant advertisements alongside search results. For instance, is about laptops can obviously lead to more focused advertisements even if no advertiser has specifically bidden on these particular queries. In this study we present a methodology for query classification, nodes. Given such classifications, one can directly use them to provide better search results as well as more focused ads. The problem of query classification is extremely difficult owing to the brevity of queries. Observe, however, that in many cases a human looking at a search query and the search query results does remarkably well in making sense of it. Of course, the sheer volume of search queries does not lend itself to human supervision, and therefore we need alternate sources of knowledge about the world. For instance, in the example above, SD450 brings pages about Canon cameras, brings pages about Compaq laptops, hence to a human the intent is quite clear. Search engines index colossal amounts of information, and as such can be viewed as very comprehensive repositories of knowledge. Following the heuristic described above, we propose to use the search results themselves to gain additional insights for query interpretation. To this end, we employ the pseudo relevance feedback paradigm, and assume the top search results to be relevant to the query. Certainly, not all results are equally relevant, and thus we use elaborate voting schemes in order to obtain reliable knowledge about the query. For the purpose of this study we first dispatch the given query to a general web search engine, and collect a number of the highest-scoring URLs. We crawl the Web pages pointed by these URLs, and classify these pages. Finally, we use these result-page classifications to classify the original query. Our empirical evaluation confirms that using Web search results in this manner yields substantial improvements in the accuracy of query classification. Note that in a practical implementation of our methodology within a commercial search engine, all indexed pages can be pre-classified using the normal text-processing and indexing pipeline. Thus, at run-time we only need to run the voting procedure, without doing any crawling or classification. This additional overhead is minimal, and therefore the use of search results to improve query classification is entirely feasible in run-time. Another important aspect of our work lies in the choice of queries. The volume of queries in today\"s search engines follows the familiar power law, where a few queries appear very often while most queries appear only a few times. While individual queries in this long tail are rare, together they account for a considerable mass of all searches. Furthermore, the aggregate volume of such queries provides a substantial opportunity for income through on-line advertising.1 Searching and advertising platforms can be trained to yield good results for frequent queries, including auxiliary data such as maps, shortcuts to related structured information, successful ads, and so on. However, the tail queries simply do not have enough occurrences to allow statistical learning on a per-query basis. Therefore, we need to aggregate such queries in some way, and to reason at the level of aggregated query clusters. A natural choice for such aggregation is to classify the queries into a topical taxonomy. Knowing which taxonomy nodes are most relevant to the given query will aid us to provide the same type of support for rare queries as for frequent queries. Consequently, in this work we focus on the classification of rare queries, whose correct classification is likely to be particularly beneficial. Early studies in query interpretation focused on query augmentation through external dictionaries . More recent studies also attempted to gather some additional knowledge from the Web. However, these studies had a number of shortcomings, which we overcome in this paper. Specifically, earlier works in the field used very small query classification taxonomies of only a few dozens of nodes, which do not allow ample specificity for online advertising . They also used a separate ancillary taxonomy for Web documents, so that an extra level of indirection had to be employed to establish the correspondence between the ancillary and the main taxonomies . The main contributions of this paper are as follows. First, we build the query classifier directly for the target taxonomy, instead of using a secondary auxiliary structure; this greatly simplifies taxonomy maintenance and development. The taxonomy used in this work is two orders of magnitude larger than that used in prior studies. The empirical evaluation demonstrates that our methodology for using external knowledge achieves greater improvements than those previously reported. Since our taxonomy is considerably larger, the classification problem we face is much more difficult, making the improvements we achieve particularly notable. We also report the results of a thorough empirical study of different voting schemes and different depths of knowledge (e.g., using search summaries vs. entire crawled pages). We found that crawling the search results yields deeper knowledge and leads to greater improvements than mere summaries. This result is in contrast with prior findings in query classification , but is supported by research in mainstream text classification .", "conclusion": "Query classification is an important information retrieval task.. Accurate classification of search queries is likely to benefit a number of higher-level tasks such as Web search and ad matching.. Since search queries are usually short, by themselves they usually carry insufficient information for adequate classification accuracy.. To address this problem, we proposed a methodology for using search results as a source of external knowledge.. To this end, we send the query to a search engine, and assume that a plurality of the highestranking search results are relevant to the query.. Classifying these results then allows us to classify the original query with substantially higher accuracy.. The results of our empirical evaluation definitively confirmed that using the Web as a repository of world knowledge contributes valuable information about the query, and aids in its correct classification.. Notably, our method exhibits significantly higher accuracy than methods described in prior studies3 Compared to prior studies, our approach does not require any auxiliary taxonomy, and we produce a query classifier directly for the target taxonomy.. Furthermore, the taxonomy used in this study is approximately 2 orders of magnitude larger than that used in prior works.. We also experimented with different values of parameters that characterize our method.. When using search results, one can either use only summaries of the results provided by Since the field of query classification does not yet have established and agreed upon benchmarks, direct comparison of results is admittedly tricky.. the search engine, or actually crawl the results pages for even deeper knowledge.. Overall, query classification performance was the best when using the full crawled pages (Table 1).. These results are consistent with prior studies , which found that using full crawled pages is superior for document classification than using only brief summaries.. Our findings, however, are different from those reported by Shen et al.. , who found summaries to yield better results.. We attribute our observations to using a more elaborate voting scheme among the classifications of individual search results, as well as to using a more difficult set of rare queries.. In this study we used two major search engines, A and B. Interestingly, we found notable distinctions in the quality of their output.. Notably, for engine A the overall results were better when using the full crawled pages of the search results, while for engine B it seems to be more beneficial to use the summaries of results.. This implies that while the quality of search results returned by engine A is apparently better, engine B does a better work in summarizing the pages.. We also found that the best results were obtained by using full crawled pages and performing voting among their individual classifications.. For a classifier that is external to the search engine, retrieving full pages may be prohibitively costly, in which case one might prefer to use summaries to gain computational efficiency.. On the other hand, for the owners of a search engine, full page classification is much more efficient, since it is easy to preprocess all indexed pages by classifying them once onto the (fixed) taxonomy.. Then, page classifications are obtained as part of the meta-data associated with each search result, and query classification can be nearly instantaneous.. When using summaries it appears that better results are obtained by first concatenating individual summaries into a meta-document, and then using its classification as a whole.. We believe the reason for this observation is that summaries are short and inherently noisier, and hence their aggregation helps to correctly identify the main theme.. Consistent with our intuition, using too few search results yields useful but insufficient knowledge, and using too many search results leads to inclusion of marginally relevant Web pages.. The best results were obtained when using 40 top search hits.. In this work, we first classify search results, and then use their classifications directly to classify the original query.. Alternatively, one can use the classifications of search results as features in order to learn a second-level classifier.. In Section 3.6, we did some preliminary experiments in this direction, and found that learning such a secondary classifier did not yield considerably advantages.. We plan to further investigate this direction in our future work.. It is also essential to note that implementing our methodology incurs little overhead.. If the search engine classifies crawled pages during indexing, then at query time we only need to fetch these classifications and do the voting.. To conclude, we believe our methodology for using Web search results holds considerable promise for substantially improving the accuracy of Web search queries.. This is particularly important for rare queries, for which little perquery learning can be done, and in this study we proved that such scarceness of information could be addressed by leveraging the knowledge found on the Web.. We believe our findings will have immediate applications to improving the handling of rare queries, both for improving the search results as well as yielding better matched advertisements.. In our further research we also plan to make use of session information in order to leverage knowledge about previous queries to better classify subsequent ones."}
{"id": "I-74", "keywords": ["argument", "dialogu", "relev"], "title": "On the relevance of utterances in formal inter-agent dialogues", "abstract": "Work on argumentation-based dialogue has defined frameworks within which dialogues can be carried out, established protocols that govern dialogues, and studied different properties of dialogues. This work has established the space in which agents are permitted to interact through dialogues. Recently, there has been increasing interest in the mechanisms agents might use to choose how to act \u2014 the rhetorical maneuvering that they use to navigate through the space defined by the rules of the dialogue. Key in such considerations is the idea of relevance, since a usual requirement is that agents stay focussed on the subject of the dialogue and only make relevant remarks. Here we study several notions of relevance, showing how they can be related to both the rules for carrying out dialogues and to rhetorical maneuvering.", "references": ["On the acceptability of arguments in preference-based argumentation framework", "Arguments, dialogue, and negotiation", "Strategic and tactic reasoning for communicating agents", "A logic-based theory of deductive arguments", "Handling controversial arguments in bipolar argumentation frameworks", "Trends in agent communication language", "Agent theory for team formation by dialogue", "On the acceptability of arguments and its fundamental role in nonmonotonic reasoning, logic programming and n-person games", "Dialectic proof procedures for assumption-based, admissable argumentation", "To commit or not to commit", "More on non-cooperation in Dialogue Logic", "Non-cooperation in Dialogue Logic", "Mathematical models of dialogue", "Reaching agreements through argumentation: a logical model and implementation", "Loose lips sink ships: A heuristic for argumentation", "Negotiation through argumentation - a preliminary report", "An analysis of formal inter-agent dialogues", "On the outcomes of formal inter-agent dialogues", "On dialogue systems with speech acts, arguments, and counterarguments", "Relating protocols for dynamic dispute with logics for defeasible argumentation", "Modelling reasoning with precedents in a formal dialogue game", "Towards a theory of negotiation strategy", "Dialogue frames in agent communications", "Adaptive strategies for practical argument-based negotiation", "Ultima ratio: should Hamlet kill Claudius", "Argumentation: Planning other agents\" plans", "Commitment in Dialogue: Basic Concepts of Interpersonal Reasoning"], "full_text": "1. INTRODUCTION Finding ways for agents to reach agreements in multiagent systems is an area of active research. One mechanism for achieving agreement is through the use of argumentation - where one agent tries to convince another agent of something during the course of some dialogue. Early examples of argumentation-based approaches to multiagent agreement include the work of Dignum et al. , Kraus , Parsons and Jennings , Reed , Schroeder et al. and Sycara . The work of Walton and Krabbe , popularised in the multiagent systems community by Reed , has been particularly influential in the field of argumentation-based dialogue. This work influenced the field in a number of ways, perhaps most deeply in framing multi-agent interactions as dialogue games in the tradition of Hamblin . Viewing dialogues in this way, as in , provides a powerful framework for analysing the formal properties of dialogues, and for identifying suitable protocols under which dialogues can be conducted . The dialogue game view overlaps with work on conversation policies (see, for example, ), but differs in considering the entire dialogue rather than dialogue segments. In this paper, we extend the work of by considering the role of relevance - the relationship between utterances in a dialogue. Relevance is a topic of increasing interest in argumentation-based dialogue because it relates to the scope that an agent has for applying strategic manoeuvering to obtain the outcomes that it requires . Our work identifes the limits on such rhetorical manoeuvering, showing when it can and cannot have an effect. 2. BACKGROUND We begin by introducing the formal system of argumentation that underpins our approach, as well as the corresponding terminology and notation, all taken from . A dialogue is a sequence of messages passed between two or more members of a set of agents A. An agent \u03b1 maintains a knowledge base, \u03a3\u03b1, containing formulas of a propositional language L and having no deductive closure. Agent \u03b1 also maintains the set of its past utterances, called the commitment store, CS\u03b1. We refer to this as an agent\"s public knowledge, since it contains information that is shared with other agents. In contrast, the contents of \u03a3\u03b1 are private to \u03b1. Note that in the description that follows, we assume that is the classical inference relation, that \u2261 stands for logical equivalence, and we use \u0394 to denote all the information available to an agent. Thus in a dialogue between two agents \u03b1 and \u03b2, \u0394\u03b1 = \u03a3\u03b1 \u222a CS\u03b1 \u222a CS\u03b2, so the commitment store CS\u03b1 can be loosely thought of as a subset of \u0394\u03b1 consisting of the assertions that have been made public. In some dialogue games, such as those in anything in CS\u03b1 is either in \u03a3\u03b1 or can be derived from it. In other dialogue games, 978-81--7-5 (RPS) IFAAMAS those in , CS\u03b1 may contain things that cannot be derived from \u03a3\u03b1. Definition 2.1. An argument A is a pair (S, p) where p is a formula of L and S a subset of \u0394 such that (i) S is consistent; (ii) S p; and (iii) S is minimal, so no proper subset of S satisfying both (1) and (2) exists. S is called the support of A, written S = Support(A) and p is the conclusion of A, written p = Conclusion(A). Thus we talk of p being supported by the argument (S, p). In general, since \u0394 may be inconsistent, arguments in A(\u0394), the set of all arguments which can be made from \u0394, may conflict, and we make this idea precise with the notion of undercutting: Definition 2.2. Let A1 and A2 be arguments in A(\u0394). A1 undercuts A2 iff \u2203\u00acp \u2208 Support(A2) such that p \u2261 Conclusion(A1). In other words, an argument is undercut if and only if there is another argument which has as its conclusion the negation of an element of the support for the first argument. To capture the fact that some beliefs are more strongly held than others, we assume that any set of beliefs has a preference order over it. We consider all information available to an agent, \u0394, to be stratified into non-overlapping subsets \u03941, . . . , \u0394n such that beliefs in \u0394i are all equally preferred and are preferred over elements in \u0394j where i > j. The preference level of a nonempty subset S \u2282 \u0394, where different elements s \u2208 S may belong to different layers \u0394i, is valued at the highest numbered layer which has a member in S and is referred to as level(S). In other words, S is only as strong as its weakest member. Note that the strength of a belief as used in this context is a separate concept from the notion of support discussed earlier. Definition 2.3. Let A1 and A2 be arguments in A(\u0394). A1 is preferred to A2 according to Pref , A1 Pref A2, iff level(Support(A1)) > level(Support(A2)). If A1 is preferred to A2, we say that A1 is stronger than A2. We can now define the argumentation system we will use: Definition 2.4. An argumentation system is a triple: A(\u0394), Undercut, Pref such that: \u2022 A(\u0394) is a set of the arguments built from \u0394, \u2022 Undercut is a binary relation representing the defeat relationship between arguments, Undercut \u2286 A(\u0394) \u00d7 A(\u0394), and \u2022 Pref is a pre-ordering on A(\u0394) \u00d7 A(\u0394). The preference order makes it possible to distinguish different types of relations between arguments: Definition 2.5. Let A1, A2 be two arguments of A(\u0394). \u2022 If A2 undercuts A1 then A1 defends itself against A2 iff A1 Pref A2. Otherwise, A1 does not defend itself. \u2022 A set of arguments A defends A1 iff for every A2 that undercuts A1, where A1 does not defend itself against A2, then there is some A3 \u2208 A such that A3 undercuts A2 and A2 does not defend itself against A3. We write AUndercut,Pref to denote the set of all non-undercut arguments and arguments defending themselves against all their undercutting arguments. The set A(\u0394) of acceptable arguments of the argumentation system A(\u0394), Undercut, Pref is the least fixpoint of a function F: A \u2286 A(\u0394) F(A) = {(S, p) \u2208 A(\u0394) | (S, p) is defended by A} Definition 2.6. The set of acceptable arguments for an argumentation system A(\u0394), Undercut, Pref is recursively defined as: A(\u0394) = Fi\u22650(\u2205) = AUndercut,Pref \u222a h[ Fi\u22651(AUndercut,Pref ) An argument is acceptable if it is a member of the acceptable set, and a proposition is acceptable if it is the conclusion of an acceptable argument. An acceptable argument is one which is, in some sense, proven since all the arguments which might undermine it are themselves undermined. Definition 2.7. If there is an acceptable argument for a proposition p, then the status of p is accepted, while if there is not an acceptable argument for p, the status of p is not accepted. Argument A is said to affect the status of another argument A if changing the status of A will change the status of A . 3. DIALOGUES Systems like those described in , lay down sets of locutions that agents can make to put forward propositions and the arguments that support them, and protocols that define precisely which locutions can be made at which points in the dialogue. We are not concerned with such a level of detail here. Instead we are interested in the interplay between arguments that agents put forth. As a result, we will consider only that agents are allowed to put forward arguments. We do not discuss the detail of the mechanism that is used to put these arguments forward - we just assume that arguments of the form (S, p) are inserted into an agent\"s commitment store where they are then visible to other agents. We then have a typical definition of a dialogue: Definition 3.1. A dialogue D is a sequence of moves: m1, m2, . . . , mn. A given move mi is a pair \u03b1, Ai where Ai is an argument that \u03b1 places into its commitment store CS\u03b1. Moves in an argumentation-based dialogue typically attack moves that have been made previously. While, in general, a dialogue can include moves that undercut several arguments, in the remainder of this paper, we will only consider dialogues that put forward moves that undercut at most one argument. For now we place no additional constraints on the moves that make up a dialogue. Later we will see how different restrictions on moves lead to different kinds of dialogue. The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) The sequence of arguments put forward in the dialogue is determined by the agents who are taking part in the dialogue, but they are usually not completely free to choose what arguments they make. As indicated earlier, their choice is typically limited by a protocol. If we write the sequence of n moves m1, m2, . . . , mn as mn, and denote the empty sequence as m0, then we can define a profocol in the following way: Definition 3.2. A protocol P is a function on a sequence of moves mi in a dialogue D that, for all i \u2265 0, identifies a set of possible moves Mi+1 from which the mi+1th move may be drawn: P : mi \u2192 Mi+1 In other words, for our purposes here, at every point in a dialogue, a protocol determines a set of possible moves that agents may make as part of the dialogue. If a dialogue D always picks its moves m from the set M identified by protocol P, then D is said to conform to P. Even if a dialogue conforms to a protocol, it is typically the case that the agent engaging in the dialogue has to make a choice of move - it has to choose which of the moves in M to make. This excercise of choice is what we refer to as an agent\"s use of rhetoric (in its oratorical sense of influencing the thought and conduct of an audience). Some of our results will give a sense of how much scope an agent has to exercise rhetoric under different protocols. As arguments are placed into commitment stores, and hence become public, agents can determine the relationships between them. In general, after several moves in a dialogue, some arguments will undercut others. We will denote the set of arguments {A1, A2, . . . , Aj} asserted after moves m1, m2, . . . , mj of a dialogue to be Aj - the relationship of the arguments in Aj can be described as an argumentation graph, similar to those described in, for example, : Definition 3.3. An argumentation graph AG over a set of arguments A is a directed graph (V, E) such that every vertex v, v \u2208 V denotes one argument A \u2208 A, every argument A is denoted by one vertex v, and every directed edge e \u2208 E from v to v denotes that v undercuts v . We will use the term argument graph as a synonym for argumentation graph. Note that we do not require that the argumentation graph is connected. In other words the notion of an argumentation graph allows for the representation of arguments that do not relate, by undercutting or being undercut, to any other arguments (we will come back to this point very shortly). We adapt some standard graph theoretic notions in order to describe various aspects of the argumentation graph. If there is an edge e from vertex v to vertex v , then v is said to be the parent of v and v is said to be the child of v. In a reversal of the usual notion, we define a root of an argumentation graph1 as follows: Definition 3.4. A root of an argumentation graph AG = (V, E) is a node v \u2208 V that has no children. Thus a root of a graph is a node to which directed edges may be connected, but from which no directed edges connect to other nodes. Thus a root is a node representing an Note that we talk of a root rather than the root - as defined, an argumentation graph need not be a tree. v v\" Figure 1: An example argument graph argument that is undercut, but which itself does no undercutting. Similarly: Definition 3.5. A leaf of an argumentation graph AG = (V, E) is a node v \u2208 V that has no parents. Thus a leaf in an argumentation graph represents an argument that undercuts another argument, but does no undercutting. Thus in Figure 1, v is a root, and v is a leaf. The reason for the reversal of the usual notions of root and leaf is that, as we shall see, we will consider dialogues to construct argumentation graphs from the roots (in our sense) to the leaves. The reversal of the terminology means that it matches the natural process of tree construction. Since, as described above, argumentation graphs are allowed to be not connected (in the usual graph theory sense), it is helpful to distinguish nodes that are connected to other nodes, in particular to the root of the tree. We say that node v is connected to node v if and only if there is a path from v to v . Since edges represent undercut relations, the notion of connectedness between nodes captures the influence that one argument may have on another: Proposition 3.1. Given an argumentation graph AG, if there is any argument A, denoted by node v that affects the status of another argument A , denoted by v , then v is connected to v . The converse does not hold. Proof. Given Definitions 2.5 and 2.6, the only ways in which A can affect the status of A is if A either undercuts A , or if A undercuts some argument A that undercuts A , or if A undercuts some A that undercuts some A that undercuts A , and so on. In all such cases, a sequence of undercut relations relates the two arguments, and if they are both in an argumentation graph, this means that they are connected. Since the notion of path ignores the direction of the directed arcs, nodes v and v are connected whether the edge between them runs from v to v or vice versa. Since A only undercuts A if the edge runs from v to v , we cannot infer that A will affect the status of A from information about whether or not they are connected. The reason that we need the concept of the argumentation graph is that the properties of the argumentation graph tell us something about the set of arguments A the graph represents. When that set of arguments is constructed through a dialogue, there is a relationship between the structure of the argumentation graph and the protocol that governs the dialogue. It is the extent of the relationship between structure and protocol that is the main subject of this paper. To study this relationship, we need to establish a correspondence between a dialogue and an argumentation graph. Given the definitions we have so far, this is simple: Definition 3.6. A dialogue D, consisting of a sequence of moves mn, and an argument graph AG = (V, E) correspond to one another iff \u2200m \u2208 mn, The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) is advanced at move mi is represented by exactly one node v \u2208 V , and \u2200v \u2208 V , v represents exactly one argument Ai that has been advanced by a move m \u2208 mn. Thus a dialogue corresponds to an argumentation graph if and only if every argument made in the dialogue corresponds to a node in the graph, and every node in the graph corresponds to an argument made in the dialogue. This one-toone correspondence allows us to consider each node v in the graph to have an index i which is the index of the move in the dialogue that put forward the argument which that node represents. Thus we can, for example, refer to the third node in the argumentation graph, meaning the node that represents the argument put forward in the third move of the dialogue. 4. RELEVANCE Most work on dialogues is concerned with what we might call coherent dialogues, that is dialogues in which the participants are, as in the work of Walton and Krabbe , focused on resolving some question through the dialogue2 To capture this coherence, it seems we need a notion of relevance to constrain the statements made by agents. Here we study three notions of relevance: Definition 4.1. Consider a dialogue D, consisting of a sequence of moves mi, with a corresponding argument graph AG. The move mi+1, i > 1, is said to be relevant if one or more of the following hold: R1 Making mi+1 will change the status of the argument denoted by the first node of AG. R2 Making mi+1 will add a node vi+1 that is connected to the first node of AG. R3 Making mi+1 will add a node vi+1 that is connected to the last node to be added to AG. R2-relevance is the form of relevance defined by in their study of strategic and tactical reasoning3 . R1-relevance was suggested by the notion used in , and though it differs somewhat from that suggested there, we believe it captures the essence of its predecessor. Note that we only define relevance for the second move of the dialogue onwards because the first move is taken to identify the subject of the dialogue, that is, the central question that the dialogue is intended to answer, and hence it must be relevant to the dialogue, no matter what it is. In assuming this, we focus our attention on the same kind of dialogues as . We can think of relevance as enforcing a form of parsimony on a dialogue - it prevents agents from making statements that do not bear on the current state of the dialogue. This promotes efficiency, in the sense of limiting the number of moves in the dialogue, and, as in , prevents agents revealing information that they might better keep hidden. Another form of parsimony is to insist that agents are not allowed to put forward arguments that will be undercut by arguments that have already been made during the dialogue. We therefore distinguish such arguments. See for examples of dialogues where this is not the case. We consider such reasoning sub-types of rhetoric. Definition 4.2. Consider a dialogue D, consisting of a sequence of moves mi, with a corresponding argument graph AG. The move mi+1 and the argument it puts forward, Ai+1, are both said to be pre-empted, if Ai+1 is undercut by some A \u2208 Ai. We use the term pre-empted because if such an argument is put forward, it can seem as though another agent anticipated the argument being made, and already made an argument that would render it useless. In the rest of this paper, we will only deal with protocols that permit moves that are relevant, in any of the senses introduced above, and are not allowed to be pre-empted. We call such protocols basic protocols, and dialogues carried out under such protocols basic dialogues. The argument graph of a basic dialogue is somewhat restricted. Proposition 4.1. Consider a basic dialogue D. The argumentation graph AG that corresponds to D is a tree with a single root. Proof. Recall that Definition 3.3 requires only that AG be a directed graph. To show that it is a tree, we have to show that it is acyclic and connected. That the graph is connected follows from the construction of the graph under a protocol that enforces relevance. If the notion of relevance is R3, each move adds a node that is connected to the previous node. If the notion of relevance is R2, then every move adds a node that is connected to the root, and thus is connected to some node in the graph. If the notion of relevance is R1, then every move has to change the status of the argument denoted by the root. Proposition 3.1 tells us that to affect the status of an argument A , the node v representing the argument A that is effecting the change has to be connected to v , the node representing A , and so it follows that every new node added as a result of an R1relevant move will be connected to the argumentation graph. Thus AG is connected. Since a basic dialogue does not allow moves that are preempted, every edge that is added during construction is directed from the node that is added to one already in the graph (thus denoting that the argument A denoted by the added node, v, undercuts the argument A denoted by the node to which the connection is made, v , rather than the other way around). Since every edge that is added is directed from the new node to the rest of the graph, there can be no cycles. Thus AG is a tree. To show that AG has a single root, consider its construction from the initial node. After m1 the graph has one node, v1 that is both a root and a leaf. After m2, the graph is two nodes connected by an edge, and v1 is now a root and not a leaf. v2 is a leaf and not a root. However the third node is added, the argument earlier in this proof demonstrates that there will be a directed edge from it to some other node, making it a leaf. Thus v1 will always be the only root. The ruling out of pre-empted moves means that v1 will never cease to be a root, and so the argumentation graph will always have one root. Since every argumentation graph constructed by a basic dialogue is a tree with a single root, this means that the first node of every argumentation graph is the root. Although these results are straightforward to obtain, they allow us to show how the notions of relevance are related. The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Proposition 4.2. Consider a basic dialogue D, consisting of a sequence of moves mi, with a corresponding argument graph AG. 1. Every move mi+1 that is R1-relevant is R2-relevant. The converse does not hold. 2. Every move mi+1 that is R3-relevant is R2-relevant. The converse does not hold. 3. Not every move mi+1 that is R1-relevant is R3-relevant, and not every move mi+1 that is R3-relevant is R1relevant Proof. For 1, consider how move mi+1 can satisfy R1. Proposition 3.1 tells us that if Ai+1 can change the status of the argument denoted by the root v1 (which, as observed above, is the first node) of AG, then vi+1 must be connected to the root. This is precisely what is required to satisfy R2, and the relatiosnhip is proved to hold. To see that the converse does not hold, we have to consider what it takes to change the status of r (since Proposition 3.1 tells us that connectedness is not enough to ensure a change of status - if it did, R1 and R2 relevance would coincide). For mi+1 to change the status of the root, it will have to (1) make the argument A represented by r either unacceptable, if it were acceptable before the move, or (2) acceptable if it were unacceptable before the move. Given the definition of acceptability, it can achieve (1) either by directly undercutting the argument represented by r, in which case vi+1 will be directly connected to r by some edge, or by undercutting some argument A that is part of the set of non-undercut arguments defending A. In the latter case, vi+1 will be directly connected to the node representing A and by Proposition 4.1 to r. To achieve (2), vi+1 will have to undercut an argument A that is either currently undercutting A, or is undercutting an argument that would otherwise defend A. Now, further consider that mi+1 puts forward an argument Ai+1 that undercuts the argument denoted by some node v , but this latter argument defends itself against Ai+1. In such a case, the set of acceptable arguments will not change, and so the status of Ar will not change. Thus a move that is R2-relevant need not be R1-relevant. For 2, consider that mi+1 can satisfy R3 simply by adding a node that is connected to vi, the last node to be added to AG. By Proposition 4.1, it is connected to r and so is R2-relevant. To see that the converse does not hold, consider that an R2-relevant move can connect to any node in AG. The first part of 3 follows by a similar argument to that we just used - an R1-relevant move does not have to connect to vi, just to some v that is part of the graph - and the second part follows since a move that is R3-relevant may introduce an argument Ai+1 that undercuts the argument Ai put forward by the previous move (and so vi+1 is connected to vi), but finds that Ai defends itself against Ai+1, preventing a change of status at the root. What is most interesting is not so much the results but why they hold, since this reveals some aspects of the interplay between relevance and the structure of argument graphs. For example, to restate a case from the proof of Proposition 4.2, a move that is R3-relevant by definition has to add a node to the argument graph that is connected to the last node that was added. Since a move that is R2-relevant can add a node that connects anywhere on an argument graph, any move that is R3-relevant will be R2-relevant, but the converse does not hold. It turns out that we can exploit the interplay between structure and relevance that Propositions 4.1 and 4.2 have started to illuminate to establish relationships between the protocols that govern dialogues and the argument graphs constructed during such dialogues. To do this we need to define protocols in such a way that they refer to the structure of the graph. We have: Definition 4.3. A protocol is single-path if all dialogues that conform to it construct argument graphs that have only one branch. Proposition 4.3. A basic protocol P is single-path if, for all i, the set of permitted moves Mi at move i are all R3relevant. The converse does not hold. Proof. R3-relevance requires that every node added to the argument graph be connected to the previous node. Starting from the first node this recursively constructs a tree with just one branch, and the relationship holds. The converse does not hold because even if one or more moves in the protocol are R1- or R2-relevant, it may be the case that, because of an agent\"s rhetorical choice or because of its knowledge, every argument that is chosen to be put forward will undercut the previous argument and so the argument graph is a one-branch tree. Looking for more complex kinds of protocol that construct more complex kinds of argument graph, it is an obvious move to turn to: Definition 4.4. A basic protocol is multi-path if all dialogues that conform to it can construct argument graphs that are trees. But, on reflection, since any graph with only one branch is also a tree: Proposition 4.4. Any single-path protocol is an instance of a multi-path protocol. and, furthermore: Proposition 4.5. Any basic protocol P is multi-path. Proof. Immediate from Proposition 4.1 So the notion of a multi-path protocol does not have much traction. As a result we distinguish multi-path protocols that permit dialogues that can construct trees that have more than one branch as bushy protocols. We then have: Proposition 4.6. A basic protocol P is bushy if, for some i, the set of permitted moves Mi at move i are all R1- or R2-relevant. Proof. From Proposition 4.3 we know that if all moves are R3-relevant then we\"ll get a tree with one branch, and from Proposition 4.1 we know that all basic protocols will build an argument graph that is a tree, so providing we exclude R3-relevant moves, we will get protocols that can build multi- The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Of course, since, by Proposition 4.2, any move that is R3relevant is R2-relevant and can quite possibly be R1-relevant (all that Proposition 4.2 tells us is that there is no guarantee that it will be), all that Proposition 4.6 tells us is that dialogues that conform to bushy protocols may have more than one branch. All we can do is to identify a bound on the number of branches: Proposition 4.7. Consider a basic dialogue D that includes m moves that are not R3-relevant, and has a corresponding argumentation graph AG. The number of branches in AG is less than or equal to m + 1. Proof. Since it must connect a node to the last node added to AG, an R3-relevant move can only extend an existing branch. Since they do not have the same restriction, R1 and R2-relevant moves may create a new branch by connecting to a node that is not the last node added. Every such move could create a new branch, and if they do, we will have m branches. If there were R3-relevant moves before any of these new-branch-creating moves, then these m branches are in addition to the initial branch created by the R3-relevant moves, and we have a maximum of m + 1 possible branches. We distinguish bushy protocols from multi-path protocols, and hence R1- and R2-relevance from R3-relevance, because of the kinds of dialogue that R3-relevance enforces. In a dialogue in which all moves must be R3-relevant, the argumentation graph has a single branch - the dialogue consists of a sequence of arguments each of which undercuts the previous one and the last move to be made is the one that settles the dialogue. This, as we will see next, means that such a dialogue only allows a subset of all the moves that would otherwise be possible. 5. COMPLETENESS The above discussion of the difference between dialogues carried out under single-path and bushy protocols brings us to the consideration of what called predeterminism, but we now prefer to describe using the term completeness. The idea of predeterminism, as described in , captures the notion that, under some circumstances, the result of a dialogue can be established without actually having the dialogue - the agents have sufficiently little room for rhetorical manoeuver that were one able to see the contents of all the \u03a3i of all the \u03b1i \u2208 A, one would be able to identify the outcome of any dialogue on a given subject4 . We develop this idea by considering how the argument graphs constructed by dialogues under different protocols compare to benchmark complete dialogues. We start by developing ideas of what complete might mean. One reasonable definition is that: Definition 5.1. A basic dialogue D between the set of agents A with a corresponding argumentation graph AG is topic-complete if no agent can construct an argument A that undercuts any argument A represented by a node in AG. The argumentation graph constructed by a topic-complete dialogue is called a topic-complete argumentation graph and is denoted AG(D)T . Assuming that the \u03a3i do not change during the dialogue, which is the usual assumption in this kind of dialogue. A dialogue is topic-complete when no agent can add anything that is directly connected to the subject of the dialogue. Some protocols will prevent agents from making moves even though the dialogue is not topic-complete. To distinguish such cases we have: Definition 5.2. A basic dialogue D between the set of agents A with a corresponding argumentation graph AG is protocol-complete under a protocol P if no agent can make a move that adds a node to the argumentation graph that is permitted by P. The argumentation graph constructed by a protocol-complete dialogue is called a protocol-complete argumentation graph and is denoted AG(D)P . Clearly: Proposition 5.1. Any dialogue D under a basic protocol P is protocol-complete if it is topic-complete. The converse does not hold in general. Proof. If D is topic-complete, no agent can make a move that will extend the argumentation graph. This means that no agent can make a move that is permitted by a basic protocol, and so D is also protocol complete. The converse does not hold since some basic dialogues (under a protocol that only permits R3-relevant moves, for example) will not permit certain moves (like the addition of a node that connects to the root of the argumentation graph after more than two moves) that would be allowed in a topiccomplete dialogue. Corollary 5.1. For a basic dialogue D, AG(D)P is a sub-graph of AG(D)T . Obviously, from the definition of a sub-graph, the converse of Corollary 5.1 does not hold in general. The important distinction between topic- and protocolcompleteness is that the former is determined purely by the state of the dialogue - as captured by the argumentation graph - and is thus independent of the protocol, while the latter is determined entirely by the protocol. Any time that a dialogue ends in a state of protocol-completeness rather than topic completeness, it is ending when agents still have things to say but can\"t because the protocol won\"t allow them to. With these definitions of completeness, our task is to relate topic-completeness - the property that ensures that agents can say everything that they have to say in a dialogue that is, in some sense, important - to the notions of relevance we have developed - which determine what agents are allowed to say. When we need very specific conditions to make protocol-complete dialogues topic-complete, it means that agents have lots of room for rhetorical maneouver when those conditions are not in force. That is there are many ways they can bring dialogues to a close before everything that can be said has been said. Where few conditions are required, or conditions are absent, then dialogues between agents with the same knowledge will always play out the same way, and rhetoric has no place. We have: Proposition 5.2. A protocol-complete basic dialogue D under a protocol which only allows R3-relevant moves will be topic-complete only when AG(D)T has a single branch in which the nodes are labelled in increasing order from the root. The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Proof. Given what we know about R3-relevance, the condition on AG(D)P having a single branch is obvious. This is not a sufficient condition on its own because certain protocols may prevent - through additional restrictions, like strict turn-taking in a multi-party dialogue - all the nodes in AG(D)T , which is not subject to such restrictions, being added to the graph. Only when AG(D)T includes the nodes in the exact order that the corresponding arguments are put forward is it necessary that a topic-complete argumentation graph be constructed. Given Proposition 5.1, these are the conditions under which dialogues conducted under the notion of R3-relevance will always be predetermined, and given how restrictive the conditions are, such dialogues seem to have plenty of room for rhetoric to play a part. To find similar conditions for dialogues composed of R1and R2-relevant moves, we first need to distinguish between them. We can do this in terms of the structure of the argumentation graph: Proposition 5.3. Consider a basic dialogue D, with argumentation graph AG which has root r denoting an argument A. If argument A , denoted by node v is an an R2relevant move m, m is not R1-relevant if and only if: 1. there are two nodes v and v on the path between v and r, and the argument denoted by v defends itself against the argument denoted by v ; or 2. there is an argument A , denoted by node v , that affects the status of A, and the path from v to r has one or more nodes in common with the path from v to r. Proof. For the first condition, consider that since AG is a tree, v is connected to r. Thus there is a series of undercut relations between A and A , and this corrresponds to a path through AG. If this path is the only branch in the tree, then A will affect the status of A unless the chain of affect is broken by an undercut that can\"t change the status of the undercut argument because the latter defends itself. For the second condition, as for the first, the only way that A cannot affect the status of A is if something is blocking its influence. If this is not due to defending against, it must be because there is some node u on the path that represents an argument whose status is fixed somehow, and that must mean that there is another chain of undercut relations, another branch of the tree, that is incident at u. Since this second branch denotes another chain of arguments, and these affect the status of the argument denoted by u, they must also affect the status of A. Any of these are the A in the condition. So an R2-relevant move m is not R1-relevant if either its effect is blocked because an argument upstream is not strong enough, or because there is another line of argument that is currently determining the status of the argument at the root. This, in turn, means that if the effect is not due to defending against, then there is an alternative move that is R1-relevant - a move that undercuts A in the second condition above5 . We can now show Though whether the agent in question can make such a move is another question. Proposition 5.4. A protocol-complete basic dialogue D will always be topic-complete under a protocol which only includes R2-relevant moves and allows every R2-relevant move to be made. The restriction on R2-relevant rules is exactly that for topiccompleteness, so a dialogue that has only R2-relevant moves will continue until every argument that any agent can make has been put forward. Given this, and what we revealed about R1-relevance in Proposition 5.3, we can see that: Proposition 5.5. A protocol-complete basic dialogue D under a protocol which only includes R1-relevant moves will be topic-complete if AG(D)T : 1. includes no path with adjacent nodes v, denoting A, and v , denoting A , such that A undercuts A and A is stronger that A; and 2. is such that the nodes in every branch have consecutive indices and no node with degree greater than two is an odd number of arcs from a leaf node. Proof. The first condition rules out the first condition in Proposition 5.3, and the second deals with the situation that leads to the second condition in Proposition 5.3. The second condition ensures that each branch is constructed in full before any new branch is added, and when a new branch is added, the argument that is undercut as part of the addition will be acceptable, and so the addition will change the status of the argument denoted by that node, and hence the root. With these conditions, every move required to construct AG(D)T will be permitted and so the dialogue will be topic-complete when every move has been completed. The second part of this result only identifies one possible way to ensure that the second condition in Proposition 5.3 is met, so the converse of this result does not hold. However, what we have is sufficient to answer the question about predetermination that we started with. For dialogues to be predetermined, every move that is R2-relevant must be made. In such cases every dialogue is topic complete. If we do not require that all R2-relevant moves are made, then there is some room for rhetoric - the way in which alternative lines of argument are presented becomes an issue. If moves are forced to be R3-relevant, then there is considerable room for rhetorical play. 6. SUMMARY This paper has studied the different ideas of relevance in argumentation-based dialogue, identifying the relationship between these ideas, and showing how they can impact the extent to which the way that agents choose moves in a dialogue - what some authors have called the strategy and tactics of a dialogue. This extends existing work on relvance, such as by showing how different notions of relevance can have an effect on the outcome of a dialogue, in particular when they render the outcome predetermined. This connection extends the work of which considered dialogue outcome, but stopped short of identifying the conditions under which it is predetermined. There are two ways we are currently trying to extend this work, both of which will generalise the results and extend its applicability. First, The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) we have imposed, the exclusion of moves that attack several arguments (without which the argument graph can be mulitply-connected) and the exclusion of pre-empted moves, without which the argument graph can have cycles. Second, we want to extend the ideas of relevance to cope with moves that do not only add undercutting arguments, but also supporting arguments, thus taking account of bipolar argumentation frameworks . Acknowledgments The authors are grateful for financial support received from the EC, through project IST-FP6- and from the NSF under grants REC-02- and NSF IIS-. They are also grateful to Peter Stone for a question, now several years old, which this paper has finally answered.", "body1": "Finding ways for agents to reach agreements in multiagent systems is an area of active research. The work of Walton and Krabbe , popularised in the multiagent systems community by Reed , has been particularly influential in the field of argumentation-based dialogue. In this paper, we extend the work of by considering the role of relevance - the relationship between utterances in a dialogue. We begin by introducing the formal system of argumentation that underpins our approach, as well as the corresponding terminology and notation, all taken from . A dialogue is a sequence of messages passed between two or more members of a set of agents A. Note that in the description that follows, we assume that is the classical inference relation, that \u2261 stands for logical equivalence, and we use \u0394 to denote all the information available to an agent. S is called the support of A, written S = Support(A) and p is the conclusion of A, written p = Conclusion(A). In general, since \u0394 may be inconsistent, arguments in A(\u0394), the set of all arguments which can be made from \u0394, may conflict, and we make this idea precise with the notion of undercutting: Definition 2.2. A1 undercuts A2 iff \u2203\u00acp \u2208 Support(A2) such that p \u2261 Conclusion(A1). In other words, an argument is undercut if and only if there is another argument which has as its conclusion the negation of an element of the support for the first argument. To capture the fact that some beliefs are more strongly held than others, we assume that any set of beliefs has a preference order over it. The preference level of a nonempty subset S \u2282 \u0394, where different elements s \u2208 S may belong to different layers \u0394i, is valued at the highest numbered layer which has a member in S and is referred to as level(S). A1 is preferred to A2 according to Pref , A1 Pref A2, iff level(Support(A1)) > level(Support(A2)). We can now define the argumentation system we will use: Definition 2.4. The preference order makes it possible to distinguish different types of relations between arguments: Definition 2.5. \u2022 If A2 undercuts A1 then A1 defends itself against A2 iff A1 Pref A2. \u2022 A set of arguments A defends A1 iff for every A2 that undercuts A1, where A1 does not defend itself against A2, then there is some A3 \u2208 A such that A3 undercuts A2 and A2 does not defend itself against A3. We write AUndercut,Pref to denote the set of all non-undercut arguments and arguments defending themselves against all their undercutting arguments. Definition 2.7. Argument A is said to affect the status of another argument A if changing the status of A will change the status of A . Systems like those described in , lay down sets of locutions that agents can make to put forward propositions and the arguments that support them, and protocols that define precisely which locutions can be made at which points in the dialogue. We then have a typical definition of a dialogue: Definition 3.1. Moves in an argumentation-based dialogue typically attack moves that have been made previously. The Sixth Intl. Even if a dialogue conforms to a protocol, it is typically the case that the agent engaging in the dialogue has to make a choice of move - it has to choose which of the moves in M to make. As arguments are placed into commitment stores, and hence become public, agents can determine the relationships between them. Note that we do not require that the argumentation graph is connected. We adapt some standard graph theoretic notions in order to describe various aspects of the argumentation graph. In a reversal of the usual notion, we define a root of an argumentation graph1 as follows: Definition 3.4. Thus a root of a graph is a node to which directed edges may be connected, but from which no directed edges connect to other nodes. v v\" Figure 1: An example argument graph argument that is undercut, but which itself does no undercutting. Thus a leaf in an argumentation graph represents an argument that undercuts another argument, but does no undercutting. Since, as described above, argumentation graphs are allowed to be not connected (in the usual graph theory sense), it is helpful to distinguish nodes that are connected to other nodes, in particular to the root of the tree. Proof. The reason that we need the concept of the argumentation graph is that the properties of the argumentation graph tell us something about the set of arguments A the graph represents. Thus a dialogue corresponds to an argumentation graph if and only if every argument made in the dialogue corresponds to a node in the graph, and every node in the graph corresponds to an argument made in the dialogue. Most work on dialogues is concerned with what we might call coherent dialogues, that is dialogues in which the participants are, as in the work of Walton and Krabbe , focused on resolving some question through the dialogue2 To capture this coherence, it seems we need a notion of relevance to constrain the statements made by agents. R3 Making mi+1 will add a node vi+1 that is connected to the last node to be added to AG. R2-relevance is the form of relevance defined by in their study of strategic and tactical reasoning3 . Note that we only define relevance for the second move of the dialogue onwards because the first move is taken to identify the subject of the dialogue, that is, the central question that the dialogue is intended to answer, and hence it must be relevant to the dialogue, no matter what it is. We can think of relevance as enforcing a form of parsimony on a dialogue - it prevents agents from making statements that do not bear on the current state of the dialogue. This promotes efficiency, in the sense of limiting the number of moves in the dialogue, and, as in , prevents agents revealing information that they might better keep hidden. Another form of parsimony is to insist that agents are not allowed to put forward arguments that will be undercut by arguments that have already been made during the dialogue. We therefore distinguish such arguments. See for examples of dialogues where this is not the case. We consider such reasoning sub-types of rhetoric. Definition 4.2. We use the term pre-empted because if such an argument is put forward, it can seem as though another agent anticipated the argument being made, and already made an argument that would render it useless. Proposition 4.1. That the graph is connected follows from the construction of the graph under a protocol that enforces relevance. Since a basic dialogue does not allow moves that are preempted, every edge that is added during construction is directed from the node that is added to one already in the graph (thus denoting that the argument A denoted by the added node, v, undercuts the argument A denoted by the node to which the connection is made, v , rather than the other way around). Thus AG is a tree. To show that AG has a single root, consider its construction from the initial node. Although these results are straightforward to obtain, they allow us to show how the notions of relevance are related. The converse does not hold. 2. 3. To see that the converse does not hold, we have to consider what it takes to change the status of r (since Proposition 3.1 tells us that connectedness is not enough to ensure a change of status - if it did, R1 and R2 relevance would coincide). For mi+1 to change the status of the root, it will have to (1) make the argument A represented by r either unacceptable, if it were acceptable before the move, or (2) acceptable if it were unacceptable before the move. Now, further consider that mi+1 puts forward an argument Ai+1 that undercuts the argument denoted by some node v , but this latter argument defends itself against Ai+1. To see that the converse does not hold, consider that an R2-relevant move can connect to any node in AG. The first part of 3 follows by a similar argument to that we just used - an R1-relevant move does not have to connect to vi, just to some v that is part of the graph - and the second part follows since a move that is R3-relevant may introduce an argument Ai+1 that undercuts the argument Ai put forward by the previous move (and so vi+1 is connected to vi), but finds that Ai defends itself against Ai+1, preventing a change of status at the root. What is most interesting is not so much the results but why they hold, since this reveals some aspects of the interplay between relevance and the structure of argument graphs. It turns out that we can exploit the interplay between structure and relevance that Propositions 4.1 and 4.2 have started to illuminate to establish relationships between the protocols that govern dialogues and the argument graphs constructed during such dialogues. Proposition 4.3. Starting from the first node this recursively constructs a tree with just one branch, and the relationship holds. Looking for more complex kinds of protocol that construct more complex kinds of argument graph, it is an obvious move to turn to: Definition 4.4. But, on reflection, since any graph with only one branch is also a tree: Proposition 4.4. and, furthermore: Proposition 4. The Sixth Intl. We distinguish bushy protocols from multi-path protocols, and hence R1- and R2-relevance from R3-relevance, because of the kinds of dialogue that R3-relevance enforces. The above discussion of the difference between dialogues carried out under single-path and bushy protocols brings us to the consideration of what called predeterminism, but we now prefer to describe using the term completeness. Assuming that the \u03a3i do not change during the dialogue, which is the usual assumption in this kind of dialogue. A dialogue is topic-complete when no agent can add anything that is directly connected to the subject of the dialogue. The argumentation graph constructed by a protocol-complete dialogue is called a protocol-complete argumentation graph and is denoted AG(D)P . Corollary 5.1. Obviously, from the definition of a sub-graph, the converse of Corollary 5.1 does not hold in general. The important distinction between topic- and protocolcompleteness is that the former is determined purely by the state of the dialogue - as captured by the argumentation graph - and is thus independent of the protocol, while the latter is determined entirely by the protocol. With these definitions of completeness, our task is to relate topic-completeness - the property that ensures that agents can say everything that they have to say in a dialogue that is, in some sense, important - to the notions of relevance we have developed - which determine what agents are allowed to say. To find similar conditions for dialogues composed of R1and R2-relevant moves, we first need to distinguish between them. For the second condition, as for the first, the only way that A cannot affect the status of A is if something is blocking its influence. So an R2-relevant move m is not R1-relevant if either its effect is blocked because an argument upstream is not strong enough, or because there is another line of argument that is currently determining the status of the argument at the root. The restriction on R2-relevant rules is exactly that for topiccompleteness, so a dialogue that has only R2-relevant moves will continue until every argument that any agent can make has been put forward. However, what we have is sufficient to answer the question about predetermination that we started with.", "body2": "and Sycara . The dialogue game view overlaps with work on conversation policies (see, for example, ), but differs in considering the entire dialogue rather than dialogue segments. Our work identifes the limits on such rhetorical manoeuvering, showing when it can and cannot have an effect. We begin by introducing the formal system of argumentation that underpins our approach, as well as the corresponding terminology and notation, all taken from . In contrast, the contents of \u03a3\u03b1 are private to \u03b1. An argument A is a pair (S, p) where p is a formula of L and S a subset of \u0394 such that (i) S is consistent; (ii) S p; and (iii) S is minimal, so no proper subset of S satisfying both (1) and (2) exists. Thus we talk of p being supported by the argument (S, p). Let A1 and A2 be arguments in A(\u0394). A1 undercuts A2 iff \u2203\u00acp \u2208 Support(A2) such that p \u2261 Conclusion(A1). In other words, an argument is undercut if and only if there is another argument which has as its conclusion the negation of an element of the support for the first argument. , \u0394n such that beliefs in \u0394i are all equally preferred and are preferred over elements in \u0394j where i > j. If A1 is preferred to A2, we say that A1 is stronger than A2. An argumentation system is a triple: A(\u0394), Undercut, Pref such that: \u2022 A(\u0394) is a set of the arguments built from \u0394, \u2022 Undercut is a binary relation representing the defeat relationship between arguments, Undercut \u2286 A(\u0394) \u00d7 A(\u0394), and \u2022 Pref is a pre-ordering on A(\u0394) \u00d7 A(\u0394). Let A1, A2 be two arguments of A(\u0394). Otherwise, A1 does not defend itself. \u2022 A set of arguments A defends A1 iff for every A2 that undercuts A1, where A1 does not defend itself against A2, then there is some A3 \u2208 A such that A3 undercuts A2 and A2 does not defend itself against A3. An acceptable argument is one which is, in some sense, proven since all the arguments which might undermine it are themselves undermined. If there is an acceptable argument for a proposition p, then the status of p is accepted, while if there is not an acceptable argument for p, the status of p is not accepted. Argument A is said to affect the status of another argument A if changing the status of A will change the status of A . We do not discuss the detail of the mechanism that is used to put these arguments forward - we just assume that arguments of the form (S, p) are inserted into an agent\"s commitment store where they are then visible to other agents. A given move mi is a pair \u03b1, Ai where Ai is an argument that \u03b1 places into its commitment store CS\u03b1. Later we will see how different restrictions on moves lead to different kinds of dialogue. If a dialogue D always picks its moves m from the set M identified by protocol P, then D is said to conform to P. Some of our results will give a sense of how much scope an agent has to exercise rhetoric under different protocols. We will use the term argument graph as a synonym for argumentation graph. In other words the notion of an argumentation graph allows for the representation of arguments that do not relate, by undercutting or being undercut, to any other arguments (we will come back to this point very shortly). If there is an edge e from vertex v to vertex v , then v is said to be the parent of v and v is said to be the child of v. A root of an argumentation graph AG = (V, E) is a node v \u2208 V that has no children. Thus a root is a node representing an Note that we talk of a root rather than the root - as defined, an argumentation graph need not be a tree. A leaf of an argumentation graph AG = (V, E) is a node v \u2208 V that has no parents. The reversal of the terminology means that it matches the natural process of tree construction. The converse does not hold. Since A only undercuts A if the edge runs from v to v , we cannot infer that A will affect the status of A from information about whether or not they are connected. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) is advanced at move mi is represented by exactly one node v \u2208 V , and \u2200v \u2208 V , v represents exactly one argument Ai that has been advanced by a move m \u2208 mn. Thus we can, for example, refer to the third node in the argumentation graph, meaning the node that represents the argument put forward in the third move of the dialogue. R2 Making mi+1 will add a node vi+1 that is connected to the first node of AG. R3 Making mi+1 will add a node vi+1 that is connected to the last node to be added to AG. R1-relevance was suggested by the notion used in , and though it differs somewhat from that suggested there, we believe it captures the essence of its predecessor. In assuming this, we focus our attention on the same kind of dialogues as . We can think of relevance as enforcing a form of parsimony on a dialogue - it prevents agents from making statements that do not bear on the current state of the dialogue. This promotes efficiency, in the sense of limiting the number of moves in the dialogue, and, as in , prevents agents revealing information that they might better keep hidden. Another form of parsimony is to insist that agents are not allowed to put forward arguments that will be undercut by arguments that have already been made during the dialogue. We therefore distinguish such arguments. See for examples of dialogues where this is not the case. We consider such reasoning sub-types of rhetoric. The move mi+1 and the argument it puts forward, Ai+1, are both said to be pre-empted, if Ai+1 is undercut by some A \u2208 Ai. The argument graph of a basic dialogue is somewhat restricted. Consider a basic dialogue D. The argumentation graph AG that corresponds to D is a tree with a single root. To show that it is a tree, we have to show that it is acyclic and connected. Thus AG is connected. Since every edge that is added is directed from the new node to the rest of the graph, there can be no cycles. Thus AG is a tree. Since every argumentation graph constructed by a basic dialogue is a tree with a single root, this means that the first node of every argumentation graph is the root. Although these results are straightforward to obtain, they allow us to show how the notions of relevance are related. Every move mi+1 that is R1-relevant is R2-relevant. The converse does not hold. Every move mi+1 that is R3-relevant is R2-relevant. This is precisely what is required to satisfy R2, and the relatiosnhip is proved to hold. To see that the converse does not hold, we have to consider what it takes to change the status of r (since Proposition 3.1 tells us that connectedness is not enough to ensure a change of status - if it did, R1 and R2 relevance would coincide). In the latter case, vi+1 will be directly connected to the node representing A and by Proposition 4.1 to r. To achieve (2), vi+1 will have to undercut an argument A that is either currently undercutting A, or is undercutting an argument that would otherwise defend A. By Proposition 4.1, it is connected to r and so is R2-relevant. To see that the converse does not hold, consider that an R2-relevant move can connect to any node in AG. The first part of 3 follows by a similar argument to that we just used - an R1-relevant move does not have to connect to vi, just to some v that is part of the graph - and the second part follows since a move that is R3-relevant may introduce an argument Ai+1 that undercuts the argument Ai put forward by the previous move (and so vi+1 is connected to vi), but finds that Ai defends itself against Ai+1, preventing a change of status at the root. Since a move that is R2-relevant can add a node that connects anywhere on an argument graph, any move that is R3-relevant will be R2-relevant, but the converse does not hold. A protocol is single-path if all dialogues that conform to it construct argument graphs that have only one branch. R3-relevance requires that every node added to the argument graph be connected to the previous node. The converse does not hold because even if one or more moves in the protocol are R1- or R2-relevant, it may be the case that, because of an agent\"s rhetorical choice or because of its knowledge, every argument that is chosen to be put forward will undercut the previous argument and so the argument graph is a one-branch tree. A basic protocol is multi-path if all dialogues that conform to it can construct argument graphs that are trees. Any single-path protocol is an instance of a multi-path protocol. Any basic protocol P is multi-path. From Proposition 4.3 we know that if all moves are R3-relevant then we\"ll get a tree with one branch, and from Proposition 4.1 we know that all basic protocols will build an argument graph that is a tree, so providing we exclude R3-relevant moves, we will get protocols that can build multi-branch trees. The number of branches in AG is less than or equal to m + 1. If there were R3-relevant moves before any of these new-branch-creating moves, then these m branches are in addition to the initial branch created by the R3-relevant moves, and we have a maximum of m + 1 possible branches. This, as we will see next, means that such a dialogue only allows a subset of all the moves that would otherwise be possible. The argumentation graph constructed by a topic-complete dialogue is called a topic-complete argumentation graph and is denoted AG(D)T . Assuming that the \u03a3i do not change during the dialogue, which is the usual assumption in this kind of dialogue. A basic dialogue D between the set of agents A with a corresponding argumentation graph AG is protocol-complete under a protocol P if no agent can make a move that adds a node to the argumentation graph that is permitted by P. The converse does not hold in general. The converse does not hold since some basic dialogues (under a protocol that only permits R3-relevant moves, for example) will not permit certain moves (like the addition of a node that connects to the root of the argumentation graph after more than two moves) that would be allowed in a topiccomplete dialogue. For a basic dialogue D, AG(D)P is a sub-graph of AG(D)T . Obviously, from the definition of a sub-graph, the converse of Corollary 5.1 does not hold in general. Any time that a dialogue ends in a state of protocol-completeness rather than topic completeness, it is ending when agents still have things to say but can\"t because the protocol won\"t allow them to. A protocol-complete basic dialogue D under a protocol which only allows R3-relevant moves will be topic-complete only when AG(D)T has a single branch in which the nodes are labelled in increasing order from the root. Given Proposition 5.1, these are the conditions under which dialogues conducted under the notion of R3-relevance will always be predetermined, and given how restrictive the conditions are, such dialogues seem to have plenty of room for rhetoric to play a part. If argument A , denoted by node v is an an R2relevant move m, m is not R1-relevant if and only if: 1. there are two nodes v and v on the path between v and r, and the argument denoted by v defends itself against the argument denoted by v ; or 2. there is an argument A , denoted by node v , that affects the status of A, and the path from v to r has one or more nodes in common with the path from v to r. If this path is the only branch in the tree, then A will affect the status of A unless the chain of affect is broken by an undercut that can\"t change the status of the undercut argument because the latter defends itself. Any of these are the A in the condition. A protocol-complete basic dialogue D will always be topic-complete under a protocol which only includes R2-relevant moves and allows every R2-relevant move to be made. A protocol-complete basic dialogue D under a protocol which only includes R1-relevant moves will be topic-complete if AG(D)T : 1. includes no path with adjacent nodes v, denoting A, and v , denoting A , such that A undercuts A and A is stronger that A; and 2. is such that the nodes in every branch have consecutive indices and no node with degree greater than two is an odd number of arcs from a leaf node. The second part of this result only identifies one possible way to ensure that the second condition in Proposition 5.3 is met, so the converse of this result does not hold. If moves are forced to be R3-relevant, then there is considerable room for rhetorical play.", "introduction": "Finding ways for agents to reach agreements in multiagent systems is an area of active research. One mechanism for achieving agreement is through the use of argumentation - where one agent tries to convince another agent of something during the course of some dialogue. Early examples of argumentation-based approaches to multiagent agreement include the work of Dignum et al. , Kraus , Parsons and Jennings , Reed , Schroeder et al. The work of Walton and Krabbe , popularised in the multiagent systems community by Reed , has been particularly influential in the field of argumentation-based dialogue. This work influenced the field in a number of ways, perhaps most deeply in framing multi-agent interactions as dialogue games in the tradition of Hamblin . Viewing dialogues in this way, as in , provides a powerful framework for analysing the formal properties of dialogues, and for identifying suitable protocols under which dialogues can be conducted . The dialogue game view overlaps with work on conversation policies (see, for example, ), but differs in considering the entire dialogue rather than dialogue segments. In this paper, we extend the work of by considering the role of relevance - the relationship between utterances in a dialogue. Relevance is a topic of increasing interest in argumentation-based dialogue because it relates to the scope that an agent has for applying strategic manoeuvering to obtain the outcomes that it requires . Our work identifes the limits on such rhetorical manoeuvering, showing when it can and cannot have an effect.", "conclusion": "This paper has studied the different ideas of relevance in argumentation-based dialogue, identifying the relationship between these ideas, and showing how they can impact the extent to which the way that agents choose moves in a dialogue - what some authors have called the strategy and tactics of a dialogue.. This extends existing work on relvance, such as by showing how different notions of relevance can have an effect on the outcome of a dialogue, in particular when they render the outcome predetermined.. This connection extends the work of which considered dialogue outcome, but stopped short of identifying the conditions under which it is predetermined.. There are two ways we are currently trying to extend this work, both of which will generalise the results and extend its applicability.. First, The Sixth Intl.. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) we have imposed, the exclusion of moves that attack several arguments (without which the argument graph can be mulitply-connected) and the exclusion of pre-empted moves, without which the argument graph can have cycles.. Second, we want to extend the ideas of relevance to cope with moves that do not only add undercutting arguments, but also supporting arguments, thus taking account of bipolar argumentation frameworks .. Acknowledgments The authors are grateful for financial support received from the EC, through project IST-FP6- and from the NSF under grants REC-02- and NSF IIS-.. They are also grateful to Peter Stone for a question, now several years old, which this paper has finally answered."}
{"id": "H-25", "keywords": ["queri expans", "interact retriev"], "title": "Term Feedback for Information Retrieval with Language Models", "abstract": "In this paper we study term-based feedback for information re- trieval in the language modeling approach. With term feedback a user directly judges the relevance of individual terms without in- teraction with feedback documents, taking full control of the query expansion process. We propose a cluster-based method for select- ing terms to present to the user for judgment, as well as effective algorithms for constructing refined query language models from user term feedback. Our algorithms are shown to bring significant improvement in retrieval accuracy over a non-feedback baseline, and achieve comparable performance to relevance feedback. They are helpful even when there are no relevant documents in the top.", "references": ["Relevance feedback with too much data", "HARD track overview in TREC 2005 - High Accuracy Retrieval from Documents", "Using terminological feedback for web search refinement: a log-based study", "The paraphrase search assistant: terminological feedback for iterative information seeking", "Automatic query expansion using SMART", "Towards interactive query expansion", "UMass at TREC 2003: HARD and QA", "Hierarchical presentation of expansion terms", "A probabilistic model of information retrieval: development and status", "The loquacious user: a document-independent source of terms for query expansion", "Elicitation of term relevance feedback: an investigation of term source and context", "A case for interaction: A study of interactive information retrieval behavior and effectiveness", "Relevance-based language models", "Evaluation of the real and perceived value of automatic and interactive query expansion", "A Language Modeling Approach to Information Retrieval", "Okapi at TREC-3", "Relevance feedback in information retrieval", "Re-examining the potential effectiveness of interactive query expansion", "Improving retrieval performance by relevance feedback", "Implicit user modeling for personalized search", "Active feedback in ad-hoc information retrieval", "Term relevance feedback and query expansion: relation to design", "Query expansion using local and global document analysis", "Microsoft cambridge at TREC-13: Web and HARD tracks", "Model-based feedback in the language modeling approach to information retrieval", "A cross-collection mixture model for comparative text mining"], "full_text": "1. INTRODUCTION In the language modeling approach to information retrieval, feedback is often modeled as estimating an improved query model or relevance model based on a set of feedback documents . This is in line with the traditional way of doing relevance feedback - presenting a user with documents/passages for relevance judgment and then extracting terms from the judged documents or passages to expand the initial query. It is an indirect way of seeking user\"s assistance for query model construction, in the sense that the refined query model (based on terms) is learned through feedback documents/passages, which are high-level structures of terms. It has the disadvantage that irrelevant terms, which occur along with relevant ones in the judged content, may be erroneously used for query expansion, causing undesired effects. For example, for the TREC query Hubble telescope achievements, when a relevant document talks more about the telescope\"s repair than its discoveries, irrelevant terms such as spacewalk can be added into the modified query. We can consider a more direct way to involve a user in query model improvement, without an intermediary step of document feedback that can introduce noise. The idea is to present a (reasonable) number of individual terms to the user and ask him/her to judge the relevance of each term or directly specify their probabilities in the query model. This strategy has been discussed in , but to our knowledge, it has not been seriously studied in existing language modeling literature. Compared to traditional relevance feedback, this term-based approach to interactive query model refinement has several advantages. First, the user has better control of the final query model through direct manipulation of terms: he/she can dictate which terms are relevant, irrelevant, and possibly, to what degree. This avoids the risk of bringing unwanted terms into the query model, although sometimes the user introduces low-quality terms. Second, because a term takes less time to judge than a document\"s full text or summary, and as few as around 20 presented terms can bring significant improvement in retrieval performance (as we will show later), term feedback makes it faster to gather user feedback. This is especially helpful for interactive adhoc search. Third, sometimes there are no relevant documents in the top N of the initially retrieved results if the topic is hard. This is often true when N is constrained to be small, which arises from the fact that the user is unwilling to judge too many documents. In this case, relevance feedback is useless, as no relevant document can be leveraged on, but term feedback is still often helpful, by allowing relevant terms to be picked from irrelevant documents. HARD Track and continued study afterward, we explored how to exploit term feedback from the user to construct improved query models for information retrieval in the language modeling approach. We identified two key subtasks of term-based feedback, i.e., pre-feedback presentation term selection and post-feedback query model construction, with effective algorithms developed for both. We imposed a secondary cluster structure on terms and found that a cluster view sheds additional insight into the user\"s information need, and provides a good way of utilizing term feedback. Through experiments we found that term feedback improves significantly over the nonfeedback baseline, even though the user often makes mistakes in relevance judgment. Among our algorithms, the one with best retrieval performance is TCFB, the combination of TFB, the direct term feedback algorithm, and CFB, the cluster-based feedback algorithm. We also varied the number of feedback terms and observed reasonable improvement even at low numbers. Finally, by comparing term feedback with document-level feedback, we found it to be a viable alternative to the latter with competitive retrieval performance. The rest of the paper is organized as follows. Section 2 discusses some related work. Section 4 outlines our general approach to term feedback. We present our method for presentation term selection in Section 3 and algorithms for query model construction in Section 5. The experiment results are given in Section 6. Section 7 concludes this paper. 2. RELATED WORK Relevance feedback has long been recognized as an effective method for improving retrieval performance. Normally, the top N documents retrieved using the original query are presented to the user for judgment, after which terms are extracted from the judged relevant documents, weighted by their potential of attracting more relevant documents, and added into the query model. The expanded query usually represents the user\"s information need better than the original one, which is often just a short keyword query. A second iteration of retrieval using this modified query usually produces significant increase in retrieval accuracy. In cases where true relevance judgment is unavailable and all top N documents are assumed to be relevant, it is called blind or pseudo feedback and usually still brings performance improvement. Because document is a large text unit, when it is used for relevance feedback many irrelevant terms can be introduced into the feedback process. To overcome this, passage feedback is proposed and shown to improve feedback performance. A more direct solution is to ask the user for their relevance judgment of feedback terms. For example, in some relevance feedback systems such as , there is an interaction step that allows the user to add or remove expansion terms after they are automatically extracted from relevant documents. This is categorized as interactive query expansion, where the original query is augmented with user-provided terms, which can come from direct user input (free-form text or keywords) or user selection of system-suggested terms (using thesauri or extracted from feedback documents). In many cases term relevance feedback has been found to effectively improve retrieval performance. For example, the study in shows that the user prefers to have explicit knowledge and direct control of which terms are used for query expansion, and the penetrable interface that provides this freedom is shown to perform better than other interfaces. However, in some other cases there is no significant benefit, even if the user likes interacting with expansion terms. In a simulated study carried out in , the author compares the retrieval performance of interactive query expansion and automatic query expansion with a simulated study, and suggests that the potential benefits of the former can be hard to achieve. The user is found to be not good at identifying useful terms for query expansion, when a simple term presentation interface is unable to provide sufficient semantic context of the feedback terms. Our work differs from the previous ones in two important aspects. First, when we choose terms to present to the user for relevance judgment, we not only consider single-term value (e.g., the relative frequency of a term in the top documents, which can be measured by metrics such as Robertson Selection Value and Simplified Kullback-Leibler Distance as listed in ), but also examine the cluster structure of the terms, so as to produce a balanced coverage of the different topic aspects. Second, with the language modelling framework, we allow an elaborate construction of the updated query model, by setting different probabilities for different terms based on whether it is a query term, its significance in the top documents, and its cluster membership. Although techniques for adjusting query term weights exist for vector space models and probablistic relevance models, most of the aforementioned works do not use them, choosing to just append feedback terms to the original query (thus using equal weights for them), which can lead to poorer retrieval performance. The combination of the two aspects allows our method to perform much better than the baseline. The usual way for feedback term presentation is just to display the terms in a list. There have been some works on alternative user interfaces. arranges terms in a hierarchy, and compares three different interfaces, including terms + checkboxes, terms + context (sentences) + checkboxes, sentences + input text box. In both studies, however, there is no significant performance difference. In our work we adopt the simplest approach of terms + checkboxes. We focus on term presentation and query model construction from feedback terms, and believe using contexts to improve feedback term quality should be orthogonal to our method. 3. GENERAL APPROACH We follow the language modeling approach, and base our method on the KL-divergence retrieval model proposed in . With this model, the retrieval task involves estimating a query language model \u03b8q from a given query, a document language model \u03b8d from each document, and calculating their KL-divergence D(\u03b8q||\u03b8d), which is then used to score the documents. treats relevance feedback as a query model re-estimation problem, i.e., computing an updated query model \u03b8q given the original query text and the extra evidence carried by the judged relevant documents. We adopt this view, and cast our task as updating the query model from user term feedback. There are two key subtasks here: First, how to choose the best terms to present to the user for judgment, in order to gather maximal evidence about the user\"s information need. Second, how to compute an updated query model based on this term feedback evidence, so that it captures the user\"s information need and translates into good retrieval performance. 4. PRESENTATION TERM SELECTION Proper selection of terms to be presented to the user for judgment is crucial to the success of term feedback. If the terms are poorly chosen and there are few relevant ones, the user will have a hard time looking for useful terms to help clarify his/her information need. If the relevant terms are plentiful, but all concentrate on a single aspect of the query topic, then we will only be able to get feedback on that aspect and missing others, resulting in a breadth loss in retrieved results. Therefore, it is important to carefully select presentation terms to maximize expected gain from user feedback, i.e., those that can potentially reveal most evidence of the user\"s information need. This is similar to active feedback, which suggests that a retrieval system should actively probe the user\"s information need, and in the case of relevance feedback, the feedback documents should be chosen to maximize learning benefits (e.g. diversely so as to increase coverage). In our approach, the top N documents from an initial retrieval using the original query form the source of feedback terms: all terms that appear in them are considered candidates to present to the user. These documents serve as pseudo-feedback, since they provide a much richer context than the original query (usually very short), while the user is not asked to judge their relevance. Due to the latter reason, it is possible to make N quite large (e.g., in our experiments we set N = 60) to increase its coverage of different aspects in the topic. The simplest way of selecting feedback terms is to choose the most frequent M terms from the N documents. This method, however, has two drawbacks. First, a lot of common noisy terms will be selected due to their high frequencies in the document collection, unless a stop-word list is used for filtering. Second, the presentation list will tend to be filled by terms from major aspects of the topic; those from a minor aspect are likely to be missed due to their relatively low frequencies. We solve the above problems by two corresponding measures. First, we introduce a background model \u03b8B that is estimated from collection statistics and explains the common terms, so that they are much less likely to appear in the presentation list. Second, the terms are selected from multiple clusters in the pseudo-feedback documents, to ensure sufficient representation of different aspects of the topic. We rely on the mixture multinomial model, which is used for theme discovery in . Specifically, we assume the N documents contain K clusters {Ci| i = 1, 2, \u00b7 \u00b7 \u00b7 K}, each characterized by a multinomial word distribution (also known as unigram language model) \u03b8i and corresponding to an aspect of the topic. The documents are regarded as sampled from a mixture of K + 1 components, including the K clusters and the background model: p(w|d) = \u03bbBp(w|\u03b8B) + (1 \u2212 \u03bbB) i=1 \u03c0d,ip(w|\u03b8i) where w is a word, \u03bbB is the mixture weight for the background model \u03b8B, and \u03c0d,i is the document-specific mixture weight for the i-th cluster model \u03b8i. We then estimate the cluster models by maximizing the probability of the pseudo-feedback documents being generated from the multinomial mixture model: log p(D|\u039b) = d\u2208D w\u2208V c(w; d) log p(w|d) where D = {di| i = 1, 2, \u00b7 \u00b7 \u00b7 N} is the set of the N documents, V is the vocabulary, c(w; d) is w\"s frequency in d and \u039b = {\u03b8i| i = 1, 2, \u00b7 \u00b7 \u00b7 K} \u222a {\u03c0dij | i = 1, 2, \u00b7 \u00b7 \u00b7 N, j = 1, 2, \u00b7 \u00b7 \u00b7 K} is the set of model parameters to estimate. The cluster models can be efficiently estimated using the Expectation-Maximization (EM) algorithm. For its details, we refer the reader to . Table 1 shows the cluster models for TREC query Transportation tunnel disasters (K = 3). Note that only the middle cluster is relevant. Table 1: \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 From each of the K estimated clusters, we choose the L = M/K terms with highest probabilities to form a total of M presentation terms. If a term happens to be in top L in multiple clusters, we assign it to the cluster where it has highest probability and let the other clusters take one more term as compensation. We also filter out terms in the original query text because they tend to always be relevant when the query is short. The selected terms are then presented to the user for judgment. A sample (completed) feedback form is shown in Figure 1. In this study we only deal with binary judgment: a presented term is by default unchecked, and a user may check it to indicate relevance. We also do not explicitly exploit negative feedback (i.e., penalizing irrelevant terms), because with binary feedback an unchecked term is not necessarily irrelevant (maybe the user is unsure about its relevance). We could ask the user for finer judgment (e.g., choosing from highly relevant, somewhat relevant, do not know, somewhat irrelevant and highly irrelevant), but binary feedback is more compact, taking less space to display and less user effort to make judgment. 5. ESTIMATING QUERY MODELS FROM TERM FEEDBACK In this section, we present several algorithms for exploiting term feedback. The algorithms take as input the original query q, the clusters {\u03b8i} as generated by the theme discovery algorithm, the set of feedback terms T and their relevance judgment R, and outputs an updated query language model \u03b8q that makes best use of the feedback evidence to capture the user\"s information need. First we describe our notations: \u2022 \u03b8q: The original query model, derived from query terms only: p(w|\u03b8q) = c(w; q) |q| where c(w; q) is the count of w in q, and |q| = w\u2208q c(w; q) is the query length. \u2022 \u03b8q : The updated query model which we need to estimate from term feedback. \u2022 \u03b8i (i = 1, 2, . . . K): The unigram language model of cluster Ci, as estimated using the theme discovery algorithm. \u2022 T = {ti,j} (i = 1 . . . K, j = 1 . . . L): The set of terms presented to the user for judgment. ti,j is the j-th term chosen from cluster Ci. \u2022 R = {\u03b4w|w \u2208 T}: \u03b4w is an indicator variable that is 1 if w is judged relevant or 0 otherwise. 5.1 TFB (Direct Term Feedback) This is a straight-forward form of term feedback that does not involve any secondary structure. We give a weight of 1 to terms judged relevant by the user, a weight of \u03bc to query terms, zero weight to other terms, and then apply normalization: p(w|\u03b8q ) = \u03b4w + \u03bc c(w; q) w \u2208T \u03b4w + \u03bc|q| where w \u2208T \u03b4w is the total number of terms that are judged relevant. We call this method TFB (direct Term FeedBack). If we let \u03bc = 1, this approach is equivalent to appending the relevant terms after the original query, which is what standard query expansion (without term reweighting) does. If we set \u03bc > 1, we are putting more emphasis on the query terms than the checked ones. Note that the result model will be more biased toward \u03b8q if the original query is long or the user feedback is weak, which makes sense, as we can trust more on the original query in either case. Figure 1: Filled clarification form for Topic 363 363 transportation tunnel disasters Please select all terms that are relevant to the topic. traffic railway harbor rail bridge kilometer construct swiss cross link kong hong river project meter shanghai fire truck french smoke car italian firefights blaze blanc mont victim franc rescue driver chamonix emerge toll amtrak train airport turnpike lui jersey pass rome z center electron road boston speed bu submit 5.2 CFB (Cluster Feedback) Here we exploit the cluster structure that played an important role when we selected the presentation terms. The clusters represent different aspects of the query topic, each of which may or may not be relevant. If we are able to identify the relevant clusters, we can combine them to generate a query model that is good at discovering documents belonging to these clusters (instead of the irrelevant ones). We could ask the user to directly judge the relevance of a cluster after viewing representative terms in that cluster, but this would sometimes be a difficult task for the user, who has to guess the semantics of a cluster via its set of terms, which may not be well connected to one another due to a lack of context. Therefore, we propose to learn cluster feedback indirectly, inferring the relevance of a cluster through the relevance of its feedback terms. Because each cluster has an equal number of terms presented to the user, the simplest measure of a cluster\"s relevance is the number of terms that are judged relevant in it. Intuitively, the more terms are marked relevant in a cluster, the closer the cluster is to the query topic, and the more the cluster should participate in query modification. If we combine the cluster models using weights determined this way and then interpolate with the original query model, we get the following formula for query updating, which we call CFB (Cluster FeedBack): p(w|\u03b8q ) = \u03bbp(w|\u03b8q) + (1 \u2212 \u03bb) i=1 j=1 \u03b4ti,j k=1 j=1 \u03b4tk,j p(w|\u03b8i) where L j=1 \u03b4ti,j is the number of relevant terms in cluster Ci, and k=1 j=1 \u03b4tk,j is the total number of relevant terms. We note that when there is only one cluster (K = 1), the above formula degenerates to p(w|\u03b8q ) = \u03bbp(w|\u03b8q) + (1 \u2212 \u03bb)p(w|\u03b81) which is merely pseudo-feedback of the form proposed in . 5.3 TCFB (Term-cluster Feedback) TFB and CFB both have their drawbacks. TFB assigns non-zero probabilities to the presented terms that are marked relevant, but completely ignores (a lot more) others, which may be left unchecked due to the user\"s ignorance, or simply not included in the presentation list, but we should be able to infer their relevance from the checked ones. For example, in Figure 1, since as many as 5 terms in the middle cluster (the third and fourth columns) are checked, we should have high confidence in the relevance of other terms in that cluster. CFB remedies TFB\"s problem by treating the terms in a cluster collectively, so that unchecked/unpresented terms receive weights when presented terms in their clusters are judged as relevant, but it does not distinguish which terms in a cluster are presented or judged. Intuitively, the judged relevant terms should receive larger weights because they are explicitly indicated as relevant by the user. Therefore, we try to combine the two methods, hoping to get the best out of both. We do this by interpolating the TFB model with the CFB model, and call it TCFB: p(w|\u03b8q ) = \u03b1p(w|\u03b8qT F B ) + (1 \u2212 \u03b1)p(w|\u03b8qCF B 6. EXPERIMENTS In this section, we describe our experiment results. We first describe our experiment setup and present an overview of various methods\" performance. Then we discuss the effects of varying the parameter setting in the algorithms, as well as the number of presentation terms. Next we analyze user term feedback behavior and its relation to retrieval performance. Finally we compare term feedback to relevance feedback and show that it has its particular advantage. 6. HARD Track for the evaluation of our algorithms. The tracks used the AQUAINT collection, a 3GB corpus of English newswire text. The topics included 50 ones previously known to be hard, i.e. with low retrieval performance. It is for these hard topics that user feedback is most helpful, as it can provide information to disambiguate the queries; with easy topics the user may be unwilling to spend efforts for feedback if the automatic retrieval results are good enough. Participants of the track were able to submit custom-designed clarification forms (CF) to solicit feedback from human assessors provided by Table 2: Retrieval performance for different methods and CF types. The last row is the percentage of MAP improvement over the baseline. The parameter settings \u03bc = 4, \u03bb = 0.1, \u03b1 = 0.3 are near optimal. Baseline TFB1C TFB3C TFB6C CFB1C CFB3C CFB6C TCFB1C TCFB3C TCFB6C MAP 0.219 0.288 0.288 0.278 0.254 0.305 0.301 0.274 0.309 0.304 Pr@30 0.393 0.467 0.475 0.457 0.399 0.480 0.473 0.431 0.491 0. % 0% 31.5% 31.5% 26.9% 16.0% 39.3% 37.4% 25.1% 41.1% 38.8% Table 3: MAP variation with the number of presented terms. # terms TFB1C TFB3C TFB6C CFB3C CFB6C TCFB3C TCFB6C 6 0.245 0.240 0.227 0.279 0.279 0.281 0.274 12 0.261 0.261 0.242 0.299 0.286 0.297 0.281 18 0.275 0.274 0.256 0.301 0.282 0.300 0.286 24 0.276 0.281 0.265 0.303 0.292 0.305 0.292 30 0.280 0.285 0.270 0.304 0.296 0.307 0.296 36 0.282 0.288 0.272 0.307 0.297 0.309 0.297 42 0.283 0.288 0.275 0.306 0.298 0.309 0.300 48 0.288 0.288 0.278 0.305 0.301 0.309 0.303 NIST. We designed three sets of clarification forms for term feedback, differing in the choice of K, the number of clusters, and L, the number of presented terms from each cluster. They are: 1\u00d7 48, a big cluster with 48 terms, 3 \u00d7 16, 3 clusters with 16 terms each, and 6 \u00d7 8, 6 clusters with 8 terms each. The total number of presented terms (M) is fixed at 48, so by comparing the performance of different types of clarification forms we can know the effects of different degree of clustering. For each topic, an assessor would complete the forms ordered by 6 \u00d7 8, 1 \u00d7 48 and 3 \u00d7 16, spending up to three minutes on each form. The sample clarification form shown in Figure 1 is of type 3 \u00d7 16. It is a simple and compact interface in which the user can check relevant terms. The form is self-explanatory; there is no need for extra user training on how to use it. Our initinal queries are constructed only using the topic title descriptions, which are on average 2.7 words in length. As our baseline we use the KL divergence retrieval method implemented in the Lemur Toolkit1 with 5 pseudo-feedback documents. We stem the terms, and truncate query language models to 50 terms (these settings are used throughout the experiments). For all other parameters we use Lemur\"s default settings. The baseline turns out to perform above average among the track participants. After an initial run using this baseline retrieval method, we take the top 60 documents for each topic and apply the theme discovery algorithm to output the clusters (1, 3, or 6 of them), based on which we generate clarification forms. After user feedback is received, we run the term feedback algorithms (TFB, CFB or TCFB) to estimate updated query models, which are then used for a second iteration of retrieval. We evaluate the different retrieval methods\" documents. The evaluation metrics we adopt include mean average (non-interpolated) precision (MAP), precision at top 30 (Pr@30) and total relevant retrieved (RR). Table 2 shows the performance of various methods and configurations of K \u00d7 L. The suffixes (1C, 3C, 6C) after TFB,CFB,TCFB stand for the number of clusters (K). For example, TCFB3C means the TCFB method on the 3 \u00d7 16 clarification forms. From Table 2 we can make the following observations: 1. All methods perform considerably better than the pseudofeedback baseline, with TCFB3C achieving a highest 41.1% improvement in MAP, indicating significant contribution of term feedback for clarification of the user\"s information need. In other words, term feedback is truly helpful for improving retrieval accuracy. 2. For TFB, the performance is almost equal on the 1 \u00d7 48 and 3 \u00d7 16 clarification forms in terms of MAP (although the latter is slightly better in Pr@30 and RR), and a little worse on the 6 \u00d7 8 ones. 3. Both CFB3C and CFB6C perform better than their TFB counterparts in all three metrics, suggesting that feedback on a secondary cluster structure is indeed beneficial. CFB1C is actually worse because it cannot adjust the weight of its (single) cluster from term feedback and it is merely pseudofeedback. 4. Although TCFB is just a simple mixture of TFB and CFB by interpolation, it is able to outperform both. This supports our speculation that TCFB overcomes the drawbacks of TFB (paying attention only to checked terms) and CFB (not distinguishing checked and unchecked terms in a cluster). Except for TCFB6C v.s. CFB6C, the performance advantage of TCFB over TFB/CFB is significant at p < 0.05 using the Wilcoxon signed rank test. This is not true in the case of TFB v.s. CFB, each of which is better than the other in nearly half of the topics. 6.2 Reduction of Presentation Terms In some situations we may have to reduce the number of presentation terms due to limits in display space or user feedback efforts. It is interesting to know whether our algorithms\" performance deteriorates when the user is presented with fewer terms. Because the presentation terms within each cluster are generated in decreasing order of their frequencies, the presentation list forms a subset of the original one if its size is reduced2 . Therefore, we can easily simulate what happens when the number of presentation terms decreases There are complexities arising from terms appearing in top L of multiple clusters, but these are exceptions from M to M : we will keep all judgments of the top L = M /K terms in each cluster and discard those of others. Table 3 shows the performance of various algorithms as the number of presentation terms ranges from 6 to 48. We find that the performance of TFB is more susceptible to presentation term reduction than that of CFB or TCFB. For example, at 12 terms the MAP of TFB3C is 90.6% of that at 48 terms, while the numbers for CFB3C and TCFB3C are 98.0% and 96.1% respectively. We conjecture the reason to be that while TFB\"s performance heavily depends on how many good terms are chosen for query expansion, CFB only needs a rough estimate of cluster weights to work. Also, the 3 \u00d7 16 clarification forms seem to be more robust than the 6 \u00d7 8 ones: at 12 terms the MAP of TFB6C is 87.1% of that at 48 terms, lower than 90.6% for TFB3C. Similarly, for CFB it is 95.0% against 98.0%. This is natual, as for a large cluster number of 6, it is easier to get into the situation where each cluster gets too few presentation terms to make topic diversification useful. Overall, we are surprised to see that the algorithms are still able to perform reasonably well when the number of presentation terms is small. For example, at only 12 terms CFB3C (the clarification form is of size 3 \u00d7 4) can still improve 36.5% over the baseline, dropping slightly from 39.3% at 48 terms. 6.3 User Feedback Analysis In this part we study several aspects of user\"s term feedback behavior, and whether they are connected to retrieval performance. Figure 2: Clarification form completion time distributions 0\u221230 30\u221260 60\u221290 90\u2212120 120\u2212150 150\u2212180 10 15 20 25 30 35 completion time (seconds) #topics 1\u00d748 3\u00d716 6\u00d78 Figure 2 shows the distribution of time needed to complete a clarification form3 . We see that the user is usually able to finish term feedback within a reasonably short amount of time: for more than half of the topics the clarification form is completed in just 1 minute, and only a small fraction of topics (less than 10% for 1 \u00d7 48 and 3 \u00d7 16) take more than 2 minutes. This suggests that term feedback is suitable for interactive ad-hoc retrieval, where a user usually does not want to spend too much time on providing feedback. We find that a user often makes mistakes when judging term relevance. Sometimes a relevant term may be left out because its connection to the query topic is not obvious to the user. Other times a dubious term may be included but turns out to be irrelevant. Take the topic in Figure 1 for example. There was a fire disaster in Mont The maximal time is 180 seconds, as the NIST assessor would be forced to submit the form at that moment. Table 4: Term selection statistics (topic average) CF Type 1 \u00d7 48 3 \u00d7 16 6 \u00d7 8 # checked terms 14.8 13.3 11.2 # rel. terms 15.0 12.6 11.2 # rel. checked terms 7.9 6.9 5.9 precision 0.534 0.519 0.527 recall 0.526 0.548 0. but the user failed to select such keywords as mont, blanc, french and italian due to his/her ignorance of the event. Indeed, without proper context it would be hard to make perfect judgment. What is then, the extent to which the user is good at term feedback? Does it have serious impact on retrieval performance? To answer these questions, we need a measure of individual terms\" true relevance. We adopt the Simplified KL Divergence metric used in to decide query expansion terms as our term relevance measure: \u03c3KLD(w) = p(w|R) log p(w|R) p(w|\u00acR) where p(w|R) is the probability that a relevant document contains term w, and p(w|\u00acR) is the probability that an irrelevant document contains w, both of which can be easily computed via maximum likelihood estimate given document-level relevance judgment. If \u03c3KLD(w) > 0, w is more likely to appear in relevant documents than irrelevant ones. We consider a term relevant if its Simplified KL Divergence value is greater than a certain threshold \u03c30. We can then define precision and recall of user term judgment accordingly: precision is the fraction of terms checked by the user that are relevant; recall is the fraction of presented relevant terms that are checked by the user. Table 4 shows the number of checked terms, relevant terms and relevant checked terms when \u03c30 is set to 1.0, as well as the precision/recall of user term judgment. Note that when the clarification forms contain more clusters, fewer terms are checked: 14.8 for 1 \u00d7 48, 13.3 for 3 \u00d7 16 and 11.2 for 6\u00d78. Similar pattern holds for relevant terms and relevant checked terms. There seems to be a trade-off between increasing topic diversity by clustering and losing extra relevant terms: when there are more clusters, each of them gets fewer terms to present, which can hurt a major relevant cluster that contains many relevant terms. Therefore, it is not always helpful to have more clusters, e.g., TFB6C is actually worse than TFB1C. The major finding we can make from Table 4 is that the user is not particularly good at identifying relevant terms, which echoes the discovery in . In the case of 3 \u00d7 16 clarification forms, the average number of terms checked as relevant by the user is 13.3 per topic, and the average number of relevant terms whose \u03c3KLD value exceed 1.0 is 12.6. The user is able to recognize only 6.9 of these terms on average. Indeed, the precision and recall of user feedback terms (as defined previously) are far from perfect. On the other hand, If the user had correctly checked all such relevant terms, the performance of our algorithms would have increased a lot, as shown in Table 5. We see that TFB gets big improvement when there is an oracle who checks all relevant terms, while CFB meets a bottleneck around MAP of 0.325, since all it does is adjust cluster weights, and when the learned weights are close to being accurate, it cannot benefit more from term feedback. Also note that TCFB fails to outperform TFB, probably because TFB is sufficiently accurate. Table 5: Change of MAP when using all (and only) relevant terms (\u03c3KLD > 1.0) for feedback. original term feedback relevant term feedback TF1 0.288 0.354 TF3 0.288 0.354 TF6 0.278 0.346 CF3 0.305 0.325 CF6 0.301 0.326 TCF3 0.309 0.345 TCF6 0.304 0.341 6.4 Comparison with Relevance Feedback Now we compare term feedback with document-level relevance feedback, in which the user is presented with the top N documents from an initial retrieval and asked to judge their relevance. The feedback process is simulated using document relevance judgment from NIST. We use the mixture model based feedback method proposed in , with mixture noise set to 0.95 and feedback coefficient set to 0.9. Comparative evaluation of relevance feedback against other methods is complicated by the fact that some documents have already been viewed during feedback, so it makes no sense to include them in the retrieval results of the second run. However, this does not hold for term feedback. Thus, to make it fair w.r.t. user\"s information gain, if the feedback documents are relevant, they should be kept in the top of the ranking; if they are irrelevant, they should be left out. Therefore, retrieved documents but with every feedback document excluded, and then prepend the relevant feedback documents at the front. Table 6 shows the performance of relevance feedback for different values of N and compares it with TCFB3C. Table 6: Performance of relevance feedback for different number of feedback documents (N). N MAP Pr@30 RR 5 0.302 0. 10 0.345 0. 20 0.389 0. TCFB3C 0.309 0. We see that the performance of TCFB3C is comparable to that of relevance feedback using 5 documents. Although it is poorer than when there are 10 feedback documents in terms of MAP and Pr@30, it does retrieve more documents when going down the ranked list. We try to compare the quality of automatically inserted terms in relevance feedback with that of manually selected terms in term feedback. This is done by truncating the relevance feedback modified query model to a size equal to the number of checked terms for the same topic. We can then compare the terms in the truncated model with the checked terms. Figure 3 shows the distribution of the terms\" \u03c3KLD scores. We find that term feedback tends to produce expansion terms of higher quality(those with \u03c3KLD > 1) compared to relevance feedback (with 10 feedback documents). This does not contradict the fact that the latter yields higher retrieval performance. Actually, when we use the truncated query model instead of the intact one refined from relevance feedback, the MAP is only 0.304. The truth Figure 3: Comparison of expansion term quality between relevance feedback (with 10 feedback documents) and term feedback (with 3 \u00d7 16 CFs) \u22121\u22120 0\u22121 1\u22122 2\u22123 3\u22124 4\u22125 5\u22126 50 100 150 200 250 300 350 \u03c3KLD #terms relevance feedback term feedback is, although there are many unwanted terms in the expanded query model from feedback documents, there are also more relevant terms than what the user can possibly select from the list of presentation terms generated with pseudo-feedback documents, and the positive effects often outweights the negative ones. We are interested to know under what circumstances term feedback has advantage over relevance feedback. One such situation is when none of the top N feedback documents is relevant, rendering relevance feedback useless. This is not infrequent, as one might have thought: out of the 50 topics, there are 13 such cases when N = 5, 10 when N = 10, and still 3 when N = 20. When this happens, one can only back off to the original retrieval method; the power of relevance feedback is lost. Surprisingly, in 11 out of 13 such cases where relevance feedback seems impossible, the user is able to check at least 2 relevant terms from the 3 \u00d7 16 clarification form (we consider term t to be relevant if \u03c3KLD(t) > 1.0). Furthermore, in 10 out of them TCFB3C outperforms the pseudo-feedback baseline, increasing MAP from 0.076 to 0.146 on average (these are particularly hard topics). We think that there are two possible explanations for this phenomenon of term feedback being active even when relevance feedback does not work: First, even if none of the top N (suppose it is a small number) documents are relevant, we may still find relevant documents in top 60, which is more inclusive but usually unreachable when people are doing relevance feedback in interactive ad-hoc search, from which we can draw feedback terms. This is true for topic 367 piracy, where the top 10 feedback documents are all about software piracy, yet there are documents between 10-60 that are about piracy on the seas (which is about the real information need), contributing terms such as pirate, ship for selection in the clarification form. Second, for some topics, a document needs to meet some special condition in order to be relevant. The top N documents may be related to the topic, but nonetheless irrelevant. In this case, we may still extract useful terms from these documents, even if they do not qualify as relevant ones. For example, in topic 639 consumer online shopping, a document needs to mention what contributes to shopping growth to really match the specified information need, hence none of the top 10 feedback documents are regarded as relevant. But nevertheless, the feedback terms such as retail, commerce are good for query expansion. 7. CONCLUSIONS In this paper we studied the use of term feedback for interactive information retrieval in the language modeling approach. We proposed a cluster-based method for selecting presentation terms as well as algorithms to estimate refined query models from user term feedback. We saw significant improvement in retrieval accuracy brought by term feedback, in spite of the fact that a user often makes mistakes in relevance judgment that hurts its performance. We found the best-performing algorithm to be TCFB, which benefits from the combination of directly observed term evidence with TFB and indirectly learned cluster relevance with CFB. When we reduced the number of presentation terms, term feedback is still able to keep much of its performance gain over the baseline. Finally, we compared term feedback to document-level relevance feedback, and found that TCFB3C\"s performance is on a par with the latter with 5 feedback documents. We regarded term feedback as a viable alternative to traditional relevance feedback, especially when there are no relevant documents in the top. We propose to extend our work in several ways. First, we want to study whether the use of various contexts can help the user to better identify term relevance, while not sacrificing the simplicity and compactness of term feedback. Second, currently all terms are presented to the user in a single batch. We could instead consider iterative term feedback, by presenting a small number of terms first, and show more terms after receiving user feedback or stop when the refined query is good enough. The presented terms should be selected dynamically to maximize learning benefits at any moment. Third, we have plans to incorporate term feedback into our UCAIR toolbar, an Internet Explorer plugin, to make it work for web search. We are also interested in studying how to combine term feedback with relevance feedback or implicit feedback. We could, for example, allow the user to dynamically modify terms in a language model learned from feedback documents. 8. ACKNOWLEDGMENT This work is supported in part by the National Science Foundation grants IIS- and IIS-.", "body1": "In the language modeling approach to information retrieval, feedback is often modeled as estimating an improved query model or relevance model based on a set of feedback documents . This is in line with the traditional way of doing relevance feedback - presenting a user with documents/passages for relevance judgment and then extracting terms from the judged documents or passages to expand the initial query. We can consider a more direct way to involve a user in query model improvement, without an intermediary step of document feedback that can introduce noise. HARD Track and continued study afterward, we explored how to exploit term feedback from the user to construct improved query models for information retrieval in the language modeling approach. The rest of the paper is organized as follows. Relevance feedback has long been recognized as an effective method for improving retrieval performance. Because document is a large text unit, when it is used for relevance feedback many irrelevant terms can be introduced into the feedback process. In many cases term relevance feedback has been found to effectively improve retrieval performance. Our work differs from the previous ones in two important aspects. The usual way for feedback term presentation is just to display the terms in a list. We follow the language modeling approach, and base our method on the KL-divergence retrieval model proposed in . Proper selection of terms to be presented to the user for judgment is crucial to the success of term feedback. In our approach, the top N documents from an initial retrieval using the original query form the source of feedback terms: all terms that appear in them are considered candidates to present to the user. The simplest way of selecting feedback terms is to choose the most frequent M terms from the N documents. First, we introduce a background model \u03b8B that is estimated from collection statistics and explains the common terms, so that they are much less likely to appear in the presentation list. We rely on the mixture multinomial model, which is used for theme discovery in . Table 1: \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 From each of the K estimated clusters, we choose the L = M/K terms with highest probabilities to form a total of M presentation terms. In this study we only deal with binary judgment: a presented term is by default unchecked, and a user may check it to indicate relevance. In this section, we present several algorithms for exploiting term feedback. \u2022 \u03b8q : The updated query model which we need to estimate from term feedback. \u2022 \u03b8i (i = 1, 2, . \u2022 T = {ti,j} (i = 1 . 5.1 TFB (Direct Term Feedback) This is a straight-forward form of term feedback that does not involve any secondary structure. Note that the result model will be more biased toward \u03b8q if the original query is long or the user feedback is weak, which makes sense, as we can trust more on the original query in either case. Figure 1: Filled clarification form for Topic 363 363 transportation tunnel disasters Please select all terms that are relevant to the topic. traffic railway harbor rail bridge kilometer construct swiss cross link kong hong river project meter shanghai fire truck french smoke car italian firefights blaze blanc mont victim franc rescue driver chamonix emerge toll amtrak train airport turnpike lui jersey pass rome z center electron road boston speed bu submit 5.2 CFB (Cluster Feedback) Here we exploit the cluster structure that played an important role when we selected the presentation terms. Because each cluster has an equal number of terms presented to the user, the simplest measure of a cluster\"s relevance is the number of terms that are judged relevant in it. 5.3 TCFB (Term-cluster Feedback) TFB and CFB both have their drawbacks. In this section, we describe our experiment results. 6. HARD Track for the evaluation of our algorithms. Participants of the track were able to submit custom-designed clarification forms (CF) to solicit feedback from human assessors provided by Table 2: Retrieval performance for different methods and CF types. # terms TFB1C TFB3C TFB6C CFB3C CFB6C TCFB3C TCFB6C 6 0.245 0.240 0.227 0.279 0.279 0.281 0.274 12 0.261 0.261 0.242 0.299 0.286 0.297 0.281 18 0.275 0.274 0.256 0.301 0.282 0.300 0.286 24 0.276 0.281 0.265 0.303 0.292 0.305 0.292 30 0.280 0.285 0.270 0.304 0.296 0.307 0.296 36 0.282 0.288 0.272 0.307 0.297 0.309 0.297 42 0.283 0.288 0.275 0.306 0.298 0.309 0.300 48 0.288 0.288 0.278 0.305 0.301 0.309 0.303 NIST. Our initinal queries are constructed only using the topic title descriptions, which are on average 2.7 words in length. We evaluate the different retrieval methods\" documents. In other words, term feedback is truly helpful for improving retrieval accuracy. 2. 3. 4. Except for TCFB6C v.s. It is interesting to know whether our algorithms\" performance deteriorates when the user is presented with fewer terms. We find that the performance of TFB is more susceptible to presentation term reduction than that of CFB or TCFB. 6.3 User Feedback Analysis In this part we study several aspects of user\"s term feedback behavior, and whether they are connected to retrieval performance. Figure 2: Clarification form completion time distributions 0\u221230 30\u221260 60\u221290 90\u2212120 120\u2212150 150\u2212180 10 15 20 25 30 35 completion time (seconds) #topics 1\u00d748 3\u00d716 6\u00d78 Figure 2 shows the distribution of time needed to complete a clarification form3 . We find that a user often makes mistakes when judging term relevance. Table 4: Term selection statistics (topic average) CF Type 1 \u00d7 48 3 \u00d7 16 6 \u00d7 8 # checked terms 14.8 13.3 11.2 # rel. What is then, the extent to which the user is good at term feedback? We consider a term relevant if its Simplified KL Divergence value is greater than a certain threshold \u03c30. Note that when the clarification forms contain more clusters, fewer terms are checked: 14.8 for 1 \u00d7 48, 13.3 for 3 \u00d7 16 and 11.2 for 6\u00d78. The major finding we can make from Table 4 is that the user is not particularly good at identifying relevant terms, which echoes the discovery in . Table 5: Change of MAP when using all (and only) relevant terms (\u03c3KLD > 1.0) for feedback. original term feedback relevant term feedback TF1 0.288 0.354 TF3 0.288 0.354 TF6 0.278 0.346 CF3 0.305 0.325 CF6 0.301 0.326 TCF3 0.309 0.345 TCF6 0.304 0.341 6.4 Comparison with Relevance Feedback Now we compare term feedback with document-level relevance feedback, in which the user is presented with the top N documents from an initial retrieval and asked to judge their relevance. Comparative evaluation of relevance feedback against other methods is complicated by the fact that some documents have already been viewed during feedback, so it makes no sense to include them in the retrieval results of the second run. N MAP Pr@30 RR 5 0.302 0. 10 0.345 0. 20 0.389 0. TCFB3C 0.309 0. We see that the performance of TCFB3C is comparable to that of relevance feedback using 5 documents. We try to compare the quality of automatically inserted terms in relevance feedback with that of manually selected terms in term feedback. is, although there are many unwanted terms in the expanded query model from feedback documents, there are also more relevant terms than what the user can possibly select from the list of presentation terms generated with pseudo-feedback documents, and the positive effects often outweights the negative ones. We are interested to know under what circumstances term feedback has advantage over relevance feedback. Surprisingly, in 11 out of 13 such cases where relevance feedback seems impossible, the user is able to check at least 2 relevant terms from the 3 \u00d7 16 clarification form (we consider term t to be relevant if \u03c3KLD(t) > 1.0). This is true for topic 367 piracy, where the top 10 feedback documents are all about software piracy, yet there are documents between 10-60 that are about piracy on the seas (which is about the real information need), contributing terms such as pirate, ship for selection in the clarification form.", "body2": "In the language modeling approach to information retrieval, feedback is often modeled as estimating an improved query model or relevance model based on a set of feedback documents . For example, for the TREC query Hubble telescope achievements, when a relevant document talks more about the telescope\"s repair than its discoveries, irrelevant terms such as spacewalk can be added into the modified query. In this case, relevance feedback is useless, as no relevant document can be leveraged on, but term feedback is still often helpful, by allowing relevant terms to be picked from irrelevant documents. Finally, by comparing term feedback with document-level feedback, we found it to be a viable alternative to the latter with competitive retrieval performance. Section 7 concludes this paper. In cases where true relevance judgment is unavailable and all top N documents are assumed to be relevant, it is called blind or pseudo feedback and usually still brings performance improvement. This is categorized as interactive query expansion, where the original query is augmented with user-provided terms, which can come from direct user input (free-form text or keywords) or user selection of system-suggested terms (using thesauri or extracted from feedback documents). The user is found to be not good at identifying useful terms for query expansion, when a simple term presentation interface is unable to provide sufficient semantic context of the feedback terms. The combination of the two aspects allows our method to perform much better than the baseline. We focus on term presentation and query model construction from feedback terms, and believe using contexts to improve feedback term quality should be orthogonal to our method. Second, how to compute an updated query model based on this term feedback evidence, so that it captures the user\"s information need and translates into good retrieval performance. diversely so as to increase coverage). Due to the latter reason, it is possible to make N quite large (e.g., in our experiments we set N = 60) to increase its coverage of different aspects in the topic. We solve the above problems by two corresponding measures. Second, the terms are selected from multiple clusters in the pseudo-feedback documents, to ensure sufficient representation of different aspects of the topic. Note that only the middle cluster is relevant. A sample (completed) feedback form is shown in Figure 1. We could ask the user for finer judgment (e.g., choosing from highly relevant, somewhat relevant, do not know, somewhat irrelevant and highly irrelevant), but binary feedback is more compact, taking less space to display and less user effort to make judgment. First we describe our notations: \u2022 \u03b8q: The original query model, derived from query terms only: p(w|\u03b8q) = c(w; q) |q| where c(w; q) is the count of w in q, and |q| = w\u2208q c(w; q) is the query length. \u2022 \u03b8q : The updated query model which we need to estimate from term feedback. K): The unigram language model of cluster Ci, as estimated using the theme discovery algorithm. \u2022 R = {\u03b4w|w \u2208 T}: \u03b4w is an indicator variable that is 1 if w is judged relevant or 0 otherwise. If we set \u03bc > 1, we are putting more emphasis on the query terms than the checked ones. Note that the result model will be more biased toward \u03b8q if the original query is long or the user feedback is weak, which makes sense, as we can trust more on the original query in either case. Figure 1: Filled clarification form for Topic 363 363 transportation tunnel disasters Please select all terms that are relevant to the topic. Therefore, we propose to learn cluster feedback indirectly, inferring the relevance of a cluster through the relevance of its feedback terms. We note that when there is only one cluster (K = 1), the above formula degenerates to p(w|\u03b8q ) = \u03bbp(w|\u03b8q) + (1 \u2212 \u03bb)p(w|\u03b81) which is merely pseudo-feedback of the form proposed in . Therefore, we try to combine the two methods, hoping to get the best out of both. Finally we compare term feedback to relevance feedback and show that it has its particular advantage. It is for these hard topics that user feedback is most helpful, as it can provide information to disambiguate the queries; with easy topics the user may be unwilling to spend efforts for feedback if the automatic retrieval results are good enough. Baseline TFB1C TFB3C TFB6C CFB1C CFB3C CFB6C TCFB1C TCFB3C TCFB6C MAP 0.219 0.288 0.288 0.278 0.254 0.305 0.301 0.274 0.309 0.304 Pr@30 0.393 0.467 0.475 0.457 0.399 0.480 0.473 0.431 0.491 0. % 0% 31.5% 31.5% 26.9% 16.0% 39.3% 37.4% 25.1% 41.1% 38.8% Table 3: MAP variation with the number of presented terms. The form is self-explanatory; there is no need for extra user training on how to use it. After user feedback is received, we run the term feedback algorithms (TFB, CFB or TCFB) to estimate updated query models, which are then used for a second iteration of retrieval. All methods perform considerably better than the pseudofeedback baseline, with TCFB3C achieving a highest 41.1% improvement in MAP, indicating significant contribution of term feedback for clarification of the user\"s information need. In other words, term feedback is truly helpful for improving retrieval accuracy. For TFB, the performance is almost equal on the 1 \u00d7 48 and 3 \u00d7 16 clarification forms in terms of MAP (although the latter is slightly better in Pr@30 and RR), and a little worse on the 6 \u00d7 8 ones. CFB1C is actually worse because it cannot adjust the weight of its (single) cluster from term feedback and it is merely pseudofeedback. This supports our speculation that TCFB overcomes the drawbacks of TFB (paying attention only to checked terms) and CFB (not distinguishing checked and unchecked terms in a cluster). 6.2 Reduction of Presentation Terms In some situations we may have to reduce the number of presentation terms due to limits in display space or user feedback efforts. Table 3 shows the performance of various algorithms as the number of presentation terms ranges from 6 to 48. For example, at only 12 terms CFB3C (the clarification form is of size 3 \u00d7 4) can still improve 36.5% over the baseline, dropping slightly from 39.3% at 48 terms. 6.3 User Feedback Analysis In this part we study several aspects of user\"s term feedback behavior, and whether they are connected to retrieval performance. This suggests that term feedback is suitable for interactive ad-hoc retrieval, where a user usually does not want to spend too much time on providing feedback. There was a fire disaster in Mont The maximal time is 180 seconds, as the NIST assessor would be forced to submit the form at that moment. Indeed, without proper context it would be hard to make perfect judgment. If \u03c3KLD(w) > 0, w is more likely to appear in relevant documents than irrelevant ones. Table 4 shows the number of checked terms, relevant terms and relevant checked terms when \u03c30 is set to 1.0, as well as the precision/recall of user term judgment. Therefore, it is not always helpful to have more clusters, e.g., TFB6C is actually worse than TFB1C. Also note that TCFB fails to outperform TFB, probably because TFB is sufficiently accurate. Table 5: Change of MAP when using all (and only) relevant terms (\u03c3KLD > 1.0) for feedback. We use the mixture model based feedback method proposed in , with mixture noise set to 0.95 and feedback coefficient set to 0.9. Table 6: Performance of relevance feedback for different number of feedback documents (N). Although it is poorer than when there are 10 feedback documents in terms of MAP and Pr@30, it does retrieve more documents when going down the ranked list. Figure 3 shows the distribution of the terms\" \u03c3KLD scores. is, although there are many unwanted terms in the expanded query model from feedback documents, there are also more relevant terms than what the user can possibly select from the list of presentation terms generated with pseudo-feedback documents, and the positive effects often outweights the negative ones. When this happens, one can only back off to the original retrieval method; the power of relevance feedback is lost. We think that there are two possible explanations for this phenomenon of term feedback being active even when relevance feedback does not work: First, even if none of the top N (suppose it is a small number) documents are relevant, we may still find relevant documents in top 60, which is more inclusive but usually unreachable when people are doing relevance feedback in interactive ad-hoc search, from which we can draw feedback terms. But nevertheless, the feedback terms such as retail, commerce are good for query expansion.", "introduction": "In the language modeling approach to information retrieval, feedback is often modeled as estimating an improved query model or relevance model based on a set of feedback documents . This is in line with the traditional way of doing relevance feedback - presenting a user with documents/passages for relevance judgment and then extracting terms from the judged documents or passages to expand the initial query. It is an indirect way of seeking user\"s assistance for query model construction, in the sense that the refined query model (based on terms) is learned through feedback documents/passages, which are high-level structures of terms. It has the disadvantage that irrelevant terms, which occur along with relevant ones in the judged content, may be erroneously used for query expansion, causing undesired effects. For example, for the TREC query Hubble telescope achievements, when a relevant document talks more about the telescope\"s repair than its discoveries, irrelevant terms such as spacewalk can be added into the modified query. We can consider a more direct way to involve a user in query model improvement, without an intermediary step of document feedback that can introduce noise. The idea is to present a (reasonable) number of individual terms to the user and ask him/her to judge the relevance of each term or directly specify their probabilities in the query model. This strategy has been discussed in , but to our knowledge, it has not been seriously studied in existing language modeling literature. Compared to traditional relevance feedback, this term-based approach to interactive query model refinement has several advantages. First, the user has better control of the final query model through direct manipulation of terms: he/she can dictate which terms are relevant, irrelevant, and possibly, to what degree. This avoids the risk of bringing unwanted terms into the query model, although sometimes the user introduces low-quality terms. Second, because a term takes less time to judge than a document\"s full text or summary, and as few as around 20 presented terms can bring significant improvement in retrieval performance (as we will show later), term feedback makes it faster to gather user feedback. This is especially helpful for interactive adhoc search. Third, sometimes there are no relevant documents in the top N of the initially retrieved results if the topic is hard. This is often true when N is constrained to be small, which arises from the fact that the user is unwilling to judge too many documents. In this case, relevance feedback is useless, as no relevant document can be leveraged on, but term feedback is still often helpful, by allowing relevant terms to be picked from irrelevant documents. HARD Track and continued study afterward, we explored how to exploit term feedback from the user to construct improved query models for information retrieval in the language modeling approach. We identified two key subtasks of term-based feedback, i.e., pre-feedback presentation term selection and post-feedback query model construction, with effective algorithms developed for both. We imposed a secondary cluster structure on terms and found that a cluster view sheds additional insight into the user\"s information need, and provides a good way of utilizing term feedback. Through experiments we found that term feedback improves significantly over the nonfeedback baseline, even though the user often makes mistakes in relevance judgment. Among our algorithms, the one with best retrieval performance is TCFB, the combination of TFB, the direct term feedback algorithm, and CFB, the cluster-based feedback algorithm. We also varied the number of feedback terms and observed reasonable improvement even at low numbers. Finally, by comparing term feedback with document-level feedback, we found it to be a viable alternative to the latter with competitive retrieval performance. The rest of the paper is organized as follows. Section 2 discusses some related work. Section 4 outlines our general approach to term feedback. We present our method for presentation term selection in Section 3 and algorithms for query model construction in Section 5. The experiment results are given in Section 6.", "conclusion": "In this paper we studied the use of term feedback for interactive information retrieval in the language modeling approach.. We proposed a cluster-based method for selecting presentation terms as well as algorithms to estimate refined query models from user term feedback.. We saw significant improvement in retrieval accuracy brought by term feedback, in spite of the fact that a user often makes mistakes in relevance judgment that hurts its performance.. We found the best-performing algorithm to be TCFB, which benefits from the combination of directly observed term evidence with TFB and indirectly learned cluster relevance with CFB.. When we reduced the number of presentation terms, term feedback is still able to keep much of its performance gain over the baseline.. Finally, we compared term feedback to document-level relevance feedback, and found that TCFB3C\"s performance is on a par with the latter with 5 feedback documents.. We regarded term feedback as a viable alternative to traditional relevance feedback, especially when there are no relevant documents in the top.. We propose to extend our work in several ways.. First, we want to study whether the use of various contexts can help the user to better identify term relevance, while not sacrificing the simplicity and compactness of term feedback.. Second, currently all terms are presented to the user in a single batch.. We could instead consider iterative term feedback, by presenting a small number of terms first, and show more terms after receiving user feedback or stop when the refined query is good enough.. The presented terms should be selected dynamically to maximize learning benefits at any moment.. Third, we have plans to incorporate term feedback into our UCAIR toolbar, an Internet Explorer plugin, to make it work for web search.. We are also interested in studying how to combine term feedback with relevance feedback or implicit feedback.. We could, for example, allow the user to dynamically modify terms in a language model learned from feedback documents.. ACKNOWLEDGMENT This work is supported in part by the National Science Foundation grants IIS- and IIS-."}
{"id": "J-2", "keywords": ["mechan design", "vickrei-clark-grove mechan", "payment redistribut"], "title": "Worst-Case Optimal Redistribution of VCG Payments", "abstract": "For allocation problems with one or more items, the well-known Vickrey-Clarke-Groves (VCG) mechanism is efficient, strategy-proof, individually rational, and does not incur a deficit. However, the VCG mechanism is not (strongly) budget balanced: generally, the agents' payments will sum to more than 0. If there is an auctioneer who is selling the items, this may be desirable, because the surplus payment corresponds to revenue for the auctioneer. However, if the items do not have an owner and the agents are merely interested in allocating the items efficiently among themselves, any surplus payment is undesirable, because it will have to flow out of the system of agents. In 2006, Cavallo [3] proposed a mechanism that redistributes some of the VCG payment back to the agents, while maintaining efficiency, strategy-proofness, individual rationality, and the non-deficit property. In this paper, we extend this result in a restricted setting. We study allocation settings where there are multiple indistinguishable units of a single good, and agents have unit demand. (For this specific setting, Cavallo's mechanism coincides with a mechanism proposed by Bailey in 1997 [2].) Here we propose a family of mechanisms that redistribute some of the VCG payment back to the agents. All mechanisms in the family are efficient, strategy-proof, individually rational, and never incur a deficit. The family includes the Bailey-Cavallo mechanism as a special case. We then provide an optimization model for finding the optimal mechanism--that is, the mechanism that maximizes redistribution in the worst case--inside the family, and show how to cast this model as a linear program. We give both numerical and analytical solutions of this linear program, and the (unique) resulting mechanism shows significant improvement over the Bailey-Cavallo mechanism (in the worst case). Finally, we prove that the obtained mechanism is optimal among all anonymous deterministic mechanisms that satisfy the above properties.", "references": ["Derandomization of auctions", "The demand revealing process: to distribute the surplus", "Optimal decision-making with minimal waste: Strategyproof redistribution of VCG payments", "Multipart pricing of public goods", "A budget-balanced, incentive-compatible scheme for social choice", "Sharing the cost of muliticast transmissions", "Pricing WiFi at Starbucks - Issues in online mechanism design", "Competitive auctions", " Competitive auctions and digital goods", "Collusive bidder behavior at single-object second-price and English auctions", "Characterization of satisfactory mechanisms for the revelation of preferences for public goods", "Incentives in teams", "Online auctions with re-usable goods", "Adaptive limited-supply online auctions", "From optimal limited to unlimited supply auctions", "On the existence of allocation systems whose manipulative Nash equilibria are Pareto optimal", "Bidding clubs in first-price auctions", "Optimal multi-unit auctions", "Efficient and strategy-proof assignment with a cheap residual claimant", "Optimal auction design", "Efficient mechanisms for bilateral trading", "Achieving budget-balance with Vickrey-based payment schemes in exchanges", "Counterspeculation, auctions, and competitive sealed tenders"], "full_text": "1. INTRODUCTION Many important problems in computer science and electronic commerce can be modeled as resource allocation problems. In such problems, we want to allocate the resources (or items) to the agents that value them the most. Unfortunately, agents\" valuations are private knowledge, and self-interested agents will lie about their valuations if this is to their benefit. One solution is to auction off the items, possibly in a combinatorial auction where agents can bid on bundles of items. There exist ways of determining the payments that the agents make in such an auction that incentivizes the agents to report their true valuations-that is, the payments make the auction strategy-proof. One very general way of doing so is to use the VCG mechanism . (The VCG mechanism is also known as the Clarke mechanism or, in the specific context of auctions, the Generalized Vickrey Auction.) Besides strategy-proofness, the VCG mechanism has several other nice properties in the context of resource allocation problems. It is efficient: the chosen allocation always maximizes the sum of the agents\" valuations. It is also (expost) individually rational: participating in the mechanism never makes an agent worse off than not participating. Finally, it has a no-deficit property: the sum of the agents\" payments is always nonnegative. In many settings, another property that would be desirable is (strong) budget balance, meaning that the payments sum to exactly 0. Suppose the agents are trying to distribute some resources among themselves that do not have a previous owner. For example, the agents may be trying to allocate the right to use a shared good on a given day. Or, the agents may be trying to allocate a resource that they have collectively constructed, discovered, or otherwise obtained. If the agents use an auction to allocate these resources, and the sum of the agents\" payments in the auction is positive, then this surplus payment must leave the system 30 of the agents (for example, the agents must give the money to an outside party, or burn it). Na\u00a8\u0131ve redistribution of the surplus payment (e.g. each of the n agents receives 1/n of the surplus) will generally result in a mechanism that is not strategy-proof (e.g. in a Vickrey auction, the second-highest bidder would want to increase her bid to obtain a larger redistribution payment). Unfortunately, the VCG mechanism is not budget balanced: typically, there is surplus payment. Unfortunately, in general settings, it is in fact impossible to design mechanisms that satisfy budget balance in addition to the other desirable properties . In light of this impossibility result, several authors have obtained budget balance by sacrificing some of the other desirable properties . Another approach that is perhaps preferable is to use a mechanism that is more budget balanced than the VCG mechanism, and maintains all the other desirable properties. One way of trying to design such a mechanism is to redistribute some of the VCG payment back to the agents in a way that will not affect the agents\" incentives (so that strategy-proofness is maintained), and that will maintain the other properties. Cavallo pursued exactly this idea, and designed a mechanism that redistributes a large amount of the total VCG payment while maintaining all of the other desirable properties of the VCG mechanism. For example, in a single-item auction (where the VCG mechanism coincides with the second-price sealed-bid auction), the amount redistributed to bidder i by Cavallo\"s mechanism is 1/n times the second-highest bid among bids other than i\"s bid. The total redistributed is at most the second-highest bid overall, and the redistribution to agent i does not affect i\"s incentives because it does not depend on i\"s own bid. In this paper, we restrict our attention to a limited setting, and in this setting we extend Cavallo\"s result. We study allocation settings where there are multiple indistinguishable units of a single good, and all agents have unit demand, i.e. they want only a single unit. For this specific setting, Cavallo\" . Here we propose the family of linear VCG redistribution mechanisms. All mechanisms in this family are efficient, strategy-proof, individually rational, and never incur a deficit. The family includes the Bailey-Cavallo mechanism as a special case (with the caveat that we only study allocation settings with multiple indistinguishable units of a single good and unit demand, while Bailey\"s and Cavallo\"s mechanisms can be applied outside these settings as well). We then provide an optimization model for finding the optimal mechanism inside the family, based on worst-case analysis. Both numerical and analytical solutions of this model are provided, and the resulting mechanism shows significant improvement over the BaileyCavallo mechanism (in the worst case). For example, for the problem of allocating a single unit, when the number of agents is 10, our mechanism always redistributes more than 98% of the total VCG payment back to the agents (whereas the Bailey-Cavallo mechanism redistributes only 80% in the worst case). Finally, we prove that our mechanism is in fact optimal among all anonymous deterministic mechanisms (even nonlinear ones) that satisfy the desirable properties. Around the same time, the same mechanism has been independently derived by Moulin .1 Moulin actually pursues a different objective (also based on worst-case analysis): whereas our objective is to maximize the percentage of VCG payments that are redistributed, Moulin tries to minimize the overall payments from agents as a percentage of efficiency. It turns out that the resulting mechanisms are the same. Towards the end of this paper, we consider dropping the individual rationality requirement, and show that this does not change the optimal mechanism for our objective. For Moulin\"s objective, dropping individual rationality does change the optimal mechanism (but only if there are multiple units). 2. PROBLEM DESCRIPTION Let n denote the number of agents, and let m denote the number of units. We only consider the case where m < n (otherwise the problem becomes trivial). We also assume that m and n are always known. (This assumption is not harmful: in environments where anyone can join the auction, running a redistribution mechanism is typically not a good idea anyway, because everyone would want to join to collect part of the redistribution.) Let the set of agents be {a1, a2, . . . , an}, where ai is the agent with ith highest report value \u02c6vi-that is, we have \u02c6v1 \u2265 \u02c6v2 \u2265 . . . \u2265 \u02c6vn \u2265 0. Let vi denote the true value of ai. Given that the mechanism is strategy-proof, we can assume vi = \u02c6vi. Under the VCG mechanism, each agent among a1, . . . , am wins a unit, and pays \u02c6vm+1 for this unit. Thus, the total VCG payment equals m\u02c6vm+1. When m = 1, this is the second-price or Vickrey auction. We modify the mechanism as follows. After running the original VCG mechanism, the center returns to each agent ai some amount zi, agent ai\"s redistribution payment. We do not allow zi to depend on \u02c6vi; because of this, ai\"s incentives are unaffected by this redistribution payment, and the mechanism remains strategy-proof. 3. LINEAR VCG REDISTRIBUTION MECHANISMS We are now ready to introduce the family of linear VCG redistribution mechanisms. Such a mechanism is defined by a vector of constants c0, c1, . . . , cn\u22121. The amount that the mechanism returns to agent ai is zi = c0 + c1\u02c6v1 + c2\u02c6v2 + . . . + ci\u22121\u02c6vi\u22121 + ci\u02c6vi+1 + . . . + cn\u22121\u02c6vn. That is, an agent receives c0, plus c1 times the highest bid other than the agent\"s own bid, plus c2 times the second-highest other bid, etc. The mechanism is strategy-proof, because for all i, zi is independent of \u02c6vi. Also, the mechanism is anonymous. It is helpful to see the entire list of redistribution payments: z1 = c0 + c1\u02c6v2 + c2\u02c6v3 + c3\u02c6v4 + . . . + cn\u22122\u02c6vn\u22121 + cn\u22121\u02c6vn z2 = c0 + c1\u02c6v1 + c2\u02c6v3 + c3\u02c6v4 + . . . + cn\u22122\u02c6vn\u22121 + cn\u22121\u02c6vn z3 = c0 + c1\u02c6v1 + c2\u02c6v2 + c3\u02c6v4 + . . . + cn\u22122\u02c6vn\u22121 + cn\u22121\u02c6vn z4 = c0 + c1\u02c6v1 + c2\u02c6v2 + c3\u02c6v3 + . . . + cn\u22122\u02c6vn\u22121 + cn\u22121\u02c6vn . . . zi = c0 + c1\u02c6v1 + c2\u02c6v2 + . . . + ci\u22121\u02c6vi\u22121 + ci\u02c6vi+1 + . . . + cn\u22121\u02c6vn . . . zn\u22122 = c0 + c1\u02c6v1 + c2\u02c6v2 + c3\u02c6v3 + . . . + cn\u22122\u02c6vn\u22121 + cn\u22121\u02c6vn zn\u22121 = c0 + c1\u02c6v1 + c2\u02c6v2 + c3\u02c6v3 + . . . + cn\u22122\u02c6vn\u22122 + cn\u22121\u02c6vn zn = c0 + c1\u02c6v1 + c2\u02c6v2 + c3\u02c6v3 + . . . + cn\u22122\u02c6vn\u22122 + cn\u22121\u02c6vn\u22121 We thank Rakesh Vohra for pointing us to Moulin\"s working paper. 31 Not all choices of the constants c0, . . . , cn\u22121 produce a mechanism that is individually rational, and not all choices of the constants produce a mechanism that never incurs a deficit. Hence, to obtain these properties, we need to place some constraints on the constants. To satisfy the individual rationality criterion, each agent\"s utility should always be non-negative. An agent that does not win a unit obtains a utility that is equal to the agent\"s redistribution payment. An agent that wins a unit obtains a utility that is equal to the agent\"s valuation for the unit, minus the VCG payment \u02c6vm+1, plus the agent\"s redistribution payment. Consider agent an, the agent with the lowest bid. Since this agent does not win an item (m < n), her utility is just her redistribution payment zn. Hence, for the mechanism to be individually rational, the ci must be such that zn is always nonnegative. If the ci have this property, then it actually follows that zi is nonnegative for every i, for the following reason. Suppose there exists some i < n and some vector of bids \u02c6v1 \u2265 \u02c6v2 \u2265 . . . \u2265 \u02c6vn \u2265 0 such that zi < 0. Then, consider the bid vector that results from replacing \u02c6vj by \u02c6vj+1 for all j \u2265 i, and letting \u02c6vn = 0. If we omit \u02c6vn from this vector, the same vector results that results from omitting \u02c6vi from the original vector. Therefore, an\"s redistribution payment under the new vector should be the same as ai\"s redistribution payment under the old vector-but this payment is negative. If all redistribution payments are always nonnegative, then the mechanism must be individually rational (because the VCG mechanism is individually rational, and the redistribution payment only increases an agent\"s utility). Therefore, the mechanism is individually rational if and only if for any bid vector, zn \u2265 0. To satisfy the non-deficit criterion, the sum of the redistribution payments should be less than or equal to the total VCG payment. So for any bid vector \u02c6v1 \u2265 \u02c6v2 \u2265 . . . \u2265 \u02c6vn \u2265 0, the constants ci should make z1 + z2 + . . . + zn \u2264 m\u02c6vm+1. We define the family of linear VCG redistribution mechanisms to be the set of all redistribution mechanisms corresponding to constants ci that satisfy the above constraints (so that the mechanisms will be individually rational and have the no-deficit property). We now give two examples of mechanisms in this family. Example 1 (Bailey-Cavallo mechanism): Consider the mechanism corresponding to cm+1 = m and ci = 0 for all other i. Under this mechanism, each agent receives a redistribution payment of m times the (m+1)th highest bid from another agent. Hence, a1, . . . , am+1 receive a redistribution payment of m \u02c6vm+2, and the others receive m \u02c6vm+1. Thus, the total redistribution payment is (m+1)m \u02c6vm+2 +(n\u2212m\u2212 1)m \u02c6vm+1. This redistribution mechanism is individually rational, because all the redistribution payments are nonnegative, and never incurs a deficit, because (m + 1) m \u02c6vm+2 + (n\u2212m\u22121)m \u02c6vm+1 \u2264 nm \u02c6vm+1 = m\u02c6vm+1. (We note that for this mechanism to make sense, we need n \u2265 m + 2.) Example 2: Consider the mechanism corresponding to cm+1 = m n\u2212m\u22121 , cm+2 = \u2212 m(m+1) (n\u2212m\u22121)(n\u2212m\u22122) , and ci = 0 for all other i. In this mechanism, each agent receives a redistribution payment of m n\u2212m\u22121 times the (m + 1)th highest reported value from other agents, minus m(m+1) (n\u2212m\u22121)(n\u2212m\u22122) times the (m+2)th highest reported value from other agents. Thus, the total redistribution payment is m\u02c6vm+1 \u2212 m(m+1)(m+2) (n\u2212m\u22121)(n\u2212m\u22122) \u02c6vm+3. If n \u2265 2m+3 (which is equivalent to n\u2212m\u22121 \u2265 m(m+1) (n\u2212m\u22121)(n\u2212m\u22122) ), then each agent always receives a nonnegative redistribution payment, thus the mechanism is individually rational. Also, the mechanism never incurs a deficit, because the total VCG payment is m\u02c6vm+1, which is greater than the amount m\u02c6vm+1 \u2212 m(m+1)(m+2) (n\u2212m\u22121)(n\u2212m\u22122) \u02c6vm+3 that is redistributed. Which of these two mechanisms is better? Is there another mechanism that is even better? This is what we study in the next section. 4. OPTIMAL REDISTRIBUTION MECHANISMS Among all linear VCG redistribution mechanisms, we would like to be able to identify the one that redistributes the greatest percentage of the total VCG payment.2 This is not a well-defined notion: it may be that one mechanism redistributes more on some bid vectors, and another more on other bid vectors. We emphasize that we do not assume that a prior distribution over bidders\" valuations is available, so we cannot compare them based on expected redistribution. Below, we study three well-defined ways of comparing redistribution mechanisms: best-case performance, dominance, and worst-case performance. Best-case performance. One way of evaluating a mechanism is by considering the highest redistribution percentage that it achieves. Consider the previous two examples. For the first example, the total redistribution payment is (m + 1)m \u02c6vm+2 + (n \u2212 m \u2212 1)m \u02c6vm+1. When \u02c6vm+2 = \u02c6vm+1, this is equal to the total VCG payment m\u02c6vm+1. Thus, this mechanism redistributes 100% of the total VCG payment in the best case. For the second example, the total redistribution payment is m\u02c6vm+1 \u2212 m(m+1)(m+2) (n\u2212m\u22121)(n\u2212m\u22122) \u02c6vm+3. When \u02c6vm+3 = 0, this is equal to the total VCG payment m\u02c6vm+1. Thus, this mechanism also redistributes 100% of the total VCG payment in the best case. Moreover, there are actually infinitely many mechanisms that redistribute 100% of the total VCG payment in the best case-for example, any convex combination of the above two will redistribute 100% if both \u02c6vm+2 = \u02c6vm+1 and \u02c6vm+3 = 0. Dominance. Inside the family of linear VCG redistribution mechanisms, we say one mechanism dominates another mechanism if the first one redistributes at least as much as the other for any bid vector. For the previous two examples, neither dominates the other, because they each redistribute 100% in different cases. It turns out that there is no mechanism in the family that dominates all other mechanisms in the family. For suppose such a mechanism exists. Then, it should dominate both examples above. Consider the remaining VCG payment (the VCG payment failed to be redistributed). The remaining VCG payment of the dominant mechanism should be 0 whenever \u02c6vm+2 = \u02c6vm+1 or \u02c6vm+3 = 0. Now, the remaining VCG payment is a linear function of the \u02c6vi (linear redistribution), and therefore also a polynomial function. The above implies that this function can be written as (\u02c6vm+2 \u2212 \u02c6vm+1)(\u02c6vm+3)P(\u02c6v1, \u02c6v2, . . . , \u02c6vn), where P is a The percentage redistributed seems the natural criterion to use, among other things because it is scale-invariant: if we multiply all bids by the same positive constant (for example, if we change the units by re-expressing the bids in euros instead of dollars), we would not want the behavior of our mechanism to change. 32 polynomial function. But since the function must be linear (has degree at most 1), it follows that P = 0. Thus, a dominant mechanism would always redistribute all of the VCG payment, which is not possible. (If it were possible, then our worst-case optimal redistribution mechanism would also always redistribute all of the VCG payment, and we will see later that it does not.) Worst-case performance. Finally, we can evaluate a mechanism by considering the lowest redistribution percentage that it guarantees. For the first example, the total redistribution payment is (m+1)m \u02c6vm+2 +(n\u2212m\u22121)m \u02c6vm+1, which is greater than or equal to (n\u2212m\u22121) m \u02c6vm+1. So in the worst case, which is when \u02c6vm+2 = 0, the percentage redistributed is n\u2212m\u22121 . For the second example, the total redistribution payment is m\u02c6vm+1 \u2212 m(m+1)(m+2) (n\u2212m\u22121)(n\u2212m\u22122) \u02c6vm+3, which is greater than or equal to m\u02c6vm+1(1\u2212 (m+1)(m+2) (n\u2212m\u22121)(n\u2212m\u22122) ). So in the worst case, which is when \u02c6vm+3 = \u02c6vm+1, the percentage redistributed is 1 \u2212 (m+1)(m+2) (n\u2212m\u22121)(n\u2212m\u22122) . Since we assume that the number of agents n and the number of units m are known, we can determine which example mechanism has better worst-case performance by comparing the two quantities. When n = 6 and m = 1, for the first example (Bailey-Cavallo mechanism), the percentage redistributed in the worst case is 2 , and for the second example, this percentage is 1 , which implies that for this pair of n and m, the first mechanism has better worst-case performance. On the other hand, when n = 12 and m = 1, for the first example, the percentage redistributed in the worst case is 5 , and for the second example, this percentage is 14 15 , which implies that this time the second mechanism has better worst-case performance. Thus, it seems most natural to compare mechanisms by the percentage of total VCG payment that they redistribute in the worst case. This percentage is undefined when the total VCG payment is 0. To deal with this, technically, we define the worst-case redistribution percentage as the largest k so that the total amount redistributed is at least k times the total VCG payment, for all bid vectors. (Hence, as long as the total amount redistributed is at least 0 when the total VCG payment is 0, these cases do not affect the worst-case percentage.) This corresponds to the following optimization problem: Maximize k (the percentage redistributed in the worst case) Subject to: For every bid vector \u02c6v1 \u2265 \u02c6v2 \u2265 . . . \u2265 \u02c6vn \u2265 0 zn \u2265 0 (individual rationality) z1 + z2 + . . . + zn \u2264 m\u02c6vm+1 (non-deficit) z1 + z2 + . . . + zn \u2265 km\u02c6vm+1 (worst-case constraint) We recall that zi = c0 + c1\u02c6v1 + c2\u02c6v2 + . . . + ci\u22121\u02c6vi\u22121 + ci\u02c6vi+1 + . . . + cn\u22121\u02c6vn. 5. TRANSFORMATION TO LINEAR PROGRAMMING The optimization problem given in the previous section can be rewritten as a linear program, based on the following observations. Claim 1. If c0, c1, . . . , cn\u22121 satisfy both the individual rationality and the non-deficit constraints, then ci = 0 for i = 0, . . . , m. Proof. First, let us prove that c0 = 0. Consider the bid vector in which \u02c6vi = 0 for all i. To obtain individual rationality, we must have c0 \u2265 0. To satisfy the non-deficit constraint, we must have c0 \u2264 0. Thus we know c0 = 0. Now, if ci = 0 for all i, there is nothing to prove. Otherwise, let j = min{i|ci = 0}. Assume that j \u2264 m. We recall that we can write the individual rationality constraint as follows: zn = c0 +c1\u02c6v1 +c2\u02c6v2 +c3\u02c6v3 +. . .+cn\u22122\u02c6vn\u22122 +cn\u22121\u02c6vn\u22121 \u2265 0 for any bid vector. Let us consider the bid vector in which \u02c6vi = 1 for i \u2264 j and \u02c6vi = 0 for the rest. In this case zn = cj, so we must have cj \u2265 0. The non-deficit constraint can be written as follows: z1 + z2 + . . . + zn \u2264 m\u02c6vm+1 for any bid vector. Consider the same bid vector as above. We have zi = 0 for i \u2264 j, because for these bids, the jth highest other bid has value 0, so all the ci that are nonzero are multiplied by 0. For i > j, we have zi = cj, because the jth highest other bid has value 1, and all lower bids have value 0. So the non-deficit constraint tells us that cj(n \u2212 j) \u2264 m\u02c6vm+1. Because j \u2264 m, \u02c6vm+1 = 0, so the right hand side is 0. We also have n \u2212 j > 0 because j \u2264 m < n. So cj \u2264 0. Because we have already established that cj \u2265 0, it follows that cj = 0; but this is contrary to assumption. So j > m. Incidentally, this claim also shows that if m = n \u2212 1, then ci = 0 for all i. Thus, we are stuck with the VCG mechanism. From here on, we only consider the case where m < n \u2212 1. Claim 2. The individual rationality constraint can be written as follows: Pj i=m+1 ci \u2265 0 for j = m + 1, . . . , n \u2212 1. Before proving this claim, we introduce the following lemma. Lemma 1. Given a positive integer k and a set of real constants s1, s2, . . . , sk, (s1t1 + s2t2 + . . . + sktk \u2265 0 for any t1 \u2265 t2 \u2265 . . . \u2265 tk \u2265 0) if and only if ( Pj i=1 si \u2265 0 for j = 1, 2, . . . , k). Proof. Let di = ti \u2212ti+1 for i = 1, 2, . . . , k\u22121, and dk = tk. Then (s1t1 +s2t2 +. . .+sktk \u2265 0 for any t1 \u2265 t2 \u2265 . . . \u2265 tk \u2265 0) is equivalent to (( P1 i=1 si)d1 + ( P2 i=1 si)d2 + . . . + Pk i=1 si)dk \u2265 0 for any set of arbitrary non-negative dj). When Pj i=1 si \u2265 0 for j = 1, 2, . . . , k, the above inequality is obviously true. If for some j, Pj i=1 si < 0, if we set dj > 0 and di = 0 for all i = j, then the above inequality becomes false. So Pj i=1 si \u2265 0 for j = 1, 2, . . . , k is both necessary and sufficient. We are now ready to present the proof of Claim 2. Proof. The individual rationality constraint can be written as zn = c0 + c1\u02c6v1 + c2\u02c6v2 + c3\u02c6v3 + . . . + cn\u22122\u02c6vn\u22122 + cn\u22121\u02c6vn\u22121 \u2265 0 for any bid vector \u02c6v1 \u2265 \u02c6v2 \u2265 . . . \u2265 \u02c6vn\u22121 \u2265 \u02c6vn \u2265 0. We have already shown that ci = 0 for i \u2264 m. Thus, the above can be simplified to zn = cm+1\u02c6vm+1 + cm+2\u02c6vm+2+. . .+cn\u22122\u02c6vn\u22122+cn\u22121\u02c6vn\u22121 \u2265 0 for any bid vector. By the above lemma, this is equivalent to Pj i=m+1 ci \u2265 0 for j = m + 1, . . . , n \u2212 1. Claim 3. The non-deficit constraint and the worst-case constraint can also be written as linear inequalities involving only the ci and k. Proof. The non-deficit constraint requires that for any bid vector, z1 +z2 +. . .+zn \u2264 m\u02c6vm+1, where zi = c0 +c1\u02c6v1 + 33 c2\u02c6v2 +. . .+ci\u22121\u02c6vi\u22121 +ci\u02c6vi+1 +. . .+cn\u22121\u02c6vn for i = 1, 2, . . . , n. Because ci = 0 for i \u2264 m, we can simplify this inequality to qm+1\u02c6vm+1 + qm+2\u02c6vm+2 + . . . + qn\u02c6vn \u2265 0 qm+1 = m \u2212 (n \u2212 m \u2212 1)cm+1 qi = \u2212(i\u22121)ci\u22121 \u2212(n\u2212i)ci, for i = m+2, . . . , n\u22121 (when m + 2 > n \u2212 1, this set of equalities is empty) qn = \u2212(n \u2212 1)cn\u22121 By the above lemma, this is equivalent to Pj i=m+1 qi \u2265 0 for j = m + 1, . . . , n. So, we can simplify further as follows: qm+1 \u2265 0 \u21d0\u21d2 (n \u2212 m \u2212 1)cm+1 \u2264 m qm+1 + . . . + qm+i \u2265 0 \u21d0\u21d2 n Pj=m+i\u22121 j=m+1 cj + (n \u2212 m \u2212 i)cm+i \u2264 m for i = 2, . . . , n \u2212 m \u2212 1 qm+1 + . . . + qn \u2265 0 \u21d0\u21d2 n Pj=n\u22121 j=m+1 cj \u2264 m So, the non-deficit constraint can be written as a set of linear inequalities involving only the ci. The worst-case constraint can be also written as a set of linear inequalities, by the following reasoning. The worstcase constraint requires that for any bid input z1 +z2 +. . .+ zn \u2265 km\u02c6vm+1, where zi = c0 +c1\u02c6v1 +c2\u02c6v2 +. . .+ci\u22121\u02c6vi\u22121 + ci\u02c6vi+1 + . . . + cn\u22121\u02c6vn for i = 1, 2, . . . , n. Because ci = 0 for i \u2264 m, we can simplify this inequality to Qm+1\u02c6vm+1 + Qm+2\u02c6vm+2 + . . . + Qn\u02c6vn \u2265 0 Qm+1 = (n \u2212 m \u2212 1)cm+1 \u2212 km Qi = (i \u2212 1)ci\u22121 + (n \u2212 i)ci, for i = m + 2, . . . , n \u2212 1 Qn = (n \u2212 1)cn\u22121 By the above lemma, this is equivalent to Pj i=m+1 Qi \u2265 0 for j = m + 1, . . . , n. So, we can simplify further as follows: Qm+1 \u2265 0 \u21d0\u21d2 (n \u2212 m \u2212 1)cm+1 \u2265 km Qm+1 + . . . + Qm+i \u2265 0 \u21d0\u21d2 n Pj=m+i\u22121 j=m+1 cj + (n \u2212 m \u2212 i)cm+i \u2265 km for i = 2, . . . , n \u2212 m \u2212 1 Qm+1 + . . . + Qn \u2265 0 \u21d0\u21d2 n Pj=n\u22121 j=m+1 cj \u2265 km So, the worst-case constraint can also be written as a set of linear inequalities involving only the ci and k. Combining all the claims, we see that the original optimization problem can be transformed into the following linear program. Variables: cm+1, cm+2, . . . , cn\u22121, k Maximize k (the percentage redistributed in the worst case) Subject to:Pj i=m+1 ci \u2265 0 for j = m + 1, . . . , n \u2212 1 km \u2264 (n \u2212 m \u2212 1)cm+1 \u2264 m km \u2264 n Pj=m+i\u22121 j=m+1 cj + (n \u2212 m \u2212 i)cm+i \u2264 m for i = 2, . . . , n \u2212 m \u2212 1 km \u2264 n Pj=n\u22121 j=m+1 cj \u2264 m 6. NUMERICAL RESULTS For selected values of n and m, we solved the linear program using Glpk (GNU Linear Programming Kit). In the table below, we present the results for a single unit (m = 1). We present 1\u2212k (the percentage of the total VCG payment that is not redistributed by the worst-case optimal mechanism in the worst case) instead of k in the second column because writing k would require too many significant digits. Correspondingly, the third column displays the percentage 5 10 15 20 25 30 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 Number of AgentsWorst\u2212caseRedistributionPercentage 1 Unit WO 1 Unit BC 2 Units WO 2 Units BC 3 Units WO 3 Units BC 4 Units WO 4 Units BC Figure 1: A comparison of the worst-case optimal mechanism (WO) and the Bailey-Cavallo mechanism (BC). of the total VCG payment that is not redistributed by the Bailey-Cavallo mechanism in the worst case (which is equal to 2 ). n 1 \u2212 k Bailey \u2212 Cavallo Mechanism 3 66.7% 66.7% 4 42.9% 50.0% 5 26.7% 40.0% 6 16.1% 33.3% 7 9.52% 28.6% 8 5.51% 25.0% 9 3.14% 22.2% 10 1.76% 20.0% 20 3.62e \u2212 5 10.0% 30 5.40e \u2212 8 6.67e \u2212 2 40 7.09e \u2212 11 5.00e \u2212 2 The worst-case optimal mechanism significantly outperforms the Bailey-Cavallo mechanism in the worst case. Perhaps more surprisingly, the worst-case optimal mechanism sometimes does better in the worst case than the BaileyCavallo mechanism does on average, as the following example shows. Recall that the total redistribution payment of the BaileyCavallo mechanism is (m + 1)m \u02c6vm+2 + (n \u2212 m \u2212 1)m \u02c6vm+1. For the single-unit case, this simplifies to 2 \u02c6v3 + n\u22122 \u02c6v2. Hence the percentage of the total VCG payment that is not redistributed is \u02c6v2\u2212 2 \u02c6v3\u2212 n\u22122 \u02c6v2 \u02c6v2 = 2 \u2212 2 \u02c6v3 \u02c6v2 , which has an expected value of E( 2 \u2212 2 \u02c6v3 \u02c6v2 ) = 2 \u2212 2 E \u02c6v3 \u02c6v2 . Suppose the bid values are drawn from a uniform distribution over . The theory of order statistics tells us that the 34 joint probability density function of \u02c6v2 and \u02c6v3 is f(\u02c6v3, \u02c6v2) = n(n \u2212 1)(n \u2212 2)\u02c6vn\u22123 3 (1 \u2212 \u02c6v2) for \u02c6v2 \u2265 \u02c6v3. Now, E \u02c6v3 \u02c6v2 R 1 R \u02c6v2 \u02c6v3 \u02c6v2 f(\u02c6v3, \u02c6v2)d\u02c6v3d\u02c6v2 = n\u22122 n\u22121 . So, the expected value of the remaining percentage is 2 \u2212 2 n\u22122 n\u22121 = 2 n(n\u22121) . For n = 20, this is 5.26e \u2212 3, whereas the remaining percentage for the worst-case optimal mechanism is 3.62e\u22125 in the worst case. Let us present the optimal solution for the case n = 5 in detail. By solving the above linear program, we find that the optimal values for the ci are c2 = 11 45 , c3 = \u22121 , and c4 = 1 15 That is, the redistribution payment received by each agent is: 11 45 times the second highest bid among the other agents, minus 1 times the third highest bid among the other agents, plus 1 15 times the fourth highest bid among the other agents. The total amount redistributed is 11 15 \u02c6v2 + 4 15 \u02c6v3 \u2212 4 15 \u02c6v4 + 15 \u02c6v5; in the worst case, 11 15 \u02c6v2 is redistributed. Hence, the percentage of the total VCG payment that is not redistributed is never more than 4 15 = 26.7%. Finally, we compare the worst-case optimal mechanism to the Bailey-Cavallo mechanism for m = 1, 2, 3, 4, n = m + 2, . . . , 30. These results are in Figure 1. We see that for any m, when n = m + 2, the worst-case optimal mechanism has the same worst-case performance as the Bailey-Cavallo mechanism (actually, in this case, the worst-case optimal mechanism is identical to the BaileyCavallo mechanism). When n > m + 2, the worst-case optimal mechanism outperforms the Bailey-Cavallo mechanism (in the worst case). 7. ANALYTICAL CHARACTERIZATION OF THE WORST-CASE OPTIMAL MECHANISM We recall that our linear program has the following form: Variables: cm+1, cm+2, . . . , cn\u22121, k Maximize k (the percentage redistributed in the worst case) Subject to:Pj i=m+1 ci \u2265 0 for j = m + 1, . . . , n \u2212 1 km \u2264 (n \u2212 m \u2212 1)cm+1 \u2264 m km \u2264 n Pj=m+i\u22121 j=m+1 cj + (n \u2212 m \u2212 i)cm+i \u2264 m for i = 2, . . . , n \u2212 m \u2212 1 km \u2264 n Pj=n\u22121 j=m+1 cj \u2264 m A linear program has no solution if and only if either the objective is unbounded, or the constraints are contradictory (there is no feasible solution). It is easy to see that k is bounded above by 1 (redistributing more than 100% violates the non-deficit constraint). Also, a feasible solution always exists, for example, k = 0 and ci = 0 for all i. So an optimal solution always exists. Observe that the linear program model depends only on the number of agents n and the number of units m. Hence the optimal solution is a function of n and m. It turns out that this optimal solution can be analytically characterized as follows. Theorem 1. For any m and n with n \u2265 m+2, the worstcase optimal mechanism (among linear VCG redistribution mechanisms) is unique. For this mechanism, the percentage redistributed in the worst case is k\u2217 = 1 \u2212 `n\u22121 Pn\u22121 j=m `n\u22121 The worst-case optimal mechanism is characterized by the following values for the ci: c\u2217 i = (\u22121)i+m\u22121 (n \u2212 m) `n\u22121 m\u22121 Pn\u22121 j=m `n\u22121 `n\u22121 n\u22121X j=i n \u2212 1 for i = m + 1, . . . , n \u2212 1. It should be noted that we have proved ci = 0 for i \u2264 m in Claim 1. Proof. We first rewrite the linear program as follows. We introduce new variables xm+1, xm+2, . . . , xn\u22121, defined by xj = Pj i=m+1 ci for j = m + 1, . . . , n \u2212 1. The linear program then becomes: Variables: xm+1, xm+2, . . . , xn\u22121, k Maximize k Subject to: km \u2264 (n \u2212 m \u2212 1)xm+1 \u2264 m km \u2264 (m + i)xm+i\u22121 + (n \u2212 m \u2212 i)xm+i \u2264 m for i = 2, . . . , n \u2212 m \u2212 1 km \u2264 nxn\u22121 \u2264 m xi \u2265 0 for i = m + 1, m + 2, . . . , n \u2212 1 We will prove that for any optimal solution to this linear program, k = k\u2217 . Moreover, we will prove that when k = k\u2217 xj = Pj i=m+1 c\u2217 i for j = m + 1, . . . , n \u2212 1. This will prove the theorem. We first make the following observations: (n \u2212 m \u2212 1)c\u2217 m+1 = (n \u2212 m \u2212 1) (n\u2212m)(n\u22121 m\u22121) (m+1) Pn\u22121 j=m (n\u22121 j ) (n\u22121 m+1) Pn\u22121 j=m+1 `n\u22121 = (n \u2212 m \u2212 1) (n\u2212m)(n\u22121 m\u22121) (m+1) Pn\u22121 j=m (n\u22121 j ) (n\u22121 m+1) Pn\u22121 j=m `n\u22121 `n\u22121 = (n \u2212 m \u2212 1) m n\u2212m\u22121 \u2212 (n \u2212 m \u2212 1) m(n\u22121 m ) (n\u2212m\u22121) Pn\u22121 j=m (n\u22121 j ) = m \u2212 (1 \u2212 k\u2217 )m = k\u2217 For i = m + 1, . . . , n \u2212 2, ic\u2217 i + (n \u2212 i \u2212 1)c\u2217 i+1 = i (\u22121)i+m\u22121 (n\u2212m)(n\u22121 m\u22121) Pn\u22121 j=m (n\u22121 j ) (n\u22121 i ) Pn\u22121 j=i `n\u22121 (n \u2212 i \u2212 1) (\u22121)i+m (n\u2212m)(n\u22121 m\u22121) (i+1) Pn\u22121 j=m (n\u22121 j ) (n\u22121 i+1 ) Pn\u22121 j=i+1 `n\u22121 (\u22121)i+m\u22121 (n\u2212m)(n\u22121 m\u22121) Pn\u22121 j=m (n\u22121 j ) (n\u22121 i ) Pn\u22121 j=i `n\u22121 (n \u2212 i \u2212 1) (\u22121)i+m\u22121 (n\u2212m)(n\u22121 m\u22121) (i+1) Pn\u22121 j=m (n\u22121 j ) i+1 (n\u22121 i )(n\u2212i\u22121) Pn\u22121 j=i+1 `n\u22121 (\u22121)i+m\u22121 (n\u2212m)(n\u22121 m\u22121) Pn\u22121 j=m (n\u22121 j ) = (\u22121)i+m\u22121 m(1 \u2212 k\u2217 Finally, (n \u2212 1)c\u2217 n\u22121 = (n \u2212 1) (\u22121)n+m (n\u2212m)(n\u22121 m\u22121) (n\u22121) Pn\u22121 j=m (n\u22121 j ) (n\u22121 n\u22121) Pn\u22121 j=n\u22121 `n\u22121 = (\u22121)m+n m(1 \u2212 k\u2217 Summarizing the above, we have: (n \u2212 m \u2212 1)c\u2217 m+1 = k\u2217 (m + 1)c\u2217 m+1 + (n \u2212 m \u2212 2)c\u2217 m+2 = m(1 \u2212 k\u2217 (m + 2)c\u2217 m+2 + (n \u2212 m \u2212 3)c\u2217 m+3 = \u2212m(1 \u2212 k\u2217 (m + 3)c\u2217 m+3 + (n \u2212 m \u2212 4)c\u2217 m+4 = m(1 \u2212 k\u2217 ... 35 (n \u2212 3)c\u2217 n\u22123 + 2c\u2217 n\u22122 = (\u22121)m+n\u22122 m(1 \u2212 k\u2217 (n \u2212 2)c\u2217 n\u22122 + c\u2217 n\u22121 = (\u22121)m+n\u22121 m(1 \u2212 k\u2217 (n \u2212 1)c\u2217 n\u22121 = (\u22121)m+n m(1 \u2212 k\u2217 Let x\u2217 j = Pj i=m+1 c\u2217 i for j = m + 1, m + 2, . . . , n \u2212 1, the first equation in the above tells us that (n \u2212 m \u2212 1)x\u2217 m+1 = k\u2217 m. By adding the first two equations, we get (m + 2)x\u2217 m+1 + (n \u2212 m \u2212 2)x\u2217 m+2 = m By adding the first three equations, we get (m + 3)x\u2217 m+2 + (n \u2212 m \u2212 3)x\u2217 m+3 = k\u2217 By adding the first i equations, where i = 2, . . . , n\u2212m\u22121, we get (m + i)x\u2217 m+i\u22121 + (n \u2212 m \u2212 i)x\u2217 m+i = m if i is even (m + i)x\u2217 m+i\u22121 + (n \u2212 m \u2212 i)x\u2217 m+i = k\u2217 m if i is odd Finally by adding all the equations, we get nx\u2217 n\u22121 = m if n \u2212 m is even; nx\u2217 n\u22121 = k\u2217 m if n \u2212 m is odd. Thus, for all of the constraints other than the nonnegativity constraints, we have shown that they are satisfied by setting xj = x\u2217 j = Pj i=m+1 c\u2217 i and k = k\u2217 . We next show that the nonnegativity constraints are satisfied by these settings as well. For m + 1 \u2264 i, i + 1 \u2264 n \u2212 1, we have Pn\u22121 j=i (n\u22121 j ) (n\u22121 i ) = 1 Pn\u22121 j=i i!(n\u22121\u2212i)! j!(n\u22121\u2212j)! \u2265 1 i+1 Pn\u22122 j=i i!(n\u22121\u2212i)! j!(n\u22121\u2212j)! i+1 Pn\u22122 j=i (i+1)!(n\u22121\u2212i\u22121)! (j+1)!(n\u22121\u2212j\u22121)! = 1 i+1 Pn\u22121 j=i+1 (n\u22121 j ) (n\u22121 i+1 ) This implies that the absolute value of c\u2217 i is decreasing as i increases (if the c\u2217 contains more than one number). We further observe that the sign of c\u2217 i alternates, with the first element c\u2217 m+1 positive. So x\u2217 j = Pj i=m+1 c\u2217 i \u2265 0 for all j. Thus, we have shown that these xi = x\u2217 i together with k = k\u2217 form a feasible solution of the linear program. We proceed to show that it is in fact the unique optimal solution. First we prove the following claim: Claim 4. If \u02c6k, \u02c6xi, i = m + 1, m + 2, . . . , n \u2212 1 satisfy the following inequalities: \u02c6km \u2264 (n \u2212 m \u2212 1)\u02c6xm+1 \u2264 m \u02c6km \u2264 (m + i)\u02c6xm+i\u22121 + (n \u2212 m \u2212 i)\u02c6xm+i \u2264 m for i = 2, . . . , n \u2212 m \u2212 1 \u02c6km \u2264 n\u02c6xn\u22121 \u2264 m \u02c6k \u2265 k\u2217 Then we must have that \u02c6xi = \u02c6x\u2217 i and \u02c6k = k\u2217 Proof of claim. Consider the first inequality. We know that (n \u2212 m \u2212 1)x\u2217 m+1 = k\u2217 m, so (n \u2212 m \u2212 1)\u02c6xm+1 \u2265 \u02c6km \u2265 k\u2217 m = (n \u2212 m \u2212 1)x\u2217 m+1. It follows that \u02c6xm+1 \u2265 x\u2217 m+1 (n \u2212 m \u2212 1 = 0). Now, consider the next inequality for i = 2. We know that (m + 2)x\u2217 m+1 + (n \u2212 m \u2212 2)x\u2217 m+2 = m. It follows that (n\u2212m\u22122)\u02c6xm+2 \u2264 m\u2212(m+2)\u02c6xm+1 \u2264 m\u2212(m+2)x\u2217 m+1 = (n \u2212 m \u2212 2)x\u2217 m+2, so \u02c6xm+2 \u2264 x\u2217 m+2 (i = 2 \u2264 n \u2212 m \u2212 1 \u21d2 n \u2212 m \u2212 2 = 0). Now consider the next inequality for i = 3. We know that (m + 3)x\u2217 m+2 + (n \u2212 m \u2212 3)x\u2217 m+3 = m. It follows that (n\u2212m\u22123)\u02c6xm+3 \u2265 \u02c6km\u2212(m+3)\u02c6xm+2 \u2265 k\u2217 m\u2212(m+3)x\u2217 m+2 = (n \u2212 m \u2212 3)x\u2217 m+3, so \u02c6xm+3 \u2265 x\u2217 m+3 (i = 3 \u2264 n \u2212 m \u2212 1 \u21d2 n \u2212 m \u2212 3 = 0). Proceeding like this all the way up to i = n\u2212m\u22121, we get that \u02c6xm+i \u2265 x\u2217 m+i if i is odd and \u02c6xm+i \u2264 x\u2217 m+i if i is even. Moreover, if one inequality is strict, then all subsequent inequalities are strict. Now, if we can prove \u02c6xn\u22121 = x\u2217 n\u22121, it would follow that the x\u2217 i are equal to the \u02c6xi (which also implies that \u02c6k = k\u2217 ). We consider two cases: Case 1: n \u2212 m is even. We have: n \u2212 m even \u21d2 n \u2212 m \u2212 1 odd \u21d2 \u02c6xn\u22121 \u2265 x\u2217 n\u22121. We also have: n\u2212m even \u21d2 nx\u2217 n\u22121 = m. Combining these two, we get m = nx\u2217 n\u22121 \u2264 n\u02c6xn\u22121 \u2264 m \u21d2 \u02c6xn\u22121 = x\u2217 n\u22121. Case 2: n \u2212 m is odd. In this case, we have \u02c6xn\u22121 \u2264 x\u2217 n\u22121, and nx\u2217 n\u22121 = k\u2217 m. Then, we have: k\u2217 m \u2264 \u02c6km \u2264 n\u02c6xn\u22121 \u2264 nx\u2217 n\u22121 = k\u2217 m \u21d2 \u02c6xn\u22121 = x\u2217 n\u22121. This completes the proof of the claim. It follows that if \u02c6k, \u02c6xi, i = m + 1, m + 2, . . . , n \u2212 1 is a feasible solution and \u02c6k \u2265 k\u2217 , then since all the inequalities in Claim 4 are satisfied, we must have \u02c6xi = x\u2217 i and \u02c6k = k\u2217 . Hence no other feasible solution is as good as the one described in the theorem. Knowing the analytical characterization of the worst-case optimal mechanism provides us with at least two major benefits. First, using these formulas is computationally more efficient than solving the linear program using a generalpurpose solver. Second, we can derive the following corollary. Corollary 1. If the number of units m is fixed, then as the number of agents n increases, the worst-case percentage redistributed linearly converges to 1, with a rate of convergence 1 . (That is, limn\u2192\u221e 1\u2212k\u2217 n+1 1\u2212k\u2217 = 1 . That is, in the limit, the percentage that is not redistributed halves for every additional agent.) We note that this is consistent with the experimental data for the single-unit case, where the worst-case remaining percentage roughly halves each time we add another agent. The worst-case percentage that is redistributed under the Bailey-Cavallo mechanism also converges to 1 as the number of agents goes to infinity, but the convergence is much slower-it does not converge linearly (that is, letting kC n be the percentage redistributed by the Bailey-Cavallo mechanism in the worst case for n agents, limn\u2192\u221e 1\u2212kC n+1 1\u2212kC limn\u2192\u221e n+1 = 1). We now present the proof of the corollary. Proof. When the number of agents is n, the worst-case percentage redistributed is k\u2217 n = 1 \u2212 (n\u22121 m ) Pn\u22121 j=m (n\u22121 j ) . When the number of agents is n + 1, the percentage becomes k\u2217 n+1 = 1 \u2212 (n m) Pn j=m (n j ) . For n sufficiently large, we will have 2n mnm\u22121 > 0, and hence 1\u2212k\u2217 n+1 1\u2212k\u2217 (n m) Pn\u22121 j=m (n\u22121 j ) (n\u22121 m ) Pn j=m (n j ) n\u2212m 2n\u22121 Pm\u22121 j=0 (n\u22121 j ) 2n\u2212 Pm\u22121 j=0 (n j ) , and n n\u2212m 2n\u22121 \u2212m(n\u22121)m\u22121 2n \u2264 1\u2212k\u2217 n+1 1\u2212k\u2217 \u2264 n n\u2212m 2n\u22121 2n\u2212mnm\u22121 (because `n \u2264 ni if j \u2264 i). Since we have limn\u2192\u221e n\u2212m 2n\u22121 \u2212m(n\u22121)m\u22121 2n = 1 , and limn\u2192\u221e n\u2212m 2n\u22121 2n\u2212mnm\u22121 = 1 it follows that limn\u2192\u221e 1\u2212k\u2217 n+1 1\u2212k\u2217 = 1 36 8. WORST-CASE OPTIMALITY OUTSIDE THE FAMILY In this section, we prove that the worst-case optimal redistribution mechanism among linear VCG redistribution mechanisms is in fact optimal (in the worst case) among all redistribution mechanisms that are deterministic, anonymous, strategy-proof, efficient and satisfy the non-deficit constraint. Thus, restricting our attention to linear VCG redistribution mechanisms did not come at a loss. To prove this theorem, we need the following lemma. This lemma is not new: it was informally stated by Cavallo . For completeness, we present it here with a detailed proof. Lemma 2. A VCG redistribution mechanism is deterministic, anonymous and strategy-proof if and only if there exists a function f : Rn\u22121 \u2192 R, so that the redistribution payment zi received by ai satisfies zi = f(\u02c6v1, \u02c6v2, . . . , \u02c6vi\u22121, \u02c6vi+1, . . . , \u02c6vn) for all i and all bid vectors. Proof. First, let us prove the only if direction, that is, if a VCG redistribution mechanism is deterministic, anonymous and strategy-proof then there exists a deterministic function f : Rn\u22121 \u2192 R, which makes zi = f(\u02c6v1, \u02c6v2, . . . , \u02c6vi\u22121, \u02c6vi+1, . . . , \u02c6vn) for all i and all bid vectors. If a VCG redistribution mechanism is deterministic and anonymous, then for any bid vector \u02c6v1 \u2265 \u02c6v2 \u2265 . . . \u2265 \u02c6vn, the mechanism outputs a unique redistribution payment list: z1, z2, . . . , zn. Let G : Rn \u2192 Rn be the function that maps \u02c6v1, \u02c6v2, . . . , \u02c6vn to z1, z2, . . . , zn for all bid vectors. Let H(i, x1, x2, . . . , xn) be the ith element of G(x1, x2, . . . , xn), so that zi = H(i, \u02c6v1, \u02c6v2, . . . , \u02c6vn) for all bid vectors and all 1 \u2264 i \u2264 n. Because the mechanism is anonymous, two agents should receive the same redistribution payment if their bids are the same. So, if \u02c6vi = \u02c6vj, H(i, \u02c6v1, \u02c6v2, . . . , \u02c6vn) = H(j, \u02c6v1, \u02c6v2, . . . , \u02c6vn). Hence, if we let j = min{t|\u02c6vt = \u02c6vi}, then H(i, \u02c6v1, \u02c6v2, . . . , \u02c6vn) = H(j, \u02c6v1, \u02c6v2, . . . , \u02c6vn). Let us define K : Rn \u2192 N \u00d7 Rn as follows: K(y, x1, x2, . . . , xn\u22121) = [j, w1, w2, . . . , wn], where w1, w2, . . . , wn are y, x1, x2, . . . , xn\u22121 sorted in descending order, and j = min{t|wt = y}. ({t|wt = y} = \u2205 because y \u2208 {w1, w2, . . . , wn}). Also let us define F : Rn \u2192 R by F(\u02c6vi, \u02c6v1, \u02c6v2, . . . , \u02c6vi\u22121, \u02c6vi+1, . . . , \u02c6vn) = H \u25e6 K(\u02c6vi, \u02c6v1, \u02c6v2, . . . , \u02c6vi\u22121, \u02c6vi+1, . . . , \u02c6vn) = H(min{t|\u02c6vt = \u02c6vi}, \u02c6v1, \u02c6v2, . . . , \u02c6vn) = H(i, \u02c6v1, \u02c6v2, . . . , \u02c6vn) = zi. That is, F is the redistribution payment to an agent that bids \u02c6vi when the other bids are \u02c6v1, \u02c6v2, . . . , \u02c6vi\u22121, \u02c6vi+1, . . . , \u02c6vn. Since our mechanism is required to be strategy-proof, and the space of valuations is unrestricted, zi should be independent of \u02c6vi by Lemma 1 in Cavallo . Hence, we can simply ignore the first variable input to F; let f(x1, x2, . . . , xn\u22121) = F(0, x1, x2, . . . , xn\u22121). So, we have for all bid vectors and i, zi = f(\u02c6v1, \u02c6v2, . . . , \u02c6vi\u22121, \u02c6vi+1, . . . , \u02c6vn). This completes the proof for the only if direction. For the if direction, if the redistribution payment received by ai satisfies zi = f(\u02c6v1, \u02c6v2, . . . , \u02c6vi\u22121, \u02c6vi+1, . . . , \u02c6vn) for all bid vectors and i, then this is clearly a deterministic and anonymous mechanism. To prove strategy-proofness, we observe that because an agent\"s redistribution payment is not affected by her own bid, her incentives are the same as in the VCG mechanism, which is strategy-proof. Now we are ready to introduce the next theorem: Theorem 2. For any m and n with n \u2265 m+2, the worstcase optimal mechanism among the family of linear VCG redistribution mechanisms is worst-case optimal among all mechanisms that are deterministic, anonymous, strategy-proof, efficient and satisfy the non-deficit constraint. While we needed individual rationality earlier in the paper, this theorem does not mention it, that is, we can not find a mechanism with better worst-case performance even if we sacrifice individual rationality. (The worst-case optimal linear VCG redistribution mechanism is of course individually rational.) Proof. Suppose there is a redistribution mechanism (when the number of units is m and the number of agents is n) that satisfies all of the above properties and has a better worstcase performance than the worst-case optimal linear VCG redistribution mechanism, that is, its worst-case redistribution percentage \u02c6k is strictly greater than k\u2217 By Lemma 2, for this mechanism, there is a function f : Rn\u22121 \u2192 R so that zi = f(\u02c6v1, \u02c6v2, . . . , \u02c6vi\u22121, \u02c6vi+1, . . . , \u02c6vn) for all i and all bid vectors. We first prove that f has the following properties. Claim 5. f(1, 1, . . . , 1, 0, 0, . . . , 0) = 0 if the number of 1s is less than or equal to m. Proof of claim. We assumed that for this mechanism, the worst-case redistribution percentage satisfies \u02c6k > k\u2217 0. If the total VCG payment is x, the total redistribution payment should be in [\u02c6kx, x] (non-deficit criterion). Consider the case where all agents bid 0, so that the total VCG payment is also 0. Hence, the total redistribution payment should be in [\u02c6k \u00b7 0, 0]-that is, it should be 0. Hence every agent\"s redistribution payment f(0, 0, . . . , 0) must be 0. Now, let ti = f(1, 1, . . . , 1, 0, 0, . . . , 0) where the number of 1s equals i. We proved t0 = 0. If tn\u22121 = 0, consider the bid vector where everyone bids 1. The total VCG payment is m and the total redistribution payment is nf(1, 1, . . . , 1) = ntn\u22121 = 0. This corresponds to 0% redistribution, which is contrary to our assumption that \u02c6k > k\u2217 \u2265 0. Now, consider j = min{i|ti = 0} (which is well-defined because tn\u22121 = 0). If j > m, the property is satisfied. If j \u2264 m, consider the bid vector where \u02c6vi = 1 for i \u2264 j and \u02c6vi = 0 for all other i. Under this bid vector, the first j agents each get redistribution payment tj\u22121 = 0, and the remaining n \u2212 j agents each get tj. Thus, the total redistribution payment is (n \u2212 j)tj. Because the total VCG payment for this bid vector is 0, we must have (n \u2212 j)tj = 0. So tj = 0 (j \u2264 m < n). But this is contrary to the definition of j. Hence f(1, 1, . . . , 1, 0, 0, . . . , 0) = 0 if the number of 1s is less than or equal to m. Claim 6. f satisfies the following inequalities: \u02c6km \u2264 (n \u2212 m \u2212 1)tm+1 \u2264 m \u02c6km \u2264 (m + i)tm+i\u22121 + (n \u2212 m \u2212 i)tm+i \u2264 m for i = 2, 3, . . . , n \u2212 m \u2212 1 \u02c6km \u2264 ntn\u22121 \u2264 m Here ti is defined as in the proof of Claim 5. 37 Proof of claim. For j = m + 1, . . . , n, consider the bid vectors where \u02c6vi = 1 for i \u2264 j and \u02c6vi = 0 for all other i. These bid vectors together with the non-deficit constraint and worst-case constraint produce the above set of inequalities: for example, when j = m + 1, we consider the bid vector \u02c6vi = 1 for i \u2264 m + 1 and \u02c6vi = 0 for all other i. The first m+1 agents each receive a redistribution payment of tm = 0, and all other agents each receive tm+1. Thus, the total VCG redistribution is (n \u2212 m \u2212 1)tm+1. The nondeficit constraint gives (n \u2212 m \u2212 1)tm+1 \u2264 m (because the total VCG payment is m). The worst-case constraint gives (n \u2212 m \u2212 1)tm+1 \u2265 \u02c6km. Combining these two, we get the first inequality. The other inequalities can be obtained in the same way. We now observe that the inequalities in Claim 6, together with \u02c6k \u2265 k\u2217 , are the same as those in Claim 4 (where the ti are replaced by the \u02c6xi). Thus, we can conclude that \u02c6k = k\u2217 which is contrary to our assumption \u02c6k > k\u2217 . Hence no mechanism satisfying all the listed properties has a redistribution percentage greater than k\u2217 in the worst case. So far we have only talked about the case where n \u2265 m+2. For the purpose of completeness, we provide the following claim for the n = m + 1 case. Claim 7. For any m and n with n = m + 1, the original VCG mechanism (that is, redistributing nothing) is (uniquely) worst-case optimal among all redistribution mechanisms that are deterministic, anonymous, strategy-proof, efficient and satisfy the non-deficit constraint. We recall that when n = m+1, Claim 1 tells us that the only mechanism inside the family of linear redistribution mechanisms is the original VCG mechanism, so that this mechanism is automatically worst-case optimal inside this family. However, to prove the above claim, we need to show that it is worst-case optimal among all redistribution mechanisms that have the desired properties. Proof. Suppose a redistribution mechanism exists that satisfies all of the above properties and has a worst-case performance as good as the original VCG mechanism, that is, its worst-case redistribution percentage is greater than or equal to 0. This implies that the total redistribution payment of this mechanism is always nonnegative. By Lemma 2, for this mechanism, there is a function f : Rn\u22121 \u2192 R so that zi = f(\u02c6v1, \u02c6v2, . . . , \u02c6vi\u22121, \u02c6vi+1, . . . , \u02c6vn) for all i and all bid vectors. We will prove that f(x1, x2, . . . , xn\u22121) = 0 for all x1 \u2265 x2 \u2265 . . . \u2265 xn\u22121 \u2265 0. First, consider the bid vector where \u02c6vi = 0 for all i. Here, each agent receives a redistribution payment f(0, 0, . . . , 0). The total redistribution payment is then nf(0, 0, . . . , 0), which should be both greater than or equal to 0 (by the above observation) as well less than or equal to 0 (using the nondeficit criterion and the fact that the total VCG payment is 0). It follows that f(0, 0, . . . , 0) = 0. Now, let us consider the bid vector where \u02c6v1 = x1 \u2265 0 and \u02c6vi = 0 for all other i. For this bid vector, the agent with the highest bid receives a redistribution payment of f(0, 0, . . . , 0) = 0, and the other n \u2212 1 agents each receive f(x1, 0, . . . , 0). By the same reasoning as above, the total redistribution payment should be both greater than or equal to 0 and less than or equal to 0, hence f(x1, 0, . . . , 0) = 0 for all x1 \u2265 0. Proceeding by induction, let us assume f(x1, x2, . . . , xk, 0, . . . , 0) = 0 for all x1 \u2265 x2 \u2265 . . . \u2265 xk \u2265 0, for some k < n \u2212 1. Consider the bid vector where \u02c6vi = xi for i \u2264 k + 1, and \u02c6vi = 0 for all other i, where the xi are arbitrary numbers satisfying x1 \u2265 x2 \u2265 . . . \u2265 xk \u2265 xk+1 \u2265 0. For the agents with the highest k + 1 bids, their redistribution payment is specified by f acting on an input with only k non-zero variables. Hence they all receive 0 by induction assumption. The other n \u2212 k \u2212 1 agents each receive f(x1, x2, . . . , xk, xk+1, 0, . . . , 0). The total redistribution payment is then (n\u2212k\u22121)f(x1, x2, . . . , xk, xk+1, 0, . . . , 0), which should be both greater than or equal to 0, and less than or equal to the total VCG payment. Now, in this bid vector, the lowest bid is 0 because k + 1 < n. But since n = m + 1, the total VCG payment is m\u02c6vn = 0. So we have f(x1, x2, . . . , xk, xk+1, 0, . . . , 0) = 0 for all x1 \u2265 x2 \u2265 . . . \u2265 xk \u2265 xk+1 \u2265 0. By induction, this statement holds for all k < n \u2212 1; when k + 1 = n \u2212 1, we have f(x1, x2, . . . , xn\u22122, xn\u22121) = 0 for all x1 \u2265 x2 \u2265 . . . \u2265 xn\u22122 \u2265 xn\u22121 \u2265 0. Hence, in this mechanism, the redistribution payment is always 0; that is, the mechanism is just the original VCG mechanism. Incidentally, we obtain the following corollary: Corollary 2. No VCG redistribution mechanism satisfies all of the following: determinism, anonymity, strategyproofness, efficiency, and (strong) budget balance. This holds for any n \u2265 m + 1. Proof. For the case n \u2265 m + 2: If such a mechanism exists, its worst-case performance would be better than that of the worst-case optimal linear VCG redistribution mechanism, which by Theorem 1 obtains a redistribution percentage strictly less than 1. But Theorem 2 shows that it is impossible to outperform this mechanism in the worst case. For the case n = m + 1: If such a mechanism exists, it would perform as well as the original VCG mechanism in the worst case, which implies that it is identical to the VCG mechanism by Claim 7. But the VCG mechanism is not (strongly) budget balanced. 9. CONCLUSIONS For allocation problems with one or more items, the wellknown Vickrey-Clarke-Groves (VCG) mechanism is efficient, strategy-proof, individually rational, and does not incur a deficit. However, the VCG mechanism is not (strongly) budget balanced: generally, the agents\" payments will sum to more than 0. If there is an auctioneer who is selling the items, this may be desirable, because the surplus payment corresponds to revenue for the auctioneer. However, if the items do not have an owner and the agents are merely interested in allocating the items efficiently among themselves, any surplus payment is undesirable, because it will have to flow out of the system of agents. Cavallo proposed a mechanism that redistributes some of the VCG payment back to the agents, while maintaining efficiency, strategy-proofness, individual rationality, and the non-deficit property. In this paper, we extended this result in a restricted setting. We studied allocation settings where there are multiple indistinguishable units of a single good, and agents have unit demand. (For this specific setting, Cavallo\" .) Here we proposed a family of mechanisms that redistribute some of the VCG payment 38 back to the agents. All mechanisms in the family are efficient, strategy-proof, individually rational, and never incur a deficit. The family includes the Bailey-Cavallo mechanism as a special case. We then provided an optimization model for finding the optimal mechanism-that is, the mechanism that maximizes redistribution in the worst case-inside the family, and showed how to cast this model as a linear program. We gave both numerical and analytical solutions of this linear program, and the (unique) resulting mechanism shows significant improvement over the Bailey-Cavallo mechanism (in the worst case). Finally, we proved that the obtained mechanism is optimal among all anonymous deterministic mechanisms that satisfy the above properties. One important direction for future research is to try to extend these results beyond multi-unit auctions with unit demand. However, it turns out that in sufficiently general settings, the worst-case optimal redistribution percentage is 0. In such settings, the worst-case criterion provides no guidance in determining a good redistribution mechanism (even redistributing nothing achieves the optimal worst-case percentage), so it becomes necessary to pursue other criteria. Alternatively, one can try to identify other special settings in which positive redistribution in the worst case is possible. Another direction for future research is to consider whether this mechanism has applications to collusion. For example, in a typical collusive scheme, there is a bidding ring consisting of a number of colluders, who submit only a single bid . If this bid wins, the colluders must allocate the item amongst themselves, perhaps using payments-but of course they do not want payments to flow out of the ring. This work is part of a growing literature on designing mechanisms that obtain good results in the worst case. Traditionally, economists have mostly focused either on designing mechanisms that always obtain certain properties (such as the VCG mechanism), or on designing mechanisms that are optimal with respect to some prior distribution over the agents\" preferences (such as the Myerson auction and the Maskin-Riley auction for maximizing expected revenue). Some more recent papers have focused on designing mechanisms for profit maximization using worst-case competitive analysis (e.g. ). There has also been growing interest in the design of online mechanisms where the agents arrive over time and decisions must be taken before all the agents have arrived. Such work often also takes a worst-case competitive analysis approach . It does not appear that there are direct connections between our work and these other works that focus on designing mechanisms that perform well in the worst case. Nevertheless, it seems likely that future research will continue to investigate mechanism design for the worst case, and hopefully a coherent framework will emerge.", "body1": "Many important problems in computer science and electronic commerce can be modeled as resource allocation problems. Unfortunately, agents\" valuations are private knowledge, and self-interested agents will lie about their valuations if this is to their benefit. In many settings, another property that would be desirable is (strong) budget balance, meaning that the payments sum to exactly 0. Or, the agents may be trying to allocate a resource that they have collectively constructed, discovered, or otherwise obtained. In light of this impossibility result, several authors have obtained budget balance by sacrificing some of the other desirable properties . In this paper, we restrict our attention to a limited setting, and in this setting we extend Cavallo\"s result. Around the same time, the same mechanism has been independently derived by Moulin .1 Moulin actually pursues a different objective (also based on worst-case analysis): whereas our objective is to maximize the percentage of VCG payments that are redistributed, Moulin tries to minimize the overall payments from agents as a percentage of efficiency. Let n denote the number of agents, and let m denote the number of units. Under the VCG mechanism, each agent among a1, . We modify the mechanism as follows. MECHANISMS We are now ready to introduce the family of linear VCG redistribution mechanisms. It is helpful to see the entire list of redistribution payments: z1 = c0 + c1\u02c6v2 + c2\u02c6v3 + c3\u02c6v4 + . zi = c0 + c1\u02c6v1 + c2\u02c6v2 + . zn\u22122 = c0 + c1\u02c6v1 + c2\u02c6v2 + c3\u02c6v3 + . 31 Not all choices of the constants c0, . To satisfy the individual rationality criterion, each agent\"s utility should always be non-negative. Consider agent an, the agent with the lowest bid. Then, consider the bid vector that results from replacing \u02c6vj by \u02c6vj+1 for all j \u2265 i, and letting \u02c6vn = 0. To satisfy the non-deficit criterion, the sum of the redistribution payments should be less than or equal to the total VCG payment. Example 1 (Bailey-Cavallo mechanism): Consider the mechanism corresponding to cm+1 = m and ci = 0 for all other i. Thus, the total redistribution payment is m\u02c6vm+1 \u2212 m(m+1)(m+2) (n\u2212m\u22121)(n\u2212m\u22122) \u02c6vm+3. Which of these two mechanisms is better? MECHANISMS Among all linear VCG redistribution mechanisms, we would like to be able to identify the one that redistributes the greatest percentage of the total VCG payment.2 This is not a well-defined notion: it may be that one mechanism redistributes more on some bid vectors, and another more on other bid vectors. Best-case performance. For the first example, the total redistribution payment is (m + 1)m \u02c6vm+2 + (n \u2212 m \u2212 1)m \u02c6vm+1. Moreover, there are actually infinitely many mechanisms that redistribute 100% of the total VCG payment in the best case-for example, any convex combination of the above two will redistribute 100% if both \u02c6vm+2 = \u02c6vm+1 and \u02c6vm+3 = 0. Dominance. Now, the remaining VCG payment is a linear function of the \u02c6vi (linear redistribution), and therefore also a polynomial function. 32 polynomial function. Thus, it seems most natural to compare mechanisms by the percentage of total VCG payment that they redistribute in the worst case. PROGRAMMING The optimization problem given in the previous section can be rewritten as a linear program, based on the following observations. Claim 1. Proof. Now, if ci = 0 for all i, there is nothing to prove. Because j \u2264 m, \u02c6vm+1 = 0, so the right hand side is 0. Incidentally, this claim also shows that if m = n \u2212 1, then ci = 0 for all i. Claim 2. Lemma 1. When Pj i=1 si \u2265 0 for j = 1, 2, . Thus, the above can be simplified to zn = cm+1\u02c6vm+1 + cm+2\u02c6vm+2+. By the above lemma, this is equivalent to Pj i=m+1 ci \u2265 0 for j = m + 1, . Because ci = 0 for i \u2264 m, we can simplify this inequality to qm+1\u02c6vm+1 + qm+2\u02c6vm+2 + . The worst-case constraint can be also written as a set of linear inequalities, by the following reasoning. For selected values of n and m, we solved the linear program using Glpk (GNU Linear Programming Kit). Correspondingly, the third column displays the percentage 5 10 15 20 25 30 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 Number of AgentsWorst\u2212caseRedistributionPercentage 1 Unit WO 1 Unit BC 2 Units WO 2 Units BC 3 Units WO 3 Units BC 4 Units WO 4 Units BC Figure 1: A comparison of the worst-case optimal mechanism (WO) and the Bailey-Cavallo mechanism (BC). of the total VCG payment that is not redistributed by the Bailey-Cavallo mechanism in the worst case (which is equal to 2 ). n 1 \u2212 k Bailey \u2212 Cavallo Mechanism 3 66.7% 66.7% 4 42.9% 50.0% 5 26.7% 40.0% 6 16.1% 33.3% 7 9.52% 28.6% 8 5.51% 25.0% 9 3.14% 22.2% 10 1.76% 20.0% 20 3.62e \u2212 5 10.0% 30 5.40e \u2212 8 6.67e \u2212 2 40 7.09e \u2212 11 5.00e \u2212 2 The worst-case optimal mechanism significantly outperforms the Bailey-Cavallo mechanism in the worst case. Perhaps more surprisingly, the worst-case optimal mechanism sometimes does better in the worst case than the BaileyCavallo mechanism does on average, as the following example shows. Recall that the total redistribution payment of the BaileyCavallo mechanism is (m + 1)m \u02c6vm+2 + (n \u2212 m \u2212 1)m \u02c6vm+1. For the single-unit case, this simplifies to 2 \u02c6v3 + n\u22122 \u02c6v2. Hence the percentage of the total VCG payment that is not redistributed is \u02c6v2\u2212 2 \u02c6v3\u2212 n\u22122 \u02c6v2 \u02c6v2 = 2 \u2212 2 \u02c6v3 \u02c6v2 , which has an expected value of E( 2 \u2212 2 \u02c6v3 \u02c6v2 ) = 2 \u2212 2 E \u02c6v3 \u02c6v2 . Suppose the bid values are drawn from a uniform distribution over . The total amount redistributed is 11 15 \u02c6v2 + 4 15 \u02c6v3 \u2212 4 15 \u02c6v4 + 15 \u02c6v5; in the worst case, 11 15 \u02c6v2 is redistributed. Finally, we compare the worst-case optimal mechanism to the Bailey-Cavallo mechanism for m = 1, 2, 3, 4, n = m + 2, . MECHANISM We recall that our linear program has the following form: Variables: cm+1, cm+2, . Theorem 1. Proof. We introduce new variables xm+1, xm+2, . We first make the following observations: (n \u2212 m \u2212 1)c\u2217 m+1 = (n \u2212 m \u2212 1) (n\u2212m)(n\u22121 m\u22121) (m+1) Pn\u22121 j=m (n\u22121 j ) (n\u22121 m+1) Pn\u22121 j=m+1 `n\u22121 = (n \u2212 m \u2212 1) (n\u2212m)(n\u22121 m\u22121) (m+1) Pn\u22121 j=m (n\u22121 j ) (n\u22121 m+1) Pn\u22121 j=m `n\u22121 `n\u22121 = (n \u2212 m \u2212 1) m n\u2212m\u22121 \u2212 (n \u2212 m \u2212 1) m(n\u22121 m ) (n\u2212m\u22121) Pn\u22121 j=m (n\u22121 j ) = m \u2212 (1 \u2212 k\u2217 )m = k\u2217 For i = m + 1, . 35 (n \u2212 3)c\u2217 n\u22123 + 2c\u2217 n\u22122 = (\u22121)m+n\u22122 m(1 \u2212 k\u2217 (n \u2212 2)c\u2217 n\u22122 + c\u2217 n\u22121 = (\u22121)m+n\u22121 m(1 \u2212 k\u2217 (n \u2212 1)c\u2217 n\u22121 = (\u22121)m+n m(1 \u2212 k\u2217 Let x\u2217 j = Pj i=m+1 c\u2217 i for j = m + 1, m + 2, . By adding the first two equations, we get (m + 2)x\u2217 m+1 + (n \u2212 m \u2212 2)x\u2217 m+2 = m By adding the first three equations, we get (m + 3)x\u2217 m+2 + (n \u2212 m \u2212 3)x\u2217 m+3 = k\u2217 By adding the first i equations, where i = 2, . For m + 1 \u2264 i, i + 1 \u2264 n \u2212 1, we have Pn\u22121 j=i (n\u22121 j ) (n\u22121 i ) = 1 Pn\u22121 j=i i!(n\u22121\u2212i)! We further observe that the sign of c\u2217 i alternates, with the first element c\u2217 m+1 positive. First we prove the following claim: Claim 4. Now consider the next inequality for i = 3. Proceeding like this all the way up to i = n\u2212m\u22121, we get that \u02c6xm+i \u2265 x\u2217 m+i if i is odd and \u02c6xm+i \u2264 x\u2217 m+i if i is even. Moreover, if one inequality is strict, then all subsequent inequalities are strict. This completes the proof of the claim. It follows that if \u02c6k, \u02c6xi, i = m + 1, m + 2, . Knowing the analytical characterization of the worst-case optimal mechanism provides us with at least two major benefits. Corollary 1.", "body2": "In such problems, we want to allocate the resources (or items) to the agents that value them the most. Finally, it has a no-deficit property: the sum of the agents\" payments is always nonnegative. For example, the agents may be trying to allocate the right to use a shared good on a given day. Unfortunately, in general settings, it is in fact impossible to design mechanisms that satisfy budget balance in addition to the other desirable properties . The total redistributed is at most the second-highest bid overall, and the redistribution to agent i does not affect i\"s incentives because it does not depend on i\"s own bid. Finally, we prove that our mechanism is in fact optimal among all anonymous deterministic mechanisms (even nonlinear ones) that satisfy the desirable properties. For Moulin\"s objective, dropping individual rationality does change the optimal mechanism (but only if there are multiple units). Given that the mechanism is strategy-proof, we can assume vi = \u02c6vi. When m = 1, this is the second-price or Vickrey auction. We do not allow zi to depend on \u02c6vi; because of this, ai\"s incentives are unaffected by this redistribution payment, and the mechanism remains strategy-proof. Also, the mechanism is anonymous. . + cn\u22122\u02c6vn\u22122 + cn\u22121\u02c6vn\u22121 We thank Rakesh Vohra for pointing us to Moulin\"s working paper. Hence, to obtain these properties, we need to place some constraints on the constants. An agent that wins a unit obtains a utility that is equal to the agent\"s valuation for the unit, minus the VCG payment \u02c6vm+1, plus the agent\"s redistribution payment. \u2265 \u02c6vn \u2265 0 such that zi < 0. Therefore, the mechanism is individually rational if and only if for any bid vector, zn \u2265 0. We now give two examples of mechanisms in this family. In this mechanism, each agent receives a redistribution payment of m n\u2212m\u22121 times the (m + 1)th highest reported value from other agents, minus m(m+1) (n\u2212m\u22121)(n\u2212m\u22122) times the (m+2)th highest reported value from other agents. Also, the mechanism never incurs a deficit, because the total VCG payment is m\u02c6vm+1, which is greater than the amount m\u02c6vm+1 \u2212 m(m+1)(m+2) (n\u2212m\u22121)(n\u2212m\u22122) \u02c6vm+3 that is redistributed. This is what we study in the next section. Below, we study three well-defined ways of comparing redistribution mechanisms: best-case performance, dominance, and worst-case performance. Consider the previous two examples. Thus, this mechanism also redistributes 100% of the total VCG payment in the best case. Moreover, there are actually infinitely many mechanisms that redistribute 100% of the total VCG payment in the best case-for example, any convex combination of the above two will redistribute 100% if both \u02c6vm+2 = \u02c6vm+1 and \u02c6vm+3 = 0. The remaining VCG payment of the dominant mechanism should be 0 whenever \u02c6vm+2 = \u02c6vm+1 or \u02c6vm+3 = 0. , \u02c6vn), where P is a The percentage redistributed seems the natural criterion to use, among other things because it is scale-invariant: if we multiply all bids by the same positive constant (for example, if we change the units by re-expressing the bids in euros instead of dollars), we would not want the behavior of our mechanism to change. On the other hand, when n = 12 and m = 1, for the first example, the percentage redistributed in the worst case is 5 , and for the second example, this percentage is 14 15 , which implies that this time the second mechanism has better worst-case performance. + cn\u22121\u02c6vn. PROGRAMMING The optimization problem given in the previous section can be rewritten as a linear program, based on the following observations. , m. Thus we know c0 = 0. So the non-deficit constraint tells us that cj(n \u2212 j) \u2264 m\u02c6vm+1. So j > m. From here on, we only consider the case where m < n \u2212 1. Before proving this claim, we introduce the following lemma. , k). + Pk i=1 si)dk \u2265 0 for any set of arbitrary non-negative dj). We are now ready to present the proof of Claim 2. We have already shown that ci = 0 for i \u2264 m. .+cn\u22122\u02c6vn\u22122+cn\u22121\u02c6vn\u22121 \u2265 0 for any bid vector. The non-deficit constraint and the worst-case constraint can also be written as linear inequalities involving only the ci and k. , n. + qn \u2265 0 \u21d0\u21d2 n Pj=n\u22121 j=m+1 cj \u2264 m So, the non-deficit constraint can be written as a set of linear inequalities involving only the ci. + Qn \u2265 0 \u21d0\u21d2 n Pj=n\u22121 j=m+1 cj \u2265 km So, the worst-case constraint can also be written as a set of linear inequalities involving only the ci and k. Combining all the claims, we see that the original optimization problem can be transformed into the following linear program. We present 1\u2212k (the percentage of the total VCG payment that is not redistributed by the worst-case optimal mechanism in the worst case) instead of k in the second column because writing k would require too many significant digits. Correspondingly, the third column displays the percentage 5 10 15 20 25 30 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 Number of AgentsWorst\u2212caseRedistributionPercentage 1 Unit WO 1 Unit BC 2 Units WO 2 Units BC 3 Units WO 3 Units BC 4 Units WO 4 Units BC Figure 1: A comparison of the worst-case optimal mechanism (WO) and the Bailey-Cavallo mechanism (BC). of the total VCG payment that is not redistributed by the Bailey-Cavallo mechanism in the worst case (which is equal to 2 ). n 1 \u2212 k Bailey \u2212 Cavallo Mechanism 3 66.7% 66.7% 4 42.9% 50.0% 5 26.7% 40.0% 6 16.1% 33.3% 7 9.52% 28.6% 8 5.51% 25.0% 9 3.14% 22.2% 10 1.76% 20.0% 20 3.62e \u2212 5 10.0% 30 5.40e \u2212 8 6.67e \u2212 2 40 7.09e \u2212 11 5.00e \u2212 2 The worst-case optimal mechanism significantly outperforms the Bailey-Cavallo mechanism in the worst case. Perhaps more surprisingly, the worst-case optimal mechanism sometimes does better in the worst case than the BaileyCavallo mechanism does on average, as the following example shows. Recall that the total redistribution payment of the BaileyCavallo mechanism is (m + 1)m \u02c6vm+2 + (n \u2212 m \u2212 1)m \u02c6vm+1. For the single-unit case, this simplifies to 2 \u02c6v3 + n\u22122 \u02c6v2. Hence the percentage of the total VCG payment that is not redistributed is \u02c6v2\u2212 2 \u02c6v3\u2212 n\u22122 \u02c6v2 \u02c6v2 = 2 \u2212 2 \u02c6v3 \u02c6v2 , which has an expected value of E( 2 \u2212 2 \u02c6v3 \u02c6v2 ) = 2 \u2212 2 E \u02c6v3 \u02c6v2 . By solving the above linear program, we find that the optimal values for the ci are c2 = 11 45 , c3 = \u22121 , and c4 = 1 15 That is, the redistribution payment received by each agent is: 11 45 times the second highest bid among the other agents, minus 1 times the third highest bid among the other agents, plus 1 15 times the fourth highest bid among the other agents. Hence, the percentage of the total VCG payment that is not redistributed is never more than 4 15 = 26.7%. When n > m + 2, the worst-case optimal mechanism outperforms the Bailey-Cavallo mechanism (in the worst case). Observe that the linear program model depends only on the number of agents n and the number of units m. Hence the optimal solution is a function of n and m. It turns out that this optimal solution can be analytically characterized as follows. It should be noted that we have proved ci = 0 for i \u2264 m in Claim 1. We first rewrite the linear program as follows. This will prove the theorem. , n \u2212 2, ic\u2217 i + (n \u2212 i \u2212 1)c\u2217 i+1 = i (\u22121)i+m\u22121 (n\u2212m)(n\u22121 m\u22121) Pn\u22121 j=m (n\u22121 j ) (n\u22121 i ) Pn\u22121 j=i `n\u22121 (n \u2212 i \u2212 1) (\u22121)i+m (n\u2212m)(n\u22121 m\u22121) (i+1) Pn\u22121 j=m (n\u22121 j ) (n\u22121 i+1 ) Pn\u22121 j=i+1 `n\u22121 (\u22121)i+m\u22121 (n\u2212m)(n\u22121 m\u22121) Pn\u22121 j=m (n\u22121 j ) (n\u22121 i ) Pn\u22121 j=i `n\u22121 (n \u2212 i \u2212 1) (\u22121)i+m\u22121 (n\u2212m)(n\u22121 m\u22121) (i+1) Pn\u22121 j=m (n\u22121 j ) i+1 (n\u22121 i )(n\u2212i\u22121) Pn\u22121 j=i+1 `n\u22121 (\u22121)i+m\u22121 (n\u2212m)(n\u22121 m\u22121) Pn\u22121 j=m (n\u22121 j ) = (\u22121)i+m\u22121 m(1 \u2212 k\u2217 Finally, (n \u2212 1)c\u2217 n\u22121 = (n \u2212 1) (\u22121)n+m (n\u2212m)(n\u22121 m\u22121) (n\u22121) Pn\u22121 j=m (n\u22121 j ) (n\u22121 n\u22121) Pn\u22121 j=n\u22121 `n\u22121 = (\u22121)m+n m(1 \u2212 k\u2217 Summarizing the above, we have: (n \u2212 m \u2212 1)c\u2217 m+1 = k\u2217 (m + 1)c\u2217 m+1 + (n \u2212 m \u2212 2)c\u2217 m+2 = m(1 \u2212 k\u2217 (m + 2)c\u2217 m+2 + (n \u2212 m \u2212 3)c\u2217 m+3 = \u2212m(1 \u2212 k\u2217 (m + 3)c\u2217 m+3 + (n \u2212 m \u2212 4)c\u2217 m+4 = m(1 \u2212 k\u2217 ... , n \u2212 1, the first equation in the above tells us that (n \u2212 m \u2212 1)x\u2217 m+1 = k\u2217 m. We next show that the nonnegativity constraints are satisfied by these settings as well. = 1 i+1 Pn\u22121 j=i+1 (n\u22121 j ) (n\u22121 i+1 ) This implies that the absolute value of c\u2217 i is decreasing as i increases (if the c\u2217 contains more than one number). We proceed to show that it is in fact the unique optimal solution. We know that (m + 2)x\u2217 m+1 + (n \u2212 m \u2212 2)x\u2217 m+2 = m. It follows that (n\u2212m\u22122)\u02c6xm+2 \u2264 m\u2212(m+2)\u02c6xm+1 \u2264 m\u2212(m+2)x\u2217 m+1 = (n \u2212 m \u2212 2)x\u2217 m+2, so \u02c6xm+2 \u2264 x\u2217 m+2 (i = 2 \u2264 n \u2212 m \u2212 1 \u21d2 n \u2212 m \u2212 2 = 0). We know that (m + 3)x\u2217 m+2 + (n \u2212 m \u2212 3)x\u2217 m+3 = m. It follows that (n\u2212m\u22123)\u02c6xm+3 \u2265 \u02c6km\u2212(m+3)\u02c6xm+2 \u2265 k\u2217 m\u2212(m+3)x\u2217 m+2 = (n \u2212 m \u2212 3)x\u2217 m+3, so \u02c6xm+3 \u2265 x\u2217 m+3 (i = 3 \u2264 n \u2212 m \u2212 1 \u21d2 n \u2212 m \u2212 3 = 0). Proceeding like this all the way up to i = n\u2212m\u22121, we get that \u02c6xm+i \u2265 x\u2217 m+i if i is odd and \u02c6xm+i \u2264 x\u2217 m+i if i is even. In this case, we have \u02c6xn\u22121 \u2264 x\u2217 n\u22121, and nx\u2217 n\u22121 = k\u2217 m. Then, we have: k\u2217 m \u2264 \u02c6km \u2264 n\u02c6xn\u22121 \u2264 nx\u2217 n\u22121 = k\u2217 m \u21d2 \u02c6xn\u22121 = x\u2217 n\u22121. This completes the proof of the claim. Hence no other feasible solution is as good as the one described in the theorem. Second, we can derive the following corollary. We now present the proof of the corollary. For n sufficiently large, we will have 2n mnm\u22121 > 0, and hence 1\u2212k\u2217 n+1 1\u2212k\u2217 (n m) Pn\u22121 j=m (n\u22121 j ) (n\u22121 m ) Pn j=m (n j ) n\u2212m 2n\u22121 Pm\u22121 j=0 (n\u22121 j ) 2n\u2212 Pm\u22121 j=0 (n j ) , and n n\u2212m 2n\u22121 \u2212m(n\u22121)m\u22121 2n \u2264 1\u2212k\u2217 n+1 1\u2212k\u2217 \u2264 n n\u2212m 2n\u22121 2n\u2212mnm\u22121 (because `n \u2264 ni if j \u2264 i).", "introduction": "Many important problems in computer science and electronic commerce can be modeled as resource allocation problems. In such problems, we want to allocate the resources (or items) to the agents that value them the most. Unfortunately, agents\" valuations are private knowledge, and self-interested agents will lie about their valuations if this is to their benefit. One solution is to auction off the items, possibly in a combinatorial auction where agents can bid on bundles of items. There exist ways of determining the payments that the agents make in such an auction that incentivizes the agents to report their true valuations-that is, the payments make the auction strategy-proof. One very general way of doing so is to use the VCG mechanism . (The VCG mechanism is also known as the Clarke mechanism or, in the specific context of auctions, the Generalized Vickrey Auction.) Besides strategy-proofness, the VCG mechanism has several other nice properties in the context of resource allocation problems. It is efficient: the chosen allocation always maximizes the sum of the agents\" valuations. It is also (expost) individually rational: participating in the mechanism never makes an agent worse off than not participating. Finally, it has a no-deficit property: the sum of the agents\" payments is always nonnegative. In many settings, another property that would be desirable is (strong) budget balance, meaning that the payments sum to exactly 0. Suppose the agents are trying to distribute some resources among themselves that do not have a previous owner. For example, the agents may be trying to allocate the right to use a shared good on a given day. Or, the agents may be trying to allocate a resource that they have collectively constructed, discovered, or otherwise obtained. If the agents use an auction to allocate these resources, and the sum of the agents\" payments in the auction is positive, then this surplus payment must leave the system 30 of the agents (for example, the agents must give the money to an outside party, or burn it). Na\u00a8\u0131ve redistribution of the surplus payment (e.g. each of the n agents receives 1/n of the surplus) will generally result in a mechanism that is not strategy-proof (e.g. in a Vickrey auction, the second-highest bidder would want to increase her bid to obtain a larger redistribution payment). Unfortunately, the VCG mechanism is not budget balanced: typically, there is surplus payment. Unfortunately, in general settings, it is in fact impossible to design mechanisms that satisfy budget balance in addition to the other desirable properties . In light of this impossibility result, several authors have obtained budget balance by sacrificing some of the other desirable properties . Another approach that is perhaps preferable is to use a mechanism that is more budget balanced than the VCG mechanism, and maintains all the other desirable properties. One way of trying to design such a mechanism is to redistribute some of the VCG payment back to the agents in a way that will not affect the agents\" incentives (so that strategy-proofness is maintained), and that will maintain the other properties. Cavallo pursued exactly this idea, and designed a mechanism that redistributes a large amount of the total VCG payment while maintaining all of the other desirable properties of the VCG mechanism. For example, in a single-item auction (where the VCG mechanism coincides with the second-price sealed-bid auction), the amount redistributed to bidder i by Cavallo\"s mechanism is 1/n times the second-highest bid among bids other than i\"s bid. The total redistributed is at most the second-highest bid overall, and the redistribution to agent i does not affect i\"s incentives because it does not depend on i\"s own bid. In this paper, we restrict our attention to a limited setting, and in this setting we extend Cavallo\"s result. We study allocation settings where there are multiple indistinguishable units of a single good, and all agents have unit demand, i.e. they want only a single unit. For this specific setting, Cavallo\" . Here we propose the family of linear VCG redistribution mechanisms. All mechanisms in this family are efficient, strategy-proof, individually rational, and never incur a deficit. The family includes the Bailey-Cavallo mechanism as a special case (with the caveat that we only study allocation settings with multiple indistinguishable units of a single good and unit demand, while Bailey\"s and Cavallo\"s mechanisms can be applied outside these settings as well). We then provide an optimization model for finding the optimal mechanism inside the family, based on worst-case analysis. Both numerical and analytical solutions of this model are provided, and the resulting mechanism shows significant improvement over the BaileyCavallo mechanism (in the worst case). For example, for the problem of allocating a single unit, when the number of agents is 10, our mechanism always redistributes more than 98% of the total VCG payment back to the agents (whereas the Bailey-Cavallo mechanism redistributes only 80% in the worst case). Finally, we prove that our mechanism is in fact optimal among all anonymous deterministic mechanisms (even nonlinear ones) that satisfy the desirable properties. Around the same time, the same mechanism has been independently derived by Moulin .1 Moulin actually pursues a different objective (also based on worst-case analysis): whereas our objective is to maximize the percentage of VCG payments that are redistributed, Moulin tries to minimize the overall payments from agents as a percentage of efficiency. It turns out that the resulting mechanisms are the same. Towards the end of this paper, we consider dropping the individual rationality requirement, and show that this does not change the optimal mechanism for our objective. For Moulin\"s objective, dropping individual rationality does change the optimal mechanism (but only if there are multiple units).", "conclusion": "In this section, we prove that the worst-case optimal redistribution mechanism among linear VCG redistribution mechanisms is in fact optimal (in the worst case) among all redistribution mechanisms that are deterministic, anonymous, strategy-proof, efficient and satisfy the non-deficit constraint.. Thus, restricting our attention to linear VCG redistribution mechanisms did not come at a loss.. To prove this theorem, we need the following lemma.. This lemma is not new: it was informally stated by Cavallo .. For completeness, we present it here with a detailed proof.. A VCG redistribution mechanism is deterministic, anonymous and strategy-proof if and only if there exists a function f : Rn\u22121 \u2192 R, so that the redistribution payment zi received by ai satisfies zi = f(\u02c6v1, \u02c6v2, .. , \u02c6vn) for all i and all bid vectors.. First, let us prove the only if direction, that is, if a VCG redistribution mechanism is deterministic, anonymous and strategy-proof then there exists a deterministic function f : Rn\u22121 \u2192 R, which makes zi = f(\u02c6v1, \u02c6v2, .. , \u02c6vn) for all i and all bid vectors.. If a VCG redistribution mechanism is deterministic and anonymous, then for any bid vector \u02c6v1 \u2265 \u02c6v2 \u2265 .. \u2265 \u02c6vn, the mechanism outputs a unique redistribution payment list: z1, z2, .. Let G : Rn \u2192 Rn be the function that maps \u02c6v1, \u02c6v2, .. , \u02c6vn to z1, z2, .. , zn for all bid vectors.. , xn) be the ith element of G(x1, x2, .. , xn), so that zi = H(i, \u02c6v1, \u02c6v2, .. , \u02c6vn) for all bid vectors and all 1 \u2264 i \u2264 n. Because the mechanism is anonymous, two agents should receive the same redistribution payment if their bids are the same.. So, if \u02c6vi = \u02c6vj, H(i, \u02c6v1, \u02c6v2, .. , \u02c6vn) = H(j, \u02c6v1, \u02c6v2, .. Hence, if we let j = min{t|\u02c6vt = \u02c6vi}, then H(i, \u02c6v1, \u02c6v2, .. , \u02c6vn) = H(j, \u02c6v1, \u02c6v2, .. Let us define K : Rn \u2192 N \u00d7 Rn as follows: K(y, x1, x2, .. , xn\u22121) = [j, w1, w2, .. , wn], where w1, w2, .. , wn are y, x1, x2, .. , xn\u22121 sorted in descending order, and j = min{t|wt = y}.. ({t|wt = y} = \u2205 because y \u2208 {w1, w2, .. Also let us define F : Rn \u2192 R by F(\u02c6vi, \u02c6v1, \u02c6v2, .. , \u02c6vn) = H \u25e6 K(\u02c6vi, \u02c6v1, \u02c6v2, .. , \u02c6vn) = H(min{t|\u02c6vt = \u02c6vi}, \u02c6v1, \u02c6v2, .. , \u02c6vn) = H(i, \u02c6v1, \u02c6v2, .. That is, F is the redistribution payment to an agent that bids \u02c6vi when the other bids are \u02c6v1, \u02c6v2, .. Since our mechanism is required to be strategy-proof, and the space of valuations is unrestricted, zi should be independent of \u02c6vi by Lemma 1 in Cavallo .. Hence, we can simply ignore the first variable input to F; let f(x1, x2, .. , xn\u22121) = F(0, x1, x2, .. So, we have for all bid vectors and i, zi = f(\u02c6v1, \u02c6v2, .. This completes the proof for the only if direction.. For the if direction, if the redistribution payment received by ai satisfies zi = f(\u02c6v1, \u02c6v2, .. , \u02c6vn) for all bid vectors and i, then this is clearly a deterministic and anonymous mechanism.. To prove strategy-proofness, we observe that because an agent\"s redistribution payment is not affected by her own bid, her incentives are the same as in the VCG mechanism, which is strategy-proof.. Now we are ready to introduce the next theorem: Theorem 2.. For any m and n with n \u2265 m+2, the worstcase optimal mechanism among the family of linear VCG redistribution mechanisms is worst-case optimal among all mechanisms that are deterministic, anonymous, strategy-proof, efficient and satisfy the non-deficit constraint.. While we needed individual rationality earlier in the paper, this theorem does not mention it, that is, we can not find a mechanism with better worst-case performance even if we sacrifice individual rationality.. (The worst-case optimal linear VCG redistribution mechanism is of course individually rational.). Suppose there is a redistribution mechanism (when the number of units is m and the number of agents is n) that satisfies all of the above properties and has a better worstcase performance than the worst-case optimal linear VCG redistribution mechanism, that is, its worst-case redistribution percentage \u02c6k is strictly greater than k\u2217 By Lemma 2, for this mechanism, there is a function f : Rn\u22121 \u2192 R so that zi = f(\u02c6v1, \u02c6v2, .. , \u02c6vn) for all i and all bid vectors.. We first prove that f has the following properties.. , 0) = 0 if the number of 1s is less than or equal to m. Proof of claim.. We assumed that for this mechanism, the worst-case redistribution percentage satisfies \u02c6k > k\u2217 0.. If the total VCG payment is x, the total redistribution payment should be in [\u02c6kx, x] (non-deficit criterion).. Consider the case where all agents bid 0, so that the total VCG payment is also 0.. Hence, the total redistribution payment should be in [\u02c6k \u00b7 0, 0]-that is, it should be 0.. Hence every agent\"s redistribution payment f(0, 0, .. Now, let ti = f(1, 1, .. , 0) where the number of 1s equals i.. If tn\u22121 = 0, consider the bid vector where everyone bids 1.. The total VCG payment is m and the total redistribution payment is nf(1, 1, .. , 1) = ntn\u22121 = 0.. This corresponds to 0% redistribution, which is contrary to our assumption that \u02c6k > k\u2217 \u2265 0.. Now, consider j = min{i|ti = 0} (which is well-defined because tn\u22121 = 0).. If j > m, the property is satisfied.. If j \u2264 m, consider the bid vector where \u02c6vi = 1 for i \u2264 j and \u02c6vi = 0 for all other i.. Under this bid vector, the first j agents each get redistribution payment tj\u22121 = 0, and the remaining n \u2212 j agents each get tj.. Thus, the total redistribution payment is (n \u2212 j)tj.. Because the total VCG payment for this bid vector is 0, we must have (n \u2212 j)tj = 0.. So tj = 0 (j \u2264 m < n).. But this is contrary to the definition of j.. , 0) = 0 if the number of 1s is less than or equal to m. Claim 6. f satisfies the following inequalities: \u02c6km \u2264 (n \u2212 m \u2212 1)tm+1 \u2264 m \u02c6km \u2264 (m + i)tm+i\u22121 + (n \u2212 m \u2212 i)tm+i \u2264 m for i = 2, 3, .. , n \u2212 m \u2212 1 \u02c6km \u2264 ntn\u22121 \u2264 m Here ti is defined as in the proof of Claim 5.. For j = m + 1, .. , n, consider the bid vectors where \u02c6vi = 1 for i \u2264 j and \u02c6vi = 0 for all other i.. These bid vectors together with the non-deficit constraint and worst-case constraint produce the above set of inequalities: for example, when j = m + 1, we consider the bid vector \u02c6vi = 1 for i \u2264 m + 1 and \u02c6vi = 0 for all other i.. The first m+1 agents each receive a redistribution payment of tm = 0, and all other agents each receive tm+1.. Thus, the total VCG redistribution is (n \u2212 m \u2212 1)tm+1.. The nondeficit constraint gives (n \u2212 m \u2212 1)tm+1 \u2264 m (because the total VCG payment is m).. The worst-case constraint gives (n \u2212 m \u2212 1)tm+1 \u2265 \u02c6km.. Combining these two, we get the first inequality.. The other inequalities can be obtained in the same way.. We now observe that the inequalities in Claim 6, together with \u02c6k \u2265 k\u2217 , are the same as those in Claim 4 (where the ti are replaced by the \u02c6xi).. Thus, we can conclude that \u02c6k = k\u2217 which is contrary to our assumption \u02c6k > k\u2217 .. Hence no mechanism satisfying all the listed properties has a redistribution percentage greater than k\u2217 in the worst case.. So far we have only talked about the case where n \u2265 m+2.. For the purpose of completeness, we provide the following claim for the n = m + 1 case.. For any m and n with n = m + 1, the original VCG mechanism (that is, redistributing nothing) is (uniquely) worst-case optimal among all redistribution mechanisms that are deterministic, anonymous, strategy-proof, efficient and satisfy the non-deficit constraint.. We recall that when n = m+1, Claim 1 tells us that the only mechanism inside the family of linear redistribution mechanisms is the original VCG mechanism, so that this mechanism is automatically worst-case optimal inside this family.. However, to prove the above claim, we need to show that it is worst-case optimal among all redistribution mechanisms that have the desired properties.. Suppose a redistribution mechanism exists that satisfies all of the above properties and has a worst-case performance as good as the original VCG mechanism, that is, its worst-case redistribution percentage is greater than or equal to 0.. This implies that the total redistribution payment of this mechanism is always nonnegative.. By Lemma 2, for this mechanism, there is a function f : Rn\u22121 \u2192 R so that zi = f(\u02c6v1, \u02c6v2, .. , \u02c6vn) for all i and all bid vectors.. We will prove that f(x1, x2, .. , xn\u22121) = 0 for all x1 \u2265 x2 \u2265 .. First, consider the bid vector where \u02c6vi = 0 for all i.. Here, each agent receives a redistribution payment f(0, 0, .. The total redistribution payment is then nf(0, 0, .. , 0), which should be both greater than or equal to 0 (by the above observation) as well less than or equal to 0 (using the nondeficit criterion and the fact that the total VCG payment is 0).. It follows that f(0, 0, .. Now, let us consider the bid vector where \u02c6v1 = x1 \u2265 0 and \u02c6vi = 0 for all other i.. For this bid vector, the agent with the highest bid receives a redistribution payment of f(0, 0, .. , 0) = 0, and the other n \u2212 1 agents each receive f(x1, 0, .. By the same reasoning as above, the total redistribution payment should be both greater than or equal to 0 and less than or equal to 0, hence f(x1, 0, .. , 0) = 0 for all x1 \u2265 0.. Proceeding by induction, let us assume f(x1, x2, .. , 0) = 0 for all x1 \u2265 x2 \u2265 .. \u2265 xk \u2265 0, for some k < n \u2212 1.. Consider the bid vector where \u02c6vi = xi for i \u2264 k + 1, and \u02c6vi = 0 for all other i, where the xi are arbitrary numbers satisfying x1 \u2265 x2 \u2265 .. \u2265 xk \u2265 xk+1 \u2265 0.. For the agents with the highest k + 1 bids, their redistribution payment is specified by f acting on an input with only k non-zero variables.. Hence they all receive 0 by induction assumption.. The other n \u2212 k \u2212 1 agents each receive f(x1, x2, .. The total redistribution payment is then (n\u2212k\u22121)f(x1, x2, .. , 0), which should be both greater than or equal to 0, and less than or equal to the total VCG payment.. Now, in this bid vector, the lowest bid is 0 because k + 1 < n. But since n = m + 1, the total VCG payment is m\u02c6vn = 0.. So we have f(x1, x2, .. , 0) = 0 for all x1 \u2265 x2 \u2265 .. \u2265 xk \u2265 xk+1 \u2265 0.. By induction, this statement holds for all k < n \u2212 1; when k + 1 = n \u2212 1, we have f(x1, x2, .. , xn\u22122, xn\u22121) = 0 for all x1 \u2265 x2 \u2265 .. \u2265 xn\u22122 \u2265 xn\u22121 \u2265 0.. Hence, in this mechanism, the redistribution payment is always 0; that is, the mechanism is just the original VCG mechanism.. Incidentally, we obtain the following corollary: Corollary 2.. No VCG redistribution mechanism satisfies all of the following: determinism, anonymity, strategyproofness, efficiency, and (strong) budget balance.. This holds for any n \u2265 m + 1.. For the case n \u2265 m + 2: If such a mechanism exists, its worst-case performance would be better than that of the worst-case optimal linear VCG redistribution mechanism, which by Theorem 1 obtains a redistribution percentage strictly less than 1.. But Theorem 2 shows that it is impossible to outperform this mechanism in the worst case.. For the case n = m + 1: If such a mechanism exists, it would perform as well as the original VCG mechanism in the worst case, which implies that it is identical to the VCG mechanism by Claim 7.. But the VCG mechanism is not (strongly) budget balanced.. CONCLUSIONS For allocation problems with one or more items, the wellknown Vickrey-Clarke-Groves (VCG) mechanism is efficient, strategy-proof, individually rational, and does not incur a deficit.. However, the VCG mechanism is not (strongly) budget balanced: generally, the agents\" payments will sum to more than 0.. If there is an auctioneer who is selling the items, this may be desirable, because the surplus payment corresponds to revenue for the auctioneer.. However, if the items do not have an owner and the agents are merely interested in allocating the items efficiently among themselves, any surplus payment is undesirable, because it will have to flow out of the system of agents.. Cavallo proposed a mechanism that redistributes some of the VCG payment back to the agents, while maintaining efficiency, strategy-proofness, individual rationality, and the non-deficit property.. In this paper, we extended this result in a restricted setting.. We studied allocation settings where there are multiple indistinguishable units of a single good, and agents have unit demand.. (For this specific setting, Cavallo\" .). Here we proposed a family of mechanisms that redistribute some of the VCG payment 38 back to the agents.. All mechanisms in the family are efficient, strategy-proof, individually rational, and never incur a deficit.. The family includes the Bailey-Cavallo mechanism as a special case.. We then provided an optimization model for finding the optimal mechanism-that is, the mechanism that maximizes redistribution in the worst case-inside the family, and showed how to cast this model as a linear program.. We gave both numerical and analytical solutions of this linear program, and the (unique) resulting mechanism shows significant improvement over the Bailey-Cavallo mechanism (in the worst case).. Finally, we proved that the obtained mechanism is optimal among all anonymous deterministic mechanisms that satisfy the above properties.. One important direction for future research is to try to extend these results beyond multi-unit auctions with unit demand.. However, it turns out that in sufficiently general settings, the worst-case optimal redistribution percentage is 0.. In such settings, the worst-case criterion provides no guidance in determining a good redistribution mechanism (even redistributing nothing achieves the optimal worst-case percentage), so it becomes necessary to pursue other criteria.. Alternatively, one can try to identify other special settings in which positive redistribution in the worst case is possible.. Another direction for future research is to consider whether this mechanism has applications to collusion.. For example, in a typical collusive scheme, there is a bidding ring consisting of a number of colluders, who submit only a single bid .. If this bid wins, the colluders must allocate the item amongst themselves, perhaps using payments-but of course they do not want payments to flow out of the ring.. This work is part of a growing literature on designing mechanisms that obtain good results in the worst case.. Traditionally, economists have mostly focused either on designing mechanisms that always obtain certain properties (such as the VCG mechanism), or on designing mechanisms that are optimal with respect to some prior distribution over the agents\" preferences (such as the Myerson auction and the Maskin-Riley auction for maximizing expected revenue).. Some more recent papers have focused on designing mechanisms for profit maximization using worst-case competitive analysis (e.g.. There has also been growing interest in the design of online mechanisms where the agents arrive over time and decisions must be taken before all the agents have arrived.. Such work often also takes a worst-case competitive analysis approach .. It does not appear that there are direct connections between our work and these other works that focus on designing mechanisms that perform well in the worst case.. Nevertheless, it seems likely that future research will continue to investigate mechanism design for the worst case, and hopefully a coherent framework will emerge."}
{"id": "H-52", "keywords": ["speech retriev", "speak term detect", "out-of-vocabulari"], "title": "Vocabulary Independent Spoken Term Detection", "abstract": "We are interested in retrieving information from speech data like broadcast news, telephone conversations and roundtable meetings. Today, most systems use large vocabulary continuous speech recognition tools to produce word transcripts; the transcripts are indexed and query terms are retrieved from the index. However, query terms that are not part of the recognizer's vocabulary cannot be retrieved, and the recall of the search is affected. In addition to the output word transcript, advanced systems provide also phonetic transcripts, against which query terms can be matched phonetically. Such phonetic transcripts suffer from lower accuracy and cannot be an alternative to word transcripts.We present a vocabulary independent system that can handle arbitrary queries, exploiting the information provided by having both word transcripts and phonetic transcripts. A speech recognizer generates word confusion networks and phonetic lattices. The transcripts are indexed for query processing and ranking purpose.The value of the proposed method is demonstrated by the relative high performance ofour system, which received the highest overall ranking for US English speech data in the recent NIST Spoken Term Detection evaluation.", "references": ["NIST Spoken Term Detection 2006 Evaluation Website", "NIST Spoken Term Detection (STD) 2006 Evaluation Plan", "General indexation of weighted automata -- application to spoken utterance retrieval", "Mutual relevance feedback for multimodal query formulation in video retrieval", "Advances in phonetic word spotting", "Open-vocabulary speech indexing for voice and video mail retrieval", "Juru at TREC 10 -- Experiments with Index Pruning", "Indexing uncertainty for spoken document search", "Position specific posterior lattices for indexing speech", "Conditional and joint models for grapheme-to-phoneme conversion", "Phonetic searching applied to on-line distance learning modules", "The TREC spoken document retrieval track: A success story", "A general algorithm for word graph matrix decomposition", "The application of classical information retrieval techniques to spoken documents", "A system for unrestricted topic retrieval from radio news broadcasts", "An experimental study of an audio indexing system for the web", "Spoken document retrieval from call-center conversations", "Finding consensus in speech recognition: word error minimization and other applications of confusion networks", "The DET curve in assessment of detection task performance", "Subword-based approaches for spoken document retrieval", "Fast two-stage vocabulary-independent search in spontaneous speech", "Lattice-based search for spoken utterance retrieval", "Vocabulary-independent search in spontaneous speech", "AT&T at TREC-7", "Document expansion for speech retrieval", "The IBM 2004 conversational telephony system for rich transcription", "Dynamic match phone-lattice searches for very fast and accurate unrestricted vocabulary keyword spotting", "Effects of out of vocabulary words in spoken document retrieval"], "full_text": "1. INTRODUCTION The rapidly increasing amount of spoken data calls for solutions to index and search this data. The classical approach consists of converting the speech to word transcripts using a large vocabulary continuous speech recognition (LVCSR) tool. In the past decade, most of the research efforts on spoken data retrieval have focused on extending classical IR techniques to word transcripts. Some of these works have been done in the framework of the NIST TREC Spoken Document Retrieval tracks and are described by Garofolo et al. . These tracks focused on retrieval from a corpus of broadcast news stories spoken by professionals. One of the conclusions of those tracks was that the effectiveness of retrieval mostly depends on the accuracy of the transcripts. While the accuracy of automatic speech recognition (ASR) systems depends on the scenario and environment, state-of-the-art systems achieved better than 90% accuracy in transcription of such data. Garofolo et al. concluded that Spoken document retrieval is a solved problem . However, a significant drawback of such approaches is that search on queries containing out-of-vocabulary (OOV) terms will not return any results. OOV terms are missing words from the ASR system vocabulary and are replaced in the output transcript by alternatives that are probable, given the recognition acoustic model and the language model. It has been experimentally observed that over 10% of user queries can contain OOV terms , as queries often relate to named entities that typically have a poor coverage in the ASR vocabulary. The effects of OOV query terms in spoken data retrieval are discussed by Woodland et al. . In many applications the OOV rate may get worse over time unless the recognizer\"s vocabulary is periodically updated. Another approach consists of converting the speech to phonetic transcripts and representing the query as a sequence of phones. The retrieval is based on searching the sequence of phones representing the query in the phonetic transcripts. The main drawback of this approach is the inherent high error rate of the transcripts. Therefore, such approach cannot be an alternative to word transcripts, especially for in-vocabulary (IV) query terms that are part of the vocabulary of the ASR system. A solution would be to combine the two different approaches presented above: we index both word transcripts and phonetic transcripts; during query processing, the information is retrieved from the word index for IV terms and from the phonetic index for OOV terms. We would like to be able to process also hybrid queries, i.e, queries that include both IV and OOV terms. Consequently, we need to merge pieces of information retrieved from word index and phonetic index. Proximity information on the occurrences of the query terms is required for phrase search and for proximity-based ranking. In classical IR, the index stores for each occurrence of a term, its offset. Therefore, we cannot merge posting lists retrieved by phonetic index with those retrieved by word index since the offset of the occurrences retrieved from the two different indices are not comparable. The only element of comparison between phonetic and word transcripts are the timestamps. No previous work combining word and phonetic approach has been done on phrase search. We present a novel scheme for information retrieval that consists of storing, during the indexing process, for each unit of indexing (phone or word) its timestamp. We search queries by merging the information retrieved from the two different indices, word index and phonetic index, according to the timestamps of the query terms. evaluation data . The paper is organized as follows. We describe the audio processing in Section 2. The indexing and retrieval methods are presented in section 3. Experimental setup and results are given in Section 4. In Section 5, we give an overview of related work. Finally, we conclude in Section 6. 2. AUTOMATIC SPEECH RECOGNITION SYSTEM We use an ASR system for transcribing speech data. It works in speaker-independent mode. For best recognition results, a speaker-independent acoustic model and a language model are trained in advance on data with similar characteristics. Typically, ASR generates lattices that can be considered as directed acyclic graphs. Each vertex in a lattice is associated with a timestamp and each edge (u, v) is labeled with a word or phone hypothesis and its prior probability, which is the probability of the signal delimited by the timestamps of the vertices u and v, given the hypothesis. The 1-best path transcript is obtained from the lattice using dynamic programming techniques. Mangu et al. and Hakkani-Tur et al. propose a compact representation of a word lattice called word confusion network (WCN). Each edge (u, v) is labeled with a word hypothesis and its posterior probability, i.e., the probability of the word given the signal. One of the main advantages of WCN is that it also provides an alignment for all of the words in the lattice. As explained in , the three main steps for building a WCN from a word lattice are as follows: 1. Compute the posterior probabilities for all edges in the word lattice. 2. Extract a path from the word lattice (which can be the 1-best, the longest or any random path), and call it the pivot path of the alignment. 3. Traverse the word lattice, and align all the transitions with the pivot, merging the transitions that correspond to the same word (or label) and occur in the same time interval by summing their posterior probabilities. The 1-best path of a WCN is obtained from the path containing the best hypotheses. As stated in , although WCNs are more compact than word lattices, in general the 1-best path obtained from WCN has a better word accuracy than the 1-best path obtained from the corresponding word lattice. Typical structures of a lattice and a WCN are given in Figure 1. Figure 1: Typical structures of a lattice and a WCN. 3. RETRIEVAL MODEL The main problem with retrieving information from spoken data is the low accuracy of the transcription particularly on terms of interest such as named entities and content words. Generally, the accuracy of a word transcript is characterized by its word error rate (WER). There are three kinds of errors that can occur in a transcript: substitution of a term that is part of the speech by another term, deletion of a spoken term that is part of the speech and insertion of a term that is not part of the speech. Substitutions and deletions reflect the fact that an occurrence of a term in the speech signal is not recognized. These misses reduce the recall of the search. Substitutions and insertions reflect the fact that a term which is not part of the speech signal appears in the transcript. These misses reduce the precision of the search. Search recall can be enhanced by expanding the transcript with extra words. These words can be taken from the other alternatives provided by the WCN; these alternatives may have been spoken but were not the top choice of the ASR. Such an expansion tends to correct the substitutions and the deletions and consequently, might improve recall but will probably reduce precision. Using an appropriate ranking model, we can avoid the decrease in precision. Mamou et al. have presented in the enhancement in the recall and the MAP by searching on WCN instead of considering only the 1-best path word transcript in the context of spoken document retrieval. We have adapted this model of IV search to term detection. In word transcripts, OOV terms are deleted or substituted. Therefore, the usage of phonetic transcripts is more desirable. However, due to their low accuracy, we have preferred to use only the 1-best path extracted from the phonetic lattices. We will show that the usage of phonetic transcripts tends to improve the recall without affecting the precision too much, using an appropriate ranking. 3. evaluation plan , the task consists in finding all the exact matches of a specific query in a given corpus of speech data. A query is a phrase containing several words. The queries are text and not speech. Note that this task is different from the more classical task of spoken document retrieval. Manual transcripts of the speech are not provided but are used by the evaluators to find true occurrences. By definition, true occurrences of a query are found automatically by searching the manual transcripts using the following rule: the gap between adjacent words in a query must be less than 0.5 seconds in the corresponding speech. For evaluating the results, each system output occurrence is judged as correct or not according to whether it is close in time to a true occurrence of the query retrieved from manual transcripts; it is judged as correct if the midpoint of the system output occurrence is less than or equal to 0.5 seconds from the time span of a true occurrence of the query. 3.2 Indexing We have used the same indexing process for WCN and phonetic transcripts. Each occurrence of a unit of indexing (word or phone) u in a transcript D is indexed with the following information: \u2022 the begin time t of the occurrence of u, \u2022 the duration d of the occurrence of u. In addition, for WCN indexing, we store \u2022 the confidence level of the occurrence of u at the time t that is evaluated by its posterior probability Pr(u|t, D), \u2022 the rank of the occurrence of u among the other hypotheses beginning at the same time t, rank(u|t, D). Note that since the task is to find exact matches of the phrase queries, we have not filtered stopwords and the corpus is not stemmed before indexing. 3.3 Search In the following, we present our approach for accomplishing the STD task using the indices described above. The terms are extracted from the query. The vocabulary of the ASR system building word transcripts is given. Terms that are part of this vocabulary are IV terms; the other terms are OOV. For an IV query term, the posting list is extracted from the word index. For an OOV query term, the term is converted to a sequence of phones using a joint maximum entropy N-gram model . For example, the term prosody is converted to the sequence of phones (p, r, aa, z, ih, d, iy). The posting list of each phone is extracted from the phonetic index. The next step consists of merging the different posting lists according to the timestamp of the occurrences in order to create results matching the query. First, we check that the words and phones appear in the right order according to their begin times. Second, we check that the gap in time between adjacent words and phones is reasonable. Conforming to the requirements of the STD evaluation, the distance in time between two adjacent query terms must be less than 0.5 seconds. For OOV search, we check that the distance in time between two adjacent phones of a query term is less that 0.2 seconds; this value has been determined empirically. In such a way, we can reduce the effect of insertion errors since we allow insertions between the adjacent words and phones. Our query processing does not allow substitutions and deletions. Example: Let us consider the phrase query prosody research. The term prosody is OOV and the term research is IV. The term prosody is converted to the sequence of phones (p, r, aa, z, ih, d, iy). The posting list of each phone is extracted from the phonetic index. We merge the posting lists of the phones such that the sequence of phones appears in the right order and the gap in time between the pairs of phones (p, r), (r, aa), (aa, z), (z, ih), (ih, d), (d, iy) is less than 0.2 seconds. We obtain occurrences of the term prosody. The posting list of research is extracted from the word index and we merge it with the occurrences found for prosody such that they appear in the right order and the distance in time between prosody and research is less than 0.5 seconds. Note that our indexing model allows to search for different types of queries: 1. queries containing only IV terms using the word index. 2. queries containing only OOV terms using the phonetic index. 3. keyword queries containing both IV and OOV terms using the word index for IV terms and the phonetic index for OOV terms; for query processing, the different sets of matches are unified if the query terms have OR semantics and intersected if the query terms have AND semantics. 4. phrase queries containing both IV and OOV terms; for query processing, the posting lists of the IV terms retrieved from the word index are merged with the posting lists of the OOV terms retrieved from the phonetic index. The merging is possible since we have stored the timestamps for each unit of indexing (word and phone) in both indices. The STD evaluation has focused on the fourth query type. It is the hardest task since we need to combine posting lists retrieved from phonetic and word indices. 3.4 Ranking Since IV terms and OOV terms are retrieved from two different indices, we propose two different functions for scoring an occurrence of a term; afterward, an aggregate score is assigned to the query based on the scores of the query terms. Because the task is term detection, we do not use a document frequency criterion for ranking the occurrences. Let us consider a query Q = (k0, ..., kn), associated with a boosting vector B = (B1, ..., Bj). This vector associates a boosting factor to each rank of the different hypotheses; the boosting factors are normalized between 0 and 1. If the rank r is larger than j, we assume Br = 0. 3.4.1 In vocabulary term ranking For IV term ranking, we extend the work of Mamou et al. on spoken document retrieval to term detection. We use the information provided by the word index. We define the score score(k, t, D) of a keyword k occurring at a time t in the transcript D, by the following formula: score(k, t, D) = Brank(k|t,D) \u00d7 Pr(k|t, D) Note that 0 \u2264 score(k, t, D) \u2264 1. 3.4.2 Out of vocabulary term ranking For OOV term ranking, we use the information provided by the phonetic index. We give a higher rank to occurrences of OOV terms that contain phones close (in time) to each other. We define a scoring function that is related to the average gap in time between the different phones. Let us consider a keyword k converted to the sequence of phones (pk 0 , ..., pk l ). We define the normalized score score(k, tk 0 , D) of a keyword k = (pk 0 , ..., pk l ), where each pk i occurs at time tk i with a duration of dk i in the transcript D, by the following formula: score(k, tk 0 , D) = 1 \u2212 i=1 5 \u00d7 (tk i \u2212 (tk i\u22121 + dk i\u22121)) Note that according to what we have ex-plained in Section 3.3, we have \u22001 \u2264 i \u2264 l, 0 < tk i \u2212 (tk i\u22121 + dk i\u22121) < 0.2 sec, 0 < 5 \u00d7 (tk i \u2212 (tk i\u22121 + dk i\u22121)) < 1, and consequently, 0 < score(k, tk 0 , D) \u2264 1. The duration of the keyword occurrence is tk l \u2212 tk 0 + dk l . Example: let us consider the sequence (p, r, aa, z, ih, d, iy) and two different occurrences of the sequence. For each phone, we give the begin time and the duration in second. Occurrence 1: (p, 0.25, 0.01), (r, 0.36, 0.01), (aa, 0.37, 0.01), (z, 0.38, 0.01), (ih, 0.39, 0.01), (d, 0.4, 0.01), (iy, 0.52, 0.01). Occurrence 2: (p, 0.45, 0.01), (r, 0.46, 0.01), (aa, 0.47, 0.01), (z, 0.48, 0.01), (ih, 0.49, 0.01), (d, 0.5, 0.01), (iy, 0.51, 0.01). According to our formula, the score of the first occurrence is 0.83 and the score of the second occurrence is 1. In the first occurrence, there are probably some insertion or silence between the phone p and r, and between the phone d and iy. The silence can be due to the fact that the phones belongs to two different words ans therefore, it is not an occurrence of the term prosody. 3.4.3 Combination The score of an occurrence of a query Q at time t0 in the document D is determined by the multiplication of the score of each keyword ki, where each ki occurs at time ti with a duration di in the transcript D: score(Q, t0, D) = i=0 score(ki, ti, D)\u03b3n Note that according to what we have ex-plained in Section 3.3, we have \u22001 \u2264 i \u2264 n, 0 < ti \u2212(ti\u22121 +di\u22121) < 0.5 sec. Our goal is to estimate for each found occurrence how likely the query appears. It is different from classical IR that aims to rank the results and not to score them. Since the probability to have a false alarm is inversely proportional to the length of the phrase query, we have boosted the score of queries by a \u03b3n exponent, that is related to the number of keywords in the phrase. We have determined empirically the value of \u03b3n = 1/n. The begin time of the query occurrence is determined by the begin time t0 of the first query term and the duration of the query occurrence by tn \u2212 t0 + dn. 4. EXPERIMENTS 4. evaluation . It includes three different source types in US English: three hours of broadcast news (BNEWS), three hours of conversational telephony speech (CTS) and two hours of conference room meetings (CONFMTG). As shown in Section 4.2, these different collections have different accuracies. CTS and CONFMTG are spontaneous speech. For the experiments, queries. Each query is a phrase containing between one to five terms, common and rare terms, terms that are in the manual transcripts and those that are not. Testing and determination of empirical values have been achieved on another set of speech data and queries, the development set, also provided by NIST. We have used the IBM research prototype ASR system, described in , for transcribing speech data. We have produced WCNs for the three different source types. 1-best phonetic transcripts were generated only for BNEWS and CTS, since CONFMTG phonetic transcripts have too low accuracy. We have adapted Juru , a full-text search library written in Java, to index the transcripts and to store the timestamps of the words and phones; search results have been retrieved as described in Section 3. For each found occurrence of the given query, our system outputs: the location of the term in the audio recording (begin time and duration), the score indicating how likely is the occurrence of query, (as defined in Section 3.4) and a hard (binary) decision as to whether the detection is correct. We measure precision and recall by comparing the results obtained over the automatic transcripts (only the results having true hard decision) to the results obtained over the reference manual transcripts. Our aim is to evaluate the ability of the suggested retrieval approach to handle transcribed speech data. Thus, the closer the automatic results to the manual results is, the better the search effectiveness over the automatic transcripts will be. The results returned from the manual transcription for a given query are considered relevant and are expected to be retrieved with highest scores. This approach for measuring search effectiveness using manual data as a reference is very common in speech retrieval research . Beside the recall and the precision, STD evaluation : the Actual Term-Weighted Value (ATWV) and the Maximum Term-Weighted Value (MTWV). The term-weighted value (TWV) is computed by first computing the miss and false alarm probabilities for each query separately, then using these and an (arbitrarily chosen) prior probability to compute query-specific values, and finally averaging these query-specific values over all queries q to produce an overall system value: TWV (\u03b8) = 1 \u2212 averageq{Pmiss(q, \u03b8) + \u03b2 \u00d7 PF A(q, \u03b8)} where \u03b2 = C (Pr\u22121 q \u2212 1). \u03b8 is the detection threshold. For the evaluation, the cost/value ratio, C/V , has been determined to 0.1 and the prior probability of a query Prq to 10\u22124 . Therefore, \u03b2 = 999.9. Miss and false alarm probabilities for a given query q are functions of \u03b8: Pmiss(q, \u03b8) = 1 \u2212 Ncorrect(q, \u03b8) Ntrue(q) PF A(q, \u03b8) = Nspurious(q, \u03b8) NNT (q) corpus WER(%) SUBR(%) DELR(%) INSR(%) BNEWS WCN 12.7 49 42 9 CTS WCN 19.6 51 38 11 CONFMTG WCN 47.4 47 49 3 Table 1: WER and distribution of the error types over word 1-best path extracted from WCNs for the different source types. where: \u2022 Ncorrect(q, \u03b8) is the number of correct detections (retrieved by the system) of the query q with a score greater than or equal to \u03b8. \u2022 Nspurious(q, \u03b8) is the number of spurious detections of the query q with a score greater than or equal to \u03b8. \u2022 Ntrue(q) is the number of true occurrences of the query q in the corpus. \u2022 NNT (q) is the number of opportunities for incorrect detection of the query q in the corpus; it is the NonTarget query trials. It has been defined by the following formula: NNT (q) = Tspeech \u2212 Ntrue(q). Tspeech is the total amount of speech in the collection (in seconds). ATWV is the actual term-weighted value; it is the detection value attained by the system as a result of the system output and the binary decision output for each putative occurrence. It ranges from \u2212\u221e to +1. MTWV is the maximum term-weighted value over the range of all possible values of \u03b8. It ranges from 0 to +1. We have also provided the detection error tradeoff (DET) curve of miss probability (Pmiss) vs. false alarm probability (PF A). We have used the STDEval tool to extract the relevant results from the manual transcripts and to compute ATWV, MTWV and the DET curve. We have determined empirically the following values for the boosting vector defined in Section 3.4: Bi = 1 4.2 WER analysis We use the word error rate (WER) in order to characterize the accuracy of the transcripts. WER is defined as follows: S + D + I \u00d7 100 where N is the total number of words in the corpus, and S, I, and D are the total number of substitution, insertion, and deletion errors, respectively. The substitution error rate (SUBR) is defined by S + D + I \u00d7 100. Deletion error rate (DELR) and insertion error rate (INSR) are defined in a similar manner. Table 1 gives the WER and the distribution of the error types over 1-best path transcripts extracted from WCNs. The WER of the 1-best path phonetic transcripts is approximately two times worse than the WER of word transcripts. That is the reason why we have not retrieved from phonetic transcripts on CONFMTG speech data. 4.3 Theta threshold We have determined empirically a detection threshold \u03b8 per source type and the hard decision of the occurrences having a score less than \u03b8 is set to false; false occurrences returned by the system are not considered as retrieved and therefore, are not used for computing ATWV, precision and recall. The value of the threshold \u03b8 per source type is reported in Table 2. It is correlated to the accuracy of the transcripts. Basically, setting a threshold aims to eliminate from the retrieved occurrences, false alarms without adding misses. The higher the WER is, the higher the \u03b8 threshold should be. BNEWS CTS CONFMTG 0.4 0.61 0.91 Table 2: Values of the \u03b8 threshold per source type. 4.4 Processing resource profile We report in Table 3 the processing resource profile. Concerning the index size, note that our index is compressed using IR index compression techniques. The indexing time includes both audio processing (generation of word and phonetic transcripts) and building of the searchable indices. MB/ HP/ sec.P/ MB Table 3: Processing resource profile. (HS: Hours of Speech. HP: Processing Hours. sec.P: Processing seconds) 4.5 Retrieval measures We compare our approach (WCN phonetic) presented in Section 4.1 with another approach (1-best-WCN phonetic). The only difference between these two approaches is that, in 1-best-WCN phonetic, we index only the 1-best path extracted from the WCN instead of indexing all the WCN. WCN phonetic was our primary system for the evaluation and 1-best-WCN phonetic was one of our contrastive systems. Average precision and recall, queries are given in Table 4. We provide also the DET curve for WCN phonetic approach in Figure 2. The point that maximizes the TWV, the MTWV, is specified on each curve. Note that retrieval performance has been evaluated separately for each source type since the accuracy of the speech differs per source type as shown in Section 4.2. As expected, we can see that MTWV and ATWV decrease in higher WER. precision 0.94 0.90 0.65 recall 0.89 0.81 0.37 1-best- precision 0.95 0.91 0.66 recall 0.84 0.75 0.37 Table 4: ATWV, MTWV, precision and recall per source type. Figure 2: DET curve for WCN phonetic approach. using WCNs relatively to 1-best path. It is due to the fact that miss probability is improved by indexing all the hypotheses provided by the WCNs. This observation confirms the results shown by Mamou et al. in the context of spoken document retrieval. The ATWV that we have obtained is close to the MTWV; we have combined our ranking model with appropriate threshold \u03b8 to eliminate results with lower score. Therefore, the effect of false alarms added by WCNs is reduced. WCN phonetic approach was used in the recent NIST STD evaluation and received the highest overall ranking among eleven participants. For comparison, the system that ranked at the third place, for BNEWS, for CONFMTG. 4.6 Influence of the duration of the query on the retrieval performance We have analysed the retrieval performance according to the average duration of the occurrences in the manual transcripts. The query set was divided into three different quantiles according to the duration; we have reported in Table 5 ATWV and MTWV according to the duration. We can see that we performed better on longer queries. One of the reasons is the fact that the ASR system is more accurate on long words. Hence, it was justified to boost the score of the results with the exponent \u03b3n, as explained in Section 3.4.3, according to the length of the query. quantile 0-33 33-66 66- Table 5: ATWV, MTWV according to the duration of the query occurrences per source type. 4.7 OOV vs. IV query processing We have randomly chosen three sets of queries from the query sets provided by NIST: 50 queries containing only IV terms; 50 queries containing only OOV terms; and 50 hybrid queries containing both IV and OOV terms. The following experiment has been achieved on the BNEWS collection and IV and OOV terms has been determined according to the vocabulary of BNEWS ASR system. We would like to compare three different approaches of retrieval: using only word index; using only phonetic index; combining word and phonetic indices. Table 6 summarizes the retrieval performance according to each approach and to each type of queries. Using a word-based approach for dealing with OOV and hybrid queries affects drastically the performance of the retrieval; precision and recall are null. Using a phone-based approach for dealing with IV queries affects also the performance of the retrieval relatively to the word-based approach. As expected, the approach combining word and phonetic indices presented in Section 3 leads to the same retrieval performance as the word approach for IV queries and to the same retrieval performance as the phonetic approach for OOV queries. This approach always outperforms the others and it justifies the fact that we need to combine word and phonetic search. 5. RELATED WORK In the past decade, the research efforts on spoken data retrieval have focused on extending classical IR techniques to spoken documents. Some of these works have been done in the context of the TREC Spoken Document Retrieval evaluations and are described by Garofolo et al. . An LVCSR system is used to transcribe the speech into 1-best path word transcripts. The transcripts are indexed as clean text: for each occurrence, its document, its word offset and additional information are stored in the index. A generic IR system over the text is used for word spotting and search as described by Brown et al. and James . This stratindex word phonetic word and phonetic precision recall precision recall precision recall IV queries 0.8 0.96 0.11 0.77 0.8 0.96 OOV queries 0 0 0.13 0.79 0.13 0.79 hybrid queries 0 0 0.15 0.71 0.89 0.83 Table 6: Comparison of word and phonetic approach on IV and OOV queries egy works well for transcripts like broadcast news collections that have a low WER (in the range of 15%-30%) and are redundant by nature (the same piece of information is spoken several times in different manners). Moreover, the algorithms have been mostly tested over long queries stated in plain English and retrieval for such queries is more robust against speech recognition errors. An alternative approach consists of using word lattices in order to improve the effectiveness of SDR. Singhal et al. propose to add some terms to the transcript in order to alleviate the retrieval failures due to ASR errors. From an IR perspective, a classical way to bring new terms is document expansion using a similar corpus. Their approach consists in using word lattices in order to determine which words returned by a document expansion algorithm should be added to the original transcript. The necessity to use a document expansion algorithm was justified by the fact that the word lattices they worked with, lack information about word probabilities. Chelba and Acero in propose a more compact word lattice, the position specific posterior lattice (PSPL). This data structure is similar to WCN and leads to a more compact index. The offset of the terms in the speech documents is also stored in the index. However, the evaluation framework is carried out on lectures that are relatively planned, in contrast to conversational speech. Their ranking model is based on the term confidence level but does not take into consideration the rank of the term among the other hypotheses. Mamou et al. propose a model for spoken document retrieval using WCNs in order to improve the recall and the MAP of the search. However, in the above works, the problem of queries containing OOV terms is not addressed. Popular approaches to deal with OOV queries are based on sub-words transcripts, where the sub-words are typically phones, syllables or word fragments (sequences of phones) . The classical approach consists of using phonetic transcripts. The transcripts are indexed in the same manner as words in using classical text retrieval techniques; during query processing, the query is represented as a sequence of phones. The retrieval is based on searching the string of phones representing the query in the phonetic transcript. To account for the high recognition error rates, some other systems use richer transcripts like phonetic lattices. They are attractive as they accommodate high error rate conditions as well as allow for OOV queries to be used . However, phonetic lattices contain many edges that overlap in time with the same phonetic label, and are difficult to index. Moreover, beside the improvement in the recall of the search, the precision is affected since phonetic lattices are often inaccurate. Consequently, phonetic approaches should be used only for OOV search; for searching queries containing also IV terms, this technique affects the performance of the retrieval in comparison to the word based approach. Saraclar and Sproat in show improvement in word spotting accuracy for both IV and OOV queries, using phonetic and word lattices, where a confidence measure of a word or a phone can be derived. They propose three different retrieval strategies: search both the word and the phonetic indices and unify the two different sets of results; search the word index for IV queries, search the phonetic index for OOV queries; search the word index and if no result is returned, search the phonetic index. However, no strategy is proposed to deal with phrase queries containing both IV and OOV terms. Amir et al. in propose to merge a word approach with a phonetic approach in the context of video retrieval. However, the phonetic transcript is obtained from a text to phonetic conversion of the 1-best path of the word transcript and is not based on a phonetic decoding of the speech data. An important issue to be considered when looking at the state-of-the-art in retrieval of spoken data, is the lack of a common test set and appropriate query terms. This paper uses such a task and the STD evaluation is a good summary of the performance of different approaches on the same test conditions. 6. CONCLUSIONS This work studies how vocabulary independent spoken term detection can be performed efficiently over different data sources. Previously, phonetic-based and word-based approaches have been used for IR on speech data. The former suffers from low accuracy and the latter from limited vocabulary of the recognition system. In this paper, we have presented a vocabulary independent model of indexing and search that combines both the approaches. The system can deal with all kinds of queries although the phrases that need to combine for the retrieval, information extracted from two different indices, a word index and a phonetic index. The scoring of OOV terms is based on the proximity (in time) between the different phones. The scoring of IV terms is based on information provided by the WCNs. We have shown an improvement in the retrieval performance when using all the WCN and not only the 1-best path and when using phonetic index for search of OOV query terms. This approach always outperforms the other approaches using only word index or phonetic index. As a future work, we will compare our model for OOV search on phonetic transcripts with a retrieval model based on the edit distance.", "body1": "The rapidly increasing amount of spoken data calls for solutions to index and search this data. The classical approach consists of converting the speech to word transcripts using a large vocabulary continuous speech recognition (LVCSR) tool. However, a significant drawback of such approaches is that search on queries containing out-of-vocabulary (OOV) terms will not return any results. Another approach consists of converting the speech to phonetic transcripts and representing the query as a sequence of phones. A solution would be to combine the two different approaches presented above: we index both word transcripts and phonetic transcripts; during query processing, the information is retrieved from the word index for IV terms and from the phonetic index for OOV terms. The only element of comparison between phonetic and word transcripts are the timestamps. The paper is organized as follows. SYSTEM We use an ASR system for transcribing speech data. Typically, ASR generates lattices that can be considered as directed acyclic graphs. Mangu et al. 3. The 1-best path of a WCN is obtained from the path containing the best hypotheses. Typical structures of a lattice and a WCN are given in Figure 1. Figure 1: Typical structures of a lattice and a WCN. The main problem with retrieving information from spoken data is the low accuracy of the transcription particularly on terms of interest such as named entities and content words. Substitutions and deletions reflect the fact that an occurrence of a term in the speech signal is not recognized. Such an expansion tends to correct the substitutions and the deletions and consequently, might improve recall but will probably reduce precision. 3. evaluation plan , the task consists in finding all the exact matches of a specific query in a given corpus of speech data. Note that this task is different from the more classical task of spoken document retrieval. In addition, for WCN indexing, we store \u2022 the confidence level of the occurrence of u at the time t that is evaluated by its posterior probability Pr(u|t, D), \u2022 the rank of the occurrence of u among the other hypotheses beginning at the same time t, rank(u|t, D). Note that since the task is to find exact matches of the phrase queries, we have not filtered stopwords and the corpus is not stemmed before indexing. 3.3 Search In the following, we present our approach for accomplishing the STD task using the indices described above. The next step consists of merging the different posting lists according to the timestamp of the occurrences in order to create results matching the query. In such a way, we can reduce the effect of insertion errors since we allow insertions between the adjacent words and phones. Example: Let us consider the phrase query prosody research. 2. queries containing only OOV terms using the phonetic index. 3. keyword queries containing both IV and OOV terms using the word index for IV terms and the phonetic index for OOV terms; for query processing, the different sets of matches are unified if the query terms have OR semantics and intersected if the query terms have AND semantics. 4. phrase queries containing both IV and OOV terms; for query processing, the posting lists of the IV terms retrieved from the word index are merged with the posting lists of the OOV terms retrieved from the phonetic index. The STD evaluation has focused on the fourth query type. It is the hardest task since we need to combine posting lists retrieved from phonetic and word indices. 3.4 Ranking Since IV terms and OOV terms are retrieved from two different indices, we propose two different functions for scoring an occurrence of a term; afterward, an aggregate score is assigned to the query based on the scores of the query terms. Because the task is term detection, we do not use a document frequency criterion for ranking the occurrences. Let us consider a query Q = (k0, ..., kn), associated with a boosting vector B = (B1, ..., Bj). 3.4.1 In vocabulary term ranking For IV term ranking, we extend the work of Mamou et al. 3.4.2 Out of vocabulary term ranking For OOV term ranking, we use the information provided by the phonetic index. For each phone, we give the begin time and the duration in second. Occurrence 1: (p, 0.25, 0.01), (r, 0.36, 0.01), (aa, 0.37, 0.01), (z, 0.38, 0.01), (ih, 0.39, 0.01), (d, 0.4, 0.01), (iy, 0.52, 0.01). Occurrence 2: (p, 0.45, 0.01), (r, 0.46, 0.01), (aa, 0.47, 0.01), (z, 0.48, 0.01), (ih, 0.49, 0.01), (d, 0.5, 0.01), (iy, 0.51, 0.01). According to our formula, the score of the first occurrence is 0.83 and the score of the second occurrence is 1. The silence can be due to the fact that the phones belongs to two different words ans therefore, it is not an occurrence of the term prosody. 3.4.3 Combination The score of an occurrence of a query Q at time t0 in the document D is determined by the multiplication of the score of each keyword ki, where each ki occurs at time ti with a duration di in the transcript D: score(Q, t0, D) = i=0 score(ki, ti, D)\u03b3n Note that according to what we have ex-plained in Section 3.3, we have \u22001 \u2264 i \u2264 n, 0 < ti \u2212(ti\u22121 +di\u22121) < 0.5 sec. Our goal is to estimate for each found occurrence how likely the query appears. 4. evaluation . We have used the IBM research prototype ASR system, described in , for transcribing speech data. For each found occurrence of the given query, our system outputs: the location of the term in the audio recording (begin time and duration), the score indicating how likely is the occurrence of query, (as defined in Section 3.4) and a hard (binary) decision as to whether the detection is correct. Beside the recall and the precision, STD evaluation : the Actual Term-Weighted Value (ATWV) and the Maximum Term-Weighted Value (MTWV). Table 1: WER and distribution of the error types over word 1-best path extracted from WCNs for the different source types. \u2022 Nspurious(q, \u03b8) is the number of spurious detections of the query q with a score greater than or equal to \u03b8. \u2022 Ntrue(q) is the number of true occurrences of the query q in the corpus. \u2022 NNT (q) is the number of opportunities for incorrect detection of the query q in the corpus; it is the NonTarget query trials. ATWV is the actual term-weighted value; it is the detection value attained by the system as a result of the system output and the binary decision output for each putative occurrence. We have used the STDEval tool to extract the relevant results from the manual transcripts and to compute ATWV, MTWV and the DET curve. We have determined empirically the following values for the boosting vector defined in Section 3.4: Bi = 1 4.2 WER analysis We use the word error rate (WER) in order to characterize the accuracy of the transcripts. Table 1 gives the WER and the distribution of the error types over 1-best path transcripts extracted from WCNs. The WER of the 1-best path phonetic transcripts is approximately two times worse than the WER of word transcripts. That is the reason why we have not retrieved from phonetic transcripts on CONFMTG speech data. 4.3 Theta threshold We have determined empirically a detection threshold \u03b8 per source type and the hard decision of the occurrences having a score less than \u03b8 is set to false; false occurrences returned by the system are not considered as retrieved and therefore, are not used for computing ATWV, precision and recall. The value of the threshold \u03b8 per source type is reported in Table 2. Basically, setting a threshold aims to eliminate from the retrieved occurrences, false alarms without adding misses. The higher the WER is, the higher the \u03b8 threshold should be. 0.4 0.61 0.91 Table 2: Values of the \u03b8 threshold per source type. Concerning the index size, note that our index is compressed using IR index compression techniques. MB/ HP/ sec.P/ MB Table 3: Processing resource profile. WCN phonetic was our primary system for the evaluation and 1-best-WCN phonetic was one of our contrastive systems. Figure 2: DET curve for WCN phonetic approach. using WCNs relatively to 1-best path. 4.6 Influence of the duration of the query on the retrieval performance We have analysed the retrieval performance according to the average duration of the occurrences in the manual transcripts. Table 5: ATWV, MTWV according to the duration of the query occurrences per source type. We would like to compare three different approaches of retrieval: using only word index; using only phonetic index; combining word and phonetic indices. As expected, the approach combining word and phonetic indices presented in Section 3 leads to the same retrieval performance as the word approach for IV queries and to the same retrieval performance as the phonetic approach for OOV queries. In the past decade, the research efforts on spoken data retrieval have focused on extending classical IR techniques to spoken documents. An alternative approach consists of using word lattices in order to improve the effectiveness of SDR. Chelba and Acero in propose a more compact word lattice, the position specific posterior lattice (PSPL). Popular approaches to deal with OOV queries are based on sub-words transcripts, where the sub-words are typically phones, syllables or word fragments (sequences of phones) . They are attractive as they accommodate high error rate conditions as well as allow for OOV queries to be used . Saraclar and Sproat in show improvement in word spotting accuracy for both IV and OOV queries, using phonetic and word lattices, where a confidence measure of a word or a phone can be derived.", "body2": "The rapidly increasing amount of spoken data calls for solutions to index and search this data. concluded that Spoken document retrieval is a solved problem . In many applications the OOV rate may get worse over time unless the recognizer\"s vocabulary is periodically updated. Therefore, such approach cannot be an alternative to word transcripts, especially for in-vocabulary (IV) query terms that are part of the vocabulary of the ASR system. Therefore, we cannot merge posting lists retrieved by phonetic index with those retrieved by word index since the offset of the occurrences retrieved from the two different indices are not comparable. evaluation data . Finally, we conclude in Section 6. For best recognition results, a speaker-independent acoustic model and a language model are trained in advance on data with similar characteristics. The 1-best path transcript is obtained from the lattice using dynamic programming techniques. Extract a path from the word lattice (which can be the 1-best, the longest or any random path), and call it the pivot path of the alignment. Traverse the word lattice, and align all the transitions with the pivot, merging the transitions that correspond to the same word (or label) and occur in the same time interval by summing their posterior probabilities. As stated in , although WCNs are more compact than word lattices, in general the 1-best path obtained from WCN has a better word accuracy than the 1-best path obtained from the corresponding word lattice. Typical structures of a lattice and a WCN are given in Figure 1. Figure 1: Typical structures of a lattice and a WCN. There are three kinds of errors that can occur in a transcript: substitution of a term that is part of the speech by another term, deletion of a spoken term that is part of the speech and insertion of a term that is not part of the speech. These words can be taken from the other alternatives provided by the WCN; these alternatives may have been spoken but were not the top choice of the ASR. We will show that the usage of phonetic transcripts tends to improve the recall without affecting the precision too much, using an appropriate ranking. The queries are text and not speech. Each occurrence of a unit of indexing (word or phone) u in a transcript D is indexed with the following information: \u2022 the begin time t of the occurrence of u, \u2022 the duration d of the occurrence of u. In addition, for WCN indexing, we store \u2022 the confidence level of the occurrence of u at the time t that is evaluated by its posterior probability Pr(u|t, D), \u2022 the rank of the occurrence of u among the other hypotheses beginning at the same time t, rank(u|t, D). Note that since the task is to find exact matches of the phrase queries, we have not filtered stopwords and the corpus is not stemmed before indexing. The posting list of each phone is extracted from the phonetic index. For OOV search, we check that the distance in time between two adjacent phones of a query term is less that 0.2 seconds; this value has been determined empirically. Our query processing does not allow substitutions and deletions. Note that our indexing model allows to search for different types of queries: 1. queries containing only IV terms using the word index. 2. queries containing only OOV terms using the phonetic index. 3. keyword queries containing both IV and OOV terms using the word index for IV terms and the phonetic index for OOV terms; for query processing, the different sets of matches are unified if the query terms have OR semantics and intersected if the query terms have AND semantics. The merging is possible since we have stored the timestamps for each unit of indexing (word and phone) in both indices. The STD evaluation has focused on the fourth query type. It is the hardest task since we need to combine posting lists retrieved from phonetic and word indices. 3.4 Ranking Since IV terms and OOV terms are retrieved from two different indices, we propose two different functions for scoring an occurrence of a term; afterward, an aggregate score is assigned to the query based on the scores of the query terms. Because the task is term detection, we do not use a document frequency criterion for ranking the occurrences. If the rank r is larger than j, we assume Br = 0. We define the score score(k, t, D) of a keyword k occurring at a time t in the transcript D, by the following formula: score(k, t, D) = Brank(k|t,D) \u00d7 Pr(k|t, D) Note that 0 \u2264 score(k, t, D) \u2264 1. Example: let us consider the sequence (p, r, aa, z, ih, d, iy) and two different occurrences of the sequence. For each phone, we give the begin time and the duration in second. Occurrence 1: (p, 0.25, 0.01), (r, 0.36, 0.01), (aa, 0.37, 0.01), (z, 0.38, 0.01), (ih, 0.39, 0.01), (d, 0.4, 0.01), (iy, 0.52, 0.01). Occurrence 2: (p, 0.45, 0.01), (r, 0.46, 0.01), (aa, 0.47, 0.01), (z, 0.48, 0.01), (ih, 0.49, 0.01), (d, 0.5, 0.01), (iy, 0.51, 0.01). In the first occurrence, there are probably some insertion or silence between the phone p and r, and between the phone d and iy. The silence can be due to the fact that the phones belongs to two different words ans therefore, it is not an occurrence of the term prosody. 3.4.3 Combination The score of an occurrence of a query Q at time t0 in the document D is determined by the multiplication of the score of each keyword ki, where each ki occurs at time ti with a duration di in the transcript D: score(Q, t0, D) = i=0 score(ki, ti, D)\u03b3n Note that according to what we have ex-plained in Section 3.3, we have \u22001 \u2264 i \u2264 n, 0 < ti \u2212(ti\u22121 +di\u22121) < 0.5 sec. The begin time of the query occurrence is determined by the begin time t0 of the first query term and the duration of the query occurrence by tn \u2212 t0 + dn. Testing and determination of empirical values have been achieved on another set of speech data and queries, the development set, also provided by NIST. We have adapted Juru , a full-text search library written in Java, to index the transcripts and to store the timestamps of the words and phones; search results have been retrieved as described in Section 3. This approach for measuring search effectiveness using manual data as a reference is very common in speech retrieval research . Therefore, \u03b2 = 999.9. where: \u2022 Ncorrect(q, \u03b8) is the number of correct detections (retrieved by the system) of the query q with a score greater than or equal to \u03b8. \u2022 Nspurious(q, \u03b8) is the number of spurious detections of the query q with a score greater than or equal to \u03b8. \u2022 Ntrue(q) is the number of true occurrences of the query q in the corpus. Tspeech is the total amount of speech in the collection (in seconds). We have also provided the detection error tradeoff (DET) curve of miss probability (Pmiss) vs. false alarm probability (PF A). We have used the STDEval tool to extract the relevant results from the manual transcripts and to compute ATWV, MTWV and the DET curve. Deletion error rate (DELR) and insertion error rate (INSR) are defined in a similar manner. Table 1 gives the WER and the distribution of the error types over 1-best path transcripts extracted from WCNs. The WER of the 1-best path phonetic transcripts is approximately two times worse than the WER of word transcripts. That is the reason why we have not retrieved from phonetic transcripts on CONFMTG speech data. 4.3 Theta threshold We have determined empirically a detection threshold \u03b8 per source type and the hard decision of the occurrences having a score less than \u03b8 is set to false; false occurrences returned by the system are not considered as retrieved and therefore, are not used for computing ATWV, precision and recall. It is correlated to the accuracy of the transcripts. Basically, setting a threshold aims to eliminate from the retrieved occurrences, false alarms without adding misses. The higher the WER is, the higher the \u03b8 threshold should be. 4.4 Processing resource profile We report in Table 3 the processing resource profile. The indexing time includes both audio processing (generation of word and phonetic transcripts) and building of the searchable indices. The only difference between these two approaches is that, in 1-best-WCN phonetic, we index only the 1-best path extracted from the WCN instead of indexing all the WCN. precision 0.94 0.90 0.65 recall 0.89 0.81 0.37 1-best- precision 0.95 0.91 0.66 recall 0.84 0.75 0.37 Table 4: ATWV, MTWV, precision and recall per source type. Figure 2: DET curve for WCN phonetic approach. For comparison, the system that ranked at the third place, for BNEWS, for CONFMTG. Hence, it was justified to boost the score of the results with the exponent \u03b3n, as explained in Section 3.4.3, according to the length of the query. The following experiment has been achieved on the BNEWS collection and IV and OOV terms has been determined according to the vocabulary of BNEWS ASR system. Using a phone-based approach for dealing with IV queries affects also the performance of the retrieval relatively to the word-based approach. This approach always outperforms the others and it justifies the fact that we need to combine word and phonetic search. Moreover, the algorithms have been mostly tested over long queries stated in plain English and retrieval for such queries is more robust against speech recognition errors. The necessity to use a document expansion algorithm was justified by the fact that the word lattices they worked with, lack information about word probabilities. However, in the above works, the problem of queries containing OOV terms is not addressed. To account for the high recognition error rates, some other systems use richer transcripts like phonetic lattices. Consequently, phonetic approaches should be used only for OOV search; for searching queries containing also IV terms, this technique affects the performance of the retrieval in comparison to the word based approach. This paper uses such a task and the STD evaluation is a good summary of the performance of different approaches on the same test conditions.", "introduction": "The rapidly increasing amount of spoken data calls for solutions to index and search this data. The classical approach consists of converting the speech to word transcripts using a large vocabulary continuous speech recognition (LVCSR) tool. In the past decade, most of the research efforts on spoken data retrieval have focused on extending classical IR techniques to word transcripts. Some of these works have been done in the framework of the NIST TREC Spoken Document Retrieval tracks and are described by Garofolo et al. These tracks focused on retrieval from a corpus of broadcast news stories spoken by professionals. One of the conclusions of those tracks was that the effectiveness of retrieval mostly depends on the accuracy of the transcripts. While the accuracy of automatic speech recognition (ASR) systems depends on the scenario and environment, state-of-the-art systems achieved better than 90% accuracy in transcription of such data. concluded that Spoken document retrieval is a solved problem . However, a significant drawback of such approaches is that search on queries containing out-of-vocabulary (OOV) terms will not return any results. OOV terms are missing words from the ASR system vocabulary and are replaced in the output transcript by alternatives that are probable, given the recognition acoustic model and the language model. It has been experimentally observed that over 10% of user queries can contain OOV terms , as queries often relate to named entities that typically have a poor coverage in the ASR vocabulary. The effects of OOV query terms in spoken data retrieval are discussed by Woodland et al. In many applications the OOV rate may get worse over time unless the recognizer\"s vocabulary is periodically updated. Another approach consists of converting the speech to phonetic transcripts and representing the query as a sequence of phones. The retrieval is based on searching the sequence of phones representing the query in the phonetic transcripts. The main drawback of this approach is the inherent high error rate of the transcripts. Therefore, such approach cannot be an alternative to word transcripts, especially for in-vocabulary (IV) query terms that are part of the vocabulary of the ASR system. A solution would be to combine the two different approaches presented above: we index both word transcripts and phonetic transcripts; during query processing, the information is retrieved from the word index for IV terms and from the phonetic index for OOV terms. We would like to be able to process also hybrid queries, i.e, queries that include both IV and OOV terms. Consequently, we need to merge pieces of information retrieved from word index and phonetic index. Proximity information on the occurrences of the query terms is required for phrase search and for proximity-based ranking. In classical IR, the index stores for each occurrence of a term, its offset. Therefore, we cannot merge posting lists retrieved by phonetic index with those retrieved by word index since the offset of the occurrences retrieved from the two different indices are not comparable. The only element of comparison between phonetic and word transcripts are the timestamps. No previous work combining word and phonetic approach has been done on phrase search. We present a novel scheme for information retrieval that consists of storing, during the indexing process, for each unit of indexing (phone or word) its timestamp. We search queries by merging the information retrieved from the two different indices, word index and phonetic index, according to the timestamps of the query terms. evaluation data . The paper is organized as follows. We describe the audio processing in Section 2. The indexing and retrieval methods are presented in section 3. Experimental setup and results are given in Section 4. In Section 5, we give an overview of related work. Finally, we conclude in Section 6.", "conclusion": "This work studies how vocabulary independent spoken term detection can be performed efficiently over different data sources.. Previously, phonetic-based and word-based approaches have been used for IR on speech data.. The former suffers from low accuracy and the latter from limited vocabulary of the recognition system.. In this paper, we have presented a vocabulary independent model of indexing and search that combines both the approaches.. The system can deal with all kinds of queries although the phrases that need to combine for the retrieval, information extracted from two different indices, a word index and a phonetic index.. The scoring of OOV terms is based on the proximity (in time) between the different phones.. The scoring of IV terms is based on information provided by the WCNs.. We have shown an improvement in the retrieval performance when using all the WCN and not only the 1-best path and when using phonetic index for search of OOV query terms.. This approach always outperforms the other approaches using only word index or phonetic index.. As a future work, we will compare our model for OOV search on phonetic transcripts with a retrieval model based on the edit distance."}
{"id": "J-39", "keywords": ["onlin auction", "option", "proxi bid", "sequenti auction problem", "ebai"], "title": "The Sequential Auction Problem on eBay: An Empirical Analysis and a Solution", "abstract": "Bidders on eBay have no dominant bidding strategy when faced with multiple auctions each offering an item of interest. As seen through an analysis of 1,956 auctions on eBay for a Dell E193FP LCD monitor, some bidders win auctions at prices higher than those of other available auctions, while others never win an auction despite placing bids in losing efforts that are greater than the closing prices of other available auctions. These misqueues in strategic behavior hamper the efficiency of the system, and in so doing limit the revenue potential for sellers. This paper proposes a novel options-based extension to eBay's proxy-bidding system that resolves this strategic issue for buyers in commoditized markets. An empirical analysis of eBay provides a basis for computer simulations that investigate the market effects of the options-based scheme, and demonstrates that the options-based scheme provides greater efficiency than eBay, while also increasing seller revenue.", "references": ["Developing a bidding agent for multiple heterogeneous auctions", "User heterogeneity and its impact on electronic auctionmarket design: An empirical exploration", "Optimal bidding in on-line auctions", "Sequential auctions for the allocation of resources with complementarities", "Decision procedures for multiple auctions", "Mutually destructive bidding: The FCC auction design problem", "Consumer heterogeneity and competitive price-matching guarantees", "Investment under Uncertainty", "Managing risks in multiple online auctions: An options approach", "Shopbots and pricebots", "Inference with an incomplete model of English auctions", "Online auctions with re-usable goods", "Preemption and delay in eBay auctions", "A robust open ascending-price multi-unit auction protocol against false-name bids", "Price-matching policies: An empirical case", "Estimating bidders' valuation distributions in online auctions", "Competitive analysis of incentive compatible on-line auctions", "Price matching in a model of equilibrium price dispersion", "Business-to-business electronic commerce", "Last-minute bidding and the rules for ending second-price auctions: Evidence from eBay and Amazon auctions on the Internet", "Internet auctions with many traders", "Mechanism design for online real-time scheduling", "Innovative approaches to competitive mineral leasing", "Leveled commitment contracts and strategic breach", "Issues in automated negotiation and electronic commerce: Extending the Contract Net framework", "Mining for bidding strategies on eBay", "Late and multiple bidding in competing second price Internet auctions", "Is last minute bidding bad?", "An equilibrium model of a dynamic auction marketplace"], "full_text": "1. INTRODUCTION Electronic markets represent an application of information systems that has generated significant new trading opportunities while allowing for the dynamic pricing of goods. In addition to marketplaces such as eBay, electronic marketplaces are increasingly used for business-to-consumer auctions (e.g. to sell surplus inventory ). Many authors have written about a future in which commerce is mediated by online, automated trading agents . There is still little evidence of automated trading in e-markets, though. We believe that one leading place of resistance is in the lack of provably optimal bidding strategies for any but the simplest of market designs. Without this, we do not expect individual consumers, or firms, to be confident in placing their business in the hands of an automated agent. One of the most common examples today of an electronic marketplace is eBay, where the gross merchandise volume (i.e., the sum of all successfully closed listings) was $44B. Among items listed on eBay, many are essentially identical. This is especially true in the Consumer Electronics category , which accounted for roughly $3.5B of eBay\". This presence of essentially identical items can expose bidders, and sellers, to risks because of the sequential auction problem. For example, Alice may want an LCD monitor, and could potentially bid in either a 1 o\"clock or 3 o\"clock eBay auction. While Alice would prefer to participate in whichever auction will have the lower winning price, she cannot determine beforehand which auction that may be, and could end up winning the wrong auction. This is a problem of multiple copies. Another problem bidders may face is the exposure problem. As investigated by Bykowsky et al. , exposure problems exist when buyers desire a bundle of goods but may only participate in single-item auctions.1 For example, if Alice values a video game console by itself for $200, a video game by itself for $30, and both a console and game for $250, Alice must determine how much of the $20 of synergy value she might include in her bid for the console alone. Both problems arise in eBay as a result of sequential auctions of single items coupled with patient bidders with substitutes or complementary valuations. Why might the sequential auction problem be bad? Complex games may lead to bidders employing costly strategies and making mistakes. Potential bidders who do not wish to bear such costs may choose not to participate in the The exposure problem has been primarily investigated by Bykowsky et al. in the context of simultaneous single-item auctions. The problem is also a familiar one of online decision making. 180 market, inhibiting seller revenue opportunities. Additionally, among those bidders who do choose to participate, the mistakes made may lead to inefficient allocations, further limiting revenue opportunities. We are interested in creating modifications to eBay-style markets that simplify the bidder problem, leading to simple equilibrium strategies, and preferably better efficiency and revenue properties. 1.1 Options + Proxies: A Proposed Solution Retail stores have developed policies to assist their customers in addressing sequential purchasing problems. Return policies alleviate the exposure problem by allowing customers to return goods at the purchase price. Price matching alleviates the multiple copies problem by allowing buyers to receive from sellers after purchase the difference between the price paid for a good and a lower price found elsewhere for the same good . Furthermore, price matching can reduce the impact of exactly when a seller brings an item to market, as the price will in part be set by others selling the same item. These two retail policies provide the basis for the scheme proposed in this paper.2 We extend the proxy bidding technology currently employed by eBay. Our super-proxy extension will take advantage of a new, real options-based, market infrastructure that enables simple, yet optimal, bidding strategies. The extensions are computationally simple, handle temporal issues, and retain seller autonomy in deciding when to enter the market and conduct individual auctions. A seller sells an option for a good, which will ultimately lead to either a sale of the good or the return of the option. Buyers interact through a proxy agent, defining a value on all possible bundles of goods in which they have interest together with the latest time period in which they are willing to wait to receive the good(s). The proxy agents use this information to determine how much to bid for options, and follow a dominant bidding strategy across all relevant auctions. A proxy agent exercises options held when the buyer\"s patience has expired, choosing options that maximize a buyer\"s payoff given the reported valuation. All other options are returned to the market and not exercised. The options-based protocol makes truthful and immediate revelation to a proxy a dominant strategy for buyers, whatever the future auction dynamics. We conduct an empirical analysis of eBay, collecting data on over four months of bids for Dell LCD screens (model E193FP). LCD screens are a high-ticket item, for which we demonstrate evidence of the sequential bidding problem. We first infer a conservative model for the arrival time, departure time and value of bidders on eBay for LCD screens during this period. This model is used to simulate the performance of the optionsbased infrastructure, in order to make direct comparisons to the actual performance of eBay in this market. We also extend the work of Haile and Tamer to estimate an upper bound on the distribution of value of eBay bidders, taking into account the sequential auction problem when making the adjustments. Using this estimate, one can approximate how much greater a bidder\"s true value is Prior work has shown price matching as a potential mechanism for colluding firms to set monopoly prices. However, in our context, auction prices will be matched, which are not explicitly set by sellers but rather by buyers\" bids. from the maximum bid they were observed to have placed on eBay. Based on this approximation, revenue generated in a simulation of the options-based scheme exceeds revenue on eBay for the comparable population and sequence of auctions by 14.8%, while the options-based scheme demonstrates itself as being 7.5% more efficient. 1.2 Related Work A number of authors have analyzed the multiple copies problem, often times in the context of categorizing or modeling sniping behavior for reasons other than those first brought forward by Ockenfels and Roth . These papers perform equilibrium analysis in simpler settings, assuming bidders can participate in at most two auctions. Peters & Severinov extend these models to allow buyers to consider an arbitrary number of auctions, and characterize a perfect Bayesian equilibrium. However, their model does not allow auctions to close at distinct times and does not consider the arrival and departure of bidders. Previous work have developed a data-driven approach toward developing a taxonomy of strategies employed by bidders in practice when facing multi-unit auctions, but have not considered the sequential bidding problem . Previous work has also sought to provide agents with smarter bidding strategies . Unfortunately, it seems hard to design artificial agents with equilibrium bidding strategies, even for a simple simultaneous ascending price auction. Iwasaki et al. have considered the role of options in the context of a single, monolithic, auction design to help bidders with marginal-increasing values avoid exposure in a multi-unit, homogeneous item auction problem. In other contexts, options have been discussed for selling coal mine leases , or as leveled commitment contracts for use in a decentralized market place . Most similar to our work, Gopal et al. use options for reducing the risks of buyers and sellers in the sequential auction problem. However, their work uses costly options and does not remove the sequential bidding problem completely. Work on online mechanisms and online auctions considers agents that can dynamically arrive and depart across time. We leverage a recent price-based characterization by Hajiaghayi et al. to provide a dominant strategy equilibrium for buyers within our options-based protocol. The special case for single-unit buyers is equivalent to the protocol of Hajiaghayi et al., albeit with an options-based interpretation. Jiang and Leyton-Brown use machine learning techniques for bid identification in online auctions. 2. EBAY AND THE DELL E193FP The most common type of auction held on eBay is a singleitem proxy auction. Auctions open at a given time and remain open for a set period of time (usually one week). Bidders bid for the item by giving a proxy a value ceiling. The proxy will bid on behalf of the bidder only as much as is necessary to maintain a winning position in the auction, up to the ceiling received from the bidder. Bidders may communicate with the proxy multiple times before an auction closes. In the event that a bidder\"s proxy has been outbid, a bidder may give the proxy a higher ceiling to use in the auction. eBay\"s proxy auction implements an incremental version of a Vickrey auction, with the item sold to the highest bidder for the second-highest bid plus a small increment. 181 10 10 10 10 10 10 10 10 10 10 Number of Auctions NumberofBidders Auctions Available Auctions in Which Bid Figure 1: Histogram of number of LCD auctions available to each bidder and number of LCD auctions in which a bidder participates. The market analyzed in this paper is that of a specific model of an LCD monitor, a 19 Dell LCD model E193FP. This market was selected for a variety of reasons including: \u2022 The mean price of the monitor was $240 (with standard deviation $32), so we believe it reasonable to assume that bidders on the whole are only interested in acquiring one copy of the item on eBay.3 \u2022 The volume transacted is fairly high, at approximately 500 units sold per month. \u2022 The item is not usually bundled with other items. \u2022 The item is typically sold as new, and so suitable for the price-matching of the options-based scheme. Raw auction information was acquired via a PERL script. The script accesses the eBay search engine,4 and returns all auctions containing the terms \u2018Dell\" and \u2018LCD\" that have closed within the past month.5 Data was stored in a text file for post-processing. To isolate the auctions in the domain of interest,.6 Figure 1 provides a general sense of how many LCD auctions occur while a bidder is interested in pursuing a monitor.7 8,746 bidders (86%) had more than one auction available between when they first placed a bid on eBay and the For reference, Dell\" mail order catalogue quotes the price of the monitor as being $379 without a desktop purchase, and $240 as part of a desktop purchase upgrade. The search is not case-sensitive. Specifically, the query found all auctions where the title contained all of the following strings: \u2018Dell,\" \u2018LCD\" and \u2018E193FP,\" while excluding all auctions that contained any of the following strings: \u2018Dimension,\" \u2018GHZ,\" \u2018desktop,\" \u2018p4\" and \u2018GB.\" The exclusion terms were incorporated so that the only auctions analyzed would be those selling exclusively the LCD of interest. For example, the few bundled auctions selling both a Dell Dimension desktop and the E193FP LCD are excluded. As a reference, most auctions close on eBay between noon and midnight EDT, with almost two auctions for the Dell LCD monitor closing each hour on average during peak time periods. Bidders have an average observed patience of 3.9 days (with a standard deviation of 11.4 days). latest closing time of an auction in which they bid (with an average of 78 auctions available). Figure 1 also illustrates the number of auctions in which each bidder participates. Only 32.3% of bidders who had more than one auction available are observed to bid in more than one auction (bidding in 3.6 auctions on average). A simple regression analysis shows that bidders tend to submit maximal bids to an auction that are $1.22 higher after spending twice as much time in the system, as well as bids that are $0.27 higher in each subsequent auction. Among the 508 bidders that won exactly one monitor and participated in multiple auctions, 201 (40%) paid more than $10 more than the closing price of another auction in which they bid, paying on average $35 more (standard deviation $21) than the closing price of the cheapest auction in which they bid but did not win. Furthermore, among the 2,216 bidders that never won an item despite participating in multiple auctions, 421 (19%) placed a losing bid in one auction that was more than $10 higher than the closing price of another auction in which they bid, submitting a losing bid on average $34 more (standard deviation $23) than the closing price of the cheapest auction in which they bid but did not win. Although these measures do not say a bidder that lost could have definitively won (because we only consider the final winning price and not the bid of the winner to her proxy), or a bidder that won could have secured a better price, this is at least indicative of some bidder mistakes. 3. MODELING THE SEQUENTIAL AUCTION PROBLEM While the eBay analysis was for simple bidders who desire only a single item, let us now consider a more general scenario where people may desire multiple goods of different types, possessing general valuations over those goods. Consider a world with buyers (sometimes called bidders) B and K different types of goods G1...GK . Let T = {0, 1, ...} denote time periods. Let L denote a bundle of goods, represented as a vector of size K, where Lk \u2208 {0, 1} denotes the quantity of good type Gk in the bundle.8 The type of a buyer i \u2208 B is (ai, di, vi), with arrival time ai \u2208 T, departure time di \u2208 T, and private valuation vi(L) \u2265 0 for each bundle of goods L received between ai and di, and zero value otherwise. The arrival time models the period in which a buyer first realizes her demand and enters the market, while the departure time models the period in which a buyer loses interest in acquiring the good(s). In settings with general valuations, we need an additional assumption: an upper bound on the difference between a buyer\"s arrival and departure, denoted \u0394Max. Buyers have quasi-linear utilities, so that the utility of buyer i receiving bundle L and paying p, in some period no later than di, is ui(L, p) = vi(L) \u2212 p. Each seller j \u2208 S brings a single item kj to the market, has no intrinsic value and wants to maximize revenue. Seller j has an arrival time, aj, which models the period in which she is first interested in listing the item, while the departure time, dj, models the latest period in which she is willing to consider having an auction for the item close. A seller will receive payment by the end of the reported departure of the winning buyer. We extend notation whereby a single item k of type Gk refers to a vector L : Lk = 1. 182 We say an individual auction in a sequence is locally strategyproof (LSP) if truthful bidding is a dominant strategy for a buyer that can only bid in that auction. Consider the following example to see that LSP is insufficient for the existence of a dominant bidding strategy for buyers facing a sequence of auctions. Example 1. Alice values one ton of Sand with one ton of Stone at $2, 000. Bob holds a Vickrey auction for one ton of Sand on Monday and a Vickrey auction for one ton of Stone on Tuesday. Alice has no dominant bidding strategy because she needs to know the price for Stone on Tuesday to know her maximum willingness to pay for Sand on Monday. Definition 1. The sequential auction problem. Given a sequence of auctions, despite each auction being locally strategyproof, a bidder has no dominant bidding strategy. Consider a sequence of auctions. Generally, auctions selling the same item will be uncertainly-ordered, because a buyer will not know the ordering of closing prices among the auctions. Define the interesting bundles for a buyer as all bundles that could maximize the buyer\"s profit for some combination of auctions and bids of other buyers.9 Within the interesting bundles, say that an item has uncertain marginal value if the marginal value of an item depends on the other goods held by the buyer.10 Say that an item is oversupplied if there is more than one auction offering an item of that type. Say two bundles are substitutes if one of those bundles has the same value as the union of both bundles.11 Proposition 1. Given locally strategyproof single-item auctions, the sequential auction problem exists for a bidder if and only if either of the following two conditions is true: (1) within the set of interesting bundles (a) there are two bundles that are substitutes, (b) there is an item with uncertain marginal value, or (c) there is an item that is over-supplied; (2) a bidder faces competitors\" bids that are conditioned on the bidder\"s past bids. Proof. (Sketch.)(\u21d0) A bidder does not have a dominant strategy when (a) she does not know which bundle among substitutes to pursue, (b) she faces the exposure problem, or (c) she faces the multiple copies problem. Additionally, a bidder does not have a dominant strategy when she does not how to optimally influence the bids of competitors.(\u21d2) By contradiction. A bidder has a dominant strategy to bid its constant marginal value for a given item in each auction available when conditions (1) and (2) are both false. For example, the following buyers all face the sequential auction problem as a result of condition (a), (b) and (c) respectively: a buyer who values one ton of Sand for $1,000, or one ton of Stone for $2,000, but not both Sand and Stone; a buyer who values one ton of Sand for $1,000, one ton of Stone for $300, and one ton of Sand and one ton of Stone for $1,500, and can participate in an auction for Sand before an auction for Stone; a buyer who values one ton of Sand for $1,000 and can participate in many auctions selling Sand. Assume that the empty set is an interesting bundle. 10 Formally, an item k has uncertain marginal value if |{m : m = vi(Q) \u2212 vi(Q \u2212 k), \u2200Q \u2286 L \u2208 InterestingBundle, Q \u2287 k}| > 1. 11 Formally, two bundles A and B are substitutes if vi(A \u222a B) = max(vi(A), vi(B)), where A \u222a B = L where Lk = max(Ak, Bk). 4. SUPER PROXIES AND OPTIONS The novel solution proposed in this work to resolve the sequential auction problem consists of two primary components: richer proxy agents, and options with price matching. In finance, a real option is a right to acquire a real good at a certain price, called the exercise price. For instance, Alice may obtain from Bob the right to buy Sand from him at an exercise price of $1, 000. An option provides the right to purchase a good at an exercise price but not the obligation. This flexibility allows buyers to put together a collection of options on goods and then decide which to exercise. Options are typically sold at a price called the option price. However, options obtained at a non-zero option price cannot generally support a simple, dominant bidding strategy, as a buyer must compute the expected value of an option to justify the cost . This computation requires a model of the future, which in our setting requires a model of the bidding strategies and the values of other bidders. This is the very kind of game-theoretic reasoning that we want to avoid. Instead, we consider costless options with an option price of zero. This will require some care as buyers are weakly better off with a costless option than without one, whatever its exercise price. However, multiple bidders pursuing options with no intention of exercising them would cause the efficiency of an auction for options to unravel. This is the role of the mandatory proxy agents, which intermediate between buyers and the market. A proxy agent forces a link between the valuation function used to acquire options and the valuation used to exercise options. If a buyer tells her proxy an inflated value for an item, she runs the risk of having the proxy exercise options at a price greater than her value. 4.1 Buyer Proxies 4.1.1 Acquiring Options After her arrival, a buyer submits her valuation \u02c6vi (perhaps untruthfully) to her proxy in some period \u02c6ai \u2265 ai, along with a claim about her departure time \u02c6di \u2265 \u02c6ai. All transactions are intermediated via proxy agents. Each auction is modified to sell an option on that good to the highest bidding proxy, with an initial exercise price set to the second-highest bid received.12 When an option in which a buyer is interested becomes available for the first time, the proxy determines its bid by computing the buyer\"s maximum marginal value for the item, and then submits a bid in this amount. A proxy does not bid for an item when it already holds an option. The bid price is: bidt i(k) = max [\u02c6vi(L + k) \u2212 \u02c6vi(L)] (1) By having a proxy compute a buyer\"s maximum marginal value for an item and then bidding only that amount, a buyer\"s proxy will win any auction that could possibly be of benefit to the buyer and only lose those auctions that could never be of value to the buyer. 12 The system can set a reserve price for each good, provided that the reserve is universal for all auctions selling the same item. Without a universal reserve price, price matching is not possible because of the additional restrictions on prices that individual sellers will accept. 183 Buyer Type Monday Tuesday Molly (Mon, Tues, $8) 6Nancy 6Nancy \u2192 4Polly Nancy (Mon, Tues, $6) - 4Polly Polly (Mon, Tues, $4) -Table 1: Three-buyer example with each wanting a single item and one auction occurring on Monday and Tuesday. XY implies an option with exercise price X and bookkeeping that a proxy has prevented Y from currently possessing an option. \u2192 is the updating of exercise price and bookkeeping. When a proxy wins an auction for an option, the proxy will store in its local memory the identity (which may be a pseudonym) of the proxy not holding an option because of the proxy\"s win (i.e., the proxy that it \u2018bumped\" from winning, if any). This information will be used for price matching. 4.1.2 Pricing Options Sellers agree by joining the market to allow the proxy representing a buyer to adjust the exercise price of an option that it holds downwards if the proxy discovers that it could have achieved a better price by waiting to bid in a later auction for an option on the same good. To assist in the implementation of the price matching scheme each proxy tracks future auctions for an option that it has already won and will determine who would be bidding in that auction had the proxy delayed its entry into the market until this later auction. The proxy will request price matching from the seller that granted it an option if the proxy discovers that it could have secured a lower price by waiting. To reiterate, the proxy does not acquire more than one option for any good. Rather, it reduces the exercise price on its already issued option if a better deal is found. The proxy is able to discover these deals by asking each future auction to report the identities of the bidders in that auction together with their bids. This needs to be enforced by eBay, as the central authority. The highest bidder in this later auction, across those whose identity is not stored in the proxy\"s memory for the given item, is exactly the bidder against whom the proxy would be competing had it delayed its entry until this auction. If this high bid is lower than the current option price held, the proxy price matches down to this high bid price. After price matching, one of two adjustments will be made by the proxy for bookkeeping purposes. If the winner of the auction is the bidder whose identity has been in the proxy\"s local memory, the proxy will replace that local information with the identity of the bidder whose bid it just price matched, as that is now the bidder the proxy has prevented from obtaining an option. If the auction winner\"s identity is not stored in the proxy\"s local memory the memory may be cleared. In this case, the proxy will simply price match against the bids of future auction winners on this item until the proxy departs. Example 2 (Table 1). Molly\"s proxy wins the Monday auction, submitting a bid of $8 and receiving an option for $6. Molly\"s proxy adds Nancy to its local memory as Nancy\"s proxy would have won had Molly\"s proxy not bid. On Tuesday, only Nancy\"s and Polly\"s proxy bid (as Molly\"s proxy holds an option), with Nancy\"s proxy winning an opBuyer Type Monday Tuesday Truth: Molly (Mon, Mon, $8) 6NancyNancy (Mon, Tues, $6) - 4Polly Polly (Mon, Tues, $4) -Misreport: Molly (Mon, Mon, $8) -Nancy (Mon, Tues, $10) 8Molly 8Molly \u2192 4\u03c6 Polly (Mon, Tues, $4) - 0\u03c6 Misreport & match low: Molly (Mon, Mon, $8) -Nancy (Mon, Tues, $10) 8 8 \u2192 0 Polly (Mon, Tues, $4) - 0 Table 2: Examples demonstrating why bookkeeping will lead to a truthful system whereas simply matching to the lowest winning price will not. tion for $4 and noting that it bumped Polly\"s proxy. At this time, Molly\"s proxy will price match its option down to $4 and replace Nancy with Polly in its local memory as per the price match algorithm, as Polly would be holding an option had Molly never bid. 4.1.3 Exercising Options At the reported departure time the proxy chooses which options to exercise. Therefore, a seller of an option must wait until period \u02c6dw for the option to be exercised and receive payment, where w was the winner of the option.13 For bidder i, in period \u02c6di, the proxy chooses the option(s) that maximize the (reported) utility of the buyer: \u03b8\u2217 t = argmax \u03b8\u2286\u0398 (\u02c6vi(\u03b3(\u03b8)) \u2212 \u03c0(\u03b8)) (2) where \u0398 is the set of all options held, \u03b3(\u03b8) are the goods corresponding to a set of options, and \u03c0(\u03b8) is the sum of exercise prices for a set of options. All other options are returned.14 No options are exercised when no combination of options have positive utility. 4.1.4 Why bookkeep and not match winning price? One may believe that an alternative method for implementing a price matching scheme could be to simply have proxies match the lowest winning price they observe after winning an option. However, as demonstrated in Table 2, such a simple price matching scheme will not lead to a truthful system. The first scenario in Table 2 demonstrates the outcome if all agents were to truthfully report their types. Molly 13 While this appears restrictive on the seller, we believe it not significantly different than what sellers on eBay currently endure in practice. An auction on eBay closes at a specific time, but a seller must wait until a buyer relinquishes payment before being able to realize the revenue, an amount of time that could easily be days (if payment is via a money order sent through courier) to much longer (if a buyer is slow but not overtly delinquent in remitting her payment). 14 Presumably, an option returned will result in the seller holding a new auction for an option on the item it still possesses. However, the system will not allow a seller to re-auction an option until \u0394Max after the option had first been issued in order to maintain a truthful mechanism. 184 would win the Monday auction and receive an option with an exercise price of $6 (subsequently exercising that option at the end of Monday), and Nancy would win the Tuesday auction and receive an option with an exercise price of $4 (subsequently exercising that option at the end of Tuesday). The second scenario in Table 2 demonstrates the outcome if Nancy were to misreport her value for the good by reporting an inflated value of $10, using the proposed bookkeeping method. Nancy would win the Monday auction and receive an option with an exercise price of $8. On Tuesday, Polly would win the auction and receive an option with an exercise price of $0. Nancy\"s proxy would observe that the highest bid submitted on Tuesday among those proxies not stored in local memory is Polly\"s bid of $4, and so Nancy\"s proxy would price match the exercise price of its option down to $4. Note that the exercise price Nancy\"s proxy has obtained at the end of Tuesday is the same as when she truthfully revealed her type to her proxy. The third scenario in Table 2 demonstrates the outcome if Nancy were to misreport her value for the good by reporting an inflated value of $10, if the price matching scheme were for proxies to simply match their option price to the lowest winning price at any time while they are in the system. Nancy would win the Monday auction and receive an option with an exercise price of $8. On Tuesday, Polly would win the auction and receive an option with an exercise price of $0. Nancy\"s proxy would observe that the lowest price on Tuesday was $0, and so Nancy\"s proxy would price match the exercise price of its option down to $0. Note that the exercise price Nancy\"s proxy has obtained at the end of Tuesday is lower than when she truthfully revealed her type to the proxy. Therefore, a price matching policy of simply matching the lowest price paid may not elicit truthful information from buyers. 4.2 Complexity of Algorithm An XOR-valuation of size M for buyer i is a set of M terms, < L1 , v1 i > ...< LM , vM i >, that maps distinct bundles to values, where i is interested in acquiring at most one such bundle. For any bundle S, vi(S) = maxLm\u2286S(vm i ). Theorem 1. Given an XOR-valuation which possesses M terms, there is an O(KM2 ) algorithm for computing all maximum marginal values, where K is the number of different item types in which a buyer may be interested. Proof. For each item type, recall Equation 1 which defines the maximum marginal value of an item. For each bundle L in the M-term valuation, vi(L + k) may be found by iterating over the M terms. Therefore, the number of terms explored to determine the maximum marginal value for any item is O(M2 ), and so the total number of bundle comparisons to be performed to calculate all maximum marginal values is O(KM2 ). Theorem 2. The total memory required by a proxy for implementing price matching is O(K), where K is the number of distinct item types. The total work performed by a proxy to conduct price matching in each auction is O(1). Proof. By construction of the algorithm, the proxy stores one maximum marginal value for each item for bidding, of which there are O(K); at most one buyer\"s identity for each item, of which there are O(K); and one current option exercise price for each item, of which there are O(K). For each auction, the proxy either submits a precomputed bid or price matches, both of which take O(1) work. 4.3 Truthful Bidding to the Proxy Agent Proxies transform the market into a direct revelation mechanism, where each buyer i interacts with the proxy only once,15 and does so by declaring a bid, bi, which is defined as an announcement of her type, (\u02c6ai, \u02c6di, \u02c6vi), where the announcement may or may not be truthful. We denote all received bids other than i\"s as b\u2212i. Given bids, b = (bi, b\u2212i), the market determines allocations, xi(b), and payments, pi(b) \u2265 0, to each buyer (using an online algorithm). A dominant strategy equilibrium for buyers requires that vi(xi(bi, b\u2212i))\u2212pi(bi, b\u2212i) \u2265 vi(xi(bi, b\u2212i))\u2212pi(bi, b\u2212i), \u2200bi = bi, \u2200b\u2212i. We now establish that it is a dominant strategy for a buyer to reveal her true valuation and true departure time to her proxy agent immediately upon arrival to the system. The proof builds on the price-based characterization of strategyproof single-item online auctions in Hajiaghayi et al. . Define a monotonic and value-independent price function psi(ai, di, L, v\u2212i) which can depend on the values of other agents v\u2212i. Price psi(ai, di, L, v\u2212i) will represent the price available to agent i for bundle L in the mechanism if it announces arrival time ai and departure time di. The price is independent of the value vi of agent i, but can depend on ai, di and L as long as it satisfies a monotonicity condition. Definition 2. Price function psi(ai, di, L, v\u2212i) is monotonic if psi(ai, di, L , v\u2212i) \u2264 psi(ai, di, L, v\u2212i) for all ai \u2264 ai, all di \u2265 di, all bundles L \u2286 L and all v\u2212i. Lemma 1. An online combinatorial auction will be strategyproof (with truthful reports of arrival, departure and value a dominant strategy) when there exists a monotonic and value-independent price function, psi(ai, di, L, v\u2212i), such that for all i and all ai, di \u2208 T and all vi, agent i is allocated bundle L\u2217 = argmaxL [vi(L) \u2212 psi(ai, di, L, v\u2212i)] in period di and makes payment psi(ai, di, L\u2217 , v\u2212i). Proof. Agent i cannot benefit from reporting a later departure \u02c6di because the allocation is made in period \u02c6di and the agent would have no value for this allocation. Agent i cannot benefit from reporting a later arrival \u02c6ai \u2265 ai or earlier departure \u02c6di \u2264 di because of price monotonicity. Finally, the agent cannot benefit from reporting some \u02c6vi = vi because its reported valuation does not change the prices it faces and the mechanism maximizes its utility given its reported valuation and given the prices. Lemma 2. At any given time, there is at most one buyer in the system whose proxy does not hold an option for a given item type because of buyer i\"s presence in the system, and the identity of that buyer will be stored in i\"s proxy\"s local memory at that time if such a buyer exists. Proof. By induction. Consider the first proxy that a buyer prevents from winning an option. Either (a) the 15 For analysis purposes, we view the mechanism as an opaque market so that the buyer cannot condition her bid on bids placed by others. 185 bumped proxy will leave the system having never won an option, or (b) the bumped proxy will win an auction in the future. If (a), the buyer\"s presence prevented exactly that one buyer from winning an option, but will have not prevented any other proxies from winning an option (as the buyer\"s proxy will not bid on additional options upon securing one), and will have had that bumped proxy\"s identity in its local memory by definition of the algorithm. If (b), the buyer has not prevented the bumped proxy from winning an option after all, but rather has prevented only the proxy that lost to the bumped proxy from winning (if any), whose identity will now be stored in the proxy\"s local memory by definition of the algorithm. For this new identity in the buyer\"s proxy\"s local memory, either scenario (a) or (b) will be true, ad infinitum. Given this, we show that the options-based infrastructure implements a price-based auction with a monotonic and value-independent price schedule to every agent. Theorem 3. Truthful revelation of valuation, arrival and departure is a dominant strategy for a buyer in the optionsbased market. Proof. First, define a simple agent-independent price function pk i (t, v\u2212i) as the highest bid by the proxies not holding an option on an item of type Gk at time t, not including the proxy representing i herself and not including any proxies that would have already won an option had i never entered the system (i.e., whose identity is stored in i\"s proxy\"s local memory)(\u221e if no supply at t). This set of proxies is independent of any declaration i makes to its proxy (as the set explicitly excludes the at most one proxy (see Lemma 2) that i has prevented from holding an option), and each bid submitted by a proxy within this set is only a function of their own buyer\"s declared valuation (see Equation 1). Furthermore, i cannot influence the supply she faces as any options returned by bidders due to a price set by i\"s proxy\"s bid will be re-auctioned after i has departed the system. Therefore, pk i (t, v\u2212i) is independent of i\"s declaration to its proxy. Next, define psk i (\u02c6ai, \u02c6di, v\u2212i) = min\u02c6ai\u2264\u03c4\u2264 \u02c6di [pk i (\u03c4, v\u2212i)] (possibly \u221e) as the minimum price over pk i (t, v\u2212i), which is clearly monotonic. By construction of price matching, this is exactly the price obtained by a proxy on any option that it holds at departure. Define psi(\u02c6ai, \u02c6di, L, v\u2212i) = \u00c8k=K k=1 psk i (\u02c6ai, \u02c6di, v\u2212i)Lk, which is monotonic in \u02c6ai, \u02c6di and L since psk i (\u02c6ai, \u02c6di, v\u2212i) is monotonic in \u02c6ai and \u02c6di and (weakly) greater than zero for each k. Given the set of options held at \u02c6di, which may be a subset of those items with non-infinite prices, the proxy exercises options to maximize the reported utility. Left to show is that all bundles that could not be obtained with options held are priced sufficiently high as to not be preferred. For each such bundle, either there is an item priced at \u221e (in which case the bundle would not be desired) or there must be an item in that bundle for which the proxy does not hold an option that was available. In all auctions for such an item there must have been a distinct bidder with a bid greater than bidt i(k), which subsequently results in psk i (\u02c6ai, \u02c6di, v\u2212i) > bidt i(k), and so the bundle without k would be preferred to the bundle. Theorem 4. The super proxy, options-based scheme is individually-rational for both buyers and sellers. Price \u03c3(Price) Value Surplus eBay $240.24 $32 $244 $4 Options $239.66 $12 $263 $23 Table 3: Average price paid, standard deviation of prices paid, average bidder value among winners, and average winning bidder surplus on eBay for Dell E193FP LCD screens as well as the simulated options-based market using worst-case estimates of bidders\" true value. Proof. By construction, the proxy exercises the profit maximizing set of options obtained, or no options if no set of options derives non-negative surplus. Therefore, buyers are guaranteed non-negative surplus by participating in the scheme. For sellers, the price of each option is based on a non-negative bid or zero. 5. EVALUATING THE OPTIONS / PROXY INFRASTRUCTURE A goal of the empirical benchmarking and a reason to collect data from eBay is to try and build a realistic model of buyers from which to estimate seller revenue and other market effects under the options-based scheme. We simulate a sequence of auctions that match the timing of the Dell LCD auctions on eBay.16 When an auction successfully closes on eBay, we simulate a Vickrey auction for an option on the item sold in that period. Auctions that do not successfully close on eBay are not simulated. We estimate the arrival, departure and value of each bidder on eBay from their observed behavior.17 Arrival is estimated as the first time that a bidder interacts with the eBay proxy, while departure is estimated as the latest closing time among eBay auctions in which a bidder participates. We initially adopt a particularly conservative estimate for bidder value, estimating it as the highest bid a bidder was observed to make on eBay. Table 3 compares the distribution of closing prices on eBay and in the simulated options scheme. While the average revenue in both schemes is virtually the same ($239.66 in the options scheme vs. $240.24 on eBay), the winners in the options scheme tend to value the item won 7% more than the winners on eBay ($263 in the options scheme vs. $244 on eBay). 5.1 Bid Identification We extend the work of Haile and Tamer to sequential auctions to get a better view of underlying bidder values. Rather than assume for bidders an equilibrium behavior as in standard econometric techniques, Haile and Tamer do not attempt to model how bidders\" true values get mapped into a bid in any given auction. Rather, in the context of repeated 16 When running the simulations, the results of the first and final ten days of auctions are not recorded to reduce edge effects that come from viewing a discrete time window of a continuous process. 17 For the 100 bidders that won multiple times on eBay, we have each one bid a constant marginal value for each additional item in each auction until the number of options held equals the total number of LCDs won on eBay, with each option available for price matching independently. This bidding strategy is not a dominant strategy (falling outside the type space possible for buyers on which the proof of truthfulness has been built), but is believed to be the most appropriate first order action for simulation. 186 0 100 200 300 400 500 0.2 0.4 0.6 0.8 Value ($) CDF Observed Max Bids Upper Bound of True Value Figure 2: CDF of maximum bids observed and upper bound estimate of the bidding population\"s distribution for maximum willingness to pay. The true population distribution lies below the estimated upper bound. single-item auctions with distinct bidder populations, Haile and Tamer make only the following two assumptions when estimating the distribution of true bidder values: 1. Bidders do not bid more than they are willing to pay. 2. Bidders do not allow an opponent to win at a price they are willing to beat. From the first of their two assumptions, given the bids placed by each bidder in each auction, Haile and Tamer derive a method for estimating an upper bound of the bidding population\"s true value distribution (i.e., the bound that lies above the true value distribution). From the second of their two assumptions, given the winning price of each auction, Haile and Tamer derive a method for estimating a lower bound of the bidding population\"s true value distribution. It is only the upper-bound of the distribution that we utilize in our work. Haile and Tamer assume that bidders only participate in a single auction, and require independence of the bidding population from auction to auction. Neither assumption is valid here: the former because bidders are known to bid in more than one auction, and the latter because the set of bidders in an auction is in all likelihood not a true i.i.d. sampling of the overall bidding population. In particular, those who win auctions are less likely to bid in successive auctions, while those who lose auctions are more likely to remain bidders in future auctions. In applying their methods we make the following adjustments: \u2022 Within a given auction, each individual bidder\"s true willingness to pay is assumed weakly greater than the maximum bid that bidder submits across all auctions for that item (either past or future). \u2022 When estimating the upper bound of the value distribution, if a bidder bids in more than one auction, randomly select one of the auctions in which the bidder bid, and only utilize that one observation during the estimation.18 18 In current work, we assume that removing duplicate bidders is sufficient to make the buying populations independent i.i.d. draws from auction to auction. If one believes that certain portions of the population are drawn to cerPrice \u03c3(Price) Value Surplus eBay $240.24 $32 $281 $40 Options $275.80 $14 $302 $26 Table 4: Average price paid, standard deviation of prices paid, average bidder value among winners, and average winning bidder surplus on eBay for Dell E193FP LCD screens as well as in the simulated options-based market using an adjusted Haile and Tamer estimate of bidders\" true values being 15% higher than their maximum observed bid. Figure 2 provides the distribution of maximum bids placed by bidders on eBay as well as the estimated upper bound of the true value distribution of bidders based on the extended Haile and Tamer method.19 As can be seen, the smallest relative gap between the two curves meaningfully occurs near the 80th percentile, where the upper bound is 1.17 times the maximum bid. Therefore, adopted as a less conservative model of bidder values is a uniform scaling factor of 1.15. We now present results from this less conservative analysis. Table 4 shows the distribution of closing prices in auctions on eBay and in the simulated options scheme. The mean price in the options scheme is now significantly higher, 15% greater, than the prices on eBay ($276 in the options scheme vs. $240 on eBay), while the standard deviation of closing prices is lower among the options scheme auctions ($14 in the options scheme vs. $32 on eBay). Therefore, not only is the expected revenue stream higher, but the lower variance provides sellers a greater likelihood of realizing that higher revenue. The efficiency of the options scheme remains higher than on eBay. The winners in the options scheme now have an average estimated value 7.5% higher at $302. In an effort to better understand this efficiency, we formulated a mixed integer program (MIP) to determine a simple estimate of the allocative efficiency of eBay. The MIP computes the efficient value of the o\ufb04ine problem with full hindsight on all bids and all supply.20 Using a scaling of 1.15, the total value allocated to eBay winners is estimated at $551,242, while the optimal value (from the MIP) is $593,301. This suggests an allocative efficiency of 92.9%: while the typical value of a winner on eBay is $281, an average value of $303 was possible.21 Note the options-based tain auctions, then further adjustments would be required in order to utilize these techniques. 19 The estimation of the points in the curve is a minimization over many variables, many of which can have smallnumbers bias. Consequently, Haile and Tamer suggest using a weighted average over all terms yi of i yi exp(yi\u03c1)\u00c8j exp(yj \u03c1) to approximate the minimum while reducing the small number effects. We used \u03c1 = \u2212 and removed observations of auctions with 17 bidders or more as they occurred very infrequently. However, some small numbers bias still demonstrated itself with the plateau in our upper bound estimate around a value of $300. 20 Buyers who won more than one item on eBay are cloned so that they appear to be multiple bidders of identical type. 21 As long as one believes that every bidder\"s true value is a constant factor \u03b1 away from their observed maximum bid, the 92.9% efficiency calculation holds for any value of \u03b1. In practice, this belief may not be reasonable. For example, if losing bidders tend to have true values close to their observed 187 scheme comes very close to achieving this level of efficiency [at 99.7% efficient in this estimate] even though it operates without the benefit of hindsight. Finally, although the typical winning bidder surplus decreases between eBay and the options-based scheme, some surplus redistribution would be possible because the total market efficiency is improved.22 6. DISCUSSION The biggest concern with our scheme is that proxy agents who may be interested in many different items may acquire many more options than they finally exercise. This can lead to efficiency loss. Notice that this is not an issue when bidders are only interested in a single item (as in our empirical study), or have linear-additive values on items. To fix this, we would prefer to have proxy agents use more caution in acquiring options and use a more adaptive bidding strategy than that in Equation 1. For instance, if a proxy is already holding an option with an exercise price of $3 on some item for which it has value of $10, and it values some substitute item at $5, the proxy could reason that in no circumstance will it be useful to acquire an option on the second item. We formulate a more sophisticated bidding strategy along these lines. Let \u0398t be the set of all options a proxy for bidder i already possesses at time t. Let \u03b8t \u2286 \u0398t, be a subset of those options, the sum of whose exercise prices are \u03c0(\u03b8t), and the goods corresponding to those options being \u03b3(\u03b8t). Let \u03a0(\u03b8t) = \u02c6vi(\u03b3(\u03b8t)) \u2212 \u03c0(\u03b8t) be the (reported) available surplus associated with a set of options. Let \u03b8\u2217 t be the set of options currently held that would maximize the buyer\"s surplus; i.e., \u03b8\u2217 t = argmax\u03b8t\u2286\u0398t \u03a0(\u03b8t). Let the maximal willingness to pay for an item k represent a price above which the agent knows it would never exercise an option on the item given the current options held. This can be computed as follows: bidt i(k) = max [0, min[\u02c6vi(L + k) \u2212 \u03a0(\u03b8\u2217 t ), \u02c6vi(L + k) \u2212 \u02c6vi(L)]] (3) where \u02c6vi(L+k)\u2212\u03a0(\u03b8\u2217 t ) considers surplus already held, \u02c6vi(L+ k)\u2212\u02c6vi(L) considers the marginal value of a good, and taking the max[0, .] considers the overall use of pursuing the good. However, and somewhat counter intuitively, we are not able to implement this bidding scheme without forfeiting truthfulness. The \u03a0(\u03b8\u2217 t ) term in Equation 3 (i.e., the amount of guaranteed surplus bidder i has already obtained) can be influenced by proxy j\"s bid. Therefore, bidder j may have the incentive to misrepresent her valuation to her proxy if she believes doing so will cause i to bid differently in the future in a manner beneficial to j. Consider the following example where the proxy scheme is refined to bid the maximum willingness to pay. Example 3. Alice values either one ton of Sand or one ton of Stone for $2,000. Bob values either one ton of Sand or one ton of Stone for $1,500. All bidders have a patience maximum bids while eBay winners have true values much greater than their observed maximum bids then downward bias is introduced in the efficiency calculation at present. 22 The increase in eBay winner surplus between Tables 3 and 4 is to be expected as the \u03b1 scaling strictly increases the estimated value of the eBay winners while holding the prices at which they won constant. of 2 days. On day one, a Sand auction is held, where Alice\"s proxy bids $2,000 and Bob\"s bids $1,500. Alice\"s proxy wins an option to purchase Sand for $1,500. On day two, a Stone auction is held, where Alice\"s proxy bids $1,500 [as she has already obtained a guaranteed $500 of surplus from winning a Sand option, and so reduces her Stone bid by this amount], and Bob\"s bids $1,500. Either Alice\"s proxy or Bob\"s proxy will win the Stone option. At the end of the second day, Alice\"s proxy holds an option with an exercise price of $1,500 to obtain a good valued for $2,000, and so obtains $500 in surplus. Now, consider what would have happened had Alice declared that she valued only Stone. Example 4. Alice declares valuing only Stone for $2,000. Bob values either one ton of Sand or one ton of Stone for $1,500. All bidders have a patience of 2 days. On day one, a Sand auction is held, where Bob\"s proxy bids $1,500. Bob\"s proxy wins an option to purchase Sand for $0. On day two, a Stone auction is held, where Alice\"s proxy bids $2,000, and Bob\"s bids $0 [as he has already obtained a guaranteed $1,500 of surplus from winning a Sand option, and so reduces his Stone bid by this amount]. Alice\"s proxy wins the Stone option for $0. At the end of the second day, Alice\"s proxy holds an option with an exercise price of $0 to obtain a good valued for $2,000, and so obtains $2,000 in surplus. By misrepresenting her valuation (i.e., excluding her value of Sand), Alice was able to secure higher surplus by guiding Bob\"s bid for Stone to $0. An area of immediate further work by the authors is to develop a more sophisticated proxy agent that can allow for bidding of maximum willingness to pay (Equation 3) while maintaining truthfulness. An additional, practical, concern with our proxy scheme is that we assume an available, trusted, and well understood method to characterize goods (and presumably the quality of goods). We envision this happening in practice by sellers defining a classification for their item upon entering the market, for instance via a UPC code. Just as in eBay, this would allow an opportunity for sellers to improve revenue by overstating the quality of their item (new vs. like new), and raises the issue of how well a reputation scheme could address this. 7. CONCLUSIONS We introduced a new sales channel, consisting of an optionsbased and proxied auction protocol, to address the sequential auction problem that exists when bidders face multiple auctions for substitutes and complements goods. Our scheme provides bidders with a simple, dominant and truthful bidding strategy even though the market remains open and dynamic. In addition to exploring more sophisticated proxies that bid in terms of maximum willingness to pay, future work should aim to better model seller incentives and resolve the strategic problems facing sellers. For instance, does the options scheme change seller incentives from what they currently are on eBay? Acknowledgments We would like to thank Pai-Ling Yin. Helpful comments have been received from William Simpson, attendees at Har188 vard University\"s EconCS and ITM seminars, and anonymous reviewers. Thank you to Aaron L. Roth and KangXing Jin for technical support. All errors and omissions remain our own.", "body1": "Electronic markets represent an application of information systems that has generated significant new trading opportunities while allowing for the dynamic pricing of goods. Many authors have written about a future in which commerce is mediated by online, automated trading agents . One of the most common examples today of an electronic marketplace is eBay, where the gross merchandise volume (i.e., the sum of all successfully closed listings) was $44B. For example, Alice may want an LCD monitor, and could potentially bid in either a 1 o\"clock or 3 o\"clock eBay auction. Another problem bidders may face is the exposure problem. Why might the sequential auction problem be bad? Additionally, among those bidders who do choose to participate, the mistakes made may lead to inefficient allocations, further limiting revenue opportunities. We are interested in creating modifications to eBay-style markets that simplify the bidder problem, leading to simple equilibrium strategies, and preferably better efficiency and revenue properties. 1.1 Options + Proxies: A Proposed Solution Retail stores have developed policies to assist their customers in addressing sequential purchasing problems. Return policies alleviate the exposure problem by allowing customers to return goods at the purchase price. Buyers interact through a proxy agent, defining a value on all possible bundles of goods in which they have interest together with the latest time period in which they are willing to wait to receive the good(s). We conduct an empirical analysis of eBay, collecting data on over four months of bids for Dell LCD screens (model E193FP). We also extend the work of Haile and Tamer to estimate an upper bound on the distribution of value of eBay bidders, taking into account the sequential auction problem when making the adjustments. 1.2 Related Work A number of authors have analyzed the multiple copies problem, often times in the context of categorizing or modeling sniping behavior for reasons other than those first brought forward by Ockenfels and Roth . Previous work has also sought to provide agents with smarter bidding strategies . Iwasaki et al. Work on online mechanisms and online auctions considers agents that can dynamically arrive and depart across time. Jiang and Leyton-Brown use machine learning techniques for bid identification in online auctions. The most common type of auction held on eBay is a singleitem proxy auction. Bidders bid for the item by giving a proxy a value ceiling. eBay\"s proxy auction implements an incremental version of a Vickrey auction, with the item sold to the highest bidder for the second-highest bid plus a small increment. 181 10 10 10 10 10 10 10 10 10 10 Number of Auctions NumberofBidders Auctions Available Auctions in Which Bid Figure 1: Histogram of number of LCD auctions available to each bidder and number of LCD auctions in which a bidder participates. The market analyzed in this paper is that of a specific model of an LCD monitor, a 19 Dell LCD model E193FP. This market was selected for a variety of reasons including: \u2022 The mean price of the monitor was $240 (with standard deviation $32), so we believe it reasonable to assume that bidders on the whole are only interested in acquiring one copy of the item on eBay.3 \u2022 The volume transacted is fairly high, at approximately 500 units sold per month. \u2022 The item is not usually bundled with other items. \u2022 The item is typically sold as new, and so suitable for the price-matching of the options-based scheme. Raw auction information was acquired via a PERL script. The script accesses the eBay search engine,4 and returns all auctions containing the terms \u2018Dell\" and \u2018LCD\" that have closed within the past month.5 Data was stored in a text file for post-processing. The search is not case-sensitive. Specifically, the query found all auctions where the title contained all of the following strings: \u2018Dell,\" \u2018LCD\" and \u2018E193FP,\" while excluding all auctions that contained any of the following strings: \u2018Dimension,\" \u2018GHZ,\" \u2018desktop,\" \u2018p4\" and \u2018GB.\" latest closing time of an auction in which they bid (with an average of 78 auctions available). Only 32.3% of bidders who had more than one auction available are observed to bid in more than one auction (bidding in 3.6 auctions on average). Among the 508 bidders that won exactly one monitor and participated in multiple auctions, 201 (40%) paid more than $10 more than the closing price of another auction in which they bid, paying on average $35 more (standard deviation $21) than the closing price of the cheapest auction in which they bid but did not win. AUCTION PROBLEM While the eBay analysis was for simple bidders who desire only a single item, let us now consider a more general scenario where people may desire multiple goods of different types, possessing general valuations over those goods. Consider a world with buyers (sometimes called bidders) B and K different types of goods G1...GK . 182 We say an individual auction in a sequence is locally strategyproof (LSP) if truthful bidding is a dominant strategy for a buyer that can only bid in that auction. Example 1. Definition 1. Consider a sequence of auctions. Proof. Assume that the empty set is an interesting bundle. 10 Formally, an item k has uncertain marginal value if |{m : m = vi(Q) \u2212 vi(Q \u2212 k), \u2200Q \u2286 L \u2208 InterestingBundle, Q \u2287 k}| > 1. 11 Formally, two bundles A and B are substitutes if vi(A \u222a B) = max(vi(A), vi(B)), where A \u222a B = L where Lk = max(Ak, Bk). The novel solution proposed in this work to resolve the sequential auction problem consists of two primary components: richer proxy agents, and options with price matching. In finance, a real option is a right to acquire a real good at a certain price, called the exercise price. Options are typically sold at a price called the option price. Instead, we consider costless options with an option price of zero. 4.1 Buyer Proxies 4.1.1 Acquiring Options After her arrival, a buyer submits her valuation \u02c6vi (perhaps untruthfully) to her proxy in some period \u02c6ai \u2265 ai, along with a claim about her departure time \u02c6di \u2265 \u02c6ai. 183 Buyer Type Monday Tuesday Molly (Mon, Tues, $8) 6Nancy 6Nancy \u2192 4Polly Nancy (Mon, Tues, $6) - 4Polly Polly (Mon, Tues, $4) -Table 1: Three-buyer example with each wanting a single item and one auction occurring on Monday and Tuesday. 4.1.2 Pricing Options Sellers agree by joining the market to allow the proxy representing a buyer to adjust the exercise price of an option that it holds downwards if the proxy discovers that it could have achieved a better price by waiting to bid in a later auction for an option on the same good. The proxy is able to discover these deals by asking each future auction to report the identities of the bidders in that auction together with their bids. After price matching, one of two adjustments will be made by the proxy for bookkeeping purposes. Example 2 (Table 1). tion for $4 and noting that it bumped Polly\"s proxy. 4.1.3 Exercising Options At the reported departure time the proxy chooses which options to exercise. 4.1.4 Why bookkeep and not match winning price? The first scenario in Table 2 demonstrates the outcome if all agents were to truthfully report their types. 184 would win the Monday auction and receive an option with an exercise price of $6 (subsequently exercising that option at the end of Monday), and Nancy would win the Tuesday auction and receive an option with an exercise price of $4 (subsequently exercising that option at the end of Tuesday). The second scenario in Table 2 demonstrates the outcome if Nancy were to misreport her value for the good by reporting an inflated value of $10, using the proposed bookkeeping method. Nancy would win the Monday auction and receive an option with an exercise price of $8. 4.2 Complexity of Algorithm An XOR-valuation of size M for buyer i is a set of M terms, < L1 , v1 i > ...< LM , vM i >, that maps distinct bundles to values, where i is interested in acquiring at most one such bundle. Theorem 1. Theorem 2. 4.3 Truthful Bidding to the Proxy Agent Proxies transform the market into a direct revelation mechanism, where each buyer i interacts with the proxy only once,15 and does so by declaring a bid, bi, which is defined as an announcement of her type, (\u02c6ai, \u02c6di, \u02c6vi), where the announcement may or may not be truthful. We now establish that it is a dominant strategy for a buyer to reveal her true valuation and true departure time to her proxy agent immediately upon arrival to the system. Define a monotonic and value-independent price function psi(ai, di, L, v\u2212i) which can depend on the values of other agents v\u2212i. Lemma 1. Lemma 2. 185 bumped proxy will leave the system having never won an option, or (b) the bumped proxy will win an auction in the future. Theorem 3. By construction of price matching, this is exactly the price obtained by a proxy on any option that it holds at departure. Price \u03c3(Price) Value Surplus eBay $240.24 $32 $244 $4 Options $239.66 $12 $263 $23 Table 3: Average price paid, standard deviation of prices paid, average bidder value among winners, and average winning bidder surplus on eBay for Dell E193FP LCD screens as well as the simulated options-based market using worst-case estimates of bidders\" true value. INFRASTRUCTURE A goal of the empirical benchmarking and a reason to collect data from eBay is to try and build a realistic model of buyers from which to estimate seller revenue and other market effects under the options-based scheme. We simulate a sequence of auctions that match the timing of the Dell LCD auctions on eBay.16 When an auction successfully closes on eBay, we simulate a Vickrey auction for an option on the item sold in that period. We initially adopt a particularly conservative estimate for bidder value, estimating it as the highest bid a bidder was observed to make on eBay. Rather than assume for bidders an equilibrium behavior as in standard econometric techniques, Haile and Tamer do not attempt to model how bidders\" true values get mapped into a bid in any given auction. 17 For the 100 bidders that won multiple times on eBay, we have each one bid a constant marginal value for each additional item in each auction until the number of options held equals the total number of LCDs won on eBay, with each option available for price matching independently. 186 0 100 200 300 400 500 0.2 0.4 0.6 0.8 Value ($) CDF Observed Max Bids Upper Bound of True Value Figure 2: CDF of maximum bids observed and upper bound estimate of the bidding population\"s distribution for maximum willingness to pay. single-item auctions with distinct bidder populations, Haile and Tamer make only the following two assumptions when estimating the distribution of true bidder values: 1. 2. From the first of their two assumptions, given the bids placed by each bidder in each auction, Haile and Tamer derive a method for estimating an upper bound of the bidding population\"s true value distribution (i.e., the bound that lies above the true value distribution). It is only the upper-bound of the distribution that we utilize in our work. Haile and Tamer assume that bidders only participate in a single auction, and require independence of the bidding population from auction to auction. sampling of the overall bidding population. In applying their methods we make the following adjustments: \u2022 Within a given auction, each individual bidder\"s true willingness to pay is assumed weakly greater than the maximum bid that bidder submits across all auctions for that item (either past or future). \u2022 When estimating the upper bound of the value distribution, if a bidder bids in more than one auction, randomly select one of the auctions in which the bidder bid, and only utilize that one observation during the estimation.18 18 In current work, we assume that removing duplicate bidders is sufficient to make the buying populations independent i.i.d. We now present results from this less conservative analysis. In an effort to better understand this efficiency, we formulated a mixed integer program (MIP) to determine a simple estimate of the allocative efficiency of eBay. 19 The estimation of the points in the curve is a minimization over many variables, many of which can have smallnumbers bias. 21 As long as one believes that every bidder\"s true value is a constant factor \u03b1 away from their observed maximum bid, the 92.9% efficiency calculation holds for any value of \u03b1. The biggest concern with our scheme is that proxy agents who may be interested in many different items may acquire many more options than they finally exercise. We formulate a more sophisticated bidding strategy along these lines. Let \u03a0(\u03b8t) = \u02c6vi(\u03b3(\u03b8t)) \u2212 \u03c0(\u03b8t) be the (reported) available surplus associated with a set of options. Let the maximal willingness to pay for an item k represent a price above which the agent knows it would never exercise an option on the item given the current options held. However, and somewhat counter intuitively, we are not able to implement this bidding scheme without forfeiting truthfulness. Example 3. of 2 days. Example 4. Bob values either one ton of Sand or one ton of Stone for $1,500. An additional, practical, concern with our proxy scheme is that we assume an available, trusted, and well understood method to characterize goods (and presumably the quality of goods).", "body2": "to sell surplus inventory ). Without this, we do not expect individual consumers, or firms, to be confident in placing their business in the hands of an automated agent. This presence of essentially identical items can expose bidders, and sellers, to risks because of the sequential auction problem. This is a problem of multiple copies. Both problems arise in eBay as a result of sequential auctions of single items coupled with patient bidders with substitutes or complementary valuations. 180 market, inhibiting seller revenue opportunities. Additionally, among those bidders who do choose to participate, the mistakes made may lead to inefficient allocations, further limiting revenue opportunities. We are interested in creating modifications to eBay-style markets that simplify the bidder problem, leading to simple equilibrium strategies, and preferably better efficiency and revenue properties. 1.1 Options + Proxies: A Proposed Solution Retail stores have developed policies to assist their customers in addressing sequential purchasing problems. A seller sells an option for a good, which will ultimately lead to either a sale of the good or the return of the option. The options-based protocol makes truthful and immediate revelation to a proxy a dominant strategy for buyers, whatever the future auction dynamics. This model is used to simulate the performance of the optionsbased infrastructure, in order to make direct comparisons to the actual performance of eBay in this market. Based on this approximation, revenue generated in a simulation of the options-based scheme exceeds revenue on eBay for the comparable population and sequence of auctions by 14.8%, while the options-based scheme demonstrates itself as being 7.5% more efficient. Previous work have developed a data-driven approach toward developing a taxonomy of strategies employed by bidders in practice when facing multi-unit auctions, but have not considered the sequential bidding problem . Unfortunately, it seems hard to design artificial agents with equilibrium bidding strategies, even for a simple simultaneous ascending price auction. However, their work uses costly options and does not remove the sequential bidding problem completely. The special case for single-unit buyers is equivalent to the protocol of Hajiaghayi et al., albeit with an options-based interpretation. Jiang and Leyton-Brown use machine learning techniques for bid identification in online auctions. Auctions open at a given time and remain open for a set period of time (usually one week). In the event that a bidder\"s proxy has been outbid, a bidder may give the proxy a higher ceiling to use in the auction. eBay\"s proxy auction implements an incremental version of a Vickrey auction, with the item sold to the highest bidder for the second-highest bid plus a small increment. 181 10 10 10 10 10 10 10 10 10 10 Number of Auctions NumberofBidders Auctions Available Auctions in Which Bid Figure 1: Histogram of number of LCD auctions available to each bidder and number of LCD auctions in which a bidder participates. The market analyzed in this paper is that of a specific model of an LCD monitor, a 19 Dell LCD model E193FP. This market was selected for a variety of reasons including: \u2022 The mean price of the monitor was $240 (with standard deviation $32), so we believe it reasonable to assume that bidders on the whole are only interested in acquiring one copy of the item on eBay.3 \u2022 The volume transacted is fairly high, at approximately 500 units sold per month. \u2022 The item is not usually bundled with other items. \u2022 The item is typically sold as new, and so suitable for the price-matching of the options-based scheme. Raw auction information was acquired via a PERL script. To isolate the auctions in the domain of interest,.6 Figure 1 provides a general sense of how many LCD auctions occur while a bidder is interested in pursuing a monitor.7 8,746 bidders (86%) had more than one auction available between when they first placed a bid on eBay and the For reference, Dell\" mail order catalogue quotes the price of the monitor as being $379 without a desktop purchase, and $240 as part of a desktop purchase upgrade. The search is not case-sensitive. Bidders have an average observed patience of 3.9 days (with a standard deviation of 11.4 days). Figure 1 also illustrates the number of auctions in which each bidder participates. A simple regression analysis shows that bidders tend to submit maximal bids to an auction that are $1.22 higher after spending twice as much time in the system, as well as bids that are $0.27 higher in each subsequent auction. Although these measures do not say a bidder that lost could have definitively won (because we only consider the final winning price and not the bid of the winner to her proxy), or a bidder that won could have secured a better price, this is at least indicative of some bidder mistakes. AUCTION PROBLEM While the eBay analysis was for simple bidders who desire only a single item, let us now consider a more general scenario where people may desire multiple goods of different types, possessing general valuations over those goods. We extend notation whereby a single item k of type Gk refers to a vector L : Lk = 1. Consider the following example to see that LSP is insufficient for the existence of a dominant bidding strategy for buyers facing a sequence of auctions. Alice has no dominant bidding strategy because she needs to know the price for Stone on Tuesday to know her maximum willingness to pay for Sand on Monday. Given a sequence of auctions, despite each auction being locally strategyproof, a bidder has no dominant bidding strategy. Given locally strategyproof single-item auctions, the sequential auction problem exists for a bidder if and only if either of the following two conditions is true: (1) within the set of interesting bundles (a) there are two bundles that are substitutes, (b) there is an item with uncertain marginal value, or (c) there is an item that is over-supplied; (2) a bidder faces competitors\" bids that are conditioned on the bidder\"s past bids. For example, the following buyers all face the sequential auction problem as a result of condition (a), (b) and (c) respectively: a buyer who values one ton of Sand for $1,000, or one ton of Stone for $2,000, but not both Sand and Stone; a buyer who values one ton of Sand for $1,000, one ton of Stone for $300, and one ton of Sand and one ton of Stone for $1,500, and can participate in an auction for Sand before an auction for Stone; a buyer who values one ton of Sand for $1,000 and can participate in many auctions selling Sand. Assume that the empty set is an interesting bundle. 10 Formally, an item k has uncertain marginal value if |{m : m = vi(Q) \u2212 vi(Q \u2212 k), \u2200Q \u2286 L \u2208 InterestingBundle, Q \u2287 k}| > 1. 11 Formally, two bundles A and B are substitutes if vi(A \u222a B) = max(vi(A), vi(B)), where A \u222a B = L where Lk = max(Ak, Bk). The novel solution proposed in this work to resolve the sequential auction problem consists of two primary components: richer proxy agents, and options with price matching. This flexibility allows buyers to put together a collection of options on goods and then decide which to exercise. This is the very kind of game-theoretic reasoning that we want to avoid. If a buyer tells her proxy an inflated value for an item, she runs the risk of having the proxy exercise options at a price greater than her value. Without a universal reserve price, price matching is not possible because of the additional restrictions on prices that individual sellers will accept. This information will be used for price matching. Rather, it reduces the exercise price on its already issued option if a better deal is found. If this high bid is lower than the current option price held, the proxy price matches down to this high bid price. In this case, the proxy will simply price match against the bids of future auction winners on this item until the proxy departs. On Tuesday, only Nancy\"s and Polly\"s proxy bid (as Molly\"s proxy holds an option), with Nancy\"s proxy winning an opBuyer Type Monday Tuesday Truth: Molly (Mon, Mon, $8) 6NancyNancy (Mon, Tues, $6) - 4Polly Polly (Mon, Tues, $4) -Misreport: Molly (Mon, Mon, $8) -Nancy (Mon, Tues, $10) 8Molly 8Molly \u2192 4\u03c6 Polly (Mon, Tues, $4) - 0\u03c6 Misreport & match low: Molly (Mon, Mon, $8) -Nancy (Mon, Tues, $10) 8 8 \u2192 0 Polly (Mon, Tues, $4) - 0 Table 2: Examples demonstrating why bookkeeping will lead to a truthful system whereas simply matching to the lowest winning price will not. At this time, Molly\"s proxy will price match its option down to $4 and replace Nancy with Polly in its local memory as per the price match algorithm, as Polly would be holding an option had Molly never bid. All other options are returned.14 No options are exercised when no combination of options have positive utility. However, as demonstrated in Table 2, such a simple price matching scheme will not lead to a truthful system. However, the system will not allow a seller to re-auction an option until \u0394Max after the option had first been issued in order to maintain a truthful mechanism. 184 would win the Monday auction and receive an option with an exercise price of $6 (subsequently exercising that option at the end of Monday), and Nancy would win the Tuesday auction and receive an option with an exercise price of $4 (subsequently exercising that option at the end of Tuesday). The third scenario in Table 2 demonstrates the outcome if Nancy were to misreport her value for the good by reporting an inflated value of $10, if the price matching scheme were for proxies to simply match their option price to the lowest winning price at any time while they are in the system. Therefore, a price matching policy of simply matching the lowest price paid may not elicit truthful information from buyers. For any bundle S, vi(S) = maxLm\u2286S(vm i ). Given an XOR-valuation which possesses M terms, there is an O(KM2 ) algorithm for computing all maximum marginal values, where K is the number of different item types in which a buyer may be interested. Therefore, the number of terms explored to determine the maximum marginal value for any item is O(M2 ), and so the total number of bundle comparisons to be performed to calculate all maximum marginal values is O(KM2 ). The total work performed by a proxy to conduct price matching in each auction is O(1). For each auction, the proxy either submits a precomputed bid or price matches, both of which take O(1) work. A dominant strategy equilibrium for buyers requires that vi(xi(bi, b\u2212i))\u2212pi(bi, b\u2212i) \u2265 vi(xi(bi, b\u2212i))\u2212pi(bi, b\u2212i), \u2200bi = bi, \u2200b\u2212i. . Price function psi(ai, di, L, v\u2212i) is monotonic if psi(ai, di, L , v\u2212i) \u2264 psi(ai, di, L, v\u2212i) for all ai \u2264 ai, all di \u2265 di, all bundles L \u2286 L and all v\u2212i. An online combinatorial auction will be strategyproof (with truthful reports of arrival, departure and value a dominant strategy) when there exists a monotonic and value-independent price function, psi(ai, di, L, v\u2212i), such that for all i and all ai, di \u2208 T and all vi, agent i is allocated bundle L\u2217 = argmaxL [vi(L) \u2212 psi(ai, di, L, v\u2212i)] in period di and makes payment psi(ai, di, L\u2217 , v\u2212i). Finally, the agent cannot benefit from reporting some \u02c6vi = vi because its reported valuation does not change the prices it faces and the mechanism maximizes its utility given its reported valuation and given the prices. At any given time, there is at most one buyer in the system whose proxy does not hold an option for a given item type because of buyer i\"s presence in the system, and the identity of that buyer will be stored in i\"s proxy\"s local memory at that time if such a buyer exists. Either (a) the 15 For analysis purposes, we view the mechanism as an opaque market so that the buyer cannot condition her bid on bids placed by others. Given this, we show that the options-based infrastructure implements a price-based auction with a monotonic and value-independent price schedule to every agent. Truthful revelation of valuation, arrival and departure is a dominant strategy for a buyer in the optionsbased market. Next, define psk i (\u02c6ai, \u02c6di, v\u2212i) = min\u02c6ai\u2264\u03c4\u2264 \u02c6di [pk i (\u03c4, v\u2212i)] (possibly \u221e) as the minimum price over pk i (t, v\u2212i), which is clearly monotonic. The super proxy, options-based scheme is individually-rational for both buyers and sellers. Price \u03c3(Price) Value Surplus eBay $240.24 $32 $244 $4 Options $239.66 $12 $263 $23 Table 3: Average price paid, standard deviation of prices paid, average bidder value among winners, and average winning bidder surplus on eBay for Dell E193FP LCD screens as well as the simulated options-based market using worst-case estimates of bidders\" true value. For sellers, the price of each option is based on a non-negative bid or zero. INFRASTRUCTURE A goal of the empirical benchmarking and a reason to collect data from eBay is to try and build a realistic model of buyers from which to estimate seller revenue and other market effects under the options-based scheme. We estimate the arrival, departure and value of each bidder on eBay from their observed behavior.17 Arrival is estimated as the first time that a bidder interacts with the eBay proxy, while departure is estimated as the latest closing time among eBay auctions in which a bidder participates. 5.1 Bid Identification We extend the work of Haile and Tamer to sequential auctions to get a better view of underlying bidder values. Rather, in the context of repeated 16 When running the simulations, the results of the first and final ten days of auctions are not recorded to reduce edge effects that come from viewing a discrete time window of a continuous process. This bidding strategy is not a dominant strategy (falling outside the type space possible for buyers on which the proof of truthfulness has been built), but is believed to be the most appropriate first order action for simulation. The true population distribution lies below the estimated upper bound. Bidders do not bid more than they are willing to pay. Bidders do not allow an opponent to win at a price they are willing to beat. From the second of their two assumptions, given the winning price of each auction, Haile and Tamer derive a method for estimating a lower bound of the bidding population\"s true value distribution. It is only the upper-bound of the distribution that we utilize in our work. Neither assumption is valid here: the former because bidders are known to bid in more than one auction, and the latter because the set of bidders in an auction is in all likelihood not a true i.i.d. In particular, those who win auctions are less likely to bid in successive auctions, while those who lose auctions are more likely to remain bidders in future auctions. In applying their methods we make the following adjustments: \u2022 Within a given auction, each individual bidder\"s true willingness to pay is assumed weakly greater than the maximum bid that bidder submits across all auctions for that item (either past or future). Therefore, adopted as a less conservative model of bidder values is a uniform scaling factor of 1.15. The winners in the options scheme now have an average estimated value 7.5% higher at $302. This suggests an allocative efficiency of 92.9%: while the typical value of a winner on eBay is $281, an average value of $303 was possible.21 Note the options-based tain auctions, then further adjustments would be required in order to utilize these techniques. 20 Buyers who won more than one item on eBay are cloned so that they appear to be multiple bidders of identical type. For example, if losing bidders tend to have true values close to their observed 187 scheme comes very close to achieving this level of efficiency [at 99.7% efficient in this estimate] even though it operates without the benefit of hindsight. For instance, if a proxy is already holding an option with an exercise price of $3 on some item for which it has value of $10, and it values some substitute item at $5, the proxy could reason that in no circumstance will it be useful to acquire an option on the second item. Let \u0398t be the set of all options a proxy for bidder i already possesses at time t. Let \u03b8t \u2286 \u0398t, be a subset of those options, the sum of whose exercise prices are \u03c0(\u03b8t), and the goods corresponding to those options being \u03b3(\u03b8t). Let \u03b8\u2217 t be the set of options currently held that would maximize the buyer\"s surplus; i.e., \u03b8\u2217 t = argmax\u03b8t\u2286\u0398t \u03a0(\u03b8t). considers the overall use of pursuing the good. Consider the following example where the proxy scheme is refined to bid the maximum willingness to pay. 22 The increase in eBay winner surplus between Tables 3 and 4 is to be expected as the \u03b1 scaling strictly increases the estimated value of the eBay winners while holding the prices at which they won constant. Now, consider what would have happened had Alice declared that she valued only Stone. Alice declares valuing only Stone for $2,000. An area of immediate further work by the authors is to develop a more sophisticated proxy agent that can allow for bidding of maximum willingness to pay (Equation 3) while maintaining truthfulness. Just as in eBay, this would allow an opportunity for sellers to improve revenue by overstating the quality of their item (new vs. like new), and raises the issue of how well a reputation scheme could address this.", "introduction": "Electronic markets represent an application of information systems that has generated significant new trading opportunities while allowing for the dynamic pricing of goods. In addition to marketplaces such as eBay, electronic marketplaces are increasingly used for business-to-consumer auctions (e.g. Many authors have written about a future in which commerce is mediated by online, automated trading agents . There is still little evidence of automated trading in e-markets, though. We believe that one leading place of resistance is in the lack of provably optimal bidding strategies for any but the simplest of market designs. Without this, we do not expect individual consumers, or firms, to be confident in placing their business in the hands of an automated agent. One of the most common examples today of an electronic marketplace is eBay, where the gross merchandise volume (i.e., the sum of all successfully closed listings) was $44B. Among items listed on eBay, many are essentially identical. This is especially true in the Consumer Electronics category , which accounted for roughly $3.5B of eBay\". This presence of essentially identical items can expose bidders, and sellers, to risks because of the sequential auction problem. For example, Alice may want an LCD monitor, and could potentially bid in either a 1 o\"clock or 3 o\"clock eBay auction. While Alice would prefer to participate in whichever auction will have the lower winning price, she cannot determine beforehand which auction that may be, and could end up winning the wrong auction. This is a problem of multiple copies. Another problem bidders may face is the exposure problem. As investigated by Bykowsky et al. , exposure problems exist when buyers desire a bundle of goods but may only participate in single-item auctions.1 For example, if Alice values a video game console by itself for $200, a video game by itself for $30, and both a console and game for $250, Alice must determine how much of the $20 of synergy value she might include in her bid for the console alone. Both problems arise in eBay as a result of sequential auctions of single items coupled with patient bidders with substitutes or complementary valuations. Why might the sequential auction problem be bad? Complex games may lead to bidders employing costly strategies and making mistakes. Potential bidders who do not wish to bear such costs may choose not to participate in the The exposure problem has been primarily investigated by Bykowsky et al. in the context of simultaneous single-item auctions. The problem is also a familiar one of online decision making. 180 market, inhibiting seller revenue opportunities. Additionally, among those bidders who do choose to participate, the mistakes made may lead to inefficient allocations, further limiting revenue opportunities. We are interested in creating modifications to eBay-style markets that simplify the bidder problem, leading to simple equilibrium strategies, and preferably better efficiency and revenue properties. 1.1 Options + Proxies: A Proposed Solution Retail stores have developed policies to assist their customers in addressing sequential purchasing problems. Return policies alleviate the exposure problem by allowing customers to return goods at the purchase price. Price matching alleviates the multiple copies problem by allowing buyers to receive from sellers after purchase the difference between the price paid for a good and a lower price found elsewhere for the same good . Furthermore, price matching can reduce the impact of exactly when a seller brings an item to market, as the price will in part be set by others selling the same item. These two retail policies provide the basis for the scheme proposed in this paper.2 We extend the proxy bidding technology currently employed by eBay. Our super-proxy extension will take advantage of a new, real options-based, market infrastructure that enables simple, yet optimal, bidding strategies. The extensions are computationally simple, handle temporal issues, and retain seller autonomy in deciding when to enter the market and conduct individual auctions. A seller sells an option for a good, which will ultimately lead to either a sale of the good or the return of the option. Buyers interact through a proxy agent, defining a value on all possible bundles of goods in which they have interest together with the latest time period in which they are willing to wait to receive the good(s). The proxy agents use this information to determine how much to bid for options, and follow a dominant bidding strategy across all relevant auctions. A proxy agent exercises options held when the buyer\"s patience has expired, choosing options that maximize a buyer\"s payoff given the reported valuation. All other options are returned to the market and not exercised. The options-based protocol makes truthful and immediate revelation to a proxy a dominant strategy for buyers, whatever the future auction dynamics. We conduct an empirical analysis of eBay, collecting data on over four months of bids for Dell LCD screens (model E193FP). LCD screens are a high-ticket item, for which we demonstrate evidence of the sequential bidding problem. We first infer a conservative model for the arrival time, departure time and value of bidders on eBay for LCD screens during this period. This model is used to simulate the performance of the optionsbased infrastructure, in order to make direct comparisons to the actual performance of eBay in this market. We also extend the work of Haile and Tamer to estimate an upper bound on the distribution of value of eBay bidders, taking into account the sequential auction problem when making the adjustments. Using this estimate, one can approximate how much greater a bidder\"s true value is Prior work has shown price matching as a potential mechanism for colluding firms to set monopoly prices. However, in our context, auction prices will be matched, which are not explicitly set by sellers but rather by buyers\" bids. from the maximum bid they were observed to have placed on eBay. Based on this approximation, revenue generated in a simulation of the options-based scheme exceeds revenue on eBay for the comparable population and sequence of auctions by 14.8%, while the options-based scheme demonstrates itself as being 7.5% more efficient. 1.2 Related Work A number of authors have analyzed the multiple copies problem, often times in the context of categorizing or modeling sniping behavior for reasons other than those first brought forward by Ockenfels and Roth . These papers perform equilibrium analysis in simpler settings, assuming bidders can participate in at most two auctions. Peters & Severinov extend these models to allow buyers to consider an arbitrary number of auctions, and characterize a perfect Bayesian equilibrium. However, their model does not allow auctions to close at distinct times and does not consider the arrival and departure of bidders. Previous work have developed a data-driven approach toward developing a taxonomy of strategies employed by bidders in practice when facing multi-unit auctions, but have not considered the sequential bidding problem . Previous work has also sought to provide agents with smarter bidding strategies . Unfortunately, it seems hard to design artificial agents with equilibrium bidding strategies, even for a simple simultaneous ascending price auction. have considered the role of options in the context of a single, monolithic, auction design to help bidders with marginal-increasing values avoid exposure in a multi-unit, homogeneous item auction problem. In other contexts, options have been discussed for selling coal mine leases , or as leveled commitment contracts for use in a decentralized market place . Most similar to our work, Gopal et al. use options for reducing the risks of buyers and sellers in the sequential auction problem. However, their work uses costly options and does not remove the sequential bidding problem completely. Work on online mechanisms and online auctions considers agents that can dynamically arrive and depart across time. We leverage a recent price-based characterization by Hajiaghayi et al. to provide a dominant strategy equilibrium for buyers within our options-based protocol. The special case for single-unit buyers is equivalent to the protocol of Hajiaghayi et al., albeit with an options-based interpretation. Jiang and Leyton-Brown use machine learning techniques for bid identification in online auctions.", "conclusion": "We introduced a new sales channel, consisting of an optionsbased and proxied auction protocol, to address the sequential auction problem that exists when bidders face multiple auctions for substitutes and complements goods.. Our scheme provides bidders with a simple, dominant and truthful bidding strategy even though the market remains open and dynamic.. In addition to exploring more sophisticated proxies that bid in terms of maximum willingness to pay, future work should aim to better model seller incentives and resolve the strategic problems facing sellers.. For instance, does the options scheme change seller incentives from what they currently are on eBay?. Acknowledgments We would like to thank Pai-Ling Yin.. Helpful comments have been received from William Simpson, attendees at Har188 vard University\"s EconCS and ITM seminars, and anonymous reviewers.. Thank you to Aaron L. Roth and KangXing Jin for technical support.. All errors and omissions remain our own."}
{"id": "C-55", "keywords": ["context awar", "group interact", "locat sens", "sensor fusion"], "title": "Context Awareness for Group Interaction Support", "abstract": "In this paper, we present an implemented system for supporting group interaction in mobile distributed computing environments. First, an introduction to context computing and a motivation for using contextual information to facilitate group interaction is given. We then present the architecture of our system, which consists of two parts: a subsystem for location sensing that acquires information about the location of users as well as spatial proximities between them, and one for the actual context-aware application, which provides services for group interaction.", "references": ["Modeling Context-aware Behavior by Interpreted ECA Rules", "Context-Aware Applications: From the Laboratory to the Marketplace", "A Survey of Context-Aware Mobile Computing Research", "Providing Architectural Support for Building Context-Aware Applications", "Location Modeling: State of the Art and Challenges", "Workspace Awareness in Mobile Virtual Teams", "Coordination in Pervasive Computing Environments", "Supporting Location Awareness in Open Distributed Systems", "Enhanced Reality Fieldwork: the Context-Aware Archaeological Assistant", "Disseminating Active Map Information to Mobile Hosts", "A System Architecture for Context-Aware Mobile Computing", "Supporting Persistent Social Groups in Ubiquitous Computing Environments Using Context-Aware Ephemeral Group Service", "The Stick-e Note Architecture: Extending the Interface Beyond the User", "CybreMinder: A Context-Aware System for Supporting Re-minders"], "full_text": "1. INTRODUCTION Today\"s computing environments are characterized by an increasing number of powerful, wirelessly connected mobile devices. Users can move throughout an environment while carrying their computers with them and having remote access to information and services, anytime and anywhere. New situations appear, where the user\"s context - for example his current location or nearby people - is more dynamic; computation does not occur at a single location and in a single context any longer, but comprises a multitude of situations and locations. This development leads to a new class of applications, which are aware of the context in which they run in and thus bringing virtual and real worlds together. Motivated by this and the fact, that only a few studies have been done for supporting group communication in such computing environments , we have developed a system, which we refer to as Group Interaction Support System (GISS). It supports group interaction in mobile distributed computing environments in a way that group members need not to at the same place any longer in order to interact with each other or just to be aware of the others situation. In the following subchapters, we will give a short overview on context aware computing and motivate its benefits for supporting group interaction. A software framework for developing contextsensitive applications is presented, which serves as middleware for GISS. Chapter 2 presents the architecture of GISS, and chapter 3 and 4 discuss the location sensing and group interaction concepts of GISS in more detail. Chapter 5 gives a final summary of our work. 1.1 What is Context Computing? According to Merriam-Webster\"s Online Dictionary1 , context is defined as the interrelated conditions in which something exists or occurs. Because this definition is very general, many approaches have been made to define the notion of context with respect to computing environments. Most definitions of context are done by enumerating examples or by choosing synonyms for context. The term context-aware has been introduced first in where context is referred to as location, identities of nearby people and objects, and changes to those objects. In , context is also defined by an enumeration of examples, namely location, identities of the people around the user, the time of the day, season, temperature etc. defines context as the user\"s location, environment, identity and time. Here we conform to a widely accepted and more formal definition, which defines context as any information than can be used to characterize the situation of an entity. An entity is a person, place, or object that is considered relevant to the interaction between a user and an application, including the user and applications themselves. identifies four primary types of context information (sometimes referred to as context dimensions), that are - with respect to characterizing the situation of an entity - more important than others. These are location, identity, time and activity, which can also be used to derive other sources of contextual information (secondary context types). For example, if we know a person\"s identity, we can easily derive related information about this person from several data sources (e.g. day of birth or e-mail address). According to this definition, defines a system to be contextaware if it uses context to provide relevant information and/or services to the user, where relevancy depends on the user\"s task. also gives a classification of features for context-aware applications, which comprises presentation of information and services to a user, automatic execution of a service and tagging of context to information for later retrieval. Figure 1. Layers of a context-aware system Context computing is based on two major issues, namely identifying relevant context (identity, location, time, activity) and using obtained context (automatic execution, presentation, tagging). In order to do this, there are a few layers between (see Figure 1). First, the obtained low-level context information has to be transformed, aggregated and interpreted (context transformation) and represented in an abstract context world model (context representation), either centralized or decentralized. Finally, the stored context information is used to trigger certain context events (context triggering). 1.2 Group Interaction in Context After these abstract and formal definitions about what context and context computing is, we will now focus on the main goal of this work, namely how the interaction of mobile group members can be supported by using context information. In we have identified organizational systems to be crucial for supporting mobile groups (see Figure 2). First, there has to be an Information and Knowledge Management System, which is capable of supporting a team with its information processing- and knowledge gathering needs. The next part is the Awareness System, which is dedicated to the perceptualisation of the effects of team activity. It does this by communicating work context, agenda and workspace information to the users. The Interaction Systems provide support for the communication among team members, either synchronous or asynchronous, and for the shared access to artefacts, such as documents. Mobility Systems deploy mechanisms to enable any-place access to team memory as well as the capturing and delivery of awareness information from and to any places. Finally yet importantly, the organisational innovation system integrates aspects of the team itself, like roles, leadership and shared facilities. With respect to these five aspects of team support, we focus on interaction and partly cover mobility- and awareness-support. Group interaction includes all means that enable group members to communicate freely with all the other members. At this point, the question how context information can be used for supporting group interaction comes up. We believe that information about the current situation of a person provides a surplus value to existing group interaction systems. Context information facilitates group interaction by allowing each member to be aware of the availability status or the current location of each other group member, which again makes it possible to form groups dynamically, to place virtual post-its in the real world or to determine which people are around. Figure 2. Support for Mobile Groups Most of today\"s context-aware applications use location and time only, and location is referred to as a crucial type of context information . We also see the importance of location information in mobile and ubiquitous environments, wherefore a main focus of our work is on the utilization of location information and information about users in spatial proximity. Nevertheless, we believe that location, as the only used type of context information, is not sufficient to support group interaction, wherefore we also take advantage of the other three primary types, namely identity, time and activity. This provides a comprehensive description of a user\"s current situation and thus enabling numerous means for supporting group interaction, which are described in detail in chapter 4.4. When we look at the types of context information stated above, we can see that all of them are single user-centred, taking into account only the context of the user itself. We believe, that for the support of group interaction, the status of the group itself has also be taken into account. Therefore, we have added a fifth contextdimension group-context, which comprises more than the sum of the individual member\"s contexts. Group context includes any information about the situation of a whole group, for example how many members a group currently has or if a certain group meets right now. 1.3 Context Middleware The Group Interaction Support System (GISS) uses the softwareframework introduced in , which serves as a middleware for developing context-sensitive applications. This so-called Context Framework is based on a distributed communication architecture and it supports different kinds of transport protocols and message coding mechanisms. 89 A main feature of the framework is the abstraction of context information retrieval via various sensors and its delivery to a level where no difference appears, for the application designer, between these different kinds of context retrieval mechanisms; the information retrieval is hidden from the application developer. This is achieved by so-called entities, which describe objectse.g. a human user - that are important for a certain context scenario. Entities express their functionality by the use of so-called attributes, which can be loaded into the entity. These attributes are complex pieces of software, which are implemented as Java classes. Typical attributes are encapsulations of sensors, but they can also be used to implement context services, for example to notify other entities about location changes of users. Each entity can contain a collection of such attributes, where an entity itself is an attribute. The initial set of attributes an entity contains can change dynamically at runtime, if an entity loads or unloads attributes from the local storage or over the network. In order to load and deploy new attributes, an entity has to reference a class loader and a transport and lookup layer, which manages the lookup mechanism for discovering other entities and the transport. XML configuration files specify which initial set of entities should be loaded and which attributes these entities own. The communication between entities and attributes is based on context events. Each attribute is able to trigger events, which are addressed to other attributes and entities respectively, independently on which physical computer they are running. Among other things, and event contains the name of the event and a list of parameters delivering information about the event itself. Related with this event-based architecture is the use of ECA (Event-Condition-Action)-rules for defining the behaviour of the context system. Therefore, every entity has a rule-interpreter, which catches triggered events, checks conditions associated with them and causes certain actions. These rules are referenced by the entity\"s XML configuration. A rule itself is even able to trigger the insertion of new rules or the unloading of existing rules at runtime in order to change the behaviour of the context system dynamically. To sum up, the context framework provides a flexible, distributed architecture for hiding low-level sensor data from high-level applications and it hides external communication details from the application developer. Furthermore, it is able to adapt its behaviour dynamically by loading attributes, entities or ECArules at runtime. 2. ARCHITECTURE OVERVIEW As GISS uses the Context Framework described in chapter 1.3 as middleware, every user is represented by an entity, as well as the central server, which is responsible for context transformation, context representation and context triggering (cf. Figure 1). A main part of our work is about the automated acquisition of position information and its sensor-independent provision at application level. We do not only sense the current location of users, but also determine spatial proximities between them. Developing the architecture, we focused on keeping the client as simple as possible and reducing the communication between client and server to a minimum. Each client may have various location and/or proximity sensors attached, which are encapsulated by respective Context Framework-attributes (Sensor Encapsulation). These attributes are responsible for integrating native sensor-implementations into the Context Framework and sending sensor-dependent position information to the server. We consider it very important to support different types of sensors even at the same time, in order to improve location accuracy on the one hand, while providing a pervasive location-sensing environment with seamless transition between different location sensing techniques on the other hand. All location- and proximity-sensors supported are represented by server-side context-attributes, which correspond to the client-side sensor encapsulation-attributes and abstract the sensor-dependent position information received from all users via the wireless network (sensor abstraction). This requires a context repository, where the mapping of diverse physical positions to standardized locations is stored. The standardized location- and proximity-information of each user is then passed to the so-called Sensor Fusion-attributes, one for symbolic locations and a second one for spatial proximities. Their job is to merge location- and proximityinformation of clients, respectively, which is described in detail in Chapter 3.3. Every time the symbolic location of a user or the spatial proximity between two users changes, the Sensor Fusion-attributes notify the GISS Core-attribute, which controls the application. Because of the abstraction of sensor-dependent position information, the system can easily be extended by additional sensors, just by implementing the (typically two) attributes for encapsulating sensors (some sensors may not need a client-side part), abstracting physical positions and observing the interface to GISS Core. Figure 3. Architecture of the Group Interaction Support System (GISS) The GISS Core-attribute is the central coordinator of the application as it shows to the user. It not only serves as an interface to the location-sensing subsystem, but also collects further context information in other dimensions (time, identity or activity). 90 Every time a change in the context of one or more users is detected, GISS Core evaluates the effect of these changes on the user, on the groups he belongs to and on the other members of these groups. Whenever necessary, events are thrown to the affected clients to trigger context-aware activities, like changing the presentation of awareness information or the execution of services. The client-side part of the application is kept as simple as possible. Furthermore, modular design was not only an issue on the sensor side but also when designing the user interface architecture. Thus, the complete user interface can be easily exchanged, if all of the defined events are taken into account and understood by the new interface-attribute. The currently implemented user interface is split up in two parts, which are also represented by two attributes. The central attribute on client-side is the so-called Instant Messenger Encapsulation, which on the one hand interacts with the server through events and on the other hand serves as a proxy for the external application the user interface is built on. As external application, we use an existing open source instant messenger - the ICQ2 -compliant Simple Instant Messenger (SIM)3 . We have chosen and instant messenger as front-end because it provides a well-known interface for most users and facilitates a seamless integration of group interaction support, thus increasing acceptance and ease of use. As the basic functionality of the instant messenger - to serve as a client in an instant messenger network - remains fully functional, our application is able to use the features already provided by the messenger. For example, the contexts activity and identity are derived from the messenger network as it is described later. The Instant Messenger Encapsulation is also responsible for supporting group communication. Through the interface of the messenger, it provides means of synchronous and asynchronous communication as well as a context-aware reminder system and tools for managing groups and the own availability status. The second part of the user interface is a visualisation of the user\"s locations, which is implemented in the attribute Viewer. The current implementation provides a two-dimensional map of the campus, but it can easily be replaced by other visualisations, a three-dimensional VRML-model for example. Furthermore, this visualisation is used to show the artefacts for asynchronous communication. Based on a floor plan-view of the geographical area the user currently resides in, it gives a quick overview of which people are nearby, their state and provides means to interact with them. In the following chapters 3 and 4, we describe the location sensing-backend and the application front-end for supporting group interaction in more detail. 3. LOCATION SENSING In the following chapter, we will introduce a location model, which is used for representing locations; afterwards, we will describe the integration of location- and proximity-sensors in more detail. Finally, we will have a closer look on the fusion of location- and proximity-information, acquired by various sensors. 3.1 Location Model A location model (i.e. a context representation for the contextinformation location) is needed to represent the locations of users, in order to be able to facilitate location-related queries like given a location, return a list of all the objects there or given an object, return its current location. In general, there are two approaches : symbolic models, which represent location as abstract symbols, and a geometric model, which represent location as coordinates. We have chosen a symbolic location model, which refers to locations as abstract symbols like Room P111 or Physics Building, because we do not require geometric location data. Instead, abstract symbols are more convenient for human interaction at application level. Furthermore, we use a symbolic location containment hierarchy similar to the one introduced in , which consists of top-level regions, which contain buildings, which contain floors, and the floors again contain rooms. We also distinguish four types, namely region (e.g. a whole campus), section (e.g. a building or an outdoor section), level (e.g. a certain floor in a building) and area (e.g. a certain room). We introduce a fifth type of location, which we refer to as semantic. These socalled semantic locations can appear at any level in the hierarchy and they can be nested, but they do not necessarily have a geographic representation. Examples for such semantic locations are tagged objects within a room (e.g. a desk and a printer on this desk) or the name of a department, which contains certain rooms. Figure 4. Symbolic Location Containment Hierarchy The hierarchy of symbolic locations as well as the type of each position is stored in the context repository. 3.2 Sensors Our architecture supports two different kinds of sensors: location sensors, which acquire location information, and proximity sensors, which detect spatial proximities between users. As described above, each sensor has a server- and in most cases a corresponding client-side-implementation, too. While the clientattributes (Sensor Abstraction) are responsible for acquiring low-level sensor-data and transmitting it to the server, the corresponding Sensor Encapsulation-attributes transform them into a uniform and sensor-independent format, namely symbolic locations and IDs of users in spatial proximity, respectively. 91 Afterwards, the respective attribute Sensor Fusion is being triggered with this sensor-independent information of a certain user, detected by a particular sensor. Such notifications are performed every time the sensor acquired new information. Accordingly, Sensor Abstraction-attributes are responsible to detect when a certain sensor is no longer available on the client side (e.g. if it has been unplugged by the user) or when position respectively proximity could not be determined any longer (e.g. RFID reader cannot detect tags) and notify the corresponding sensor fusion about this. 3.2.1 Location Sensors In order to sense physical positions, the Sensor Encapsulationattributes asynchronously transmit sensor-dependent position information to the server. The corresponding location Sensor Abstraction-attributes collect these physical positions delivered by the sensors of all users, and perform a repository-lookup in order to get the associated symbolic location. This requires certain tables for each sensor, which map physical positions to symbolic locations. One physical position may have multiple symbolic locations at different accuracy-levels in the location hierarchy assigned to, for example if a sensor covers several rooms. If such a mapping could be found, an event is thrown in order to notify the attribute Location Sensor Fusion about the symbolic locations a certain sensor of a particular user determined. We have prototypically implemented three kinds of location sensors, which are based on WLAN (IEEE 802.11), Bluetooth and RFID (Radio Frequency Identification). We have chosen these three completely different sensors because of their differences concerning accuracy, coverage and administrative effort, in order to evaluate the flexibility of our system (see Table 1). The most accurate one is an RFID sensor, which is based on an active RFID-reader. As soon as the reader is plugged into the client, it scans for active RFID tags in range and transmits their serial numbers to the server, where they are mapped to symbolic locations. We also take into account RSSI (Radio Signal Strength Information), which provides position accuracy of few centimetres and thus enables us to determine which RFID-tag is nearest. Due to this high accuracy, RFID is used for locating users within rooms. The administration is quite simple; once a new RFID tag is placed, its serial number simply has to be assigned to a single symbolic location. A drawback is the poor availability, which can be traced back to the fact that RFID readers are still very expensive. The second one is an 802.11 WLAN sensor. Therefore, we integrated a purely software-based, commercial WLAN positioning system for tracking clients on the university campuswide WLAN infrastructure. The reached position accuracy is in the range of few meters and thus is suitable for location sensing at the granularity of rooms. A big disadvantage is that a map of the whole area has to be calibrated with measuring points at a distance of 5 meters each. Because most mobile computers are equipped with WLAN technology and the positioning-system is a software-only solution, nearly everyone is able to use this kind of sensor. Finally, we have implemented a Bluetooth sensor, which detects Bluetooth tags (i.e. Bluetooth-modules with known position) in range and transmits them to the server that maps to symbolic locations. Because of the fact that we do not use signal strengthinformation in the current implementation, the accuracy is above 10 meters and therefore a single Bluetooth MAC address is associated with several symbolic locations, according to the physical locations such a Bluetooth module covers. This leads to the disadvantage that the range of each Bluetooth-tag has to be determined and mapped to symbolic locations within this range. Table 1. Comparison of implemented sensors Sensor Accuracy Coverage Administration RFID < 10 cm poor easy WLAN 1-4 m very well very timeconsuming Bluetooth ~ 10 m well time-consuming 3.2.2 Proximity Sensors Any sensor that is able to detect whether two users are in spatial proximity is referred to as proximity sensor. Similar to the location sensors, the Proximity Sensor Abstraction-attributes collect physical proximity information of all users and transform them to mappings of user-IDs. We have implemented two types of proximity-sensors, which are based on Bluetooth on the one hand and on fused symbolic locations (see chapter 3.3.1) on the other hand. The Bluetooth-implementation goes along with the implementation of the Bluetooth-based location sensor. The already determined Bluetooth MAC addresses in range of a certain client are being compared with those of all other clients, and each time the attribute Bluetooth Sensor Abstraction detects congruence, it notifies the proximity sensor fusion about this. The second sensor is based on symbolic locations processed by Location Sensor Fusion, wherefore it does not need a client-side implementation. Each time the fused symbolic location of a certain user changes, it checks whether he is at the same symbolic location like another user and again notifies the proximity sensor fusion about the proximity between these two users. The range can be restricted to any level of the location containment hierarchy, for example to room granularity. A currently unresolved issue is the incomparable granularity of different proximity sensors. For example, the symbolic locations at same level in the location hierarchy mostly do not cover the same geographic area. 3.3 Sensor Fusion Core of the location sensing subsystem is the sensor fusion. It merges data of various sensors, while coping with differences concerning accuracy, coverage and sample-rate. According to the two kinds of sensors described in chapter 3.2, we distinguish between fusion of location sensors on the one hand, and fusion of proximity sensors on the other hand. The fusion of symbolic locations as well as the fusion of spatial proximities operates on standardized information (cf. Figure 3). This has the advantage, that additional position- and proximitysensors can be added easily or the fusion algorithms can be replaced by ones that are more sophisticated. 92 Fusion is performed for each user separately and takes into account the measurements at a single point in time only (i.e. no history information is used for determining the current location of a certain user). The algorithm collects all events thrown by the Sensor Abstraction-attributes, performs fusion and triggers the GISS Core-attribute if the symbolic location of a certain user or the spatial proximity between users changed. An important feature is the persistent storage of location- and proximity-history in a database in order to allow future retrieval. This enables applications to visualize the movement of users for example. 3.3.1 Location Sensor Fusion Goal of the fusion of location information is to improve precision and accuracy by merging the set of symbolic locations supplied by various location sensors, in order to reduce the number of these locations to a minimum, ideally to a single symbolic location per user. This is quite difficult, because different sensors may differ in accuracy and sample rate as well. The Location Sensor Fusion-attribute is triggered by events, which are thrown by the Location Sensor Abstractionattributes. These events contain information about the identity of the user concerned, his current location and the sensor by which the location has been determined. If the attribute Location Sensor Fusion receives such an event, it checks if the amount of symbolic locations of the user concerned has changed (compared with the last event). If this is the case, it notifies the GISS Core-attribute about all symbolic locations this user is currently associated with. However, this information is not very useful on its own if a certain user is associated with several locations. As described in chapter 3.2.1, a single location sensor may deliver multiple symbolic locations. Moreover, a certain user may have several location sensors, which supply symbolic locations differing in accuracy (i.e. different levels in the location containment hierarchy). To cope with this challenge, we implemented a fusion algorithm in order to reduce the number of symbolic locations to a minimum (ideally to a single location). In a first step, each symbolic location is associated with its number of occurrences. A symbolic location may occur several times if it is referred to by more than one sensor or if a single sensor detects multiple tags, which again refer to several locations. Furthermore, this number is added to the previously calculated number of occurrences of each symbolic location, which is a child-location of the considered one in the location containment hierarchy. For example, if - in Figure 4 - room2 occurs two times and desk occurs a single time, the value 2 of room2 is added to the value 1 of desk, whereby desk finally gets the value 3. In a final step, only those symbolic locations are left which are assigned with the highest number of occurrences. A further reduction can be achieved by assigning priorities to sensors (based on accuracy and confidence) and cumulating these priorities for each symbolic location instead of just counting the number of occurrences. If the remaining fused locations have changed (i.e. if they differ from the fused locations the considered user is currently associated with), they are provided with the current timestamp, written to the database and the GISS-attribute is notified about where the user is probably located. Finally, the most accurate, common location in the location hierarchy is calculated (i.e. the least upper bound of these symbolic locations) in order to get a single symbolic location. If it changes, the GISS Core-attribute is triggered again. 3.3.2 Proximity Sensor Fusion Proximity sensor fusion is much simpler than the fusion of symbolic locations. The corresponding proximity sensor fusionattribute is triggered by events, which are thrown by the Proximity Sensor Abstraction-attributes. These special events contain information about the identity of the two users concerned, if they are currently in spatial proximity or if proximity no longer persists, and by which proximity-sensor this has been detected. If the sensor fusion-attribute is notified by a certain Proximity Sensor Abstraction-attribute about an existing spatial proximity, it first checks if these two users are already known to be in proximity (detected either by another user or by another proximity-sensor of the user, which caused the event). If not, this change in proximity is written to the context repository with current timestamp. Similarly, if the attribute Proximity Fusion is notified about an ended proximity, it checks if the users are still known to be in proximity, and writes this change to the repository if not. Finally, if spatial proximity between the two users actually changed, an event is thrown to notify the GISS Core-attribute about this. 4. CONTEXTSENSITIVE INTERACTION 4.1 Overview In most of today\"s systems supporting interaction in groups, the provided means lack any awareness of the user\"s current context, thus being unable to adapt to his needs. In our approach, we use context information to enhance interaction and provide further services, which offer new possibilities to the user. Furthermore, we believe that interaction in groups also has to take into account the current context of the group itself and not only the context of individual group members. For this reason, we also retrieve information about the group\"s current context, derived from the contexts of the group members together with some sort of meta-information (see chapter 4.3). The sources of context used for our application correspond with the four primary context types given in chapter 1.1 - identity (I), location (L), time (T) and activity (A). As stated before, we also take into account the context of the group the user is interaction with, so that we could add a fifth type of context informationgroup awareness (G) - to the classification. Using this context information, we can trigger context-aware activities in all of the three categories described in chapter 1.1 - presentation of information (P), automatic execution of services (A) and tagging of context to information for later retrieval (T). Table 2 gives an overview of activities we have already implemented; they are described comprehensively in chapter 4.4. The table also shows which types of context information are used for each activity and the category the activity could be classified in. 93 Table 2. Classification of implemented context-aware activities Service L T I A G P A T Location Visualisation X X X Group Building Support X X X X Support for Synchronous Communication X X X X Support for Asynchronous Communication X X X X X X X Availability Management X X X Task Management Support X X X X Meeting Support X X X X X X Reasons for implementing these very features are to take advantage of all four types of context information in order to support group interaction by utilizing a comprehensive knowledge about the situation a single user or a whole group is in. A critical issue for the user acceptance of such a system is the usability of its interface. We have evaluated several ways of presenting context-aware means of interaction to the user, until we came to the solution we use right now. Although we think that the user interface that has been implemented now offers the best trade-off between seamless integration of features and ease of use, it would be no problem to extend the architecture with other user interfaces, even on different platforms. The chosen solution is based on an existing instant messenger, which offers several possibilities to integrate our system (see chapter 4.2). The biggest advantage of this approach is that the user is confronted with a graphical user interface he is already used to in most cases. Furthermore, our system uses an instant messenger account as an identifier, so that the user does not have to register a further account anywhere else (for example, the user can use his already existing ICQ2 -account). 4.2 Instant Messenger Integration Our system is based upon an existing instant messenger, the socalled Simple Instant Messenger (SIM)3 . The implementation of this messenger is carried out as a project at Sourceforge4 SIM supports multiple messenger protocols such as AIM5 , ICQ2 and MSN6 . It also supports connections to multiple accounts at the same time. Furthermore, full support for SMS-notification (where provided from the used protocol) is given. SIM is based on a plug-in concept. All protocols as well as parts of the user-interface are implemented as plug-ins. Its architecture is also used to extend the application\"s abilities to communicate with external applications. For this purpose, a remote control plug-in is provided, by which SIM can be controlled from external applications via socket connection. This remote control interface is extensively used by GISS for retrieving the contact list, setting the user\"s availability-state or sending messages. The functionality of the plug-in was extended in several ways, for example to accept messages for an account (as if they would have been sent via the messenger network). The messenger, more exactly the contact list (i.e. a list of profiles of all people registered with the instant messenger, which is visualized by listing their names as it can be seen in Figure 5), is also used to display locations of other members of the groups a user belongs to. This provides location awareness without taking too much space or requesting the user\"s full attention. A more comprehensive description of these features is given in chapter 4.4. 4.3 Sources of Context Information While the location-context of a user is obtained from our location sensing subsystem described in chapter 3, we consider further types of context than location relevant for the support of group interaction, too. Local time as a very important context dimension can be easily retrieved from the real time clock of the user\"s system. Besides location and time, we also use context information of user\"s activity and identity, where we exploit the functionality provided by the underlying instant messenger system. Identity (or more exactly, the mapping of IDs to names as well as additional information from the user\"s profile) can be distilled out of the contents of the user\"s contact list. Information about the activity or a certain user is only available in a very restricted area, namely the activity at the computer itself. Other activities like making a phone call or something similar, cannot be recognized with the current implementation of the activity sensor. The only context-information used is the instant messenger\"s availability state, thus only providing a very coarse classification of the user\"s activity (online, offline, away, busy etc.). Although this may not seem to be very much information, it is surely relevant and can be used to improve or even enable several services. Having collected the context information from all available users, it is now possible to distil some information about the context of a certain group. Information about the context of a group includes how many members the group currently has, if the group meets right now, which members are participating at a meeting, how many members have read which of the available posts from other team members and so on. Therefore, some additional information like a list of members for each group is needed. These lists can be assembled manually (by users joining and leaving groups) or retrieved automatically. The context of a group is secondary context and is aggregated from the available contexts of the group members. Every time the context of a single group member changes, the context of the whole group is changing and has to be recalculated. With knowledge about a user\"s context and the context of the groups he belongs to, we can provide several context-aware services to the user, which enhance his interaction abilities. A brief description of these services is given in chapter 4.4. 94 4.4 Group Interaction Support 4.4.1 Visualisation of Location Information An important feature is the visualisation of location information, thus allowing users to be aware of the location of other users and members of groups he joined, respectively. As already described in chapter 2, we use two different forms of visualisation. The maybe more important one is to display location information in the contact list of the instant messenger, right beside the name, thus being always visible while not drawing the user\"s attention on it (compared with a twodimensional view for example, which requires a own window for displaying a map of the environment). Due to the restricted space in the contact list, it has been necessary to implement some sort of level-of-detail concept. As we use a hierarchical location model, we are able to determine the most accurate common location of two users. In the contact list, the current symbolic location one level below the previously calculated common location is then displayed. If, for example, user A currently resides in room P121 at the first floor of a building and user B, which has to be displayed in the contact list of user A, is in room P304 at the third floor, the most accurate common location of these two users is the building they are in. For that reason, the floor (i.e. one level beyond the common location, namely the building) of user B is displayed in the contact list of user A. If both people reside on the same floor or even in the same room, the room would be taken. Figure 5 shows a screenshot of the Simple Instant Messenger3 where the current location of those people, whose location is known by GISS, is displayed in brackets right beside their name. On top of the image, the heightened, integrated GISS-toolbar is shown, which currently contains the following, implemented functionality (from left to right): Asynchronous communication for groups (see chapter 4.4.4), context-aware reminders (see chapter 4.4.6), two-dimensional visualisation of locationinformation, forming and managing groups (see chapter 4.4.2), context-aware availability-management (see chapter 4.4.5) and finally a button for terminating GISS. Figure 5. GISS integration in Simple Instant Messenger3 As displaying just this short form of location may not be enough for the user, because he may want to see the most accurate position available, a fully qualified position is shown if a name in the contact-list is clicked (e.g. in the form of desk@room2@department1@1stfloor@building 1@campus). The second possible form of visualisation is a graphical one. We have evaluated a three-dimensional view, which was based on a VRML model of the respective area (cf. Figure 6). Due to lacks in navigational and usability issues, we decided to use a twodimensional view of the floor (it is referred to as level in the location hierarchy, cf. Figure 4). Other levels of granularity like section (e.g. building) and region (e.g. campus) are also provided. In this floor-plan-based view, the current locations are shown in the manner of ICQ2 contacts, which are placed at the currently sensed location of the respective person. The availability-status of a user, for example away if he is not on the computer right now, or busy if he does not want to be disturbed, is visualized by colour-coding the ICQ2 -flower left beside the name. Furthermore, the floor-plan-view shows so-called the virtual post-its, which are virtual counterparts of real-life post-its and serve as our means of asynchronous communication (more about virtual post-its can be found in chapter 4.4.4). Figure 6. 3D-view of the floor (VRML) Figure 7 shows the two-dimensional map of a certain floor, where several users are currently located (visualized by their name and the flower left beside). The location of the client, on which the map is displayed, is visualized by a green circle. Down to the right, two virtual post-its can be seen. Figure 7. 2D view of the floor Another feature of the 2D-view is the visualisation of locationhistory of users. As we store the complete history of a user\"s locations together with a timestamp, we are able to provide information about the locations he has been back in time. When the mouse is moved over the name of a certain user in the 2Dview, footprints of a user, placed at the locations he has been, are faded out the stronger, the older the location information is. 95 4.4.2 Forming and Managing Groups To support interaction in groups, it is first necessary to form groups. As groups can have different purposes, we distinguish two types of groups. So-called static groups are groups, which are built up manually by people joining and leaving them. Static groups can be further divided into two subtypes. In open static groups, everybody can join and leave anytime, useful for example to form a group of lecture attendees of some sort of interest group. Closed static groups have an owner, who decides, which persons are allowed to join, although everybody could leave again at any time. Closed groups enable users for example to create a group of their friends, thus being able to communicate with them easily. In contrast to that, we also support the creation of dynamic groups. They are formed among persons, who are at the same location at the same time. The creation of dynamic groups is only performed at locations, where it makes sense to form groups, for example in lecture halls or meeting rooms, but not on corridors or outdoor. It would also be not very meaningful to form a group only of the people residing in the left front sector of a hall; instead, the complete hall should be considered. For these reasons, all the defined locations in the hierarchy are tagged, whether they allow the formation of groups or not. Dynamic groups are also not only formed granularity of rooms, but also on higher levels in the hierarchy, for example with the people currently residing in the area of a department. As the members of dynamic groups constantly change, it is possible to create an open static group out of them. 4.4.3 Synchronous Communication for Groups The most important form of synchronous communication on computers today is instant messaging; some people even see instant messaging to be the real killer application on the Internet. This has also motivated the decision to build GISS upon an instant messaging system. In today\"s messenger systems, peer-to-peer-communication is extensively supported. However, when it comes to communication in groups, the support is rather poor most of the time. Often, only sending a message to multiple recipients is supported, lacking means to take into account the current state of the recipients. Furthermore, groups can only be formed of members in one\"s contact list, thus being not able to send messages to a group, where not all of its members are known (which may be the case in settings, where the participants of a lecture form a group). Our approach does not have the mentioned restrictions. We introduce group-entries in the user\"s contact list; enable him or his to send messages to this group easily, without knowing who exactly is currently a member of this group. Furthermore, group messages are only delivered to persons, who are currently not busy, thus preventing a disturbance by a message, which is possibly unimportant for the user. These features cannot be carried out in the messenger network itself, so whenever a message to a group account is sent, we intercept it and route it through our system to all the recipients, which are available at a certain time. Communication via a group account is also stored centrally, enabling people to query missed messages or simply viewing the message history. 4.4.4 Asynchronous Communication for Groups Asynchronous communication in groups is not a new idea. The goal of this approach is not to reinvent the wheel, as email is maybe the most widely used form of asynchronous communication on computers and is broadly accepted and standardized. In out work, we aim at the combination of asynchronous communication with location awareness. For this reason, we introduce the concept of so-called virtual postits (cp. ), which are messages that are bound to physical locations. These virtual post-its could be either visible for all users that are passing by or they can be restricted to be visible for certain groups of people only. Moreover, a virtual post-it can also have an expiry date after which it is dropped and not displayed anymore. Virtual post-its can also be commented by others, thus providing some from of forum-like interaction, where each post-it forms a thread. Virtual post-its are displayed automatically, whenever a user (available) passes by the first time. Afterwards, post-its can be accessed via the 2D-viewer, where all visible post-its are shown. All readers of a post-it are logged and displayed when viewing it, providing some sort of awareness about the group members\" activities in the past. 4.4.5 Context-aware Availability Management Instant messengers in general provide some kind of availability information about a user. Although this information can be only defined in a very coarse granularity, we have decided to use these means of gathering activity context, because the introduction of an additional one would strongly decrease the usability of the system. To support the user managing his availability, we provide an interface that lets the user define rules to adapt his availability to the current context. These rules follow the form on event (E) if condition (C) then action (A), which is directly supported by the ECA-rules of the Context Framework described in chapter 1.3. The testing of conditions is periodically triggered by throwing events (whenever the context of a user changes). The condition itself is defined by the user, who can demand the change of his availability status as the action in the rule. As a condition, the user can define his location, a certain time (also triggering daily, every week or every month) or any logical combination of these criteria. 4.4.6 Context-Aware Reminders Reminders are used to give the user the opportunity of defining tasks and being reminded of those, when certain criteria are fulfilled. Thus, a reminder can be seen as a post-it to oneself, which is only visible in certain cases. Reminders can be bound to a certain place or time, but also to spatial proximity of users or groups. These criteria can be combined with Boolean operators, thus providing a powerful means to remind the user of tasks that he wants to carry out when a certain context occurs. A reminder will only pop up the first time the actual context meets the defined criterion. On showing up the reminder, the user has the chance to resubmit it to be reminded again, for example five minutes later or the next time a certain user is in spatial proximity. 96 4.4.7 Context-Aware Recognition and Notification of Group Meetings With the available context information, we try to recognize meetings of a group. The determination of the criteria, when the system recognizes a group having a meeting, is part of the ongoing work. In a first approach, we use the location- and activity-context of the group members to determine a meeting. Whenever more than 50 % of the members of a group are available at a location, where a meeting is considered to make sense (e.g. not on a corridor), a meeting minutes post-it is created at this location and all absent group members are notified of the meeting and the location it takes place. During the meeting, the comment-feature of virtual post-its provides a means to take notes for all of the participants. When members are joining or leaving the meeting, this is automatically added as a note to the list of comments. Like the recognition of the beginning of a meeting, the recognition of its end is still part of ongoing work. If the end of the meeting is recognized, all group members get the complete list of comments as a meeting protocol at the end of the meeting. 5. CONCLUSIONS This paper discussed the potentials of support for group interaction by using context information. First, we introduced the notions of context and context computing and motivated their value for supporting group interaction. An architecture is presented to support context-aware group interaction in mobile, distributed environments. It is built upon a flexible and extensible framework, thus enabling an easy adoption to available context sources (e.g. by adding additional sensors) as well as the required form of representation. We have prototypically developed a set of services, which enhance group interaction by taking into account the current context of the users as well as the context of groups itself. Important features are dynamic formation of groups, visualization of location on a two-dimensional map as well as unobtrusively integrated in an instant-messenger, asynchronous communication by virtual post-its, which are bound to certain locations, and a context-aware availability-management, which adapts the availability-status of a user to his current situation. To provide location information, we have implemented a subsystem for automated acquisition of location- and proximityinformation provided by various sensors, which provides a technology-independent presentation of locations and spatial proximities between users and merges this information using sensor-independent fusion algorithms. A history of locations as well as of spatial proximities is stored in a database, thus enabling context history-based services.", "body1": "Today\"s computing environments are characterized by an increasing number of powerful, wirelessly connected mobile devices. In the following subchapters, we will give a short overview on context aware computing and motivate its benefits for supporting group interaction. 1.1 What is Context Computing? Most definitions of context are done by enumerating examples or by choosing synonyms for context. Here we conform to a widely accepted and more formal definition, which defines context as any information than can be used to characterize the situation of an entity. also gives a classification of features for context-aware applications, which comprises presentation of information and services to a user, automatic execution of a service and tagging of context to information for later retrieval. Figure 1. In we have identified organizational systems to be crucial for supporting mobile groups (see Figure 2). Group interaction includes all means that enable group members to communicate freely with all the other members. Figure 2. When we look at the types of context information stated above, we can see that all of them are single user-centred, taking into account only the context of the user itself. 89 A main feature of the framework is the abstraction of context information retrieval via various sensors and its delivery to a level where no difference appears, for the application designer, between these different kinds of context retrieval mechanisms; the information retrieval is hidden from the application developer. This is achieved by so-called entities, which describe objectse.g. Entities express their functionality by the use of so-called attributes, which can be loaded into the entity. Each entity can contain a collection of such attributes, where an entity itself is an attribute. Among other things, and event contains the name of the event and a list of parameters delivering information about the event itself. Related with this event-based architecture is the use of ECA (Event-Condition-Action)-rules for defining the behaviour of the context system. As GISS uses the Context Framework described in chapter 1.3 as middleware, every user is represented by an entity, as well as the central server, which is responsible for context transformation, context representation and context triggering (cf. Developing the architecture, we focused on keeping the client as simple as possible and reducing the communication between client and server to a minimum. Each client may have various location and/or proximity sensors attached, which are encapsulated by respective Context Framework-attributes (Sensor Encapsulation). The standardized location- and proximity-information of each user is then passed to the so-called Sensor Fusion-attributes, one for symbolic locations and a second one for spatial proximities. Figure 3. The client-side part of the application is kept as simple as possible. As external application, we use an existing open source instant messenger - the ICQ2 -compliant Simple Instant Messenger (SIM)3 . The second part of the user interface is a visualisation of the user\"s locations, which is implemented in the attribute Viewer. The current implementation provides a two-dimensional map of the campus, but it can easily be replaced by other visualisations, a three-dimensional VRML-model for example. In the following chapter, we will introduce a location model, which is used for representing locations; afterwards, we will describe the integration of location- and proximity-sensors in more detail. 3.1 Location Model A location model (i.e. Instead, abstract symbols are more convenient for human interaction at application level. 3.2 Sensors Our architecture supports two different kinds of sensors: location sensors, which acquire location information, and proximity sensors, which detect spatial proximities between users. As described above, each sensor has a server- and in most cases a corresponding client-side-implementation, too. 91 Afterwards, the respective attribute Sensor Fusion is being triggered with this sensor-independent information of a certain user, detected by a particular sensor. Accordingly, Sensor Abstraction-attributes are responsible to detect when a certain sensor is no longer available on the client side (e.g. RFID reader cannot detect tags) and notify the corresponding sensor fusion about this. 3.2.1 Location Sensors In order to sense physical positions, the Sensor Encapsulationattributes asynchronously transmit sensor-dependent position information to the server. The most accurate one is an RFID sensor, which is based on an active RFID-reader. The second one is an 802.11 WLAN sensor. Finally, we have implemented a Bluetooth sensor, which detects Bluetooth tags (i.e. Table 1. The Bluetooth-implementation goes along with the implementation of the Bluetooth-based location sensor. The second sensor is based on symbolic locations processed by Location Sensor Fusion, wherefore it does not need a client-side implementation. 3.3 Sensor Fusion Core of the location sensing subsystem is the sensor fusion. This has the advantage, that additional position- and proximitysensors can be added easily or the fusion algorithms can be replaced by ones that are more sophisticated. 92 Fusion is performed for each user separately and takes into account the measurements at a single point in time only (i.e. This enables applications to visualize the movement of users for example. 3.3.1 Location Sensor Fusion Goal of the fusion of location information is to improve precision and accuracy by merging the set of symbolic locations supplied by various location sensors, in order to reduce the number of these locations to a minimum, ideally to a single symbolic location per user. The Location Sensor Fusion-attribute is triggered by events, which are thrown by the Location Sensor Abstractionattributes. If the attribute Location Sensor Fusion receives such an event, it checks if the amount of symbolic locations of the user concerned has changed (compared with the last event). However, this information is not very useful on its own if a certain user is associated with several locations. In a first step, each symbolic location is associated with its number of occurrences. If the remaining fused locations have changed (i.e. Finally, the most accurate, common location in the location hierarchy is calculated (i.e. 3.3.2 Proximity Sensor Fusion Proximity sensor fusion is much simpler than the fusion of symbolic locations. If the sensor fusion-attribute is notified by a certain Proximity Sensor Abstraction-attribute about an existing spatial proximity, it first checks if these two users are already known to be in proximity (detected either by another user or by another proximity-sensor of the user, which caused the event). 4.1 Overview In most of today\"s systems supporting interaction in groups, the provided means lack any awareness of the user\"s current context, thus being unable to adapt to his needs. In our approach, we use context information to enhance interaction and provide further services, which offer new possibilities to the user. The sources of context used for our application correspond with the four primary context types given in chapter 1.1 - identity (I), location (L), time (T) and activity (A). The table also shows which types of context information are used for each activity and the category the activity could be classified in. 93 Table 2. A critical issue for the user acceptance of such a system is the usability of its interface. The chosen solution is based on an existing instant messenger, which offers several possibilities to integrate our system (see chapter 4.2). 4.2 Instant Messenger Integration Our system is based upon an existing instant messenger, the socalled Simple Instant Messenger (SIM)3 . SIM is based on a plug-in concept. The messenger, more exactly the contact list (i.e. Local time as a very important context dimension can be easily retrieved from the real time clock of the user\"s system. Other activities like making a phone call or something similar, cannot be recognized with the current implementation of the activity sensor. Therefore, some additional information like a list of members for each group is needed. 94 4.4 Group Interaction Support 4.4.1 Visualisation of Location Information An important feature is the visualisation of location information, thus allowing users to be aware of the location of other users and members of groups he joined, respectively. As already described in chapter 2, we use two different forms of visualisation. Due to the restricted space in the contact list, it has been necessary to implement some sort of level-of-detail concept. For that reason, the floor (i.e. On top of the image, the heightened, integrated GISS-toolbar is shown, which currently contains the following, implemented functionality (from left to right): Asynchronous communication for groups (see chapter 4.4.4), context-aware reminders (see chapter 4.4.6), two-dimensional visualisation of locationinformation, forming and managing groups (see chapter 4.4.2), context-aware availability-management (see chapter 4.4.5) and finally a button for terminating GISS. Figure 5. The second possible form of visualisation is a graphical one. In this floor-plan-based view, the current locations are shown in the manner of ICQ2 contacts, which are placed at the currently sensed location of the respective person. Figure 6. Figure 7. So-called static groups are groups, which are built up manually by people joining and leaving them. In contrast to that, we also support the creation of dynamic groups. 4.4.3 Synchronous Communication for Groups The most important form of synchronous communication on computers today is instant messaging; some people even see instant messaging to be the real killer application on the Internet. This has also motivated the decision to build GISS upon an instant messaging system. In today\"s messenger systems, peer-to-peer-communication is extensively supported. Our approach does not have the mentioned restrictions. 4.4.4 Asynchronous Communication for Groups Asynchronous communication in groups is not a new idea. For this reason, we introduce the concept of so-called virtual postits (cp. All readers of a post-it are logged and displayed when viewing it, providing some sort of awareness about the group members\" activities in the past. 4.4.5 Context-aware Availability Management Instant messengers in general provide some kind of availability information about a user. To support the user managing his availability, we provide an interface that lets the user define rules to adapt his availability to the current context. The testing of conditions is periodically triggered by throwing events (whenever the context of a user changes). 4.4.6 Context-Aware Reminders Reminders are used to give the user the opportunity of defining tasks and being reminded of those, when certain criteria are fulfilled. 96 4.4.7 Context-Aware Recognition and Notification of Group Meetings With the available context information, we try to recognize meetings of a group. During the meeting, the comment-feature of virtual post-its provides a means to take notes for all of the participants. Like the recognition of the beginning of a meeting, the recognition of its end is still part of ongoing work.", "body2": "It supports group interaction in mobile distributed computing environments in a way that group members need not to at the same place any longer in order to interact with each other or just to be aware of the others situation. Chapter 5 gives a final summary of our work. Because this definition is very general, many approaches have been made to define the notion of context with respect to computing environments. defines context as the user\"s location, environment, identity and time. According to this definition, defines a system to be contextaware if it uses context to provide relevant information and/or services to the user, where relevancy depends on the user\"s task. also gives a classification of features for context-aware applications, which comprises presentation of information and services to a user, automatic execution of a service and tagging of context to information for later retrieval. 1.2 Group Interaction in Context After these abstract and formal definitions about what context and context computing is, we will now focus on the main goal of this work, namely how the interaction of mobile group members can be supported by using context information. With respect to these five aspects of team support, we focus on interaction and partly cover mobility- and awareness-support. Context information facilitates group interaction by allowing each member to be aware of the availability status or the current location of each other group member, which again makes it possible to form groups dynamically, to place virtual post-its in the real world or to determine which people are around. This provides a comprehensive description of a user\"s current situation and thus enabling numerous means for supporting group interaction, which are described in detail in chapter 4.4. This so-called Context Framework is based on a distributed communication architecture and it supports different kinds of transport protocols and message coding mechanisms. 89 A main feature of the framework is the abstraction of context information retrieval via various sensors and its delivery to a level where no difference appears, for the application designer, between these different kinds of context retrieval mechanisms; the information retrieval is hidden from the application developer. a human user - that are important for a certain context scenario. Typical attributes are encapsulations of sensors, but they can also be used to implement context services, for example to notify other entities about location changes of users. Each attribute is able to trigger events, which are addressed to other attributes and entities respectively, independently on which physical computer they are running. Among other things, and event contains the name of the event and a list of parameters delivering information about the event itself. Furthermore, it is able to adapt its behaviour dynamically by loading attributes, entities or ECArules at runtime. We do not only sense the current location of users, but also determine spatial proximities between them. Developing the architecture, we focused on keeping the client as simple as possible and reducing the communication between client and server to a minimum. This requires a context repository, where the mapping of diverse physical positions to standardized locations is stored. Because of the abstraction of sensor-dependent position information, the system can easily be extended by additional sensors, just by implementing the (typically two) attributes for encapsulating sensors (some sensors may not need a client-side part), abstracting physical positions and observing the interface to GISS Core. Whenever necessary, events are thrown to the affected clients to trigger context-aware activities, like changing the presentation of awareness information or the execution of services. The central attribute on client-side is the so-called Instant Messenger Encapsulation, which on the one hand interacts with the server through events and on the other hand serves as a proxy for the external application the user interface is built on. Through the interface of the messenger, it provides means of synchronous and asynchronous communication as well as a context-aware reminder system and tools for managing groups and the own availability status. The second part of the user interface is a visualisation of the user\"s locations, which is implemented in the attribute Viewer. In the following chapters 3 and 4, we describe the location sensing-backend and the application front-end for supporting group interaction in more detail. Finally, we will have a closer look on the fusion of location- and proximity-information, acquired by various sensors. We have chosen a symbolic location model, which refers to locations as abstract symbols like Room P111 or Physics Building, because we do not require geometric location data. Symbolic Location Containment Hierarchy The hierarchy of symbolic locations as well as the type of each position is stored in the context repository. 3.2 Sensors Our architecture supports two different kinds of sensors: location sensors, which acquire location information, and proximity sensors, which detect spatial proximities between users. While the clientattributes (Sensor Abstraction) are responsible for acquiring low-level sensor-data and transmitting it to the server, the corresponding Sensor Encapsulation-attributes transform them into a uniform and sensor-independent format, namely symbolic locations and IDs of users in spatial proximity, respectively. Such notifications are performed every time the sensor acquired new information. if it has been unplugged by the user) or when position respectively proximity could not be determined any longer (e.g. RFID reader cannot detect tags) and notify the corresponding sensor fusion about this. We have chosen these three completely different sensors because of their differences concerning accuracy, coverage and administrative effort, in order to evaluate the flexibility of our system (see Table 1). A drawback is the poor availability, which can be traced back to the fact that RFID readers are still very expensive. Because most mobile computers are equipped with WLAN technology and the positioning-system is a software-only solution, nearly everyone is able to use this kind of sensor. This leads to the disadvantage that the range of each Bluetooth-tag has to be determined and mapped to symbolic locations within this range. We have implemented two types of proximity-sensors, which are based on Bluetooth on the one hand and on fused symbolic locations (see chapter 3.3.1) on the other hand. The already determined Bluetooth MAC addresses in range of a certain client are being compared with those of all other clients, and each time the attribute Bluetooth Sensor Abstraction detects congruence, it notifies the proximity sensor fusion about this. For example, the symbolic locations at same level in the location hierarchy mostly do not cover the same geographic area. Figure 3). This has the advantage, that additional position- and proximitysensors can be added easily or the fusion algorithms can be replaced by ones that are more sophisticated. An important feature is the persistent storage of location- and proximity-history in a database in order to allow future retrieval. This enables applications to visualize the movement of users for example. This is quite difficult, because different sensors may differ in accuracy and sample rate as well. These events contain information about the identity of the user concerned, his current location and the sensor by which the location has been determined. If this is the case, it notifies the GISS Core-attribute about all symbolic locations this user is currently associated with. To cope with this challenge, we implemented a fusion algorithm in order to reduce the number of symbolic locations to a minimum (ideally to a single location). A further reduction can be achieved by assigning priorities to sensors (based on accuracy and confidence) and cumulating these priorities for each symbolic location instead of just counting the number of occurrences. if they differ from the fused locations the considered user is currently associated with), they are provided with the current timestamp, written to the database and the GISS-attribute is notified about where the user is probably located. If it changes, the GISS Core-attribute is triggered again. These special events contain information about the identity of the two users concerned, if they are currently in spatial proximity or if proximity no longer persists, and by which proximity-sensor this has been detected. Finally, if spatial proximity between the two users actually changed, an event is thrown to notify the GISS Core-attribute about this. 4.1 Overview In most of today\"s systems supporting interaction in groups, the provided means lack any awareness of the user\"s current context, thus being unable to adapt to his needs. For this reason, we also retrieve information about the group\"s current context, derived from the contexts of the group members together with some sort of meta-information (see chapter 4.3). Table 2 gives an overview of activities we have already implemented; they are described comprehensively in chapter 4.4. The table also shows which types of context information are used for each activity and the category the activity could be classified in. Classification of implemented context-aware activities Service L T I A G P A T Location Visualisation X X X Group Building Support X X X X Support for Synchronous Communication X X X X Support for Asynchronous Communication X X X X X X X Availability Management X X X Task Management Support X X X X Meeting Support X X X X X X Reasons for implementing these very features are to take advantage of all four types of context information in order to support group interaction by utilizing a comprehensive knowledge about the situation a single user or a whole group is in. Although we think that the user interface that has been implemented now offers the best trade-off between seamless integration of features and ease of use, it would be no problem to extend the architecture with other user interfaces, even on different platforms. Furthermore, our system uses an instant messenger account as an identifier, so that the user does not have to register a further account anywhere else (for example, the user can use his already existing ICQ2 -account). Furthermore, full support for SMS-notification (where provided from the used protocol) is given. The functionality of the plug-in was extended in several ways, for example to accept messages for an account (as if they would have been sent via the messenger network). 4.3 Sources of Context Information While the location-context of a user is obtained from our location sensing subsystem described in chapter 3, we consider further types of context than location relevant for the support of group interaction, too. Information about the activity or a certain user is only available in a very restricted area, namely the activity at the computer itself. Information about the context of a group includes how many members the group currently has, if the group meets right now, which members are participating at a meeting, how many members have read which of the available posts from other team members and so on. A brief description of these services is given in chapter 4.4. 94 4.4 Group Interaction Support 4.4.1 Visualisation of Location Information An important feature is the visualisation of location information, thus allowing users to be aware of the location of other users and members of groups he joined, respectively. The maybe more important one is to display location information in the contact list of the instant messenger, right beside the name, thus being always visible while not drawing the user\"s attention on it (compared with a twodimensional view for example, which requires a own window for displaying a map of the environment). If, for example, user A currently resides in room P121 at the first floor of a building and user B, which has to be displayed in the contact list of user A, is in room P304 at the third floor, the most accurate common location of these two users is the building they are in. Figure 5 shows a screenshot of the Simple Instant Messenger3 where the current location of those people, whose location is known by GISS, is displayed in brackets right beside their name. On top of the image, the heightened, integrated GISS-toolbar is shown, which currently contains the following, implemented functionality (from left to right): Asynchronous communication for groups (see chapter 4.4.4), context-aware reminders (see chapter 4.4.6), two-dimensional visualisation of locationinformation, forming and managing groups (see chapter 4.4.2), context-aware availability-management (see chapter 4.4.5) and finally a button for terminating GISS. in the form of desk@room2@department1@1stfloor@building 1@campus). campus) are also provided. Furthermore, the floor-plan-view shows so-called the virtual post-its, which are virtual counterparts of real-life post-its and serve as our means of asynchronous communication (more about virtual post-its can be found in chapter 4.4.4). Down to the right, two virtual post-its can be seen. As groups can have different purposes, we distinguish two types of groups. Closed groups enable users for example to create a group of their friends, thus being able to communicate with them easily. As the members of dynamic groups constantly change, it is possible to create an open static group out of them. 4.4.3 Synchronous Communication for Groups The most important form of synchronous communication on computers today is instant messaging; some people even see instant messaging to be the real killer application on the Internet. This has also motivated the decision to build GISS upon an instant messaging system. Furthermore, groups can only be formed of members in one\"s contact list, thus being not able to send messages to a group, where not all of its members are known (which may be the case in settings, where the participants of a lecture form a group). Communication via a group account is also stored centrally, enabling people to query missed messages or simply viewing the message history. In out work, we aim at the combination of asynchronous communication with location awareness. Afterwards, post-its can be accessed via the 2D-viewer, where all visible post-its are shown. All readers of a post-it are logged and displayed when viewing it, providing some sort of awareness about the group members\" activities in the past. Although this information can be only defined in a very coarse granularity, we have decided to use these means of gathering activity context, because the introduction of an additional one would strongly decrease the usability of the system. These rules follow the form on event (E) if condition (C) then action (A), which is directly supported by the ECA-rules of the Context Framework described in chapter 1.3. As a condition, the user can define his location, a certain time (also triggering daily, every week or every month) or any logical combination of these criteria. On showing up the reminder, the user has the chance to resubmit it to be reminded again, for example five minutes later or the next time a certain user is in spatial proximity. not on a corridor), a meeting minutes post-it is created at this location and all absent group members are notified of the meeting and the location it takes place. When members are joining or leaving the meeting, this is automatically added as a note to the list of comments. If the end of the meeting is recognized, all group members get the complete list of comments as a meeting protocol at the end of the meeting.", "introduction": "Today\"s computing environments are characterized by an increasing number of powerful, wirelessly connected mobile devices. Users can move throughout an environment while carrying their computers with them and having remote access to information and services, anytime and anywhere. New situations appear, where the user\"s context - for example his current location or nearby people - is more dynamic; computation does not occur at a single location and in a single context any longer, but comprises a multitude of situations and locations. This development leads to a new class of applications, which are aware of the context in which they run in and thus bringing virtual and real worlds together. Motivated by this and the fact, that only a few studies have been done for supporting group communication in such computing environments , we have developed a system, which we refer to as Group Interaction Support System (GISS). It supports group interaction in mobile distributed computing environments in a way that group members need not to at the same place any longer in order to interact with each other or just to be aware of the others situation. In the following subchapters, we will give a short overview on context aware computing and motivate its benefits for supporting group interaction. A software framework for developing contextsensitive applications is presented, which serves as middleware for GISS. Chapter 2 presents the architecture of GISS, and chapter 3 and 4 discuss the location sensing and group interaction concepts of GISS in more detail. Chapter 5 gives a final summary of our work. According to Merriam-Webster\"s Online Dictionary1 , context is defined as the interrelated conditions in which something exists or occurs. Because this definition is very general, many approaches have been made to define the notion of context with respect to computing environments. Most definitions of context are done by enumerating examples or by choosing synonyms for context. The term context-aware has been introduced first in where context is referred to as location, identities of nearby people and objects, and changes to those objects. In , context is also defined by an enumeration of examples, namely location, identities of the people around the user, the time of the day, season, temperature etc. defines context as the user\"s location, environment, identity and time. Here we conform to a widely accepted and more formal definition, which defines context as any information than can be used to characterize the situation of an entity. An entity is a person, place, or object that is considered relevant to the interaction between a user and an application, including the user and applications themselves. identifies four primary types of context information (sometimes referred to as context dimensions), that are - with respect to characterizing the situation of an entity - more important than others. These are location, identity, time and activity, which can also be used to derive other sources of contextual information (secondary context types). For example, if we know a person\"s identity, we can easily derive related information about this person from several data sources (e.g. day of birth or e-mail address). According to this definition, defines a system to be contextaware if it uses context to provide relevant information and/or services to the user, where relevancy depends on the user\"s task. also gives a classification of features for context-aware applications, which comprises presentation of information and services to a user, automatic execution of a service and tagging of context to information for later retrieval. Layers of a context-aware system Context computing is based on two major issues, namely identifying relevant context (identity, location, time, activity) and using obtained context (automatic execution, presentation, tagging). In order to do this, there are a few layers between (see Figure 1). First, the obtained low-level context information has to be transformed, aggregated and interpreted (context transformation) and represented in an abstract context world model (context representation), either centralized or decentralized. Finally, the stored context information is used to trigger certain context events (context triggering). 1.2 Group Interaction in Context After these abstract and formal definitions about what context and context computing is, we will now focus on the main goal of this work, namely how the interaction of mobile group members can be supported by using context information. In we have identified organizational systems to be crucial for supporting mobile groups (see Figure 2). First, there has to be an Information and Knowledge Management System, which is capable of supporting a team with its information processing- and knowledge gathering needs. The next part is the Awareness System, which is dedicated to the perceptualisation of the effects of team activity. It does this by communicating work context, agenda and workspace information to the users. The Interaction Systems provide support for the communication among team members, either synchronous or asynchronous, and for the shared access to artefacts, such as documents. Mobility Systems deploy mechanisms to enable any-place access to team memory as well as the capturing and delivery of awareness information from and to any places. Finally yet importantly, the organisational innovation system integrates aspects of the team itself, like roles, leadership and shared facilities. With respect to these five aspects of team support, we focus on interaction and partly cover mobility- and awareness-support. Group interaction includes all means that enable group members to communicate freely with all the other members. At this point, the question how context information can be used for supporting group interaction comes up. We believe that information about the current situation of a person provides a surplus value to existing group interaction systems. Context information facilitates group interaction by allowing each member to be aware of the availability status or the current location of each other group member, which again makes it possible to form groups dynamically, to place virtual post-its in the real world or to determine which people are around. Support for Mobile Groups Most of today\"s context-aware applications use location and time only, and location is referred to as a crucial type of context information . We also see the importance of location information in mobile and ubiquitous environments, wherefore a main focus of our work is on the utilization of location information and information about users in spatial proximity. Nevertheless, we believe that location, as the only used type of context information, is not sufficient to support group interaction, wherefore we also take advantage of the other three primary types, namely identity, time and activity. This provides a comprehensive description of a user\"s current situation and thus enabling numerous means for supporting group interaction, which are described in detail in chapter 4.4. When we look at the types of context information stated above, we can see that all of them are single user-centred, taking into account only the context of the user itself. We believe, that for the support of group interaction, the status of the group itself has also be taken into account. Therefore, we have added a fifth contextdimension group-context, which comprises more than the sum of the individual member\"s contexts. Group context includes any information about the situation of a whole group, for example how many members a group currently has or if a certain group meets right now. 1.3 Context Middleware The Group Interaction Support System (GISS) uses the softwareframework introduced in , which serves as a middleware for developing context-sensitive applications. This so-called Context Framework is based on a distributed communication architecture and it supports different kinds of transport protocols and message coding mechanisms. 89 A main feature of the framework is the abstraction of context information retrieval via various sensors and its delivery to a level where no difference appears, for the application designer, between these different kinds of context retrieval mechanisms; the information retrieval is hidden from the application developer. This is achieved by so-called entities, which describe objectse.g. a human user - that are important for a certain context scenario. Entities express their functionality by the use of so-called attributes, which can be loaded into the entity. These attributes are complex pieces of software, which are implemented as Java classes. Typical attributes are encapsulations of sensors, but they can also be used to implement context services, for example to notify other entities about location changes of users. Each entity can contain a collection of such attributes, where an entity itself is an attribute. The initial set of attributes an entity contains can change dynamically at runtime, if an entity loads or unloads attributes from the local storage or over the network. In order to load and deploy new attributes, an entity has to reference a class loader and a transport and lookup layer, which manages the lookup mechanism for discovering other entities and the transport. XML configuration files specify which initial set of entities should be loaded and which attributes these entities own. The communication between entities and attributes is based on context events. Each attribute is able to trigger events, which are addressed to other attributes and entities respectively, independently on which physical computer they are running. Among other things, and event contains the name of the event and a list of parameters delivering information about the event itself. Related with this event-based architecture is the use of ECA (Event-Condition-Action)-rules for defining the behaviour of the context system. Therefore, every entity has a rule-interpreter, which catches triggered events, checks conditions associated with them and causes certain actions. These rules are referenced by the entity\"s XML configuration. A rule itself is even able to trigger the insertion of new rules or the unloading of existing rules at runtime in order to change the behaviour of the context system dynamically. To sum up, the context framework provides a flexible, distributed architecture for hiding low-level sensor data from high-level applications and it hides external communication details from the application developer. Furthermore, it is able to adapt its behaviour dynamically by loading attributes, entities or ECArules at runtime.", "conclusion": "This paper discussed the potentials of support for group interaction by using context information.. First, we introduced the notions of context and context computing and motivated their value for supporting group interaction.. An architecture is presented to support context-aware group interaction in mobile, distributed environments.. It is built upon a flexible and extensible framework, thus enabling an easy adoption to available context sources (e.g.. by adding additional sensors) as well as the required form of representation.. We have prototypically developed a set of services, which enhance group interaction by taking into account the current context of the users as well as the context of groups itself.. Important features are dynamic formation of groups, visualization of location on a two-dimensional map as well as unobtrusively integrated in an instant-messenger, asynchronous communication by virtual post-its, which are bound to certain locations, and a context-aware availability-management, which adapts the availability-status of a user to his current situation.. To provide location information, we have implemented a subsystem for automated acquisition of location- and proximityinformation provided by various sensors, which provides a technology-independent presentation of locations and spatial proximities between users and merges this information using sensor-independent fusion algorithms.. A history of locations as well as of spatial proximities is stored in a database, thus enabling context history-based services."}
{"id": "C-67", "keywords": ["macintosh os x", "xgrid", "grid comput", "cluster", "highperform comput", "rendezv", "render"], "title": "A Holistic Approach to High-Performance Computing: Xgrid Experience", "abstract": "The Ringling School of Art and Design is a fully accredited four-year college of visual arts and design. With a student to computer ratio of better than 2-to-1, the Ringling School has achieved national recognition for its large-scale integration of technology into collegiate visual art and design education. We have found that Mac OS X is the best operating system to train future artists and designers. Moreover, we can now buy Macs to run high-end graphics, nonlinear video editing, animation, multimedia, web production, and digital video applications rather than expensive UNIX workstations. As visual artists cross from paint on canvas to creating in the digital realm, the demand for a high-performance computing environment grows. In our public computer laboratories, students use the computers most often during the workday; at night and on weekends the computers see only light use. In order to harness the lost processing time for tasks such as video rendering, we are testing Xgrid, a suite of Mac OS X applications recently developed by Apple for parallel and distributed high-performance computing. As with any new technology deployment, IT managers need to consider a number of factors as they assess, plan, and implement Xgrid. Therefore, we would like to share valuable information we learned from our implementation of an Xgrid environment with our colleagues. In our report, we will address issues such as assessing the needs for grid computing, potential applications, management tools, security, authentication, integration into existing infrastructure, application support, user training, and user support. Furthermore, we will discuss the issues that arose and the lessons learned during and after the implementation process.", "references": ["Apple Academic Research", "Search for Extraterrestrial Intelligence at home", "Alias", "Apple Computer, Xgrid", "Xgrid Guide", "Apple Mac OS X Features", "Xgrid Manual Page", "Xgrid Presentation", "Research Systems Unix Group", "Using the Radmind Command Line Tools to Maintain Multiple Mac OS X Machines", "POV-Ray", "Xgrid example: Parallel graphics rendering in POVray", "NEESgrid", "SAP", "Platform Computing", "Grid", "United Devices", "N1 Grid Engine 6", "Xgrig Users Mailing List"], "full_text": "1. INTRODUCTION Grid computing does not have a single, universally accepted definition. The technology behind grid computing model is not new.s, where scientists harnessed the computing power of idle workstations to let compute intensive applications to run on multiple workstations, which dramatically shortening processing times. Although numerous distributed computing models were available for discipline-specific scientific applications, only recently have the tools became available to use general-purpose applications on a grid. Consequently, the grid computing model is gaining popularity and has become a show piece of \"utility computing\". Since in the IT industry, various computing models are used interchangeably with grid computing, we first sort out the similarities and difference between these computing models so that grid computing can be placed in perspective. 1.1 Clustering A cluster is a group of machines in a fixed configuration united to operate and be managed as a single entity to increase robustness and performance. The cluster appears as a single high-speed system or a single highly available system. In this model, resources can not enter and leave the group as necessary. There are at least two types of clusters: parallel clusters and highavailability clusters. Clustered machines are generally in spatial proximity, such as in the same server room, and dedicated solely to their task. In a high-availability cluster, each machine provides the same service. If one machine fails, another seamlessly takes over its workload. For example, each computer could be a web server for a web site. Should one web server \"die,\" another provides the service, so that the web site rarely, if ever, goes down. A parallel cluster is a type of supercomputer. Problems are split into many parts, and individual cluster members are given part of the problem to solve. An example of a parallel cluster is composed of Apple Power Mac G5 computers at Virginia Tech University . 1.2 Distributed Computing Distributed computing spatially expands network services so that the components providing the services are separated. The major objective of this computing model is to consolidate processing power over a network. A simple example is spreading services such as file and print serving, web serving, and data storage across multiple machines rather than a single machine handling all the tasks. Distributed computing can also be more fine-grained, where even a single application is broken into parts and each part located on different machines: a word processor on one server, a spell checker on a second server, etc. 1.3 Utility Computing Literally, utility computing resembles common utilities such as telephone or electric service. A service provider makes computing resources and infrastructure management available to a customer as needed, and charges for usage rather than a flat rate. The important thing to note is that resources are only used as needed, and not dedicated to a single customer. 1.4 Grid Computing Grid computing contains aspects of clusters, distributed computing, and utility computing. In the most basic sense, grid turns a group of heterogeneous systems into a centrally managed but flexible computing environment that can work on tasks too time intensive for the individual systems. The grid members are not necessarily in proximity, but must merely be accessible over a network; the grid can access computers on a LAN, WAN, or anywhere in the world via the Internet. In addition, the computers comprising the grid need not be dedicated to the grid; rather, they can function as normal workstations, and then advertise their availability to the grid when not in use. The last characteristic is the most fundamental to the grid described in this paper. A well-known example of such an ad hoc grid is the SETI@home project of the University of California at Berkeley, which allows any person in the world with a computer and an Internet connection to donate unused processor time for analyzing radio telescope data. 1.5 Comparing the Grid and Cluster A computer grid expands the capabilities of the cluster by loosing its spatial bounds, so that any computer accessible through the network gains the potential to augment the grid. A fundamental grid feature is that it scales well. The processing power of any machine added to the grid is immediately availably for solving problems. In addition, the machines on the grid can be generalpurpose workstations, which keep down the cost of expanding the grid. 2. ASSESSING THE NEED FOR GRID COMPUTING Effective use of a grid requires a computation that can be divided into independent (i.e., parallel) tasks. The results of each task cannot depend on the results of any other task, and so the members of the grid can solve the tasks in parallel. Once the tasks have been completed, the results can be assembled into the solution. Examples of parallelizable computations are the Mandelbrot set of fractals, the Monte Carlo calculations used in disciplines such as Solid State Physics, and the individual frames of a rendered animation. This paper is concerned with the last example. 2.1 Applications Appropriate for Grid Computing The applications used in grid computing must either be specifically designed for grid use, or scriptable in such a way that they can receive data from the grid, process the data, and then return results. In other words, the best candidates for grid computing are applications that run the same or very similar computations on a large number of pieces of data without any dependencies on the previous calculated results. Applications heavily dependent on data handling rather than processing power are generally more suitable to run on a traditional environment than on a grid platform. Of course, the applications must also run on the computing platform that hosts the grid. Our interest is in using the Alias Maya application with Apple\"s Xgrid on Mac OS X. Commercial applications usually have strict license requirements. This is an important concern if we install a commercial application such as Maya on all members of our grid. By its nature, the size of the grid may change as the number of idle computers changes. How many licenses will be required? Our resolution of this issue will be discussed in a later section. 2.2 Integration into the Existing Infrastructure The grid requires a controller that recognizes when grid members are available, and parses out job to available members. The controller must be able to see members on the network. This does not require that members be on the same subnet as the controller, but if they are not, any intervening firewalls and routers must be configured to allow grid traffic. 3. XGRID Xgrid is Apple\"s grid implementation. It was inspired by Zilla, a desktop clustering application developed by NeXT and acquired by Apple. In this report we describe the Xgrid Technology Preview 2, a free download that requires Mac OS X 10.2.8 or later and a minimum 128 MB RAM . Xgrid, leverages Apple\"s traditional ease of use and configuration. If the grid members are on the same subnet, by default Xgrid automatically discovers available resources through Rendezvous . Tasks are submitted to the grid through a GUI interface or by the command line. A System Preference Pane controls when each computer is available to the grid. It may be best to view Xgrid as a facilitator. The Xgrid architecture handles software and data distribution, job execution, and result aggregation. However, Xgrid does not perform the actual calculations. 3.1 Xgrid Components Xgrid has three major components: the client, controller, and the agent. Each component is included in the default installation, and any computer can easily be configured to assume any role. In 120 fact, for testing purposes, a computer can simultaneously assume all roles in local mode. The more typical production use is called cluster mode. The client submits jobs to the controller through the Xgrid GUI or command line. The client defines how the job will be broken into tasks for the grid. If any files or executables must be sent as part of a job, they must reside on the client or at a location accessible to the client. When a job is complete, the client can retrieve the results from the controller. A client can only connect to a single controller at a time. The controller runs the GridServer process. It queues tasks received from clients, distributes those tasks to the agents, and handles failover if an agent cannot complete a task. In Xgrid Technology Preview 2, a controller can handle a maximum of 10,000 agent connections. Only one controller can exist per logical grid. The agents run the GridAgent process. When the GridAgent process starts, it registers with a controller; an agent can only be connected to one controller at a time. Agents receive tasks from their controller, perform the specified computations, and then send the results back to the controller. An agent can be configured to always accept tasks, or to just accept them when the computer is not otherwise busy. 3.2 Security and Authentication By default, Xgrid requires two passwords. First, a client needs a password to access a controller. Second, the controller needs a password to access an agent. Either password requirement can be disabled. Xgrid uses two-way-random mutual authentication protocol with MD5 hashes. At this time, data encryption is only used for passwords. As mentioned earlier, an agent registers with a controller when the GridAgent process starts. There is no native method for the controller to reject agents, and so it must accept any agent that registers. This means that any agent could submit a job that consumes excessive processor and disk space on the agents. Of course, since Mac OS X is a BSD-based operating system, the controller could employ Unix methods of restricting network connections from agents. The Xgrid daemons run as the user nobody, which means the daemons can read, write, or execute any file according to world permissions. Thus, Xgrid jobs can execute many commands and write to /tmp and /Volumes. In general, this is not a major security risk, but is does require a level of trust between all members of the grid. 3.3 Using Xgrid 3.3.1 Installation Basic Xgrid installation and configuration is described both in Apple documentation and online at the University of Utah web site . The installation is straightforward and offers no options for customization. This means that every computer on which Xgrid is installed has the potential to be a client, controller, or agent. 3.3.2 Agent and Controller Configuration The agents and controllers can be configured through the Xgrid Preference Pane in the System Preferences or XML files in /Library/Preferences. Here the GridServer and GridAgent processes are started, passwords set, and the controller discovery method used by agents is selected. By default, agents use Rendezvous to find a controller, although the agents can also be configured to look for a specific host. The Xgrid Preference Pane also sets whether the Agents will always accept jobs, or only accept jobs when idle. In Xgrid terms, idle either means that the Xgrid screen saver has activated, or the mouse and keyboard have not been used for more than 15 minutes. Even if the agent is configured to always accept tasks, if the computer is being used these tasks will run in the background at a low priority. However, if an agent only accepts jobs when idle, any unfinished task being performed when the computer ceases being idle are immediately stopped and any intermediary results lost. Then the controller assigns the task to another available member of the grid. Advertising the controller via Rendezvous can be disabled by editing /Library/Preferences/com.apple.xgrid.controller.plist. This, however, will not prevent an agent from connecting to the controller by hostname. 3.3.3 Sending Jobs from an Xgrid Client The client sends jobs to the controller either through the Xgrid GUI or the command line. The Xgrid GUI submits jobs via small applications called plug-ins. Sample plug-ins are provided by Apple, but they are only useful as simple testing or as examples of how to create a custom plug-in. If we are to employ Xgrid for useful work, we will require a custom plug-in. James Reynolds details the creation of custom plug-ins on the University of Utah Mac OS web site . Xgrid stores plug-ins in /Library/Xgrid/Plug-ins or ~/Library/Xgrid/Plug-ins, depending on whether the plug-in was installed with Xgrid or created by a user. The core plug-in parameter is the command, which includes the executable the agents will run. Another important parameter is the working directory. This directory contains necessary files that are not installed on the agents or available to them over a network. The working directory will always be copied to each agent, so it is best to keep this directory small. If the files are installed on the agents or available over a network, the working directory parameter is not needed. The command line allows the options available with the GUI plug-in, but it can be slightly more cumbersome. However, the command line probably will be the method of choice for serious work. The command arguments must be included in a script unless they are very basic. This can be a shell, perl, or python script, as long as the agent can interpret it. 3.3.4 Running the Xgrid Job When the Xgrid job is started, the command tells the controller how to break the job into tasks for the agents. Then the command is tarred and gzipped and sent to each agent; if there is a working directory, this is also tarred and gzipped and sent to the agents. 121 The agents extract these files into /tmp and run the task. Recall that since the GridAgent process runs as the user nobody, everything associated with the command must be available to nobody. Executables called by the command should be installed on the agents unless they are very simple. If the executable depends on libraries or other files, it may not function properly if transferred, even if the dependent files are referenced in the working directory. When the task is complete, the results are available to the client. In principle, the results are sent to the client, but whether this actually happens depends on the command. If the results are not sent to the client, they will be in /tmp on each agent. When available, a better solution is to direct the results to a network volume accessible to the client. 3.4 Limitations and Idiosyncrasies Since Xgrid is only in its second preview release, there are some rough edges and limitations. Apple acknowledges some limitations . For example, the controller cannot determine whether an agent is trustworthy and the controller always copies the command and working directory to the agent without checking to see if these exist on the agent. Other limitations are likely just a by-product of an unfinished work. Neither the client nor controller can specify which agents will receive the tasks, which is particularly important if the agents contain a variety of processor types and speeds and the user wants to optimize the calculations. At this time, the best solution to this problem may be to divide the computers into multiple logical grids. There is also no standard way to monitor the progress of a running job on each agent. The Xgrid GUI and command line indicate which agents are working on tasks, but gives no indication of progress. Finally, at this time only Mac OS X clients can submit jobs to the grid. The framework exists to allow third parties to write plug-ins for other Unix flavors, but Apple has not created them. 4. XGRID IMPLEMENTATION Our goal is an Xgrid render farm for Alias Maya. The Ringling School has about 400 Apple Power Mac G4\"s and G5\"s in 13 computer labs. The computers range from 733 MHz singleprocessor G4\"s and 500 MHz and 1 GHz dual-processor G4\"s to 1.8 GHz dual-processor G5\"s. All of these computers are lightly used in the evening and on weekends and represent an enormous processing resource for our student rendering projects. 4.1 Software Installation During our Xgrid testing, we loaded software on each computer multiple times, including the operating systems. We saved time by facilitating our installations with the remote administration daemon (radmind) software developed at the University of Michigan , . Everything we installed for testing was first created as a radmind base load or overload. Thus, Mac OS X, Mac OS X Developer Tools, Xgrid, POV-Ray , and Alias Maya were stored on a radmind server and then installed on our test computers when needed. 4.2 Initial Testing We used six 1.8 GHz dual-processor Apple Power Mac G5\"s for our Xgrid tests. Each computer ran Mac OS X 10.3.3 and contained 1 GB RAM. As shown in Figure 1, one computer served as both client and controller, while the other five acted as agents. Before attempting Maya rendering with Xgrid, we performed basic calculations to cement our understanding of Xgrid. Apple\"s Xgrid documentation is sparse, so finding helpful web sites facilitated our learning. We first ran the Mandelbrot set plug-in provided by Apple, which allowed us to test the basic functionality of our grid. Then we performed benchmark rendering with the Open Source Application POV-Ray, as described by Daniel C\u00f4t\u00e9 and James Reynolds . Our results showed that one dual-processor G5 rendering the benchmark POV-Ray image took 104 minutes. Breaking the image into three equal parts and using Xgrid to send the parts to three agents required 47 minutes. However, two agents finished their rendering in 30 minutes, while the third agent used 47 minutes; the entire render was only as fast as the slowest agent. These results gave us two important pieces of information. First, the much longer rendering time for one of the tasks indicated that we should be careful how we split jobs into tasks for the agents. All portions of the rendering will not take equal amounts of time, even if the pixel size is the same. Second, since POV-Ray cannot take advantage of both processors in a G5, neither can an Xgrid task running POV-Ray. Alias Maya does not have this limitation. 4.3 Rendering with Alias Maya 6 We first installed Alias Maya 6 for Mac OS X on the client/controller and each agent. Maya 6 requires licenses for use as a workstation application. However, if it is just used for rendering from the command line or a script, no license is needed. We thus created a minimal installation of Maya as a radmind overload. The application was installed in a hidden directory inside /Applications. This was done so that normal users of the workstations would not find and attempt to run Maya, which would fail because these installations are not licensed for such use. In addition, Maya requires the existence of a directory ending in the path /maya. The directory must be readable and writable by the Maya user. For a user running Maya on a Mac OS X workstation, the path would usually be ~/Documents/maya. Unless otherwise specified, this directory will be the default location for Maya data and output files. If the directory does not Figure 1. Xgrid test grid. Client/ Controller Agent 1 Agent 2 Agent 3 Agent 4 Agent 5 Network Volume Jobs Data Data 122 exist, Maya will try to create it, even if the user specifies that the data and output files exist in other locations. However, Xgrid runs as the user nobody, which does not have a home directory. Maya is unable to create the needed directory, and looks instead for /Alias/maya. This directory also does not exist, and the user nobody has insufficient rights to create it. Our solution was to manually create /Alias/maya and give the user nobody read and write permissions. We also created a network volume for storage of both the rendering data and the resulting rendered frames. This avoided sending the Maya files and associated textures to each agent as part of a working directory. Such a solution worked well for us because our computers are geographically close on a LAN; if greater distance had separated the agents from the client/controller, specifying a working directory may have been a better solution. Finally, we created a custom GUI plug-in for Xgrid. The plug-in command calls a Perl script with three arguments. Two arguments specify the beginning and end frames of the render and the third argument the number of frames in each job (which we call the cluster size). The script then calculates the total number of jobs and parses them out to the agents. For example, if we begin at frame 201 and end at frame 225, with 5 frames for each job, the plug-in will create 5 jobs and send them out to the agents. Once the jobs are sent to the agents, the script executes the /usr/sbin/Render command on each agent with the parameters appropriate for the particular job. The results are sent to the network volume. With the setup described, we were able to render with Alias Maya 6 on our test grid. Rendering speed was not important at this time; our first goal was to implement the grid, and in that we succeeded. 4.3.1 Pseudo Code for Perl Script in Custom Xgrid Plug-in In this section we summarize in simplified pseudo code format the Perl script used in our Xgrig plug-in. agent_jobs{ \u2022 Read beginning frame, end frame, and cluster size of render. \u2022 Check whether the render can be divided into an integer number of jobs based on the cluster size. \u2022 If there are not an integer number of jobs, reduce the cluster size of the last job and set its last frame to the end frame of the render. \u2022 Determine the start frame and end frame for each job. \u2022 Execute the Render command. 4.4 Lessons Learned Rendering with Maya from the Xgrid GUI was not trivial. The lack of Xgrid documentation and the requirements of Maya combined into a confusing picture, where it was difficult to decide the true cause of the problems we encountered. Trial and error was required to determine the best way to set up our grid. The first hurdle was creating the directory /Alias/maya with read and write permissions for the user nobody. The second hurdle was learning that we got the best performance by storing the rendering data on a network volume. The last major hurdle was retrieving our results from the agents. Unlike the POV-Ray rendering tests, our initial Maya results were never returned to the client; instead, Maya stored the results in /tmp on each agent. Specifying in the plug-in where to send the results would not change this behavior. We decided this was likely a Maya issue rather than an Xgrid issue, and the solution was to send the results to the network volume via the Perl script. 5. FUTURE PLANS Maya on Xgrid is not yet ready to be used by the students of Ringling School. In order to do this, we must address at least the following concerns. \u2022 Continue our rendering tests through the command line rather than the GUI plug-in. This will be essential for the following step. \u2022 Develop an appropriate interface for users to send jobs to the Xgrid controller. This will probably be an extension to the web interface of our existing render farm, where the student specifies parameters that are placed in a script that issues the Render command. \u2022 Perform timed Maya rendering tests with Xgrid. Part of this should compare the rendering times for Power Mac G4\"s and G5\"s. 6. CONCLUSION Grid computing continues to advance. Recently, the IT industry has witnessed the emergence of numerous types of contemporary grid applications in addition to the traditional grid framework for compute intensive applications. For instance, peer-to-peer applications such as Kazaa, are based on storage grids that do not share processing power but instead an elegant protocol to swap files between systems. Although in our campuses we discourage students from utilizing peer-to-peer applications from music sharing, the same protocol can be utilized on applications such as decision support and data mining. The National Virtual Collaboratory grid project will link earthquake researchers across the U.S. with computing resources, allowing them to share extremely large data sets, research equipment, and work together as virtual teams over the Internet. There is an assortment of new grid players in the IT world expanding the grid computing model and advancing the grid technology to the next level. SAP is piloting a project to grid-enable SAP ERP applications, Dell has partnered with Platform Computing to consolidate computing resources and provide grid-enabled systems for compute intensive applications, Oracle has integrated support for grid computing in their 10g release , United Devices offers hosting service for gridon-demand, and Sun Microsystems continues their research and development of Sun\"s N1 Grid engine which combines grid and clustering platforms. Simply, the grid computing is up and coming. The potential benefits of grid computing are colossal in higher education learning while the implementation costs are low. Today, it would be difficult to identify an application with as high a return on investment as grid computing in information technology divisions in higher education institutions. It is a mistake to overlook this technology with such a high payback. 123 7. ACKNOWLEDGMENTS The authors would like to thank Scott Hanselman of the IT team at the Ringling School of Art and Design for providing valuable input in the planning of our Xgrid testing. We would also like to thank the posters of the Xgrid Mailing List for providing insight into many areas of Xgrid.", "body1": "Grid computing does not have a single, universally accepted definition. 1.1 Clustering A cluster is a group of machines in a fixed configuration united to operate and be managed as a single entity to increase robustness and performance. In a high-availability cluster, each machine provides the same service. A parallel cluster is a type of supercomputer. 1.2 Distributed Computing Distributed computing spatially expands network services so that the components providing the services are separated. 1.3 Utility Computing Literally, utility computing resembles common utilities such as telephone or electric service. 1.4 Grid Computing Grid computing contains aspects of clusters, distributed computing, and utility computing. 1.5 Comparing the Grid and Cluster A computer grid expands the capabilities of the cluster by loosing its spatial bounds, so that any computer accessible through the network gains the potential to augment the grid. COMPUTING Effective use of a grid requires a computation that can be divided into independent (i.e., parallel) tasks. 2.1 Applications Appropriate for Grid Computing The applications used in grid computing must either be specifically designed for grid use, or scriptable in such a way that they can receive data from the grid, process the data, and then return results. This is an important concern if we install a commercial application such as Maya on all members of our grid. 2.2 Integration into the Existing Infrastructure The grid requires a controller that recognizes when grid members are available, and parses out job to available members. Xgrid is Apple\"s grid implementation. If the grid members are on the same subnet, by default Xgrid automatically discovers available resources through Rendezvous . It may be best to view Xgrid as a facilitator. 3.1 Xgrid Components Xgrid has three major components: the client, controller, and the agent. The client submits jobs to the controller through the Xgrid GUI or command line. The controller runs the GridServer process. The agents run the GridAgent process. 3.2 Security and Authentication By default, Xgrid requires two passwords. As mentioned earlier, an agent registers with a controller when the GridAgent process starts. The Xgrid daemons run as the user nobody, which means the daemons can read, write, or execute any file according to world permissions. 3.3 Using Xgrid 3.3.1 Installation Basic Xgrid installation and configuration is described both in Apple documentation and online at the University of Utah web site . 3.3.2 Agent and Controller Configuration The agents and controllers can be configured through the Xgrid Preference Pane in the System Preferences or XML files in /Library/Preferences. The Xgrid Preference Pane also sets whether the Agents will always accept jobs, or only accept jobs when idle. Advertising the controller via Rendezvous can be disabled by editing /Library/Preferences/com.apple.xgrid.controller.plist. 3.3.3 Sending Jobs from an Xgrid Client The client sends jobs to the controller either through the Xgrid GUI or the command line. The core plug-in parameter is the command, which includes the executable the agents will run. The command line allows the options available with the GUI plug-in, but it can be slightly more cumbersome. 121 The agents extract these files into /tmp and run the task. Executables called by the command should be installed on the agents unless they are very simple. When the task is complete, the results are available to the client. In principle, the results are sent to the client, but whether this actually happens depends on the command. 3.4 Limitations and Idiosyncrasies Since Xgrid is only in its second preview release, there are some rough edges and limitations. Other limitations are likely just a by-product of an unfinished work. Our goal is an Xgrid render farm for Alias Maya. Everything we installed for testing was first created as a radmind base load or overload. 4.2 Initial Testing We used six 1.8 GHz dual-processor Apple Power Mac G5\"s for our Xgrid tests. We first ran the Mandelbrot set plug-in provided by Apple, which allowed us to test the basic functionality of our grid. These results gave us two important pieces of information. All portions of the rendering will not take equal amounts of time, even if the pixel size is the same. 4.3 Rendering with Alias Maya 6 We first installed Alias Maya 6 for Mac OS X on the client/controller and each agent. We thus created a minimal installation of Maya as a radmind overload. In addition, Maya requires the existence of a directory ending in the path /maya. Unless otherwise specified, this directory will be the default location for Maya data and output files. However, Xgrid runs as the user nobody, which does not have a home directory. We also created a network volume for storage of both the rendering data and the resulting rendered frames. Finally, we created a custom GUI plug-in for Xgrid. With the setup described, we were able to render with Alias Maya 6 on our test grid. 4.3.1 Pseudo Code for Perl Script in Custom Xgrid Plug-in In this section we summarize in simplified pseudo code format the Perl script used in our Xgrig plug-in. agent_jobs{ \u2022 Read beginning frame, end frame, and cluster size of render. \u2022 Check whether the render can be divided into an integer number of jobs based on the cluster size. \u2022 If there are not an integer number of jobs, reduce the cluster size of the last job and set its last frame to the end frame of the render. \u2022 Determine the start frame and end frame for each job. \u2022 Execute the Render command. 4.4 Lessons Learned Rendering with Maya from the Xgrid GUI was not trivial. The last major hurdle was retrieving our results from the agents. Unlike the POV-Ray rendering tests, our initial Maya results were never returned to the client; instead, Maya stored the results in /tmp on each agent. Maya on Xgrid is not yet ready to be used by the students of Ringling School. \u2022 Develop an appropriate interface for users to send jobs to the Xgrid controller. \u2022 Perform timed Maya rendering tests with Xgrid.", "body2": "Since in the IT industry, various computing models are used interchangeably with grid computing, we first sort out the similarities and difference between these computing models so that grid computing can be placed in perspective. Clustered machines are generally in spatial proximity, such as in the same server room, and dedicated solely to their task. Should one web server \"die,\" another provides the service, so that the web site rarely, if ever, goes down. An example of a parallel cluster is composed of Apple Power Mac G5 computers at Virginia Tech University . Distributed computing can also be more fine-grained, where even a single application is broken into parts and each part located on different machines: a word processor on one server, a spell checker on a second server, etc. The important thing to note is that resources are only used as needed, and not dedicated to a single customer. A well-known example of such an ad hoc grid is the SETI@home project of the University of California at Berkeley, which allows any person in the world with a computer and an Internet connection to donate unused processor time for analyzing radio telescope data. In addition, the machines on the grid can be generalpurpose workstations, which keep down the cost of expanding the grid. This paper is concerned with the last example. Commercial applications usually have strict license requirements. Our resolution of this issue will be discussed in a later section. This does not require that members be on the same subnet as the controller, but if they are not, any intervening firewalls and routers must be configured to allow grid traffic. Xgrid, leverages Apple\"s traditional ease of use and configuration. A System Preference Pane controls when each computer is available to the grid. However, Xgrid does not perform the actual calculations. The more typical production use is called cluster mode. A client can only connect to a single controller at a time. Only one controller can exist per logical grid. An agent can be configured to always accept tasks, or to just accept them when the computer is not otherwise busy. At this time, data encryption is only used for passwords. Of course, since Mac OS X is a BSD-based operating system, the controller could employ Unix methods of restricting network connections from agents. In general, this is not a major security risk, but is does require a level of trust between all members of the grid. This means that every computer on which Xgrid is installed has the potential to be a client, controller, or agent. By default, agents use Rendezvous to find a controller, although the agents can also be configured to look for a specific host. Then the controller assigns the task to another available member of the grid. This, however, will not prevent an agent from connecting to the controller by hostname. Xgrid stores plug-ins in /Library/Xgrid/Plug-ins or ~/Library/Xgrid/Plug-ins, depending on whether the plug-in was installed with Xgrid or created by a user. If the files are installed on the agents or available over a network, the working directory parameter is not needed. Then the command is tarred and gzipped and sent to each agent; if there is a working directory, this is also tarred and gzipped and sent to the agents. Recall that since the GridAgent process runs as the user nobody, everything associated with the command must be available to nobody. If the executable depends on libraries or other files, it may not function properly if transferred, even if the dependent files are referenced in the working directory. When the task is complete, the results are available to the client. When available, a better solution is to direct the results to a network volume accessible to the client. For example, the controller cannot determine whether an agent is trustworthy and the controller always copies the command and working directory to the agent without checking to see if these exist on the agent. The framework exists to allow third parties to write plug-ins for other Unix flavors, but Apple has not created them. We saved time by facilitating our installations with the remote administration daemon (radmind) software developed at the University of Michigan , . Thus, Mac OS X, Mac OS X Developer Tools, Xgrid, POV-Ray , and Alias Maya were stored on a radmind server and then installed on our test computers when needed. Apple\"s Xgrid documentation is sparse, so finding helpful web sites facilitated our learning. However, two agents finished their rendering in 30 minutes, while the third agent used 47 minutes; the entire render was only as fast as the slowest agent. First, the much longer rendering time for one of the tasks indicated that we should be careful how we split jobs into tasks for the agents. Alias Maya does not have this limitation. However, if it is just used for rendering from the command line or a script, no license is needed. This was done so that normal users of the workstations would not find and attempt to run Maya, which would fail because these installations are not licensed for such use. For a user running Maya on a Mac OS X workstation, the path would usually be ~/Documents/maya. Client/ Controller Agent 1 Agent 2 Agent 3 Agent 4 Agent 5 Network Volume Jobs Data Data 122 exist, Maya will try to create it, even if the user specifies that the data and output files exist in other locations. Our solution was to manually create /Alias/maya and give the user nobody read and write permissions. Such a solution worked well for us because our computers are geographically close on a LAN; if greater distance had separated the agents from the client/controller, specifying a working directory may have been a better solution. The results are sent to the network volume. Rendering speed was not important at this time; our first goal was to implement the grid, and in that we succeeded. 4.3.1 Pseudo Code for Perl Script in Custom Xgrid Plug-in In this section we summarize in simplified pseudo code format the Perl script used in our Xgrig plug-in. agent_jobs{ \u2022 Read beginning frame, end frame, and cluster size of render. \u2022 Check whether the render can be divided into an integer number of jobs based on the cluster size. \u2022 If there are not an integer number of jobs, reduce the cluster size of the last job and set its last frame to the end frame of the render. \u2022 Determine the start frame and end frame for each job. \u2022 Execute the Render command. The second hurdle was learning that we got the best performance by storing the rendering data on a network volume. The last major hurdle was retrieving our results from the agents. We decided this was likely a Maya issue rather than an Xgrid issue, and the solution was to send the results to the network volume via the Perl script. This will be essential for the following step. This will probably be an extension to the web interface of our existing render farm, where the student specifies parameters that are placed in a script that issues the Render command. Part of this should compare the rendering times for Power Mac G4\"s and G5\"s.", "introduction": "Grid computing does not have a single, universally accepted definition. The technology behind grid computing model is not new.s, where scientists harnessed the computing power of idle workstations to let compute intensive applications to run on multiple workstations, which dramatically shortening processing times. Although numerous distributed computing models were available for discipline-specific scientific applications, only recently have the tools became available to use general-purpose applications on a grid. Consequently, the grid computing model is gaining popularity and has become a show piece of \"utility computing\". Since in the IT industry, various computing models are used interchangeably with grid computing, we first sort out the similarities and difference between these computing models so that grid computing can be placed in perspective. 1.1 Clustering A cluster is a group of machines in a fixed configuration united to operate and be managed as a single entity to increase robustness and performance. The cluster appears as a single high-speed system or a single highly available system. In this model, resources can not enter and leave the group as necessary. There are at least two types of clusters: parallel clusters and highavailability clusters. Clustered machines are generally in spatial proximity, such as in the same server room, and dedicated solely to their task. In a high-availability cluster, each machine provides the same service. If one machine fails, another seamlessly takes over its workload. For example, each computer could be a web server for a web site. Should one web server \"die,\" another provides the service, so that the web site rarely, if ever, goes down. A parallel cluster is a type of supercomputer. Problems are split into many parts, and individual cluster members are given part of the problem to solve. An example of a parallel cluster is composed of Apple Power Mac G5 computers at Virginia Tech University . 1.2 Distributed Computing Distributed computing spatially expands network services so that the components providing the services are separated. The major objective of this computing model is to consolidate processing power over a network. A simple example is spreading services such as file and print serving, web serving, and data storage across multiple machines rather than a single machine handling all the tasks. Distributed computing can also be more fine-grained, where even a single application is broken into parts and each part located on different machines: a word processor on one server, a spell checker on a second server, etc. 1.3 Utility Computing Literally, utility computing resembles common utilities such as telephone or electric service. A service provider makes computing resources and infrastructure management available to a customer as needed, and charges for usage rather than a flat rate. The important thing to note is that resources are only used as needed, and not dedicated to a single customer. 1.4 Grid Computing Grid computing contains aspects of clusters, distributed computing, and utility computing. In the most basic sense, grid turns a group of heterogeneous systems into a centrally managed but flexible computing environment that can work on tasks too time intensive for the individual systems. The grid members are not necessarily in proximity, but must merely be accessible over a network; the grid can access computers on a LAN, WAN, or anywhere in the world via the Internet. In addition, the computers comprising the grid need not be dedicated to the grid; rather, they can function as normal workstations, and then advertise their availability to the grid when not in use. The last characteristic is the most fundamental to the grid described in this paper. A well-known example of such an ad hoc grid is the SETI@home project of the University of California at Berkeley, which allows any person in the world with a computer and an Internet connection to donate unused processor time for analyzing radio telescope data. 1.5 Comparing the Grid and Cluster A computer grid expands the capabilities of the cluster by loosing its spatial bounds, so that any computer accessible through the network gains the potential to augment the grid. A fundamental grid feature is that it scales well. The processing power of any machine added to the grid is immediately availably for solving problems. In addition, the machines on the grid can be generalpurpose workstations, which keep down the cost of expanding the grid.", "conclusion": "Recently, the IT industry has witnessed the emergence of numerous types of contemporary grid applications in addition to the traditional grid framework for compute intensive applications.. For instance, peer-to-peer applications such as Kazaa, are based on storage grids that do not share processing power but instead an elegant protocol to swap files between systems.. Although in our campuses we discourage students from utilizing peer-to-peer applications from music sharing, the same protocol can be utilized on applications such as decision support and data mining.. The National Virtual Collaboratory grid project will link earthquake researchers across the U.S. with computing resources, allowing them to share extremely large data sets, research equipment, and work together as virtual teams over the Internet.. There is an assortment of new grid players in the IT world expanding the grid computing model and advancing the grid technology to the next level.. SAP is piloting a project to grid-enable SAP ERP applications, Dell has partnered with Platform Computing to consolidate computing resources and provide grid-enabled systems for compute intensive applications, Oracle has integrated support for grid computing in their 10g release , United Devices offers hosting service for gridon-demand, and Sun Microsystems continues their research and development of Sun\"s N1 Grid engine which combines grid and clustering platforms.. Simply, the grid computing is up and coming.. The potential benefits of grid computing are colossal in higher education learning while the implementation costs are low.. Today, it would be difficult to identify an application with as high a return on investment as grid computing in information technology divisions in higher education institutions.. It is a mistake to overlook this technology with such a high payback.. ACKNOWLEDGMENTS The authors would like to thank Scott Hanselman of the IT team at the Ringling School of Art and Design for providing valuable input in the planning of our Xgrid testing.. We would also like to thank the posters of the Xgrid Mailing List for providing insight into many areas of Xgrid."}
{"id": "J-23", "keywords": ["auction", "frugal", "vertex cover"], "title": "Frugality Ratios And Improved Truthful Mechanisms for Vertex Cover", "abstract": "In set-system auctions, there are several overlapping teams of agents, and a task that can be completed by any of these teams. The auctioneer's goal is to hire a team and pay as little as possible. Examples of this setting include shortest-path auctions and vertex-cover auctions. Recently, Karlin, Kempe and Tamir introduced a new definition of frugality ratio for this problem. Informally, the \"frugality ratio\" is the ratio of the total payment of a mechanism to a desired payment bound. The ratio captures the extent to which the mechanism overpays, relative to perceived fair cost in a truthful auction. In this paper, we propose a new truthful polynomial-time auction for the vertex cover problem and bound its frugality ratio. We show that the solution quality is with a constant factor of optimal and the frugality ratio is within a constant factor of the best possible worst-case bound; this is the first auction for this problem to have these properties. Moreover, we show how to transform any truthful auction into a frugal one while preserving the approximation ratio. Also, we consider two natural modifications of the definition of Karlin et al., and we analyse the properties of the resulting payment bounds, such as monotonicity, computational hardness, and robustness with respect to the draw-resolution rule. We study the relationships between the different payment bounds, both for general set systems and for specific set-system auctions, such as path auctions and vertex-cover auctions. We use these new definitions in the proof of our main result for vertex-cover auctions via a boot-strapping technique, which may be of independent interest.", "references": ["Frugal path mechanisms", "Local ratio: A unified framework for approximation algorithms", "A local-ratio theorem for approximating the weighted vertex cover problem", "Multipart pricing of public goods", "Bounding the payment of approximate truthful mechanisms", "On the expected payment of mechanisms for task allocation", "True costs of cheap labor are hard to measure: edge deletion and VCG payments in graphs", "Frugality ratios and improved truthful mechanisms for vertex cover", "Frugality in path auctions", "A BGP-based mechanism for lowest-cost routing", "Competitive generalized auctions", "Coalitional games on graphs: core structures, substitutes and frugality", "Competitive auctions and digital goods", "Incentives in teams", "First-price path auctions", "Beyond VCG: frugality of truthful mechanisms", "Personal communication", "Cheap labor can be expensive", "Algorithmic mechanism design", "Towards generic low payment mechanisms for decentralized task allocation", "Towards generic low payment mechanisms for decentralized task allocation", "Counterspeculation, auctions, and competitive sealed tenders"], "full_text": "1. INTRODUCTION In a set system auction there is a single buyer and many vendors that can provide various services. It is assumed that the buyer\"s requirements can be satisfied by various subsets of the vendors; these subsets are called the feasible sets. A widely-studied class of setsystem auctions is path auctions, where each vendor is able to sell access to a link in a network, and the feasible sets are those sets whose links contain a path from a given source to a given destination; the study of these auctions has been initiated in the seminal paper by Nisan and Ronen (see also ). We assume that each vendor has a cost of providing his services, but submits a possibly larger bid to the auctioneer. Based on these bids, the auctioneer selects a feasible subset of vendors, and makes payments to the vendors in this subset. Each selected vendor enjoys a profit of payment minus cost. Vendors want to maximise profit, while the buyer wants to minimise the amount he pays. A natural goal in this setting is to design a truthful auction, in which vendors have an incentive to bid their true cost. This can be achieved by paying each selected vendor a premium above her bid in such a way that the vendor has no incentive to overbid. An interesting question in mechanism design is how much the auctioneer will have to overpay in order to ensure truthful bids. In the context of path auctions this topic was first addressed by Archer and Tardos . They define the frugality ratio of a mechanism as the ratio between its total payment and the cost of the cheapest path disjoint from the path selected by the mechanism. They show that, for a large class of truthful mechanisms for this problem, the frugality ratio is as large as the number of edges in the shortest path. Talwar extends this definition of frugality ratio to general set systems, and studies the frugality ratio of the classical VCG mechanism for many specific set systems, such as minimum spanning trees and set covers. While the definition of frugality ratio proposed by is wellmotivated and has been instrumental in studying truthful mechanisms for set systems, it is not completely satisfactory. Consider, for example, the graph of Figure 1 with the costs cAB = cBC = A B Figure 1: The diamond graph 336 cCD = 0, cAC = cBD = 1. This graph is 2-connected and the VCG payment to the winning path ABCD is bounded. However, the graph contains no A-D path that is disjoint from ABCD, and hence the frugality ratio of VCG on this graph remains undefined. At the same time, there is no monopoly, that is, there is no vendor that appears in all feasible sets. In auctions for other types of set systems, the requirement that there exist a feasible solution disjoint from the selected one is even more severe: for example, for vertex-cover auctions (where vendors correspond to the vertices of some underlying graph, and the feasible sets are vertex covers) the requirement means that the graph must be bipartite. To deal with this problem, Karlin et al. suggest a better benchmark, which is defined for any monopoly-free set system. This quantity, which they denote by \u03bd, intuitively corresponds to the value of a cheapest Nash equilibrium. Based on this new definition, the authors construct new mechanisms for the shortest path problem and show that the overpayment of these mechanisms is within a constant factor of optimal. 1.1 Our results Vertex cover auctions We propose a truthful polynomial-time auction for vertex cover that outputs a solution whose cost is within a factor of 2 of optimal, and whose frugality ratio is at most 2\u0394, where \u0394 is the maximum degree of the graph (Theorem 4). We complement this result by proving (Theorem 5) that for any \u0394 and n, there are graphs of maximum degree \u0394 and size \u0398(n) for which any truthful mechanism has frugality ratio at least \u0394/2. This means that the solution quality of our auction is with a factor of 2 of optimal and the frugality ratio is within a factor of 4 of the best possible bound for worst-case inputs. To the best of our knowledge, this is the first auction for this problem that enjoys these properties. Moreover, we show how to transform any truthful mechanism for the vertex-cover problem into a frugal one while preserving the approximation ratio. Frugality ratios Our vertex cover results naturally suggest two modifications of the definition of \u03bd in . These modifications can be made independently of each other, resulting in four different payment bounds TUmax, TUmin, NTUmax, and NTUmin, where NTUmin is equal to the original payment bound \u03bd of in . All four payment bounds arise as Nash equilibria of certain games (see the full version of this paper ); the differences between them can be seen as the price of initiative and the price of cooperation (see Section 3). While our main result about vertex cover auctions (Theorem 4) is with respect to NTUmin = \u03bd, we make use of the new definitions by first comparing the payment of our mechanism to a weaker bound NTUmax, and then bootstrapping from this result to obtain the desired bound. Inspired by this application, we embark on a further study of these payment bounds. Our results here are as follows: 1. We observe (Proposition 1) that the four payment bounds always obey a particular order that is independent of the choice of the set system and the cost vector, namely, TUmin \u2264 NTUmin \u2264 NTUmax \u2264 TUmax. We provide examples (Proposition 5 and Corollaries 1 and 2) showing that for the vertex cover problem any two consecutive bounds can differ by a factor of n \u2212 2, where n is the number of agents. We then show (Theorem 2) that this separation is almost best possible for general set systems by proving that for any set system TUmax/TUmin \u2264 n. In contrast, we demonstrate (Theorem 3) that for path auctions TUmax/TUmin \u2264 2. We provide examples (Propositions 2, 3 and 4) showing that this bound is tight. We see this as an argument for the study of vertexcover auctions, as they appear to be more representative of the general team -selection problem than the widely studied path auctions. 2. We show (Theorem 1) that for any set system, if there is a cost vector for which TUmin and NTUmin differ by a factor of \u03b1, there is another cost vector that separates NTUmin and NTUmax by the same factor and vice versa; the same is true for the pairs (NTUmin, NTUmax) and (NTUmax, TUmax). This symmetry is quite surprising, since, e.g., TUmin and NTUmax are obtained from NTUmin by two very different transformations. This observation suggests that the four payment bounds should be studied in a unified framework; moreover, it leads us to believe that the bootstrapping technique of Theorem 4 may have other applications. 3. We evaluate the payment bounds introduced here with respect to a checklist of desirable features. In particular, we note that the payment bound \u03bd = NTUmin of exhibits some counterintuitive properties, such as nonmonotonicity with respect to adding a new feasible set (Proposition 7), and is NP-hard to compute (Theorem 6), while some of the other payment bounds do not suffer from these problems. This can be seen as an argument in favour of using weaker but efficiently computable bounds NTUmax and TUmax. Related work Vertex-cover auctions have been studied in the past by Talwar and Calinescu . Both of these papers are based on the definition of frugality ratio used in ; as mentioned before, this means that their results only apply to bipartite graphs. Talwar shows that the frugality ratio of VCG is at most \u0394. However, since finding the cheapest vertex cover is an NP-hard problem, the VCG mechanism is computationally infeasible. The first (and, to the best of our knowledge, only) paper to investigate polynomial-time truthful mechanisms for vertex cover is . This paper studies an auction that is based on the greedy allocation algorithm, which has an approximation ratio of log n. While the main focus of is the more general set cover problem, the results of imply a frugality ratio of 2\u03942 for vertex cover. Our results improve on those of as our mechanism is polynomial-time computable, as well as on those of , as our mechanism has a better approximation ratio, and we prove a stronger bound on the frugality ratio; moreover, this bound also applies to the mechanism of . 2. PRELIMINARIES In most of this paper, we discuss auctions for set systems. A set system is a pair (E, F), where E is the ground set, |E| = n, and F is a collection of feasible sets, which are subsets of E. Two particular types of set systems are of interest to us - shortest path systems, in which the ground set consists of all edges of a network, and the feasible sets are paths between two specified vertices s and t, and vertex cover systems, in which the elements of the ground set are the vertices of a graph, and the feasible sets are vertex covers of this graph. In set system auctions, each element e of the ground set is owned by an independent agent and has an associated non-negative cost ce. The goal of the centre is to select (purchase) a feasible set. Each element e in the selected set incurs a cost of ce. The elements that are not selected incur no costs. The auction proceeds as follows: all elements of the ground set make their bids, the centre selects a feasible set based on the bids and makes payments to the agents. Formally, an auction is defined by an allocation rule A : Rn \u2192 F and a payment rule P : Rn Rn . The allocation rule takes as input a vector of bids and decides which of the sets in F should be selected. The payment rule also takes as input a vector of bids and decides how much to pay to each agent. The standard requirements are individual rationality, i.e., the payment to each agent should be at least as high as his incurred cost (0 for agents not in the selected set and ce for agents in the 337 selected set) and incentive compatibility, or truthfulness, i.e., each agent\"s dominant strategy is to bid his true cost. An allocation rule is monotone if an agent cannot increase his chance of getting selected by raising his bid. Formally, for any bid vector b and any e \u2208 E, if e \u2208 A(b) then e \u2208 A(b1, . . . , be, . . . , bn) for any be > be. Given a monotone allocation rule A and a bid vector b, the threshold bid te of an agent e \u2208 A(b) is the highest bid of this agent that still wins the auction, given that the bids of other participants remain the same. Formally, te = sup{be \u2208 R | e \u2208 A(b1, . . . , be, . . . , bn)}. It is well known (see, e.g. ) that any auction that has a monotone allocation rule and pays each agent his threshold bid is truthful; conversely, any truthful auction has a monotone allocation rule. The VCG mechanism is a truthful mechanism that maximises the social welfare and pays 0 to the losing agents. For set system auctions, this simply means picking a cheapest feasible set, paying each agent in the selected set his threshold bid, and paying 0 to all other agents. Note, however, that the VCG mechanism may be difficult to implement, since finding a cheapest feasible set may be intractable. If U is a set of agents, c(U) denotes w\u2208U cw. Similarly, b(U) denotes w\u2208U bw. 3. FRUGALITY RATIOS We start by reproducing the definition of the quantity \u03bd from [16, Definition 4]. Let (E, F) be a set system and let S be a cheapest feasible set with respect to the true costs ce. Then \u03bd(c, S) is the solution to the following optimisation problem. Minimise B = e\u2208S be subject to (1) be \u2265 ce for all e \u2208 E (2) e\u2208S\\T be \u2264 e\u2208T \\S ce for all T \u2208 F (3) for every e \u2208 S, there is a Te \u2208 F such that e \u2208 Te andP e \u2208S\\Te be = e \u2208Te\\S ce The bound \u03bd(c, S) can be seen as an outcome of a two-stage process, where first each agent e \u2208 S makes a bid be stating how much it wants to be paid, and then the centre decides whether to accept these bids. The behaviour of both parties is affected by the following considerations. From the centre\"s point of view, the set S must remain the most attractive choice, i.e., it must be among the cheapest feasible sets under the new costs ce = ce for e \u2208 S, ce = be for e \u2208 S (condition (2)). The reason for that is that if (2) is violated for some set T, the centre would prefer T to S. On the other hand, no agent would agree to a payment that does not cover his costs (condition (1)), and moreover, each agent tries to maximise his profit by bidding as high as possible, i.e., none of the agents can increase his bid without violating condition (2) (condition (3)). The centre wants to minimise the total payout, so \u03bd(c, S) corresponds to the best possible outcome from the centre\"s point of view. This definition captures many important aspects of our intuition about \u2018fair\" payments. However, it can be modified in two ways, both of which are still quite natural, but result in different payment bounds. First, we can consider the worst rather than the best possible outcome for the centre. That is, we can consider the maximum total payment that the agents can extract by jointly selecting their bids subject to (1), (2), and (3). Such a bound corresponds to maximising B subject to (1), (2), and (3) rather than minimising it. If it is the agents who make the original bids (rather than the centre), this kind of bidding behaviour is plausible. On the other hand, in a game in which the centre proposes payments to the agents in S and the agents accept them as long as (1), (2) and (3) are satisfied, we would be likely to observe a total payment of \u03bd(c, S). Hence, the difference between these two definitions can be seen as the price of initiative. Second, the agents may be able to make payments to each other. In this case, if they can extract more money from the centre by agreeing on a vector of bids that violates individual rationality (i.e., condition (1)) for some bidders, they might be willing to do so, as the agents who are paid below their costs will be compensated by other members of the group. The bids must still be realistic, i.e., they have to satisfy be \u2265 0. The resulting change in payments can be seen as the price of co-operation and corresponds to replacing condition (1) with the following weaker condition (1\u2217 ): be \u2265 0 for all e \u2208 E. (1\u2217 By considering all possible combinations of these modifications, we obtain four different payment bounds, namely \u2022 TUmin(c, S), which is the solution to the optimisation problem Minimise B subject to (1\u2217 ), (2), and (3). \u2022 TUmax(c, S), which is the solution to the optimisation problem Maximise B subject to (1\u2217 ), (2), and (3). \u2022 NTUmin(c, S), which is the solution to the optimisation problem Minimise B subject to (1), (2), and (3). \u2022 NTUmax(c, S), which is the solution to the optimisation problem Maximise B subject to (1), (2), (3). The abbreviations TU and NTU correspond, respectively, to transferable utility and non-transferable utility, i.e., the agents\" ability/inability to make payments to each other. For concreteness, we will take TUmin(c) to be TUmin(c, S) where S is the lexicographically least amongst the cheapest feasible sets. We define TUmax(c), NTUmin(c), NTUmax(c) and \u03bd(c) similarly, though we will see in Section 6.3 that, in fact, NTUmin(c, S) and NTUmax(c, S) are independent of the choice of S. Note that the quantity \u03bd(c) from is NTUmin(c). The second modification (transferable utility) is more intuitively appealing in the context of the maximisation problem, as both assume some degree of co-operation between the agents. While the second modification can be made without the first, the resulting payment bound TUmin(c, S) is too strong to be a realistic benchmark, at least for general set systems. In particular, it can be smaller than the total cost of the cheapest feasible set S (see Section 6). Nevertheless, we provide the definition as well as some results about TUmin(c, S) in the paper, both for completeness and because we believe that it may help to understand which properties of the payment bounds are important for our proofs. Another possibility would be to introduce an additional constraint e\u2208S be \u2265P e\u2208S ce in the definition of TUmin(c, S) (note that this condition holds automatically for TUmax(c, S), as TUmax(c, S) \u2265 NTUmax(c, S)); however, such a definition would have no direct game-theoretic interpretation, and some of our results (in particular, the ones in Section 4) would no longer be true. REMARK 1. For the payment bounds that are derived from maximisation problems, (i.e., TUmax(c, S) and NTUmax(c, S)), constraints of type (3) are redundant and can be dropped. Hence, TUmax(c, S) and NTUmax(c, S) are solutions to linear programs, and therefore can be computed in polynomial time as long as we have a separation oracle for constraints in (2). In contrast, 338 NTUmin(c, S) can be NP-hard to compute even if the size of F is polynomial (see Section 6). The first and third inequalities in the following observation follow from the fact that condition (1\u2217 ) is strictly weaker than condition (1). PROPOSITION 1. TUmin(c, S) \u2264 NTUmin(c, S) \u2264 NTUmax(c, S) \u2264 TUmax(c, S). Let M be a truthful mechanism for (E, F). Let pM(c) denote the total payments of M when the actual costs are c. A frugality ratio of M with respect to a payment bound is the ratio between the payment of M and this payment bound. In particular, \u03c6TUmin(M) = sup pM(c)/TUmin(c), \u03c6TUmax(M) = sup pM(c)/TUmax(c), \u03c6NTUmin(M) = sup pM(c)/NTUmin(c), \u03c6NTUmax(M) = sup pM(c)/NTUmax(c). We conclude this section by showing that there exist set systems and respective cost vectors for which all four payment bounds are different. In the next section, we quantify this difference, both for general set systems, and for specific types of set systems, such as path auctions or vertex cover auctions. EXAMPLE 1. Consider the shortest-path auction on the graph of Figure 1. The cheapest feasible sets are all paths from A to D. It can be verified, using the reasoning of Propositions 2 and 3 below, that for the cost vector cAB = cCD = 2, cBC = 1, cAC = cBD = 5, we have \u2022 TUmax(c) = 10 (with bAB = bCD = 5, bBC = 0), \u2022 NTUmax(c) = 9 (with bAB = bCD = 4, bBC = 1), \u2022 NTUmin(c) = 7 (with bAB = bCD = 2, bBC = 3), \u2022 TUmin(c) = 5 (with bAB = bCD = 0, bBC = 5). 4. COMPARING PAYMENT BOUNDS 4.1 Path auctions We start by showing that for path auctions any two consecutive payment bounds can differ by at least a factor of 2. PROPOSITION 2. There is an instance of the shortest-path problem for which we have NTUmax(c)/NTUmin(c) \u2265 2. PROOF. This construction is due to David Kempe . Consider the graph of Figure 1 with the edge costs cAB = cBC = cCD = 0, cAC = cBD = 1. Under these costs, ABCD is the cheapest path. The inequalities in (2) are bAB + bBC \u2264 cAC = 1, bBC + bCD \u2264 cBD = 1. By condition (3), both of these inequalities must be tight (the former one is the only inequality involving bAB, and the latter one is the only inequality involving bCD). The inequalities in (1) are bAB \u2265 0, bBC \u2265 0, bCD \u2265 0. Now, if the goal is to maximise bAB + bBC + bCD, the best choice is bAB = bCD = 1, bBC = 0, so NTUmax(c) = 2. On the other hand, if the goal is to minimise bAB + bBC + bCD, one should set bAB = bCD = 0, bBC = 1, so NTUmin(c) = 1. PROPOSITION 3. There is an instance of the shortest-path problem for which we have TUmax(c)/NTUmax(c) \u2265 2. PROOF. Again, consider the graph of Figure 1. Let the edge costs be cAB = cCD = 0, cBC = 1, cAC = cBD = 1. ABCD is the lexicographically-least cheapest path, so we can assume that S = {AB, BC, CD}. The inequalities in (2) are the same as in the previous example, and by the same argument both of them are, in fact, equalities. The inequalities in (1) are bAB \u2265 0, bBC \u2265 1, bCD \u2265 0. Our goal is to maximise bAB + bBC + bCD. If we have to respect the inequalities in (1), we have to set bAB = bCD = 0, bBC = 1, so NTUmax(c) = 1. Otherwise, we can set bAB = bCD = 1, bBC = 0, so TUmax(c) \u2265 2. PROPOSITION 4. There is an instance of the shortest-path problem for which we have NTUmin(c)/TUmin(c) \u2265 2. PROOF. This construction is also based on the graph of Figure 1. The edge costs are cAB = cCD = 1, cBC = 0, cAC = cBD = 1. ABCD is the lexicographically least cheapest path, so we can assume that S = {AB, BC, CD}. Again, the inequalities in (2) are the same, and both are, in fact, equalities. The inequalities in (1) are bAB \u2265 1, bBC \u2265 0, bCD \u2265 1. Our goal is to minimise bAB + bBC +bCD. If we have to respect the inequalities in (1), we have to set bAB = bCD = 1, bBC = 0, so NTUmin(c) = 2. Otherwise, we can set bAB = bCD = 0, bBC = 1, so TUmin(c) \u2264 1. In Section 4.4 (Theorem 3), we show that the separation results in Propositions 2, 3, and 4 are optimal. 4.2 Connections between separation results The separation results for path auctions are obtained on the same graph using very similar cost vectors. It turns out that this is not coincidental. Namely, we can prove the following theorem. THEOREM 1. For any set system (E, F), and any feasible set S, max TUmax(c, S) NTUmax(c, S) = max NTUmax(c, S) NTUmin(c, S) max NTUmax(c, S) NTUmin(c, S) = max NTUmin(c, S) TUmin(c, S) where the maximum is over all cost vectors c for which S is a cheapest feasible set. The proof of the theorem follows directly from the four lemmas proved below; more precisely, the first equality in Theorem 1 is obtained by combining Lemmas 1 and 2, and the second equality is obtained by combining Lemmas 3 and 4. We prove Lemma 1 here; the proofs of Lemmas 2- 4 are similar and can be found in the full version of this paper . LEMMA 1. Suppose that c is a cost vector for (E, F) such that S is a cheapest feasible set and TUmax(c, S)/NTUmax(c, S) = \u03b1. Then there is a cost vector c such that S is a cheapest feasible set and NTUmax(c , S)/NTUmin(c , S) \u2265 \u03b1. PROOF. Suppose that TUmax(c, S) = X and NTUmax(c, S) = Y where X/Y = \u03b1. Assume without loss of generality that S consists of elements 1, . . . , k, and let b1 = (b1 1, . . . , b1 k) and b2 (b2 1, . . . , b2 k) be the bid vectors that correspond to TUmax(c, S) and NTUmax(c, S), respectively. Construct the cost vector c by setting ci = ci for i \u2208 S, ci = min{ci, b1 i } for i \u2208 S. Clearly, S is a cheapest set under c . Moreover, as the costs of elements outside of S remained the same, the right-hand sides of all constraints in (2) did not change, so any bid vector that satisfies (2) and (3) with respect to c, also satisfies them with respect to c . We will construct two bid vectors b3 and b4 that satisfy conditions (1), (2), and (3) for the cost vector c , and 339 X X X X 0 12 X 4 5 Figure 2: Graph that separates payment bounds for vertex cover, n = 7 have i\u2208S b3 i = X, i\u2208S b4 i = Y . As NTUmax(c , S) \u2265 X and NTUmin(c , S) \u2264 Y , this implies the lemma. We can set b3 i = b1 i : this bid vector satisfies conditions (2) and (3) since b1 does, and we have b1 i \u2265 min{ci, b1 i } = ci, which means that b3 satisfies condition (1). Furthermore, we can set b4 i = b2 i . Again, b4 satisfies conditions (2) and (3) since b2 does, and since b2 satisfies condition (1), we have b2 i \u2265 ci \u2265 ci, which means that b4 satisfies condition (1). LEMMA 2. Suppose c is a cost vector for (E, F) such that S is a cheapest feasible set and NTUmax(c, S)/NTUmin(c, S) = \u03b1. Then there is a cost vector c such that S is a cheapest feasible set and TUmax(c , S)/NTUmax(c , S) \u2265 \u03b1. LEMMA 3. Suppose that c is a cost vector for (E, F) such that S is a cheapest feasible set and NTUmax(c, S)/NTUmin(c, S) = \u03b1. Then there is a cost vector c such that S is a cheapest feasible set and NTUmin(c , S)/TUmin(c , S) \u2265 \u03b1. LEMMA 4. Suppose that c is a cost vector for (E, F) such that S is a cheapest feasible set and NTUmin(c, S)/TUmin(c, S) = \u03b1. Then there is a cost vector c such that S is a cheapest feasible set and NTUmax(c , S)/NTUmin(c , S) \u2265 \u03b1. 4.3 Vertex-cover auctions In contrast to the case of path auctions, for vertex-cover auctions the gap between NTUmin(c) and NTUmax(c) (and hence between NTUmax(c) and TUmax(c), and between TUmin(c) and NTUmin(c)) can be proportional to the size of the graph. PROPOSITION 5. For any n \u2265 3, there is a an n-vertex graph and a cost vector c for which TUmax(c)/NTUmax(c) \u2265 n \u2212 2. PROOF. The underlying graph consists of an (n \u2212 1)-clique on the vertices X1, . . . , Xn\u22121, and an extra vertex X0 adjacent to Xn\u22121. The costs are cX1 = cX2 = \u00b7 \u00b7 \u00b7 = cXn\u22122 = 0, cX0 = cXn\u22121 = 1. We can assume that S = {X0, X1, . . . , Xn\u22122} (this is the lexicographically first vertex cover of cost 1). For this set system, the constraints in (2) are bXi + bX0 \u2264 cXn\u22121 = 1 for i = 1, . . . , n \u2212 2. Clearly, we can satisfy conditions (2) and (3) by setting bXi = 1 for i = 1, . . . , n \u2212 2, bX0 = 0. Hence, TUmax(c) \u2265 n \u2212 2. For NTUmax(c), there is an additional constraint bX0 \u2265 1, so the best we can do is to set bXi = 0 for i = 1, . . . , n \u2212 2, bX0 = 1, which implies NTUmax(c) = 1. Combining Proposition 5 with Lemmas 1 and 3, we derive the following corollaries. COROLLARY 1. For any n \u2265 3, we can construct an instance of the vertex cover problem on a graph of size n that satisfies NTUmax(c)/NTUmin(c) \u2265 n \u2212 2. COROLLARY 2. For any n \u2265 3, we can construct an instance of the vertex cover problem on a graph of size n that satisfies NTUmin(c)/TUmin(c) \u2265 n \u2212 2. j+2ix ij P \\ P ij+2P \\ P yijixix j j+1 i j+2ij+1 y y i i j+2ie j e j+1 ij+1 P \\ P Figure 3: Proof of Theorem 3: constraints for \u02c6Pij and \u02c6Pij+2 do not overlap 4.4 Upper bounds It turns out that the lower bound proved in the previous subsection is almost tight. More precisely, the following theorem shows that no two payment bounds can differ by more than a factor of n; moreover, this is the case not just for the vertex cover problem, but for general set systems. We bound the gap between TUmax(c) and TUmin(c). Since TUmin(c) \u2264 NTUmin(c) \u2264 NTUmax(c) \u2264 TUmax(c), this bound applies to any pair of payment bounds. THEOREM 2. For any set system (E, F) and any cost vector c, we have TUmax(c)/TUmin(c) \u2264 n. PROOF. Assume wlog that the winning set S consists of elements 1, . . . , k. Let c1, . . . , ck be the true costs of elements in S, let b1, . . . , bk be their bids that correspond to TUmin(c), and let b1 , . . . , bk be their bids that correspond to TUmax(c). Consider the conditions (2) and (3) for S. One can pick a subset L of at most k inequalities in (2) so that for each i = 1, . . . , k there is at least one inequality in L that is tight for bi. Suppose that the jth inequality in L is of the form bi1 + \u00b7 \u00b7 \u00b7 + bit \u2264 c(Tj \\ S). For bi, all inequalities in L are, in fact, equalities. Hence, by adding up all of them we obtain k i=1,...,k bi \u2265 j=1,...,k c(Tj \\ S). On the other hand, all these inequalities appear in condition (2), so they must hold for bi , i.e., i=1,...,k bi \u2264 j=1,...,k c(Tj \\ S). Combining these two inequalities, we obtain nTUmin(c) \u2265 kTUmin(c) \u2265 TUmax(c). REMARK 2. The final line of the proof of Theorem 2 shows that, in fact, the upper bound on TUmax(c)/TUmin(c) can be strengthened to the size of the winning set, k. Note that in Proposition 5, as well as in Corollaries 1 and 2, k = n\u22121, so these results do not contradict each other. For path auctions, this upper bound can be improved to 2, matching the lower bounds of Section 4.1. THEOREM 3. For any instance of the shortest path problem, TUmax(c) \u2264 2 TUmin(c). PROOF. Given a network (G, s, t), assume without loss of generality that the lexicographically-least cheapest s-t path, P, in G is {e1, . . . , ek}, where e1 = (s, v1), e2 = (v1, v2), . . . , ek = (vk\u22121, t). Let c1, . . . , ck be the true costs of e1, . . . , ek, and let b = (b1, . . . , bk) and b = (b1 , . . . , bk ) be bid vectors that correspond to TUmin(c) and TUmax(c), respectively. For any i = 1, . . . , k, there is a constraint in (2) that is tight for bi with respect to the bid vector b , i.e., an s-t path Pi that avoids ei and satisfies b (P \\Pi) = c(Pi \\P). We can assume without loss of generality that Pi coincides with P up to some vertex xi, then deviates from P to avoid ei, and finally returns to P at a vertex 340 yi and coincides with P from then on (clearly, it might happen that s = xi or t = yi). Indeed, if Pi deviates from P more than once, one of these deviations is not necessary to avoid ei and can be replaced with the respective segment of P without increasing the cost of Pi. Among all paths of this form, let \u02c6Pi be the one with the largest value of yi, i.e., the rightmost one. This path corresponds to an inequality Ii of the form bxi+1 + \u00b7 \u00b7 \u00b7 + byi \u2264 c( \u02c6Pi \\ P). As in the proof of Theorem 2, we construct a set of tight constraints L such that every variable bi appears in at least one of these constraints; however, now we have to be more careful about the choice of constraints in L. We construct L inductively as follows. Start by setting L = {I1}. At the jth step, suppose that all variables up to (but not including) bij appear in at least one inequality in L. Add Iij to L. Note that for any j we have yij+1 > yij . This is because the inequalities added to L during the first j steps did not cover bij+1 See Figure 3. Since yij+2 > yij+1 , we must also have xij+2 > yij : otherwise, \u02c6Pij+1 would not be the rightmost constraint for bij+1 . Therefore, the variables in Iij+2 and Iij do not overlap, and hence no bi can appear in more than two inequalities in L. Now we follow the argument of the proof of Theorem 2 to finish. By adding up all of the (tight) inequalities in L for bi we obtain i=1,...,k bi \u2265 j=1,...,k c( \u02c6Pj \\ P). On the other hand, all these inequalities appear in condition (2), so they must hold for bi , i.e., i=1,...,k bi \u2264 j=1,...,k c( \u02c6Pj \\ P), so TUmax(c) \u2264 2TUmin(c). 5. TRUTHFUL MECHANISMS FOR VERTEX COVER Recall that for a vertex-cover auction on a graph G = (V, E), an allocation rule is an algorithm that takes as input a bid bv for each vertex and returns a vertex cover \u02c6S of G. As explained in Section 2, we can combine a monotone allocation rule with threshold payments to obtain a truthful auction. Two natural examples of monotone allocation rules are Aopt, i.e., the algorithm that finds an optimal vertex cover, and the greedy algorithm AGR. However, Aopt cannot be guaranteed to run in polynomial time unless P = NP and AGR has approximation ratio of log n. Another approximation algorithm for vertex cover, which has approximation ratio 2, is the local ratio algorithm ALR . This algorithm considers the edges of G one by one. Given an edge e = (u, v), it computes = min{bu, bv} and sets bu = bu \u2212 , bv = bv \u2212 . After all edges have been processed, ALR returns the set of vertices {v | bv = 0}. It is not hard to check that if the order in which the edges are considered is independent of the bids, then this algorithm is monotone as well. Hence, we can use it to construct a truthful auction that is guaranteed to select a vertex cover whose cost is within a factor of 2 from the optimal. However, while the quality of the solution produced by ALR is much better than that of AGR, we still need to show that its total payment is not too high. In the next subsection, we bound the frugality ratio of ALR (and, more generally, all algorithms that satisfy the condition of local optimality, defined later) by 2\u0394, where \u0394 is the maximum degree of G. We then prove a matching lower bound showing that for some graphs the frugality ratio of any truthful auction is at least \u0394/2. 5.1 Upper bound We say that an allocation rule is locally optimal if whenever bv >P w\u223cv bw, the vertex v is not chosen. Note that for any such rule the threshold bid of v satisfies tv \u2264 w\u223cv bw. CLAIM 1. The algorithms Aopt, AGR, and ALR are locally optimal. THEOREM 4. Any vertex cover auction M that has a locally optimal and monotone allocation rule and pays each agent his threshold bid has frugality ratio \u03c6NTUmin(M) \u2264 2\u0394. To prove Theorem 4, we first show that the total payment of any locally optimal mechanism does not exceed \u0394c(V ). We then demonstrate that NTUmin(c) \u2265 c(V )/2. By combining these two results, the theorem follows. LEMMA 5. Consider a graph G = (V, E) with maximum degree \u0394. Let M be a vertex-cover auction on G that satisfies the conditions of Theorem 4. Then for any cost vector c, the total payment of M satisfies pM(c) \u2264 \u0394c(V ). PROOF. First note that any such auction is truthful, so we can assume that each agent\"s bid is equal to his cost. Let \u02c6S be the vertex cover selected by M. Then by local optimality pM(c) = v\u2208 \u02c6S tv \u2264 v\u2208 \u02c6S w\u223cv cw \u2264 w\u2208V \u0394cw = \u0394c(V ). We now derive a lower bound on TUmax(c); while not essential for the proof of Theorem 4, it helps us build the intuition necessary for that proof. LEMMA 6. For a vertex cover instance G = (V, E) in which S is a minimum vertex cover, TUmax(c, S) \u2265 c(V \\ S) PROOF. For a vertex w with at least one neighbour in S, let d(w) denote the number of neighbours that w has in S. Consider the bid vector b in which, for each v \u2208 S, bv = w\u223cv,w\u2208S cw d(w) Then v\u2208S bv = v\u2208S w\u223cv,w\u2208S cw/d(w) = w /\u2208S cw = c(V \\ S). To finish we want to show that b is feasible in the sense that it satisfies (2). Consider a vertex cover T, and extend the bid vector b by assigning bv = cv for v /\u2208 S. Then b(T) = c(T \\S)+b(S\u2229T) \u2265 c(T \\S)+ v\u2208S\u2229T w\u2208S\u2229T :w\u223cv cw d(w) and since all edges between S \u2229 T and S go to S \u2229 T, the righthand-side is equal to c(T \\S)+ w\u2208S\u2229T cw = c(T \\S)+c(S \u2229T) = c(V \\S) = b(S). Next, we prove a lower bound on NTUmax(c, S); we will then use it to obtain a lower bound on NTUmin(c). LEMMA 7. For a vertex cover instance G = (V, E) in which S is a minimum vertex cover, NTUmax(c, S) \u2265 c(V \\ S) PROOF. If c(S) \u2265 c(V \\ S), by condition (1) we are done. Therefore, for the rest of the proof we assume that c(S) < c(V \\ S). We show how to construct a bid vector (be)e\u2208S that satisfies conditions (1) and (2) such that b(S) \u2265 c(V \\ S); clearly, this implies NTUmax(c, S) \u2265 c(V \\ S). Recall that a network flow problem is described by a directed graph \u0393 = (V\u0393, E\u0393), a source node s \u2208 V\u0393, a sink node t \u2208 V\u0393, and a vector of capacity constraints ae, e \u2208 E\u0393. Consider a network (V\u0393, E\u0393) such that V\u0393 = V \u222a{s, t}, E\u0393 = E1 \u222aE2 \u222aE3, where E1 = {(s, v) | v \u2208 S}, E2 = {(v, w) | v \u2208 S, w \u2208 341 V \\ S, (v, w) \u2208 E}, E3 = {(w, t) | w \u2208 V \\ S}. Since S is a vertex cover for G, no edge of E can have both of its endpoints in V \\ S, and by construction, E2 contains no edges with both endpoints in S. Therefore, the graph (V, E2) is bipartite with parts (S, V \\ S). Set the capacity constraints for e \u2208 E\u0393 as follows: a(s,v) = cv, a(w,t) = cw, a(v,w) = +\u221e for all v \u2208 S, w \u2208 V \\ S. Recall that a cut is a partition of the vertices in V\u0393 into two sets C1 and C2 so that s \u2208 C1, t \u2208 C2; we denote such a cut by C = (C1, C2). Abusing notation, we write e = (u, v) \u2208 C if u \u2208 C1, v \u2208 C2 or u \u2208 C2, v \u2208 C1, and say that such an edge e = (u, v) crosses the cut C. The capacity of a cut C is computed as cap(C) = (v,w)\u2208C a(v,w). We have cap(s, V \u222a{t}) = c(S), cap({s} \u222a V, t) = c(V \\ S). Let Cmin = ({s} \u222a S \u222a W , {t} \u222a S \u222a W ) be a minimum cut in \u0393, where S , S \u2286 S, W , W \u2286 V \\ S. See Figure 4. As cap(Cmin) \u2264 cap(s, V \u222a {t}) = c(S) < +\u221e, and any edge in E2 has infinite capacity, no edge (u, v) \u2208 E2 crosses Cmin. Consider the network \u0393 = (V\u0393 , E\u0393 ), where V\u0393 = {s} \u222a S \u222a W \u222a {t}, E\u0393 = {(u, v) \u2208 E\u0393 | u, v \u2208 V\u0393 }. Clearly, C = ({s} \u222a S \u222a W , {t}) is a minimum cut in \u0393 (otherwise, there would exist a smaller cut for \u0393). As cap(C ) = c(W ), we have c(S ) \u2265 c(W ). Now, consider the network \u0393 = (V\u0393 , E\u0393 ), where V\u0393 = {s} \u222a S \u222a W \u222a {t}, E\u0393 = {(u, v) \u2208 E\u0393 | u, v \u2208 V\u0393 }. Similarly, C = ({s}, S \u222a W \u222a {t}) is a minimum cut in \u0393 , cap(C ) = c(S ). As the size of a maximum flow from s to t is equal to the capacity of a minimum cut separating s and t, there exists a flow F = (fe)e\u2208E\u0393 of size c(S ). This flow has to saturate all edges between s and S , i.e., f(s,v) = cv for all v \u2208 S . Now, increase the capacities of all edges between s and S to +\u221e. In the modified network, the capacity of a minimum cut (and hence the size of a maximum flow) is c(W ), and a maximum flow F = (fe)e\u2208E\u0393 can be constructed by greedily augmenting F. Set bv = cv for all v \u2208 S , bv = f(s,v) for all v \u2208 S . As F is constructed by augmenting F, we have bv \u2265 cv for all v \u2208 S, i.e., condition (1) is satisfied. Now, let us check that no vertex cover T \u2286 V can violate condition (2). Set T1 = T \u2229 S , T2 = T \u2229 S , T3 = T \u2229 W , T4 = T \u2229 W ; our goal is to show that b(S \\ T1) + b(S \\ T2) \u2264 c(T3)+c(T4). Consider all edges (u, v) \u2208 E such that u \u2208 S \\T1. If (u, v) \u2208 E2 then v \u2208 T3 (no edge in E2 can cross the cut), and if u, v \u2208 S then v \u2208 T1\u222aT2. Hence, T1\u222aT3\u222aS is a vertex cover for G, and therefore c(T1)+ c(T3)+ c(S ) \u2265 c(S) = c(T1)+ c(S \\ T1) + c(S ). Consequently, c(T3) \u2265 c(S \\ T1) = b(S \\ T1). Now, consider the vertices in S \\T2. Any edge in E2 that starts in one of these vertices has to end in T4 (this edge has to be covered by T, and it cannot go across the cut). Therefore, the total flow out of S \\T2 is at most the total flow out of T4, i.e., b(S \\T2) \u2264 c(T4). Hence, b(S \\ T1) + b(S \\ T2) \u2264 c(T3) + c(T4). Finally, we derive a lower bound on the payment bound that is of interest to us, namely, NTUmin(c). LEMMA 8. For a vertex cover instance G = (V, E) in which S is a minimum vertex cover, NTUmin(c, S) \u2265 c(V \\ S) PROOF. Suppose for contradiction that c is a cost vector with minimum-cost vertex cover S and NTUmin(c, S) < c(V \\S). Let b be the corresponding bid vector and let c be a new cost vector with cv = bv for v \u2208 S and cv = cv for v \u2208 S. Condition (2) guarantees that S is an optimal solution to the cost vector c . Now compute a bid vector b corresponding to NTUmax(c , S). We S\" W\"\" S\"\" W\" Figure 4: Proof of Lemma 7. Dashed lines correspond to edges in E \\ E2 claim that bv = cv for any v \u2208 S. Indeed, suppose that bv > cv for some v \u2208 S (bv = cv for v \u2208 S by construction). As b satisfies conditions (1)-(3), among the inequalities in (2) there is one that is tight for v and the bid vector b. That is, b(S \\ T) = c(T \\ S). By the construction of c , c (S \\ T) = c (T \\ S). Now since bw \u2265 cw for all w \u2208 S, bv > cv implies b (S \\T) > c (S \\T) = c (T \\S). But this violates (2). So we now know b = c . Hence, we have NTUmax(c , S) = v\u2208S bv = NTUmin(c, S) < c(V \\ S), giving a contradiction to the fact that NTUmax(c , S) \u2265 c (V \\S) which we proved in Lemma 7. As NTUmin(c, S) satisfies condition (1), it follows that we have NTUmin(c, S) \u2265 c(S). Together will Lemma 8, this implies NTUmin(c, S) \u2265 max{c(V \\ S), c(S)} \u2265 c(V )/2. Combined with Lemma 5, this completes the proof of Theorem 4. REMARK 3. As NTUmin(c) \u2264 NTUmax(c) \u2264 TUmax(c), our bound of 2\u0394 extends to the smaller frugality ratios that we consider, i.e., \u03c6NTUmax(M) and \u03c6TUmax(M). It is not clear whether it extends to the larger frugality ratio \u03c6TUmin(M). However, the frugality ratio \u03c6TUmin(M) is not realistic because the payment bound TUmin(c) is inappropriately low - we show in Section 6 that TUmin(c) can be significantly smaller than the total cost of a cheapest vertex cover. Extensions We can also apply our results to monotone vertex-cover algorithms that do not necessarily output locally-optimal solutions. To do so, we simply take the vertex cover produced by any such algorithm and transform it into a locally-optimal one, considering the vertices in lexicographic order and replacing a vertex v with its neighbours whenever bv > u\u223cv bu. Note that if a vertex u has been added to the vertex cover during this process, it means that it has a neighbour whose bid is higher than bu, so after one pass all vertices in the vertex cover satisfy bv \u2264 u\u223cv bu. This procedure is monotone in bids, and it can only decrease the cost of the vertex cover. Therefore, using it on top of a monotone allocation rule with approx342 imation ratio \u03b1, we obtain a monotone locally-optimal allocation rule with approximation ratio \u03b1. Combining it with threshold payments, we get an auction with \u03c6NTUmin \u2264 2\u0394. Since any truthful auction has a monotone allocation rule, this procedure transforms any truthful mechanism for the vertex-cover problem into a frugal one while preserving the approximation ratio. 5.2 Lower bound In this subsection, we prove that the upper bound of Theorem 4 is essentially optimal. Our proof uses the techniques of , where the authors prove a similar result for shortest-path auctions. THEOREM 5. For any \u0394 > 0 and any n, there exist a graph G of maximum degree \u0394 and size N > n such that for any truthful mechanism M on G we have \u03c6NTUmin(M) \u2265 \u0394/2. PROOF. Given n and \u0394, set k = n/2\u0394 . Let G be the graph that consists of k blocks B1, . . . , Bk of size 2\u0394 each, where each Bi is a complete bipartite graph with parts Li and Ri, |Li| = |Ri| = \u0394. We will consider two families of cost vectors for G. Under a cost vector x \u2208 X, each block Bi has one vertex of cost 1; all other vertices cost 0. Under a cost vector y \u2208 Y , there is one block that has two vertices of cost 1, one in each part, all other blocks have one vertex of cost 1, and all other vertices cost 0. Clearly, |X| = (2\u0394)k , |Y | = k(2\u0394)k\u22121 \u03942 . We will now construct a bipartite graph W with the vertex set X \u222a Y as follows. Consider a cost vector y \u2208 Y that has two vertices of cost 1 in Bi; let these vertices be vl \u2208 Li and vr \u2208 Ri. By changing the cost of either of these vertices to 0, we obtain a cost vector in X. Let xl and xr be the cost vectors obtained by changing the cost of vl and vr, respectively. The vertex cover chosen by M(y) must either contain all vertices in Li or it must contain all vertices in Ri. In the former case, we put in W an edge from y to xl and in the latter case we put in W an edge from y to xr (if the vertex cover includes all of Bi, W contains both of these edges). The graph W has at least k(2\u0394)k\u22121 \u03942 edges, so there must exist an x \u2208 X of degree at least k\u0394/2. Let y1, . . . , yk\u0394/2 be the other endpoints of the edges incident to x, and for each i = 1, . . . , k\u0394/2, let vi be the vertex whose cost is different under x and yi; note that all vi are distinct. It is not hard to see that NTUmin(x) \u2264 k: the cheapest vertex cover contains the all-0 part of each block, and we can satisfy conditions (1)-(3) by letting one of the vertices in the all-0 part of each block to bid 1, while all other the vertices in the cheapest set bid 0. On the other hand, by monotonicity of M we have vi \u2208 M(x) for i = 1, . . . , k\u0394/2 (vi is in the winning set under yi, and x is obtained from yi by decreasing the cost of vi), and moreover, the threshold bid of each vi is at least 1, so the total payment of M on x is at least k\u0394/2. Hence, \u03c6NTUmin(M) \u2265 M(x)/NTUmin(x) \u2265 \u0394/2. REMARK 4. The lower bound of Theorem 5 can be generalised to randomised mechanisms, where a randomised mechanism is considered to be truthful if it can be represented as a probability distribution over truthful mechanisms. In this case, instead of choosing the vertex x \u2208 X with the highest degree, we put both (y, xl) and (y, xr) into W , label each edge with the probability that the respective part of the block is chosen, and pick x \u2208 X with the highest weighted degree. The argument can be further extended to a more permissive definition of truthfulness for randomised mechanisms, but this discussion is beyond the scope of this paper. 6. PROPERTIES OF PAYMENT BOUNDS In this section we consider several desirable properties of payment bounds and evaluate the four payment bounds proposed in this paper with respect to them. The particular properties that we are interested in are independence of the choice of S (Section 6.3), monotonicity (Section 6.4.1), computational hardness (Section 6.4.2), and the relationship with other reasonable bounds, such as the total cost of the cheapest set (Section 6.1), or the total VCG payment (Section 6.2). 6.1 Comparison with total cost Our first requirement is that a payment bound should not be less than the total cost of the selected set. Payment bounds are used to evaluate the performance of set-system auctions. The latter have to satisfy individual rationality, i.e., the payment to each agent must be at least as large as his incurred costs; it is only reasonable to require the payment bound to satisfy the same requirement. Clearly, NTUmax(c) and NTUmin(c) satisfy this requirement due to condition (1), and so does TUmax(c), since TUmax(c) \u2265 NTUmax(c). However, TUmin(c) fails this test. The example of Proposition 4 shows that for path auctions, TUmin(c) can be smaller than the total cost by a factor of 2. Moreover, there are set systems and cost vectors for which TUmin(c) is smaller than the cost of the cheapest set S by a factor of \u03a9(n). Consider, for example, the vertex-cover auction for the graph of Proposition 5 with the costs cX1 = \u00b7 \u00b7 \u00b7 = cXn\u22122 = cXn\u22121 = 1, cX0 = 0. The cost of a cheapest vertex cover is n \u2212 2, and the lexicographically first vertex cover of cost n\u22122 is {X0, X1, . . . , Xn\u22122}. The constraints in (2) are bXi + bX0 \u2264 cXn\u22121 = 1. Clearly, we can satisfy conditions (2) and (3) by setting bX1 = \u00b7 \u00b7 \u00b7 = bXn\u22122 = 0, bX0 = 1, which means that TUmin(c) \u2264 1. This observation suggests that the payment bound TUmin(c) is too strong to be realistic, since it can be substantially lower than the cost of the cheapest feasible set. Nevertheless, some of the positive results that were proved in for NTUmin(c) go through for TUmin(c) as well. In particular, one can show that if the feasible sets are the bases of a monopolyfree matroid, then \u03c6TUmin(VCG) = 1. To show that \u03c6TUmin(VCG) is at most 1, one must prove that the VCG payment is at most TUmin(c). This is shown for NTUmin(c) in the first paragraph of the proof of Theorem 5 in . Their argument does not use condition (1) at all, so it also applies to TUmin(c). On the other hand, \u03c6TUmin(VCG) \u2265 1 since \u03c6TUmin(VCG) \u2265 \u03c6NTUmin(VCG) and \u03c6NTUmin(VCG) \u2265 1 by Proposition 7 of (and also by Proposition 6 below). 6.2 Comparison with VCG payments Another measure of suitability for payment bounds is that they should not result in frugality ratios that are less then 1 for wellknown truthful mechanisms. If this is indeed the case, the payment bound may be too weak, as it becomes too easy to design mechanisms that perform well with respect to it. It particular, a reasonable requirement is that a payment bound should not exceed the total payment of the classical VCG mechanism. The following proposition shows that NTUmax(c), and therefore also NTUmin(c) and TUmin(c), do not exceed the VCG payment pVCG(c). The proof essentially follows the argument of Proposition 7 of and can be found in the full version of this paper . PROPOSITION 6. \u03c6NTUmax(VCG) \u2265 1. Proposition 6 shows that none of the payment bounds TUmin(c), NTUmin(c) and NTUmax(c) exceeds the payment of VCG. However, the payment bound TUmax(c) can be larger that the total 343 VCG payment. In particular, for the instance in Proposition 5, the VCG payment is smaller than TUmax(c) by a factor of n \u2212 2. We have already seen that TUmax(c) \u2265 n \u2212 2. On the other hand, under VCG, the threshold bid of any Xi, i = 1, . . . , n \u2212 2, is 0: if any such vertex bids above 0, it is deleted from the winning set together with X0 and replaced with Xn\u22121. Similarly, the threshold bid of X0 is 1, because if X0 bids above 1, it can be replaced with Xn\u22121. So the VCG payment is 1. This result is not surprising: the definition of TUmax(c) implicitly assumes there is co-operation between the agents, while the computation of VCG payments does not take into account any interaction between them. Indeed, co-operation enables the agents to extract higher payments under VCG. That is, VCG is not groupstrategyproof. This suggests that as a payment bound, TUmax(c) may be too liberal, at least in a context where there is little or no co-operation between agents. Perhaps TUmax(c) can be a good benchmark for measuring the performance of mechanisms designed for agents that can form coalitions or make side payments to each other, in particular, group-strategyproof mechanisms. Another setting in which bounding \u03c6TUmax is still of some interest is when, for the underlying problem, the optimal allocation and VCG payments are NP-hard to compute. In this case, finding a polynomial-time computable mechanism with good frugality ratio with respect to TUmax(c) is a non-trivial task, while bounding the frugality ratio with respect to more challenging payment bounds could be too difficult. To illustrate this point, compare the proofs of Lemma 6 and Lemma 7: both require some effort, but the latter is much more difficult than the former. 6.3 The choice of S All payment bounds defined in this paper correspond to the total bid of all elements in the cheapest feasible set, where ties are broken lexicographically. While this definition ensures that our payment bounds are well-defined, the particular choice of the drawresolution rule appears arbitrary, and one might wonder if our payment bounds are sufficiently robust to be independent of this choice. It turns out that is indeed the case for NTUmin(c) and NTUmax(c), i.e., these bounds do not depend on the draw-resolution rule. To see this, suppose that two feasible sets S1 and S2 have the same cost. In the computation of NTUmin(c, S1), all vertices in S1 \\S2 would have to bid their true cost, since otherwise S2 would become cheaper than S1. Hence, any bid vector for S1 can only have be = ce for e \u2208 S1 \u2229 S2, and hence constitutes a valid bid vector for S2 and vice versa. A similar argument applies to NTUmax(c). However, for TUmin(c) and TUmax(c) this is not the case. For example, consider the set system E = {e1, e2, e3, e4, e5}, F = {S1 = {e1, e2}, S2 = {e2, e3, e4}, S3 = {e4, e5}} with the costs c1 = 2, c2 = c3 = c4 = 1, c5 = 3. The cheapest sets are S1 and S2. Now TUmax(c, S1) \u2264 4, as the total bid of the elements in S1 cannot exceed the total cost of S3. On the other hand, TUmax(c, S2) \u2265 5, as we can set b2 = 3, b3 = 0, b4 = 2. Similarly, TUmin(c, S1) = 4, because the inequalities in (2) are b1 \u2264 2 and b1 + b2 \u2264 4. But TUmin(c, S2) \u2264 3 as we can set b2 = 1, b3 = 2, b4 = 0. 6.4 Negative results for NTUmin(c) and TUmin(c) The results in and our vertex cover results are proved for the frugality ratio \u03c6NTUmin. Indeed, it can be argued that \u03c6NTUmin is the best definition of frugality ratio, because among all reasonable payment bounds (i.e., ones that are at least as large as the cost of the cheapest feasible set), it is most demanding of the algorithm. However, NTUmin(c) is not always the easiest or the most natural payment bound to work with. In this subsection, we discuss several disadvantages of NTUmin(c) (and also TUmin(c)) as compared to NTUmax(c) and TUmax(c). 6.4.1 Nonmonotonicity The first problem with NTUmin(c) is that it is not monotone with respect to F, i.e., it may increase when one adds a feasible set to F. (It is, however, monotone in the sense that a losing agent cannot become a winner by raising his cost.) Intuitively, a good payment bound should satisfy this monotonicity requirement, as adding a feasible set increases the competition, so it should drive the prices down. Note that this indeed the case for NTUmax(c) and TUmax(c) since a new feasible set adds a constraint in (2), thus limiting the solution space for the respective linear program. PROPOSITION 7. Adding a feasible set to F can increase the value of NTUmin(c) by a factor of \u03a9(n). PROOF. Let E = {x, xx, y1, . . . , yn, z1, . . . , zn}. Set Y = {y1, . . . , yn}, S = Y \u222a {x}, Ti = Y \\ {yi} \u222a {zi}, i = 1, . . . , n, and suppose that F = {S, T1, . . . , Tn}. The costs are cx = 0, cxx = 0, cyi = 0, czi = 1 for i = 1, . . . , n. Note that S is the cheapest feasible set. Let F = F \u222a {T0}, where T0 = Y \u222a {xx}. For F, the bid vector by1 = \u00b7 \u00b7 \u00b7 = byn = 0, bx = 1 satisfies (1), (2), and (3), so NTUmin(c) \u2264 1. For F , S is still the lexicographically-least cheapest set. Any optimal solution has bx = 0 (by constraint in (2) with T0). Condition (3) for yi implies bx + byi = czi = 1, so byi = 1 and NTUmin(c) = n. For path auctions, it has been shown that NTUmin(c) is non-monotone in a slightly different sense, i.e., with respect to adding a new edge (agent) rather than a new feasible set (a team of existing agents). REMARK 5. We can also show that NTUmin(c) is non-monotone for vertex cover. In this case, adding a new feasible set corresponds to deleting edges from the graph. It turns out that deleting a single edge can increase NTUmin(c) by a factor of n \u2212 2; the construction is similar to that of Proposition 5. 6.4.2 NP-Hardness Another problem with NTUmin(c, S) is that it is NP-hard to compute even if the number of feasible sets is polynomial in n. Again, this puts it at a disadvantage compared to NTUmax(c, S) and TUmax(c, S) (see Remark 1). THEOREM 6. Computing NTUmin(c) is NP-hard, even when the lexicographically-least feasible set S is given in the input. PROOF. We reduce EXACT COVER BY 3-SETS(X3C) to our problem. An instance of X3C is given by a universe G = {g1, . . . , gn} and a collection of subsets C1, . . . , Cm, Ci \u2282 G, |Ci| = 3, where the goal is to decide whether one can cover G by n/3 of these sets. Observe that if this is indeed the case, each element of G is contained in exactly one set of the cover. LEMMA 9. Consider a minimisation problem P of the following form: Minimise i=1,...,n bi under conditions (1) bi \u2265 0 for all i = 1, . . . , n; (2) for any j = 1, . . . , k we have bi\u2208Sj bi \u2264 aj, where Sj \u2286 {b1, . . . , bn}; (3) for each bj , one of the constraints in (2) involving it is tight. For any such P, one can construct a set system S and a vector of costs c such that NTUmin(c) is the optimal solution to P. PROOF. The construction is straightforward: there is an element of cost 0 for each bi, an element of cost aj for each aj, the feasible solutions are {b1, . . . , bn}, or any set obtained from {b1, . . . , bn} by replacing the elements in Sj by aj. 344 By this lemma, all we have to do to prove Theorem 6 is to show how to solve X3C by using the solution to a minimisation problem of the form given in Lemma 9. We do this as follows. For each Ci, we introduce 4 variables xi, \u00afxi, ai, and bi. Also, for each element gj of G there is a variable dj. We use the following set of constraints: \u2022 In (1), we have constraints xi \u2265 0, \u00afxi \u2265 0, ai \u2265 0, bi \u2265 0, dj \u2265 0 for all i = 1, . . . , m, j = 1, . . . , n. \u2022 In (2), for all i = 1, . . . , m, we have the following 5 constraints: xi + \u00afxi \u2264 1, xi +ai \u2264 1, \u00afxi +ai \u2264 1, xi +bi \u2264 1, \u00afxi + bi \u2264 1. Also, for all j = 1, . . . , n we have a constraint of the form xi1 + \u00b7 \u00b7 \u00b7 + xik + dj \u2264 1, where Ci1 , . . . , Cik are the sets that contain gj. The goal is to minimize z = i(xi + \u00afxi + ai + bi) + j dj. Observe that for each j, there is only one constraint involving dj , so by condition (3) it must be tight. Consider the two constraints involving ai. One of them must be tight, and therefore xi +\u00afxi +ai +bi \u2265 xi +\u00afxi +ai \u2265 1. Hence, for any feasible solution to (1)-(3) we have z \u2265 m. Now, suppose that there is an exact set cover. Set dj = 0 for j = 1, . . . , n. Also, if Ci is included in this cover, set xi = 1, \u00afxi = ai = bi = 0, otherwise set \u00afxi = 1, xi = ai = bi = 0. Clearly, all inequalities in (2) are satisfied (we use the fact that each element is covered exactly once), and for each variable, one of the constraints involving it is tight. This assignment results in z = m. Conversely, suppose there is a feasible solution with z = m. As each addend of the form xi + \u00afxi + ai + bi contributes at least 1, we have xi + \u00afxi + ai + bi = 1 for all i, dj = 0 for all j. We will now show that for each i, either xi = 1 and \u00afxi = 0, or xi = 0 and \u00afxi = 1. For the sake of contradiction, suppose that xi = \u03b4 < 1, \u00afxi = \u03b4 < 1. As one of the constraints involving ai must be tight, we have ai \u2265 min{1 \u2212 \u03b4, 1 \u2212 \u03b4 }. Similarly, bi \u2265 min{1 \u2212 \u03b4, 1 \u2212 \u03b4 }. Hence, xi + \u00afxi + ai + bi = 1 = \u03b4 +\u03b4 +2 min{1\u2212\u03b4, 1\u2212\u03b4 } > 1. To finish the proof, note that for each j = 1, . . . , m we have xi1 + \u00b7 \u00b7 \u00b7 + xik + dj = 1 and dj = 0, so the subsets that correspond to xi = 1 constitute a set cover. REMARK 6. In the proofs of Proposition 7 and Theorem 6 all constraints in (1) are of the form be \u2265 0. Hence, the same results are true for TUmin(c). REMARK 7. For shortest-path auctions, the size of F can be superpolynomial. However, there is a polynomial-time separation oracle for constraints in (2) (to construct one, use any algorithm for finding shortest paths), so one can compute NTUmax(c) and TUmax(c) in polynomial time. On the other hand, recently and independently it was shown that computing NTUmin(c) for shortest-path auctions is NP-hard.", "body1": "In a set system auction there is a single buyer and many vendors that can provide various services. We assume that each vendor has a cost of providing his services, but submits a possibly larger bid to the auctioneer. They show that, for a large class of truthful mechanisms for this problem, the frugality ratio is as large as the number of edges in the shortest path. While the definition of frugality ratio proposed by is wellmotivated and has been instrumental in studying truthful mechanisms for set systems, it is not completely satisfactory. At the same time, there is no monopoly, that is, there is no vendor that appears in all feasible sets. 1.1 Our results Vertex cover auctions We propose a truthful polynomial-time auction for vertex cover that outputs a solution whose cost is within a factor of 2 of optimal, and whose frugality ratio is at most 2\u0394, where \u0394 is the maximum degree of the graph (Theorem 4). All four payment bounds arise as Nash equilibria of certain games (see the full version of this paper ); the differences between them can be seen as the price of initiative and the price of cooperation (see Section 3). Inspired by this application, we embark on a further study of these payment bounds. 2. 3. Related work Vertex-cover auctions have been studied in the past by Talwar and Calinescu . In most of this paper, we discuss auctions for set systems. The goal of the centre is to select (purchase) a feasible set. The auction proceeds as follows: all elements of the ground set make their bids, the centre selects a feasible set based on the bids and makes payments to the agents. An allocation rule is monotone if an agent cannot increase his chance of getting selected by raising his bid. The VCG mechanism is a truthful mechanism that maximises the social welfare and pays 0 to the losing agents. We start by reproducing the definition of the quantity \u03bd from [16, Definition 4]. Minimise B = e\u2208S be subject to (1) be \u2265 ce for all e \u2208 E (2) e\u2208S\\T be \u2264 e\u2208T \\S ce for all T \u2208 F (3) for every e \u2208 S, there is a Te \u2208 F such that e \u2208 Te andP e \u2208S\\Te be = e \u2208Te\\S ce The bound \u03bd(c, S) can be seen as an outcome of a two-stage process, where first each agent e \u2208 S makes a bid be stating how much it wants to be paid, and then the centre decides whether to accept these bids. This definition captures many important aspects of our intuition about \u2018fair\" payments. First, we can consider the worst rather than the best possible outcome for the centre. In this case, if they can extract more money from the centre by agreeing on a vector of bids that violates individual rationality (i.e., condition (1)) for some bidders, they might be willing to do so, as the agents who are paid below their costs will be compensated by other members of the group. \u2022 NTUmin(c, S), which is the solution to the optimisation problem Minimise B subject to (1), (2), and (3). \u2022 NTUmax(c, S), which is the solution to the optimisation problem Maximise B subject to (1), (2), (3). The abbreviations TU and NTU correspond, respectively, to transferable utility and non-transferable utility, i.e., the agents\" ability/inability to make payments to each other. The second modification (transferable utility) is more intuitively appealing in the context of the maximisation problem, as both assume some degree of co-operation between the agents. REMARK 1. PROPOSITION 1. TUmin(c, S) \u2264 NTUmin(c, S) \u2264 NTUmax(c, S) \u2264 TUmax(c, S). Let M be a truthful mechanism for (E, F). EXAMPLE 1. 4.1 Path auctions We start by showing that for path auctions any two consecutive payment bounds can differ by at least a factor of 2. PROOF. Consider the graph of Figure 1 with the edge costs cAB = cBC = cCD = 0, cAC = cBD = 1. The inequalities in (1) are bAB \u2265 0, bBC \u2265 0, bCD \u2265 0. PROOF. The edge costs are cAB = cCD = 1, cBC = 0, cAC = cBD = 1. 4.2 Connections between separation results The separation results for path auctions are obtained on the same graph using very similar cost vectors. The proof of the theorem follows directly from the four lemmas proved below; more precisely, the first equality in Theorem 1 is obtained by combining Lemmas 1 and 2, and the second equality is obtained by combining Lemmas 3 and 4. LEMMA 1. Moreover, as the costs of elements outside of S remained the same, the right-hand sides of all constraints in (2) did not change, so any bid vector that satisfies (2) and (3) with respect to c, also satisfies them with respect to c . We can set b3 i = b1 i : this bid vector satisfies conditions (2) and (3) since b1 does, and we have b1 i \u2265 min{ci, b1 i } = ci, which means that b3 satisfies condition (1). Then there is a cost vector c such that S is a cheapest feasible set and TUmax(c , S)/NTUmax(c , S) \u2265 \u03b1. LEMMA 3. LEMMA 4. PROPOSITION 5. COROLLARY 1. COROLLARY 2. j+2ix ij P \\ P ij+2P \\ P yijixix j j+1 i j+2ij+1 y y i i j+2ie j e j+1 ij+1 P \\ P Figure 3: Proof of Theorem 3: constraints for \u02c6Pij and \u02c6Pij+2 do not overlap 4.4 Upper bounds It turns out that the lower bound proved in the previous subsection is almost tight. Consider the conditions (2) and (3) for S. One can pick a subset L of at most k inequalities in (2) so that for each i = 1, . Combining these two inequalities, we obtain nTUmin(c) \u2265 kTUmin(c) \u2265 TUmax(c). REMARK 2. For path auctions, this upper bound can be improved to 2, matching the lower bounds of Section 4.1. THEOREM 3. For any i = 1, . Start by setting L = {I1}. Note that for any j we have yij+1 > yij . By adding up all of the (tight) inequalities in L for bi we obtain i=1,...,k bi \u2265 j=1,...,k c( \u02c6Pj \\ P). Recall that for a vertex-cover auction on a graph G = (V, E), an allocation rule is an algorithm that takes as input a bid bv for each vertex and returns a vertex cover \u02c6S of G. As explained in Section 2, we can combine a monotone allocation rule with threshold payments to obtain a truthful auction. Another approximation algorithm for vertex cover, which has approximation ratio 2, is the local ratio algorithm ALR . 5.1 Upper bound We say that an allocation rule is locally optimal if whenever bv >P w\u223cv bw, the vertex v is not chosen. CLAIM 1. THEOREM 4. To prove Theorem 4, we first show that the total payment of any locally optimal mechanism does not exceed \u0394c(V ). LEMMA 5. LEMMA 6. LEMMA 7. Recall that a network flow problem is described by a directed graph \u0393 = (V\u0393, E\u0393), a source node s \u2208 V\u0393, a sink node t \u2208 V\u0393, and a vector of capacity constraints ae, e \u2208 E\u0393. Recall that a cut is a partition of the vertices in V\u0393 into two sets C1 and C2 so that s \u2208 C1, t \u2208 C2; we denote such a cut by C = (C1, C2). Consider the network \u0393 = (V\u0393 , E\u0393 ), where V\u0393 = {s} \u222a S \u222a W \u222a {t}, E\u0393 = {(u, v) \u2208 E\u0393 | u, v \u2208 V\u0393 }. Similarly, C = ({s}, S \u222a W \u222a {t}) is a minimum cut in \u0393 , cap(C ) = c(S ). Now, let us check that no vertex cover T \u2286 V can violate condition (2). If (u, v) \u2208 E2 then v \u2208 T3 (no edge in E2 can cross the cut), and if u, v \u2208 S then v \u2208 T1\u222aT2. Now, consider the vertices in S \\T2. Finally, we derive a lower bound on the payment bound that is of interest to us, namely, NTUmin(c). LEMMA 8. But this violates (2). As NTUmin(c, S) satisfies condition (1), it follows that we have NTUmin(c, S) \u2265 c(S). REMARK 3. Extensions We can also apply our results to monotone vertex-cover algorithms that do not necessarily output locally-optimal solutions. Therefore, using it on top of a monotone allocation rule with approx342 imation ratio \u03b1, we obtain a monotone locally-optimal allocation rule with approximation ratio \u03b1. THEOREM 5. We will consider two families of cost vectors for G. Under a cost vector x \u2208 X, each block Bi has one vertex of cost 1; all other vertices cost 0. Let xl and xr be the cost vectors obtained by changing the cost of vl and vr, respectively. In the former case, we put in W an edge from y to xl and in the latter case we put in W an edge from y to xr (if the vertex cover includes all of Bi, W contains both of these edges). The graph W has at least k(2\u0394)k\u22121 \u03942 edges, so there must exist an x \u2208 X of degree at least k\u0394/2. On the other hand, by monotonicity of M we have vi \u2208 M(x) for i = 1, . REMARK 4.", "body2": "A widely-studied class of setsystem auctions is path auctions, where each vendor is able to sell access to a link in a network, and the feasible sets are those sets whose links contain a path from a given source to a given destination; the study of these auctions has been initiated in the seminal paper by Nisan and Ronen (see also ). They define the frugality ratio of a mechanism as the ratio between its total payment and the cost of the cheapest path disjoint from the path selected by the mechanism. Talwar extends this definition of frugality ratio to general set systems, and studies the frugality ratio of the classical VCG mechanism for many specific set systems, such as minimum spanning trees and set covers. However, the graph contains no A-D path that is disjoint from ABCD, and hence the frugality ratio of VCG on this graph remains undefined. Based on this new definition, the authors construct new mechanisms for the shortest path problem and show that the overpayment of these mechanisms is within a constant factor of optimal. These modifications can be made independently of each other, resulting in four different payment bounds TUmax, TUmin, NTUmax, and NTUmin, where NTUmin is equal to the original payment bound \u03bd of in . While our main result about vertex cover auctions (Theorem 4) is with respect to NTUmin = \u03bd, we make use of the new definitions by first comparing the payment of our mechanism to a weaker bound NTUmax, and then bootstrapping from this result to obtain the desired bound. We see this as an argument for the study of vertexcover auctions, as they appear to be more representative of the general team -selection problem than the widely studied path auctions. This observation suggests that the four payment bounds should be studied in a unified framework; moreover, it leads us to believe that the bootstrapping technique of Theorem 4 may have other applications. This can be seen as an argument in favour of using weaker but efficiently computable bounds NTUmax and TUmax. Our results improve on those of as our mechanism is polynomial-time computable, as well as on those of , as our mechanism has a better approximation ratio, and we prove a stronger bound on the frugality ratio; moreover, this bound also applies to the mechanism of . In set system auctions, each element e of the ground set is owned by an independent agent and has an associated non-negative cost ce. The elements that are not selected incur no costs. The standard requirements are individual rationality, i.e., the payment to each agent should be at least as high as his incurred cost (0 for agents not in the selected set and ce for agents in the 337 selected set) and incentive compatibility, or truthfulness, i.e., each agent\"s dominant strategy is to bid his true cost. ) that any auction that has a monotone allocation rule and pays each agent his threshold bid is truthful; conversely, any truthful auction has a monotone allocation rule. Similarly, b(U) denotes w\u2208U bw. Then \u03bd(c, S) is the solution to the following optimisation problem. The centre wants to minimise the total payout, so \u03bd(c, S) corresponds to the best possible outcome from the centre\"s point of view. However, it can be modified in two ways, both of which are still quite natural, but result in different payment bounds. Second, the agents may be able to make payments to each other. \u2022 TUmax(c, S), which is the solution to the optimisation problem Maximise B subject to (1\u2217 ), (2), and (3). \u2022 NTUmin(c, S), which is the solution to the optimisation problem Minimise B subject to (1), (2), and (3). \u2022 NTUmax(c, S), which is the solution to the optimisation problem Maximise B subject to (1), (2), (3). We define TUmax(c), NTUmin(c), NTUmax(c) and \u03bd(c) similarly, though we will see in Section 6.3 that, in fact, NTUmin(c, S) and NTUmax(c, S) are independent of the choice of S. Note that the quantity \u03bd(c) from is NTUmin(c). Another possibility would be to introduce an additional constraint e\u2208S be \u2265P e\u2208S ce in the definition of TUmin(c, S) (note that this condition holds automatically for TUmax(c, S), as TUmax(c, S) \u2265 NTUmax(c, S)); however, such a definition would have no direct game-theoretic interpretation, and some of our results (in particular, the ones in Section 4) would no longer be true. The first and third inequalities in the following observation follow from the fact that condition (1\u2217 ) is strictly weaker than condition (1). PROPOSITION 1. TUmin(c, S) \u2264 NTUmin(c, S) \u2264 NTUmax(c, S) \u2264 TUmax(c, S). In the next section, we quantify this difference, both for general set systems, and for specific types of set systems, such as path auctions or vertex cover auctions. The cheapest feasible sets are all paths from A to D. It can be verified, using the reasoning of Propositions 2 and 3 below, that for the cost vector cAB = cCD = 2, cBC = 1, cAC = cBD = 5, we have \u2022 TUmax(c) = 10 (with bAB = bCD = 5, bBC = 0), \u2022 NTUmax(c) = 9 (with bAB = bCD = 4, bBC = 1), \u2022 NTUmin(c) = 7 (with bAB = bCD = 2, bBC = 3), \u2022 TUmin(c) = 5 (with bAB = bCD = 0, bBC = 5). There is an instance of the shortest-path problem for which we have NTUmax(c)/NTUmin(c) \u2265 2. This construction is due to David Kempe . By condition (3), both of these inequalities must be tight (the former one is the only inequality involving bAB, and the latter one is the only inequality involving bCD). There is an instance of the shortest-path problem for which we have TUmax(c)/NTUmax(c) \u2265 2. There is an instance of the shortest-path problem for which we have NTUmin(c)/TUmin(c) \u2265 2. This construction is also based on the graph of Figure 1. In Section 4.4 (Theorem 3), we show that the separation results in Propositions 2, 3, and 4 are optimal. For any set system (E, F), and any feasible set S, max TUmax(c, S) NTUmax(c, S) = max NTUmax(c, S) NTUmin(c, S) max NTUmax(c, S) NTUmin(c, S) = max NTUmin(c, S) TUmin(c, S) where the maximum is over all cost vectors c for which S is a cheapest feasible set. We prove Lemma 1 here; the proofs of Lemmas 2- 4 are similar and can be found in the full version of this paper . Then there is a cost vector c such that S is a cheapest feasible set and NTUmax(c , S)/NTUmin(c , S) \u2265 \u03b1. Construct the cost vector c by setting ci = ci for i \u2208 S, ci = min{ci, b1 i } for i \u2208 S. Clearly, S is a cheapest set under c . As NTUmax(c , S) \u2265 X and NTUmin(c , S) \u2264 Y , this implies the lemma. Suppose c is a cost vector for (E, F) such that S is a cheapest feasible set and NTUmax(c, S)/NTUmin(c, S) = \u03b1. Then there is a cost vector c such that S is a cheapest feasible set and TUmax(c , S)/NTUmax(c , S) \u2265 \u03b1. Then there is a cost vector c such that S is a cheapest feasible set and NTUmin(c , S)/TUmin(c , S) \u2265 \u03b1. 4.3 Vertex-cover auctions In contrast to the case of path auctions, for vertex-cover auctions the gap between NTUmin(c) and NTUmax(c) (and hence between NTUmax(c) and TUmax(c), and between TUmin(c) and NTUmin(c)) can be proportional to the size of the graph. For any n \u2265 3, there is a an n-vertex graph and a cost vector c for which TUmax(c)/NTUmax(c) \u2265 n \u2212 2. Combining Proposition 5 with Lemmas 1 and 3, we derive the following corollaries. For any n \u2265 3, we can construct an instance of the vertex cover problem on a graph of size n that satisfies NTUmax(c)/NTUmin(c) \u2265 n \u2212 2. For any n \u2265 3, we can construct an instance of the vertex cover problem on a graph of size n that satisfies NTUmin(c)/TUmin(c) \u2265 n \u2212 2. For any set system (E, F) and any cost vector c, we have TUmax(c)/TUmin(c) \u2264 n. , bk be their bids that correspond to TUmax(c). On the other hand, all these inequalities appear in condition (2), so they must hold for bi , i.e., i=1,...,k bi \u2264 j=1,...,k c(Tj \\ S). Combining these two inequalities, we obtain nTUmin(c) \u2265 kTUmin(c) \u2265 TUmax(c). The final line of the proof of Theorem 2 shows that, in fact, the upper bound on TUmax(c)/TUmin(c) can be strengthened to the size of the winning set, k. Note that in Proposition 5, as well as in Corollaries 1 and 2, k = n\u22121, so these results do not contradict each other. For path auctions, this upper bound can be improved to 2, matching the lower bounds of Section 4.1. For any instance of the shortest path problem, TUmax(c) \u2264 2 TUmin(c). , bk ) be bid vectors that correspond to TUmin(c) and TUmax(c), respectively. As in the proof of Theorem 2, we construct a set of tight constraints L such that every variable bi appears in at least one of these constraints; however, now we have to be more careful about the choice of constraints in L. We construct L inductively as follows. At the jth step, suppose that all variables up to (but not including) bij appear in at least one inequality in L. Add Iij to L. Therefore, the variables in Iij+2 and Iij do not overlap, and hence no bi can appear in more than two inequalities in L. Now we follow the argument of the proof of Theorem 2 to finish. On the other hand, all these inequalities appear in condition (2), so they must hold for bi , i.e., i=1,...,k bi \u2264 j=1,...,k c( \u02c6Pj \\ P), so TUmax(c) \u2264 2TUmin(c). However, Aopt cannot be guaranteed to run in polynomial time unless P = NP and AGR has approximation ratio of log n. In the next subsection, we bound the frugality ratio of ALR (and, more generally, all algorithms that satisfy the condition of local optimality, defined later) by 2\u0394, where \u0394 is the maximum degree of G. We then prove a matching lower bound showing that for some graphs the frugality ratio of any truthful auction is at least \u0394/2. Note that for any such rule the threshold bid of v satisfies tv \u2264 w\u223cv bw. The algorithms Aopt, AGR, and ALR are locally optimal. Any vertex cover auction M that has a locally optimal and monotone allocation rule and pays each agent his threshold bid has frugality ratio \u03c6NTUmin(M) \u2264 2\u0394. By combining these two results, the theorem follows. Then for any cost vector c, the total payment of M satisfies pM(c) \u2264 \u0394c(V ). We now derive a lower bound on TUmax(c); while not essential for the proof of Theorem 4, it helps us build the intuition necessary for that proof. Next, we prove a lower bound on NTUmax(c, S); we will then use it to obtain a lower bound on NTUmin(c). We show how to construct a bid vector (be)e\u2208S that satisfies conditions (1) and (2) such that b(S) \u2265 c(V \\ S); clearly, this implies NTUmax(c, S) \u2265 c(V \\ S). Set the capacity constraints for e \u2208 E\u0393 as follows: a(s,v) = cv, a(w,t) = cw, a(v,w) = +\u221e for all v \u2208 S, w \u2208 V \\ S. As cap(Cmin) \u2264 cap(s, V \u222a {t}) = c(S) < +\u221e, and any edge in E2 has infinite capacity, no edge (u, v) \u2208 E2 crosses Cmin. Now, consider the network \u0393 = (V\u0393 , E\u0393 ), where V\u0393 = {s} \u222a S \u222a W \u222a {t}, E\u0393 = {(u, v) \u2208 E\u0393 | u, v \u2208 V\u0393 }. As F is constructed by augmenting F, we have bv \u2265 cv for all v \u2208 S, i.e., condition (1) is satisfied. Consider all edges (u, v) \u2208 E such that u \u2208 S \\T1. Consequently, c(T3) \u2265 c(S \\ T1) = b(S \\ T1). Hence, b(S \\ T1) + b(S \\ T2) \u2264 c(T3) + c(T4). Finally, we derive a lower bound on the payment bound that is of interest to us, namely, NTUmin(c). Now since bw \u2265 cw for all w \u2208 S, bv > cv implies b (S \\T) > c (S \\T) = c (T \\S). Hence, we have NTUmax(c , S) = v\u2208S bv = NTUmin(c, S) < c(V \\ S), giving a contradiction to the fact that NTUmax(c , S) \u2265 c (V \\S) which we proved in Lemma 7. Combined with Lemma 5, this completes the proof of Theorem 4. However, the frugality ratio \u03c6TUmin(M) is not realistic because the payment bound TUmin(c) is inappropriately low - we show in Section 6 that TUmin(c) can be significantly smaller than the total cost of a cheapest vertex cover. This procedure is monotone in bids, and it can only decrease the cost of the vertex cover. Our proof uses the techniques of , where the authors prove a similar result for shortest-path auctions. For any \u0394 > 0 and any n, there exist a graph G of maximum degree \u0394 and size N > n such that for any truthful mechanism M on G we have \u03c6NTUmin(M) \u2265 \u0394/2. , Bk of size 2\u0394 each, where each Bi is a complete bipartite graph with parts Li and Ri, |Li| = |Ri| = \u0394. By changing the cost of either of these vertices to 0, we obtain a cost vector in X. The vertex cover chosen by M(y) must either contain all vertices in Li or it must contain all vertices in Ri. In the former case, we put in W an edge from y to xl and in the latter case we put in W an edge from y to xr (if the vertex cover includes all of Bi, W contains both of these edges). It is not hard to see that NTUmin(x) \u2264 k: the cheapest vertex cover contains the all-0 part of each block, and we can satisfy conditions (1)-(3) by letting one of the vertices in the all-0 part of each block to bid 1, while all other the vertices in the cheapest set bid 0. Hence, \u03c6NTUmin(M) \u2265 M(x)/NTUmin(x) \u2265 \u0394/2. The argument can be further extended to a more permissive definition of truthfulness for randomised mechanisms, but this discussion is beyond the scope of this paper.", "introduction": "In a set system auction there is a single buyer and many vendors that can provide various services. It is assumed that the buyer\"s requirements can be satisfied by various subsets of the vendors; these subsets are called the feasible sets. A widely-studied class of setsystem auctions is path auctions, where each vendor is able to sell access to a link in a network, and the feasible sets are those sets whose links contain a path from a given source to a given destination; the study of these auctions has been initiated in the seminal paper by Nisan and Ronen (see also ). We assume that each vendor has a cost of providing his services, but submits a possibly larger bid to the auctioneer. Based on these bids, the auctioneer selects a feasible subset of vendors, and makes payments to the vendors in this subset. Each selected vendor enjoys a profit of payment minus cost. Vendors want to maximise profit, while the buyer wants to minimise the amount he pays. A natural goal in this setting is to design a truthful auction, in which vendors have an incentive to bid their true cost. This can be achieved by paying each selected vendor a premium above her bid in such a way that the vendor has no incentive to overbid. An interesting question in mechanism design is how much the auctioneer will have to overpay in order to ensure truthful bids. In the context of path auctions this topic was first addressed by Archer and Tardos . They define the frugality ratio of a mechanism as the ratio between its total payment and the cost of the cheapest path disjoint from the path selected by the mechanism. They show that, for a large class of truthful mechanisms for this problem, the frugality ratio is as large as the number of edges in the shortest path. Talwar extends this definition of frugality ratio to general set systems, and studies the frugality ratio of the classical VCG mechanism for many specific set systems, such as minimum spanning trees and set covers. While the definition of frugality ratio proposed by is wellmotivated and has been instrumental in studying truthful mechanisms for set systems, it is not completely satisfactory. Consider, for example, the graph of Figure 1 with the costs cAB = cBC = A B Figure 1: The diamond graph 336 cCD = 0, cAC = cBD = 1. This graph is 2-connected and the VCG payment to the winning path ABCD is bounded. However, the graph contains no A-D path that is disjoint from ABCD, and hence the frugality ratio of VCG on this graph remains undefined. At the same time, there is no monopoly, that is, there is no vendor that appears in all feasible sets. In auctions for other types of set systems, the requirement that there exist a feasible solution disjoint from the selected one is even more severe: for example, for vertex-cover auctions (where vendors correspond to the vertices of some underlying graph, and the feasible sets are vertex covers) the requirement means that the graph must be bipartite. To deal with this problem, Karlin et al. suggest a better benchmark, which is defined for any monopoly-free set system. This quantity, which they denote by \u03bd, intuitively corresponds to the value of a cheapest Nash equilibrium. Based on this new definition, the authors construct new mechanisms for the shortest path problem and show that the overpayment of these mechanisms is within a constant factor of optimal. 1.1 Our results Vertex cover auctions We propose a truthful polynomial-time auction for vertex cover that outputs a solution whose cost is within a factor of 2 of optimal, and whose frugality ratio is at most 2\u0394, where \u0394 is the maximum degree of the graph (Theorem 4). We complement this result by proving (Theorem 5) that for any \u0394 and n, there are graphs of maximum degree \u0394 and size \u0398(n) for which any truthful mechanism has frugality ratio at least \u0394/2. This means that the solution quality of our auction is with a factor of 2 of optimal and the frugality ratio is within a factor of 4 of the best possible bound for worst-case inputs. To the best of our knowledge, this is the first auction for this problem that enjoys these properties. Moreover, we show how to transform any truthful mechanism for the vertex-cover problem into a frugal one while preserving the approximation ratio. Frugality ratios Our vertex cover results naturally suggest two modifications of the definition of \u03bd in . These modifications can be made independently of each other, resulting in four different payment bounds TUmax, TUmin, NTUmax, and NTUmin, where NTUmin is equal to the original payment bound \u03bd of in . All four payment bounds arise as Nash equilibria of certain games (see the full version of this paper ); the differences between them can be seen as the price of initiative and the price of cooperation (see Section 3). While our main result about vertex cover auctions (Theorem 4) is with respect to NTUmin = \u03bd, we make use of the new definitions by first comparing the payment of our mechanism to a weaker bound NTUmax, and then bootstrapping from this result to obtain the desired bound. Inspired by this application, we embark on a further study of these payment bounds. Our results here are as follows: 1. We observe (Proposition 1) that the four payment bounds always obey a particular order that is independent of the choice of the set system and the cost vector, namely, TUmin \u2264 NTUmin \u2264 NTUmax \u2264 TUmax. We provide examples (Proposition 5 and Corollaries 1 and 2) showing that for the vertex cover problem any two consecutive bounds can differ by a factor of n \u2212 2, where n is the number of agents. We then show (Theorem 2) that this separation is almost best possible for general set systems by proving that for any set system TUmax/TUmin \u2264 n. In contrast, we demonstrate (Theorem 3) that for path auctions TUmax/TUmin \u2264 2. We provide examples (Propositions 2, 3 and 4) showing that this bound is tight. We see this as an argument for the study of vertexcover auctions, as they appear to be more representative of the general team -selection problem than the widely studied path auctions. We show (Theorem 1) that for any set system, if there is a cost vector for which TUmin and NTUmin differ by a factor of \u03b1, there is another cost vector that separates NTUmin and NTUmax by the same factor and vice versa; the same is true for the pairs (NTUmin, NTUmax) and (NTUmax, TUmax). This symmetry is quite surprising, since, e.g., TUmin and NTUmax are obtained from NTUmin by two very different transformations. This observation suggests that the four payment bounds should be studied in a unified framework; moreover, it leads us to believe that the bootstrapping technique of Theorem 4 may have other applications. We evaluate the payment bounds introduced here with respect to a checklist of desirable features. In particular, we note that the payment bound \u03bd = NTUmin of exhibits some counterintuitive properties, such as nonmonotonicity with respect to adding a new feasible set (Proposition 7), and is NP-hard to compute (Theorem 6), while some of the other payment bounds do not suffer from these problems. This can be seen as an argument in favour of using weaker but efficiently computable bounds NTUmax and TUmax. Related work Vertex-cover auctions have been studied in the past by Talwar and Calinescu . Both of these papers are based on the definition of frugality ratio used in ; as mentioned before, this means that their results only apply to bipartite graphs. Talwar shows that the frugality ratio of VCG is at most \u0394. However, since finding the cheapest vertex cover is an NP-hard problem, the VCG mechanism is computationally infeasible. The first (and, to the best of our knowledge, only) paper to investigate polynomial-time truthful mechanisms for vertex cover is . This paper studies an auction that is based on the greedy allocation algorithm, which has an approximation ratio of log n. While the main focus of is the more general set cover problem, the results of imply a frugality ratio of 2\u03942 for vertex cover. Our results improve on those of as our mechanism is polynomial-time computable, as well as on those of , as our mechanism has a better approximation ratio, and we prove a stronger bound on the frugality ratio; moreover, this bound also applies to the mechanism of .", "conclusion": "In this section we consider several desirable properties of payment bounds and evaluate the four payment bounds proposed in this paper with respect to them.. The particular properties that we are interested in are independence of the choice of S (Section 6.3), monotonicity (Section 6.4.1), computational hardness (Section 6.4.2), and the relationship with other reasonable bounds, such as the total cost of the cheapest set (Section 6.1), or the total VCG payment (Section 6.2).. 6.1 Comparison with total cost Our first requirement is that a payment bound should not be less than the total cost of the selected set.. Payment bounds are used to evaluate the performance of set-system auctions.. The latter have to satisfy individual rationality, i.e., the payment to each agent must be at least as large as his incurred costs; it is only reasonable to require the payment bound to satisfy the same requirement.. Clearly, NTUmax(c) and NTUmin(c) satisfy this requirement due to condition (1), and so does TUmax(c), since TUmax(c) \u2265 NTUmax(c).. The example of Proposition 4 shows that for path auctions, TUmin(c) can be smaller than the total cost by a factor of 2.. Moreover, there are set systems and cost vectors for which TUmin(c) is smaller than the cost of the cheapest set S by a factor of \u03a9(n).. Consider, for example, the vertex-cover auction for the graph of Proposition 5 with the costs cX1 = \u00b7 \u00b7 \u00b7 = cXn\u22122 = cXn\u22121 = 1, cX0 = 0.. The cost of a cheapest vertex cover is n \u2212 2, and the lexicographically first vertex cover of cost n\u22122 is {X0, X1, .. The constraints in (2) are bXi + bX0 \u2264 cXn\u22121 = 1.. Clearly, we can satisfy conditions (2) and (3) by setting bX1 = \u00b7 \u00b7 \u00b7 = bXn\u22122 = 0, bX0 = 1, which means that TUmin(c) \u2264 1.. This observation suggests that the payment bound TUmin(c) is too strong to be realistic, since it can be substantially lower than the cost of the cheapest feasible set.. Nevertheless, some of the positive results that were proved in for NTUmin(c) go through for TUmin(c) as well.. In particular, one can show that if the feasible sets are the bases of a monopolyfree matroid, then \u03c6TUmin(VCG) = 1.. To show that \u03c6TUmin(VCG) is at most 1, one must prove that the VCG payment is at most TUmin(c).. This is shown for NTUmin(c) in the first paragraph of the proof of Theorem 5 in .. Their argument does not use condition (1) at all, so it also applies to TUmin(c).. On the other hand, \u03c6TUmin(VCG) \u2265 1 since \u03c6TUmin(VCG) \u2265 \u03c6NTUmin(VCG) and \u03c6NTUmin(VCG) \u2265 1 by Proposition 7 of (and also by Proposition 6 below).. 6.2 Comparison with VCG payments Another measure of suitability for payment bounds is that they should not result in frugality ratios that are less then 1 for wellknown truthful mechanisms.. If this is indeed the case, the payment bound may be too weak, as it becomes too easy to design mechanisms that perform well with respect to it.. It particular, a reasonable requirement is that a payment bound should not exceed the total payment of the classical VCG mechanism.. The following proposition shows that NTUmax(c), and therefore also NTUmin(c) and TUmin(c), do not exceed the VCG payment pVCG(c).. The proof essentially follows the argument of Proposition 7 of and can be found in the full version of this paper .. Proposition 6 shows that none of the payment bounds TUmin(c), NTUmin(c) and NTUmax(c) exceeds the payment of VCG.. However, the payment bound TUmax(c) can be larger that the total 343 VCG payment.. In particular, for the instance in Proposition 5, the VCG payment is smaller than TUmax(c) by a factor of n \u2212 2.. We have already seen that TUmax(c) \u2265 n \u2212 2.. On the other hand, under VCG, the threshold bid of any Xi, i = 1, .. , n \u2212 2, is 0: if any such vertex bids above 0, it is deleted from the winning set together with X0 and replaced with Xn\u22121.. Similarly, the threshold bid of X0 is 1, because if X0 bids above 1, it can be replaced with Xn\u22121.. So the VCG payment is 1.. This result is not surprising: the definition of TUmax(c) implicitly assumes there is co-operation between the agents, while the computation of VCG payments does not take into account any interaction between them.. Indeed, co-operation enables the agents to extract higher payments under VCG.. That is, VCG is not groupstrategyproof.. This suggests that as a payment bound, TUmax(c) may be too liberal, at least in a context where there is little or no co-operation between agents.. Perhaps TUmax(c) can be a good benchmark for measuring the performance of mechanisms designed for agents that can form coalitions or make side payments to each other, in particular, group-strategyproof mechanisms.. Another setting in which bounding \u03c6TUmax is still of some interest is when, for the underlying problem, the optimal allocation and VCG payments are NP-hard to compute.. In this case, finding a polynomial-time computable mechanism with good frugality ratio with respect to TUmax(c) is a non-trivial task, while bounding the frugality ratio with respect to more challenging payment bounds could be too difficult.. To illustrate this point, compare the proofs of Lemma 6 and Lemma 7: both require some effort, but the latter is much more difficult than the former.. 6.3 The choice of S All payment bounds defined in this paper correspond to the total bid of all elements in the cheapest feasible set, where ties are broken lexicographically.. While this definition ensures that our payment bounds are well-defined, the particular choice of the drawresolution rule appears arbitrary, and one might wonder if our payment bounds are sufficiently robust to be independent of this choice.. It turns out that is indeed the case for NTUmin(c) and NTUmax(c), i.e., these bounds do not depend on the draw-resolution rule.. To see this, suppose that two feasible sets S1 and S2 have the same cost.. In the computation of NTUmin(c, S1), all vertices in S1 \\S2 would have to bid their true cost, since otherwise S2 would become cheaper than S1.. Hence, any bid vector for S1 can only have be = ce for e \u2208 S1 \u2229 S2, and hence constitutes a valid bid vector for S2 and vice versa.. A similar argument applies to NTUmax(c).. However, for TUmin(c) and TUmax(c) this is not the case.. For example, consider the set system E = {e1, e2, e3, e4, e5}, F = {S1 = {e1, e2}, S2 = {e2, e3, e4}, S3 = {e4, e5}} with the costs c1 = 2, c2 = c3 = c4 = 1, c5 = 3.. The cheapest sets are S1 and S2.. Now TUmax(c, S1) \u2264 4, as the total bid of the elements in S1 cannot exceed the total cost of S3.. On the other hand, TUmax(c, S2) \u2265 5, as we can set b2 = 3, b3 = 0, b4 = 2.. Similarly, TUmin(c, S1) = 4, because the inequalities in (2) are b1 \u2264 2 and b1 + b2 \u2264 4.. But TUmin(c, S2) \u2264 3 as we can set b2 = 1, b3 = 2, b4 = 0.. 6.4 Negative results for NTUmin(c) and TUmin(c) The results in and our vertex cover results are proved for the frugality ratio \u03c6NTUmin.. Indeed, it can be argued that \u03c6NTUmin is the best definition of frugality ratio, because among all reasonable payment bounds (i.e., ones that are at least as large as the cost of the cheapest feasible set), it is most demanding of the algorithm.. However, NTUmin(c) is not always the easiest or the most natural payment bound to work with.. In this subsection, we discuss several disadvantages of NTUmin(c) (and also TUmin(c)) as compared to NTUmax(c) and TUmax(c).. 6.4.1 Nonmonotonicity The first problem with NTUmin(c) is that it is not monotone with respect to F, i.e., it may increase when one adds a feasible set to F. (It is, however, monotone in the sense that a losing agent cannot become a winner by raising his cost.). Intuitively, a good payment bound should satisfy this monotonicity requirement, as adding a feasible set increases the competition, so it should drive the prices down.. Note that this indeed the case for NTUmax(c) and TUmax(c) since a new feasible set adds a constraint in (2), thus limiting the solution space for the respective linear program.. Adding a feasible set to F can increase the value of NTUmin(c) by a factor of \u03a9(n).. Let E = {x, xx, y1, .. , yn}, S = Y \u222a {x}, Ti = Y \\ {yi} \u222a {zi}, i = 1, .. , n, and suppose that F = {S, T1, .. The costs are cx = 0, cxx = 0, cyi = 0, czi = 1 for i = 1, .. , n. Note that S is the cheapest feasible set.. Let F = F \u222a {T0}, where T0 = Y \u222a {xx}.. For F, the bid vector by1 = \u00b7 \u00b7 \u00b7 = byn = 0, bx = 1 satisfies (1), (2), and (3), so NTUmin(c) \u2264 1.. For F , S is still the lexicographically-least cheapest set.. Any optimal solution has bx = 0 (by constraint in (2) with T0).. Condition (3) for yi implies bx + byi = czi = 1, so byi = 1 and NTUmin(c) = n. For path auctions, it has been shown that NTUmin(c) is non-monotone in a slightly different sense, i.e., with respect to adding a new edge (agent) rather than a new feasible set (a team of existing agents).. We can also show that NTUmin(c) is non-monotone for vertex cover.. In this case, adding a new feasible set corresponds to deleting edges from the graph.. It turns out that deleting a single edge can increase NTUmin(c) by a factor of n \u2212 2; the construction is similar to that of Proposition 5.. 6.4.2 NP-Hardness Another problem with NTUmin(c, S) is that it is NP-hard to compute even if the number of feasible sets is polynomial in n. Again, this puts it at a disadvantage compared to NTUmax(c, S) and TUmax(c, S) (see Remark 1).. Computing NTUmin(c) is NP-hard, even when the lexicographically-least feasible set S is given in the input.. We reduce EXACT COVER BY 3-SETS(X3C) to our problem.. An instance of X3C is given by a universe G = {g1, .. , gn} and a collection of subsets C1, .. , Cm, Ci \u2282 G, |Ci| = 3, where the goal is to decide whether one can cover G by n/3 of these sets.. Observe that if this is indeed the case, each element of G is contained in exactly one set of the cover.. Consider a minimisation problem P of the following form: Minimise i=1,...,n bi under conditions (1) bi \u2265 0 for all i = 1, .. , n; (2) for any j = 1, .. , k we have bi\u2208Sj bi \u2264 aj, where Sj \u2286 {b1, .. , bn}; (3) for each bj , one of the constraints in (2) involving it is tight.. For any such P, one can construct a set system S and a vector of costs c such that NTUmin(c) is the optimal solution to P. PROOF.. The construction is straightforward: there is an element of cost 0 for each bi, an element of cost aj for each aj, the feasible solutions are {b1, .. , bn}, or any set obtained from {b1, .. , bn} by replacing the elements in Sj by aj.. 344 By this lemma, all we have to do to prove Theorem 6 is to show how to solve X3C by using the solution to a minimisation problem of the form given in Lemma 9.. For each Ci, we introduce 4 variables xi, \u00afxi, ai, and bi.. Also, for each element gj of G there is a variable dj.. We use the following set of constraints: \u2022 In (1), we have constraints xi \u2265 0, \u00afxi \u2265 0, ai \u2265 0, bi \u2265 0, dj \u2265 0 for all i = 1, .. , m, j = 1, .. , n. \u2022 In (2), for all i = 1, .. , m, we have the following 5 constraints: xi + \u00afxi \u2264 1, xi +ai \u2264 1, \u00afxi +ai \u2264 1, xi +bi \u2264 1, \u00afxi + bi \u2264 1.. Also, for all j = 1, .. , n we have a constraint of the form xi1 + \u00b7 \u00b7 \u00b7 + xik + dj \u2264 1, where Ci1 , .. , Cik are the sets that contain gj.. The goal is to minimize z = i(xi + \u00afxi + ai + bi) + j dj.. Observe that for each j, there is only one constraint involving dj , so by condition (3) it must be tight.. Consider the two constraints involving ai.. One of them must be tight, and therefore xi +\u00afxi +ai +bi \u2265 xi +\u00afxi +ai \u2265 1.. Hence, for any feasible solution to (1)-(3) we have z \u2265 m. Now, suppose that there is an exact set cover.. Set dj = 0 for j = 1, .. , n. Also, if Ci is included in this cover, set xi = 1, \u00afxi = ai = bi = 0, otherwise set \u00afxi = 1, xi = ai = bi = 0.. Clearly, all inequalities in (2) are satisfied (we use the fact that each element is covered exactly once), and for each variable, one of the constraints involving it is tight.. This assignment results in z = m. Conversely, suppose there is a feasible solution with z = m. As each addend of the form xi + \u00afxi + ai + bi contributes at least 1, we have xi + \u00afxi + ai + bi = 1 for all i, dj = 0 for all j.. We will now show that for each i, either xi = 1 and \u00afxi = 0, or xi = 0 and \u00afxi = 1.. For the sake of contradiction, suppose that xi = \u03b4 < 1, \u00afxi = \u03b4 < 1.. As one of the constraints involving ai must be tight, we have ai \u2265 min{1 \u2212 \u03b4, 1 \u2212 \u03b4 }.. Similarly, bi \u2265 min{1 \u2212 \u03b4, 1 \u2212 \u03b4 }.. Hence, xi + \u00afxi + ai + bi = 1 = \u03b4 +\u03b4 +2 min{1\u2212\u03b4, 1\u2212\u03b4 } > 1.. To finish the proof, note that for each j = 1, .. , m we have xi1 + \u00b7 \u00b7 \u00b7 + xik + dj = 1 and dj = 0, so the subsets that correspond to xi = 1 constitute a set cover.. In the proofs of Proposition 7 and Theorem 6 all constraints in (1) are of the form be \u2265 0.. Hence, the same results are true for TUmin(c).. For shortest-path auctions, the size of F can be superpolynomial.. However, there is a polynomial-time separation oracle for constraints in (2) (to construct one, use any algorithm for finding shortest paths), so one can compute NTUmax(c) and TUmax(c) in polynomial time.. On the other hand, recently and independently it was shown that computing NTUmin(c) for shortest-path auctions is NP-hard."}
{"id": "C-42", "keywords": ["reservoir model", "energi explor", "enkf", "tigr"], "title": "Demonstration of Grid-Enabled Ensemble Kalman Filter Data Assimilation Methodology for Reservoir Characterization", "abstract": "Ensemble Kalman filter data assimilation methodology is a popular approach for hydrocarbon reservoir simulations in energy exploration. In this approach, an ensemble of geological models and production data of oil fields is used to forecast the dynamic response of oil wells. The Schlumberger ECLIPSE software is used for these simulations. Since models in the ensemble do not communicate, message-passing implementation is a good choice. Each model checks out an ECLIPSE license and therefore, parallelizability of reservoir simulations depends on the number licenses available. We have Grid-enabled the ensemble Kalman filter data assimilation methodology for the TIGRE Grid computing environment. By pooling the licenses and computing resources across the collaborating institutions using GridWay metascheduler and TIGRE environment, the computational accuracy can be increased while reducing the simulation runtime. In this paper, we provide an account of our efforts in Grid-enabling the ensemble Kalman Filter data assimilation methodology. Potential benefits of this approach, observations and lessons learned will be discussed.", "references": ["The Grid: Blueprint for a new computing infrastructure", "TIGRE Portal", "Demonstration of TIGRE environment for Grid enabled/suitable applications", "The High Performance Computing across Texas Consortium", "The Open Science Grid", "the TeraGrid and Beyond", "Data Assimilation: The Ensemble Kalman Filter", "Scientific Programming", "The GriPhyN project: Towards petascale virtual data grids", "The PacMan documentation and installation guide", "Case studies in identify management for virtual organizations", "The Grid User Management System", "Building grid computing portals: The NPACI grid portal toolkit, Grid computing: making the global infrastructure a reality", "Open Ticket Request System", "The MoinMoin Wiki Engine", "Integrating dynamic data into high resolution reservoir models using streamline-based analytic sensitivity coefficients", "History matching finite difference models with 3D streamlines", "Reservoir monitoring and Continuous Model Updating using Ensemble Kalman Filter", "History matching with an ensemble Kalman filter and discrete cosine parameterization", "An iterative ensemble Kalman filter for data assimilation", "Streamline assisted ensemble Kalman filter for rapid and continuous reservoir model updating", "ECLIPSE Reservoir Engineering Software"], "full_text": "1. INTRODUCTION Grid computing is an emerging collaborative computing paradigm to extend institution/organization specific high performance computing (HPC) capabilities greatly beyond local resources. Its importance stems from the fact that ground breaking research in strategic application areas such as bioscience and medicine, energy exploration and environmental modeling involve strong interdisciplinary components and often require intercampus collaborations and computational capabilities beyond institutional limitations. The Texas Internet Grid for Research and Education (TIGRE) is a state funded cyberinfrastructure development project carried out by five (Rice, A&M, TTU, UH and UT Austin) major university systems - collectively called TIGRE Institutions. The purpose of TIGRE is to create a higher education Grid to sustain and extend research and educational opportunities across Texas. TIGRE is a project of the High Performance Computing across Texas (HiPCAT) consortium. The goal of HiPCAT is to support advanced computational technologies to enhance research, development, and educational activities. The primary goal of TIGRE is to design and deploy state-of-the-art Grid middleware that enables integration of computing systems, storage systems and databases, visualization laboratories and displays, and even instruments and sensors across Texas. The secondary goal is to demonstrate the TIGRE capabilities to enhance research and educational opportunities in strategic application areas of interest to the State of Texas. These are bioscience and medicine, energy exploration and air quality modeling. Vision of the TIGRE project is to foster interdisciplinary and intercampus collaborations, identify novel approaches to extend academic-government-private partnerships, and become a competitive model for external funding opportunities. The overall goal of TIGRE is to support local, campus and regional user interests and offer avenues to connect with national Grid projects such as Open Science Grid , and TeraGrid . Within the energy exploration strategic application area, we have Grid-enabled the ensemble Kalman Filter (EnKF) approach for data assimilation in reservoir modeling and demonstrated the extensibility of the application using the TIGRE environment and the GridWay metascheduler. Section 2 provides an overview of the TIGRE environment and capabilities. Application description and the need for Grid-enabling EnKF methodology is provided in Section 3. The implementation details and merits of our approach are discussed in Section 4. Conclusions are provided in Section 5. Finally, observations and lessons learned are documented in Section 6. 2. TIGRE ENVIRONMENT The TIGRE Grid middleware consists of minimal set of components derived from a subset of the Virtual Data Toolkit (VDT) which supports a variety of operating systems. The purpose of choosing a minimal software stack is to support applications at hand, and to simplify installation and distribution of client/server stacks across TIGRE sites. Additional components will be added as they become necessary. The PacMan packaging and distribution mechanism is employed for TIGRE client/server installation and management. The PacMan distribution mechanism involves retrieval, installation, and often configuration of the packaged software. This approach allows the clients to keep current, consistent versions of TIGRE software. It also helps TIGRE sites to install the needed components on resources distributed throughout the participating sites. The TIGRE client/server stack consists of an authentication and authorization layer, Globus GRAM4-based job submission via web services (pre-web services installations are available up on request). The tools for handling Grid proxy generation, Grid-enabled file transfer and Grid-enabled remote login are supported. The pertinent details of TIGRE services and tools for job scheduling and management are provided below. 2.1. Certificate Authority The TIGRE security infrastructure includes a certificate authority (CA) accredited by the International Grid Trust Federation (IGTF) for issuing X. 509 user and resource Grid certificates . The Texas Advanced Computing Center (TACC), University of Texas at Austin is the TIGRE\"s shared CA. The TIGRE Institutions serve as Registration Authorities (RA) for their respective local user base. For up-to-date information on securing user and resource certificates and their installation instructions see ref . The users and hosts on TIGRE are identified by their distinguished name (DN) in their X.509 certificate provided by the CA. A native Grid-mapfile that contains a list of authorized DNs is used to authenticate and authorize user job scheduling and management on TIGRE site resources. At Texas Tech University, the users are dynamically allocated one of the many generic pool accounts. This is accomplished through the Grid User Management System (GUMS) . 2.2. Job Scheduling and Management The TIGRE environment supports GRAM4-based job submission via web services. The job submission scripts are generated using XML. The web services GRAM translates the XML scripts into target cluster specific batch schedulers such as LSF, PBS, or SGE. The high bandwidth file transfer protocols such as GridFTP are utilized for staging files in and out of the target machine. The login to remote hosts for compilation and debugging is only through GSISSH service which requires resource authentication through X.509 certificates. The authentication and authorization of Grid jobs are managed by issuing Grid certificates to both users and hosts. The certificate revocation lists (CRL) are updated on a daily basis to maintain high security standards of the TIGRE Grid services. The TIGRE portal documentation area provides a quick start tutorial on running jobs on TIGRE. 2.3. Metascheduler The metascheduler interoperates with the cluster level batch schedulers (such as LSF, PBS) in the overall Grid workflow management. In the present work, we have employed GridWay metascheduler - a Globus incubator project - to schedule and manage jobs across TIGRE. The GridWay is a light-weight metascheduler that fully utilizes Globus functionalities. It is designed to provide efficient use of dynamic Grid resources by multiple users for Grid infrastructures built on top of Globus services. The TIGRE site administrator can control the resource sharing through a powerful built-in scheduler provided by GridWay or by extending GridWay\"s external scheduling module to provide their own scheduling policies. Application users can write job descriptions using GridWay\"s simple and direct job template format (see Section 4 for details) or standard Job Submission Description Language (JSDL). See section 4 for implementation details. 2.4. Customer Service Management System A TIGRE portal was designed and deployed to interface users and resource providers. It was designed using GridPort and is maintained by TACC. The TIGRE environment is supported by open source tools such as the Open Ticket Request System (OTRS) for servicing trouble tickets, and MoinMoin Wiki for TIGRE content and knowledge management for education, outreach and training. The links for OTRS and Wiki are consumed by the TIGRE portal - the gateway for users and resource providers. The TIGRE resource status and loads are monitored by the Grid Port Information Repository (GPIR) service of the GridPort toolkit which interfaces with local cluster load monitoring service such as Ganglia. The GPIR utilizes cron jobs on each resource to gather site specific resource characteristics such as jobs that are running, queued and waiting for resource allocation. 3. ENSEMBLE KALMAN FILTER APPLICATION The main goal of hydrocarbon reservoir simulations is to forecast the production behavior of oil and gas field (denoted as field hereafter) for its development and optimal management. In reservoir modeling, the field is divided into several geological models as shown in Figure 1. For accurate performance forecasting of the field, it is necessary to reconcile several geological models to the dynamic response of the field through history matching . Figure 1. Cross-sectional view of the Field. Vertical layers correspond to different geological models and the nails are oil wells whose historical information will be used for forecasting the production behavior. (Figure Ref:). The EnKF is a Monte Carlo method that works with an ensemble of reservoir models. This method utilizes crosscovariances between the field measurements and the reservoir model parameters (derived from several models) to estimate prediction uncertainties. The geological model parameters in the ensemble are sequentially updated with a goal to minimize the prediction uncertainties. Historical production response of the field for over 50 years is used in these simulations. The main advantage of EnKF is that it can be readily linked to any reservoir simulator, and can assimilate latest production data without the need to re-run the simulator from initial conditions. Researchers in Texas are large subscribers of the Schlumberger ECLIPSE package for reservoir simulations. In the reservoir modeling, each geological model checks out an ECLIPSE license. The simulation runtime of the EnKF methodology depends on the number of geological models used, number of ECLIPSE licenses available, production history of the field, and propagated uncertainties in history matching. The overall EnKF workflow is shown Figure 2. Figure 2. Ensemble Kaman Filter Data Assimilation Workflow. Each site has L licenses. At START, the master/control process (EnKF main program) reads the simulation configuration file for number (N) of models, and model-specific input files. Then, N working directories are created to store the output files. At the end of iteration, the master/control process collects the output files from N models and post processes crosscovariances to estimate the prediction uncertainties. This information will be used to update models (or input files) for the next iteration. The simulation continues until the production histories are exhausted. Typical EnKF simulation with N=50 and field histories of 50-60 years, in time steps ranging from three months to a year, takes about three weeks on a serial computing environment. In parallel computing environment, there is no interprocess communication between the geological models in the ensemble. However, at the end of each simulation time-step, model-specific output files are to be collected for analyzing cross covariances and to prepare next set of input files. Therefore, master-slave model in messagepassing (MPI) environment is a suitable paradigm. In this approach, the geological models are treated as slaves and are distributed across the available processors. The master Cluster or (TIGRE/GridWay) START Read Configuration File Create N Working Directories Create N Input files Model l Model 2 Model N. . . ECLIPSE on site A ECLIPSE on Site B ECLIPSE on Site Z Collect N Model Outputs, Post-process Output files END . . . process collects model-specific output files, analyzes and prepares next set of input files for the simulation. Since each geological model checks out an ECLIPSE license, parallelizability of the simulation depends on the number of licenses available. When the available number of licenses is less than the number of models in the ensemble, one or more of the nodes in the MPI group have to handle more than one model in a serial fashion and therefore, it takes longer to complete the simulation. A Petroleum Engineering Department usually procures 10-15 ECLIPSE licenses while at least ten-fold increase in the number of licenses would be necessary for industry standard simulations. The number of licenses can be increased by involving several Petroleum Engineering Departments that support ECLIPSE package. Since MPI does not scale very well for applications that involve remote compute clusters, and to get around the firewall issues with license servers across administrative domains, Grid-enabling the EnKF workflow seems to be necessary. With this motivation, we have implemented Grid-enabled EnKF workflow for the TIGRE environment and demonstrated parallelizability of the application across TIGRE using GridWay metascheduler. Further details are provided in the next section. 4. IMPLEMENTATION DETAILS To Grid-enable the EnKF approach, we have eliminated the MPI code for parallel processing and replaced with N single processor jobs (or sub-jobs) where, N is the number of geological models in the ensemble. These model-specific sub-jobs were distributed across TIGRE sites that support ECLIPSE package using the GridWay metascheduler. For each sub-job, we have constructed a GridWay job template that specifies the executable, input and output files, and resource requirements. Since the TIGRE compute resources are not expected to change frequently, we have used static resource discovery policy for GridWay and the sub-jobs were scheduled dynamically across the TIGRE resources using GridWay. Figure 3 represents the sub-job template file for the GridWay metascheduler. Figure 3. GridWay Sub-Job Template In Figure 3, REQUIREMENTS flag is set to choose the resources that satisfy the application requirements. In the case of EnKF application, for example, we need resources that support ECLIPSE package. ARGUMENTS flag specifies the model in the ensemble that will invoke ECLIPSE at a remote site. INPUT_FILES is prepared by the EnKF main program (or master/control process) and is transferred by GridWay to the remote site where it is untared and is prepared for execution. Finally, OUTPUT_FILES specifies the name and location where the output files are to be written. The command-line features of GridWay were used to collect and process the model-specific outputs to prepare new set of input files. This step mimics MPI process synchronization in master-slave model. At the end of each iteration, the compute resources and licenses are committed back to the pool. Table 1 shows the sub-jobs in TIGRE Grid via GridWay using gwps command and for clarity, only selected columns were shown USER JID DM EM NAME HOST pingluo 88 wrap pend enkf.jt antaeus.hpcc.ttu.edu/LSF pingluo 89 wrap pend enkf.jt antaeus.hpcc.ttu.edu/LSF pingluo 90 wrap actv enkf.jt minigar.hpcc.ttu.edu/LSF pingluo 91 wrap pend enkf.jt minigar.hpcc.ttu.edu/LSF pingluo 92 wrap done enkf.jt cosmos.tamu.edu/PBS pingluo 93 wrap epil enkf.jt cosmos.tamu.edu/PBS Table 1. Job scheduling across TIGRE using GridWay Metascheduler. DM: Dispatch state, EM: Execution state, JID is the job id and HOST corresponds to site specific cluster and its local batch scheduler. When a job is submitted to GridWay, it will go through a series of dispatch (DM) and execution (EM) states. For DM, the states include pend(ing), prol(og), wrap(per), epil(og), and done. DM=prol means the job has been scheduled to a resource and the remote working directory is in preparation. DM=warp implies that GridWay is executing the wrapper which in turn executes the application. DM=epil implies the job has finished running at the remote site and results are being transferred back to the GridWay server. Similarly, when EM=pend implies the job is waiting in the queue for resource and the job is running when EM=actv. For complete list of message flags and their descriptions, see the documentation in ref . We have demonstrated the Grid-enabled EnKF runs using GridWay for TIGRE environment. The jobs are so chosen that the runtime doesn\"t exceed more than a half hour. The simulation runs involved up to 20 jobs between A&M and TTU sites with TTU serving 10 licenses. For resource information, see Table I. One of the main advantages of Grid-enabled EnKF simulation is that both the resources and licenses are released back to the pool at the end of each simulation time step unlike in the case of MPI implementation where licenses and nodes are locked until the completion of entire simulation. However, the fact that each sub-job gets scheduled independently via GridWay could possibly incur another time delay caused by waiting in queue for execution in each simulation time step. Such delays are not expected EXECUTABLE=runFORWARD REQUIREMENTS=HOSTNAME=cosmos.tamu.edu | HOSTNAME=antaeus.hpcc.ttu.edu | HOSTNAME=minigar.hpcc.ttu.edu | ARGUMENTS=001 INPUT_FILES=001.in.tar OUTPUT_FILES=001.out.tar in MPI implementation where the node is blocked for processing sub-jobs (model-specific calculation) until the end of the simulation. There are two main scenarios for comparing Grid and cluster computing approaches. Scenario I: The cluster is heavily loaded. The conceived average waiting time of job requesting large number of CPUs is usually longer than waiting time of jobs requesting single CPU. Therefore, overall waiting time could be shorter in Grid approach which requests single CPU for each sub-job many times compared to MPI implementation that requests large number of CPUs at a single time. It is apparent that Grid scheduling is beneficial especially when cluster is heavily loaded and requested number of CPUs for the MPI job is not readily available. Scenario II: The cluster is relatively less loaded or largely available. It appears the MPI implementation is favorable compared to the Grid scheduling. However, parallelizability of the EnKF application depends on the number of ECLIPSE licenses and ideally, the number of licenses should be equal to the number of models in the ensemble. Therefore, if a single institution does not have sufficient number of licenses, the cluster availability doesn\"t help as much as it is expected. Since the collaborative environment such as TIGRE can address both compute and software resource requirements for the EnKF application, Grid-enabled approach is still advantageous over the conventional MPI implementation in any of the above scenarios. 5. CONCLUSIONS AND FUTURE WORK TIGRE is a higher education Grid development project and its purpose is to sustain and extend research and educational opportunities across Texas. Within the energy exploration application area, we have Grid-enabled the MPI implementation of the ensemble Kalman filter data assimilation methodology for reservoir characterization. This task was accomplished by removing MPI code for parallel processing and replacing with single processor jobs one for each geological model in the ensemble. These single processor jobs were scheduled across TIGRE via GridWay metascheduler. We have demonstrated that by pooling licenses across TIGRE sites, more geological models can be handled in parallel and therefore conceivably better simulation accuracy. This approach has several advantages over MPI implementation especially when a site specific cluster is heavily loaded and/or the number licenses required for the simulation is more than those available at a single site. Towards the future work, it would be interesting to compare the runtime between MPI, and Grid implementations for the EnKF application. This effort could shed light on quality of service (QoS) of Grid environments in comparison with cluster computing. Another aspect of interest in the near future would be managing both compute and license resources to address the job (or processor)-to-license ratio management. 6. OBSERVATIONS AND LESSIONS LEARNED The Grid-enabling efforts for EnKF application have provided ample opportunities to gather insights on the visibility and promise of Grid computing environments for application development and support. The main issues are industry standard data security and QoS comparable to cluster computing. Since the reservoir modeling research involves proprietary data of the field, we had to invest substantial efforts initially in educating the application researchers on the ability of Grid services in supporting the industry standard data security through role- and privilege-based access using X.509 standard. With respect to QoS, application researchers expect cluster level QoS with Grid environments. Also, there is a steep learning curve in Grid computing compared to the conventional cluster computing. Since Grid computing is still an emerging technology, and it spans over several administrative domains, Grid computing is still premature especially in terms of the level of QoS although, it offers better data security standards compared to commodity clusters. It is our observation that training and outreach programs that compare and contrast the Grid and cluster computing environments would be a suitable approach for enhancing user participation in Grid computing. This approach also helps users to match their applications and abilities Grids can offer. In summary, our efforts through TIGRE in Grid-enabling the EnKF data assimilation methodology showed substantial promise in engaging Petroleum Engineering researchers through intercampus collaborations. Efforts are under way to involve more schools in this effort. These efforts may result in increased collaborative research, educational opportunities, and workforce development through graduate/faculty research programs across TIGRE Institutions. 7. ACKNOWLEDGMENTS The authors acknowledge the State of Texas for supporting the TIGRE project through the Texas Enterprise Fund, and TIGRE Institutions for providing the mechanism, in which the authors (Ravi Vadapalli, Taesung Kim, and Ping Luo) are also participating. The authors thank the application researchers Prof. Akhil Datta-Gupta of Texas A&M University and Prof. Lloyd Heinze of Texas Tech University for their discussions and interest to exploit the TIGRE environment to extend opportunities in research and development.", "body1": "Grid computing is an emerging collaborative computing paradigm to extend institution/organization specific high performance computing (HPC) capabilities greatly beyond local resources. TIGRE is a project of the High Performance Computing across Texas (HiPCAT) consortium. The primary goal of TIGRE is to design and deploy state-of-the-art Grid middleware that enables integration of computing systems, storage systems and databases, visualization laboratories and displays, and even instruments and sensors across Texas. Section 2 provides an overview of the TIGRE environment and capabilities. The implementation details and merits of our approach are discussed in Section 4. The TIGRE Grid middleware consists of minimal set of components derived from a subset of the Virtual Data Toolkit (VDT) which supports a variety of operating systems. The pertinent details of TIGRE services and tools for job scheduling and management are provided below. 2.1. 2.2. 2.3. The GridWay is a light-weight metascheduler that fully utilizes Globus functionalities. 2.4. APPLICATION The main goal of hydrocarbon reservoir simulations is to forecast the production behavior of oil and gas field (denoted as field hereafter) for its development and optimal management. Figure 1. The EnKF is a Monte Carlo method that works with an ensemble of reservoir models. Figure 2. At START, the master/control process (EnKF main program) reads the simulation configuration file for number (N) of models, and model-specific input files. Typical EnKF simulation with N=50 and field histories of 50-60 years, in time steps ranging from three months to a year, takes about three weeks on a serial computing environment. In parallel computing environment, there is no interprocess communication between the geological models in the ensemble. ECLIPSE on site A ECLIPSE on Site B ECLIPSE on Site Z Collect N Model Outputs, Post-process Output files END . process collects model-specific output files, analyzes and prepares next set of input files for the simulation. Since MPI does not scale very well for applications that involve remote compute clusters, and to get around the firewall issues with license servers across administrative domains, Grid-enabling the EnKF workflow seems to be necessary. To Grid-enable the EnKF approach, we have eliminated the MPI code for parallel processing and replaced with N single processor jobs (or sub-jobs) where, N is the number of geological models in the ensemble. For each sub-job, we have constructed a GridWay job template that specifies the executable, input and output files, and resource requirements. Figure 3. The command-line features of GridWay were used to collect and process the model-specific outputs to prepare new set of input files. When a job is submitted to GridWay, it will go through a series of dispatch (DM) and execution (EM) states. We have demonstrated the Grid-enabled EnKF runs using GridWay for TIGRE environment. One of the main advantages of Grid-enabled EnKF simulation is that both the resources and licenses are released back to the pool at the end of each simulation time step unlike in the case of MPI implementation where licenses and nodes are locked until the completion of entire simulation. Scenario I: The cluster is heavily loaded. Scenario II: The cluster is relatively less loaded or largely available. TIGRE is a higher education Grid development project and its purpose is to sustain and extend research and educational opportunities across Texas. This task was accomplished by removing MPI code for parallel processing and replacing with single processor jobs one for each geological model in the ensemble. Another aspect of interest in the near future would be managing both compute and license resources to address the job (or processor)-to-license ratio management.", "body2": "The purpose of TIGRE is to create a higher education Grid to sustain and extend research and educational opportunities across Texas. The goal of HiPCAT is to support advanced computational technologies to enhance research, development, and educational activities. Within the energy exploration strategic application area, we have Grid-enabled the ensemble Kalman Filter (EnKF) approach for data assimilation in reservoir modeling and demonstrated the extensibility of the application using the TIGRE environment and the GridWay metascheduler. Application description and the need for Grid-enabling EnKF methodology is provided in Section 3. Finally, observations and lessons learned are documented in Section 6. The tools for handling Grid proxy generation, Grid-enabled file transfer and Grid-enabled remote login are supported. The pertinent details of TIGRE services and tools for job scheduling and management are provided below. This is accomplished through the Grid User Management System (GUMS) . The TIGRE portal documentation area provides a quick start tutorial on running jobs on TIGRE. In the present work, we have employed GridWay metascheduler - a Globus incubator project - to schedule and manage jobs across TIGRE. See section 4 for implementation details. The GPIR utilizes cron jobs on each resource to gather site specific resource characteristics such as jobs that are running, queued and waiting for resource allocation. For accurate performance forecasting of the field, it is necessary to reconcile several geological models to the dynamic response of the field through history matching . (Figure Ref:). The overall EnKF workflow is shown Figure 2. Each site has L licenses. The simulation continues until the production histories are exhausted. Typical EnKF simulation with N=50 and field histories of 50-60 years, in time steps ranging from three months to a year, takes about three weeks on a serial computing environment. . The number of licenses can be increased by involving several Petroleum Engineering Departments that support ECLIPSE package. Further details are provided in the next section. These model-specific sub-jobs were distributed across TIGRE sites that support ECLIPSE package using the GridWay metascheduler. Figure 3 represents the sub-job template file for the GridWay metascheduler. Finally, OUTPUT_FILES specifies the name and location where the output files are to be written. DM: Dispatch state, EM: Execution state, JID is the job id and HOST corresponds to site specific cluster and its local batch scheduler. For complete list of message flags and their descriptions, see the documentation in ref . For resource information, see Table I. There are two main scenarios for comparing Grid and cluster computing approaches. It is apparent that Grid scheduling is beneficial especially when cluster is heavily loaded and requested number of CPUs for the MPI job is not readily available. Since the collaborative environment such as TIGRE can address both compute and software resource requirements for the EnKF application, Grid-enabled approach is still advantageous over the conventional MPI implementation in any of the above scenarios. Within the energy exploration application area, we have Grid-enabled the MPI implementation of the ensemble Kalman filter data assimilation methodology for reservoir characterization. This effort could shed light on quality of service (QoS) of Grid environments in comparison with cluster computing. Another aspect of interest in the near future would be managing both compute and license resources to address the job (or processor)-to-license ratio management.", "introduction": "Grid computing is an emerging collaborative computing paradigm to extend institution/organization specific high performance computing (HPC) capabilities greatly beyond local resources. Its importance stems from the fact that ground breaking research in strategic application areas such as bioscience and medicine, energy exploration and environmental modeling involve strong interdisciplinary components and often require intercampus collaborations and computational capabilities beyond institutional limitations. The Texas Internet Grid for Research and Education (TIGRE) is a state funded cyberinfrastructure development project carried out by five (Rice, A&M, TTU, UH and UT Austin) major university systems - collectively called TIGRE Institutions. The purpose of TIGRE is to create a higher education Grid to sustain and extend research and educational opportunities across Texas. TIGRE is a project of the High Performance Computing across Texas (HiPCAT) consortium. The goal of HiPCAT is to support advanced computational technologies to enhance research, development, and educational activities. The primary goal of TIGRE is to design and deploy state-of-the-art Grid middleware that enables integration of computing systems, storage systems and databases, visualization laboratories and displays, and even instruments and sensors across Texas. The secondary goal is to demonstrate the TIGRE capabilities to enhance research and educational opportunities in strategic application areas of interest to the State of Texas. These are bioscience and medicine, energy exploration and air quality modeling. Vision of the TIGRE project is to foster interdisciplinary and intercampus collaborations, identify novel approaches to extend academic-government-private partnerships, and become a competitive model for external funding opportunities. The overall goal of TIGRE is to support local, campus and regional user interests and offer avenues to connect with national Grid projects such as Open Science Grid , and TeraGrid . Within the energy exploration strategic application area, we have Grid-enabled the ensemble Kalman Filter (EnKF) approach for data assimilation in reservoir modeling and demonstrated the extensibility of the application using the TIGRE environment and the GridWay metascheduler. Section 2 provides an overview of the TIGRE environment and capabilities. Application description and the need for Grid-enabling EnKF methodology is provided in Section 3. The implementation details and merits of our approach are discussed in Section 4. Conclusions are provided in Section 5. Finally, observations and lessons learned are documented in Section 6.", "conclusion": "LEARNED The Grid-enabling efforts for EnKF application have provided ample opportunities to gather insights on the visibility and promise of Grid computing environments for application development and support.. The main issues are industry standard data security and QoS comparable to cluster computing.. Since the reservoir modeling research involves proprietary data of the field, we had to invest substantial efforts initially in educating the application researchers on the ability of Grid services in supporting the industry standard data security through role- and privilege-based access using X.509 standard.. With respect to QoS, application researchers expect cluster level QoS with Grid environments.. Also, there is a steep learning curve in Grid computing compared to the conventional cluster computing.. Since Grid computing is still an emerging technology, and it spans over several administrative domains, Grid computing is still premature especially in terms of the level of QoS although, it offers better data security standards compared to commodity clusters.. It is our observation that training and outreach programs that compare and contrast the Grid and cluster computing environments would be a suitable approach for enhancing user participation in Grid computing.. This approach also helps users to match their applications and abilities Grids can offer.. In summary, our efforts through TIGRE in Grid-enabling the EnKF data assimilation methodology showed substantial promise in engaging Petroleum Engineering researchers through intercampus collaborations.. Efforts are under way to involve more schools in this effort.. These efforts may result in increased collaborative research, educational opportunities, and workforce development through graduate/faculty research programs across TIGRE Institutions.. ACKNOWLEDGMENTS The authors acknowledge the State of Texas for supporting the TIGRE project through the Texas Enterprise Fund, and TIGRE Institutions for providing the mechanism, in which the authors (Ravi Vadapalli, Taesung Kim, and Ping Luo) are also participating.. The authors thank the application researchers Prof. Akhil Datta-Gupta of Texas A&M University and Prof. Lloyd Heinze of Texas Tech University for their discussions and interest to exploit the TIGRE environment to extend opportunities in research and development."}
{"id": "C-72", "keywords": ["coordin spectrum sens", "gossip protocol", "fm aggreg", "increment algorithm"], "title": "GUESS: Gossiping Updates for Efficient Spectrum Sensing", "abstract": "Wireless radios of the future will likely be frequency-agile, that is, supporting opportunistic and adaptive use of the RF spectrum. Such radios must coordinate with each other to build an accurate and consistent map of spectral utilization in their surroundings. We focus on the problem of sharing RF spectrum data among a collection of wireless devices. The inherent requirements of such data and the time-granularity at which it must be collected makes this problem both interesting and technically challenging. We propose GUESS, a novel incremental gossiping approach to coordinated spectral sensing. It (1) reduces protocol overhead by limiting the amount of information exchanged between participating nodes, (2) is resilient to network alterations, due to node movement or node failures, and (3) allows exponentially-fast information convergence. We outline an initial solution incorporating these ideas and also show how our approach reduces network overhead by up to a factor of 2.4 and results in up to 2.7 times faster information convergence than alternative approaches.", "references": ["Unlicensed Operation in the TV Broadcast Bands and Additional Spectrum for Unlicensed Devices Below 900 MHz in the 3 GHz band", "In-Stat: Covering the Full Spectrum of Digital Communications Market Research, from Vendor to End-user", "Incremental Maintenance of Global Aggregates", "CORVUS: A Cognitive Radio Approach for Usage of Virtual Unlicensed Spectrum", "Implementation Issues in Spectrum Sensing for Cognitive Radios", "Spatially-Decaying Aggregation Over a Network: Model and Algorithms", "Probabilistic Counting Algorithms for Data Base Applications", "Random Walks in Peer-to-Peer Networks", "Previewing Intel's Cognitive Radio Chip", "Gossip-Based Computation of Aggregate Information", "Sensing-based Opportunistic Channel Access", "Search and Replication in Unstructured Peer-to-Peer Networks", "BRITE: an Approach to Universal Topology Generation", "Cooperative Sensing among Cognitive Radios", "Synopsis Diffusion for Robust Aggregation in Sensor Networks", "Fundamental Tradeoffs in Robust Spectrum Sensing for Opportunistic Frequency Reuse", "Distributed Coordination in Dynamic Spectrum Allocation Networks"], "full_text": "1. INTRODUCTION There has recently been a huge surge in the growth of wireless technology, driven primarily by the availability of unlicensed spectrum. However, this has come at the cost of increased RF interference, which has caused the Federal Communications Commission (FCC) in the United States to re-evaluate its strategy on spectrum allocation. Currently, the FCC has licensed RF spectrum to a variety of public and private institutions, termed primary users. New spectrum allocation regimes implemented by the FCC use dynamic spectrum access schemes to either negotiate or opportunistically allocate RF spectrum to unlicensed secondary users Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific D1 D2 D5 D3 D4 Primary User Shadowed Secondary Users Secondary Users detect Primary's Signal Shadowed Secondary User Figure 1: Without cooperation, shadowed users are not able to detect the presence of the primary user. that can use it when the primary user is absent. The second type of allocation scheme is termed opportunistic spectrum sharing. The FCC has already legislated this access method for the 5 GHz band and is also considering the same for TV broadcast bands . As a result, a new wave of intelligent radios, termed cognitive radios (or software defined radios), is emerging that can dynamically re-tune their radio parameters based on interactions with their surrounding environment. Under the new opportunistic allocation strategy, secondary users are obligated not to interfere with primary users (senders or receivers). This can be done by sensing the environment to detect the presence of primary users. However, local sensing is not always adequate, especially in cases where a secondary user is shadowed from a primary user, as illustrated in Figure 1. Here, coordination between secondary users is the only way for shadowed users to detect the primary. In general, cooperation improves sensing accuracy by an order of magnitude when compared to not cooperating at all . To realize this vision of dynamic spectrum access, two fundamental problems must be solved: (1) Efficient and coordinated spectrum sensing and (2) Distributed spectrum allocation. In this paper, we propose strategies for coordinated spectrum sensing that are low cost, operate on timescales comparable to the agility of the RF environment, and are resilient to network failures and alterations. We defer the problem of spectrum allocation to future work. Spectrum sensing techniques for cognitive radio networks are broadly classified into three regimes; (1) centralized coordinated techniques, (2) decentralized coordinated techniques, and (3) decentralized uncoordinated techniques. We advocate a decentralized coordinated approach, similar in spirit to OSPF link-state routing used in the Internet. This is more effective than uncoordinated approaches because making decisions based only on local information is fallible (as shown in Figure 1). Moreover, compared to cen12 tralized approaches, decentralized techniques are more scalable, robust, and resistant to network failures and security attacks (e.g. jamming). Coordinating sensory data between cognitive radio devices is technically challenging because accurately assessing spectrum usage requires exchanging potentially large amounts of data with many radios at very short time scales. Data size grows rapidly due to the large number (i.e. thousands) of spectrum bands that must be scanned. This data must also be exchanged between potentially hundreds of neighboring secondary users at short time scales, to account for rapid changes in the RF environment. This paper presents GUESS, a novel approach to coordinated spectrum sensing for cognitive radio networks. Our approach is motivated by the following key observations: 1. Low-cost sensors collect approximate data: Most devices have limited sensing resolution because they are low-cost and low duty-cycle devices and thus cannot perform complex RF signal processing (e.g. matched filtering). Many are typically equipped with simple energy detectors that gather only approximate information. 2. Approximate summaries are sufficient for coordination: Approximate statistical summaries of sensed data are sufficient for correlating sensed information between radios, as relative usage information is more important than absolute usage data. Thus, exchanging exact RF information may not be necessary, and more importantly, too costly for the purposes of spectrum sensing. 3. RF spectrum changes incrementally: On most bands, RF spectrum utilization changes infrequently. Moreover, utilization of a specific RF band affects only that band and not the entire spectrum. Therefore, if the usage pattern of a particular band changes substantially, nodes detecting that change can initiate an update protocol to update the information for that band alone, leaving in place information already collected for other bands. This allows rapid detection of change while saving the overhead of exchanging unnecessary information. Based on these observations, GUESS makes the following contributions: 1. A novel approach that applies randomized gossiping algorithms to the problem of coordinated spectrum sensing. These algorithms are well suited to coordinated spectrum sensing due to the unique characteristics of the problem: i.e. radios are power-limited, mobile and have limited bandwidth to support spectrum sensing capabilities. 2. An application of in-network aggregation for dissemination of spectrum summaries. We argue that approximate summaries are adequate for performing accurate radio parameter tuning. 3. An extension of in-network aggregation and randomized gossiping to support incremental maintenance of spectrum summaries. Compared to standard gossiping approaches, incremental techniques can further reduce overhead and protocol execution time by requiring fewer radio resources. The rest of the paper is organized as follows. Section 2 motivates the need for a low cost and efficient approach to coordinated spectrum sensing. Section 3 discusses related work in the area, while Section 4 provides a background on in-network aggregation and randomized gossiping. Sections 5 and 6 discuss extensions and protocol details of these techniques for coordinated spectrum sensing. Section 7 presents simulation results showcasing the benefits of GUESS, and Section 8 presents a discussion and some directions for future work. 2. MOTIVATION To estimate the scale of the problem, In-stat predicts that the number of WiFi- . Therefore, it would be reasonable to assume that a typical dense urban environment will contain several thousand cognitive radio devices in range of each other. As a result, distributed spectrum sensing and allocation would become both important and fundamental. Coordinated sensing among secondary radios is essential due to limited device sensing resolution and physical RF effects such as shadowing. Cabric et al. illustrate the gains from cooperation and show an order of magnitude reduction in the probability of interference with the primary user when only a small fraction of secondary users cooperate. However, such coordination is non-trivial due to: (1) the limited bandwidth available for coordination, (2) the need to communicate this information on short timescales, and (3) the large amount of sensory data that needs to be exchanged. Limited Bandwidth: Due to restrictions of cost and power, most devices will likely not have dedicated hardware for supporting coordination. This implies that both data and sensory traffic will need to be time-multiplexed onto a single radio interface. Therefore, any time spent communicating sensory information takes away from the device\"s ability to perform its intended function. Thus, any such coordination must incur minimal network overhead. Short Timescales: Further compounding the problem is the need to immediately propagate updated RF sensory data, in order to allow devices to react to it in a timely fashion. This is especially true due to mobility, as rapid changes of the RF environment can occur due to device and obstacle movements. Here, fading and multi-path interference heavily impact sensing abilities. Signal level can drop to a deep null with just a \u03bb/4 movement in receiver position (3.7 cm at 2 GHz), where \u03bb is the wavelength . Coordination which does not support rapid dissemination of information will not be able to account for such RF variations. Large Sensory Data: Because cognitive radios can potentially use any part of the RF spectrum, there will be numerous channels that they need to scan. Suppose we wish to compute the average signal energy in each of 100 discretized frequency bands, and each signal can have up to 128 discrete energy levels. Exchanging complete sensory information between nodes would require 700 bits per transmission (for 100 channels, each requiring seven bits of information). Exchanging this information among even a small group of 50 devices each second would require (50 time-steps \u00d7 50 devices \u00d7 700 bits per transmission) = 1.67 Mbps of aggregate network bandwidth. Contrast this to the use of a randomized gossip protocol to disseminate such information, and the use of FM bit vectors to perform in-network aggregation. By applying gossip and FM aggregation, aggregate bandwidth requirements drop to (c\u00b7logN time-steps \u00d7 50 devices \u00d7 700 bits per transmission) = 0.40 Mbps, since 12 time-steps are needed to propagate the data (with c = 2, for illustrative purpoes1 ). This is explained further in Section 4. Based on these insights, we propose GUESS, a low-overhead approach which uses incremental extensions to FM aggregation and randomized gossiping for efficient coordination within a cognitive radio network. As we show in Section 7, Convergence time is correlated with the connectivity topology of the devices, which in turn depends on the environment. 13 Figure 2: Using FM aggregation to compute average signal level measured by a group of devices. these incremental extensions can further reduce bandwidth requirements by up to a factor of 2.4 over the standard approaches discussed above. 3. RELATED WORK Research in cognitive radio has increased rapidly over the years, and it is being projected as one of the leading enabling technologies for wireless networks of the future . As mentioned earlier, the FCC has already identified new regimes for spectrum sharing between primary users and secondary users and a variety of systems have been proposed in the literature to support such sharing . Detecting the presence of a primary user is non-trivial, especially a legacy primary user that is not cognitive radio aware. Secondary users must be able to detect the primary even if they cannot properly decode its signals. This has been shown by Sahai et al. to be extremely difficult even if the modulation scheme is known. Sophisticated and costly hardware, beyond a simple energy detector, is required to improve signal detection accuracy . Moreover, a shadowed secondary user may not even be able to detect signals from the primary. As a result, simple local sensing approaches have not gained much momentum. This has motivated the need for cooperation among cognitive radios . More recently, some researchers have proposed approaches for radio coordination. Liu et al. consider a centralized access point (or base station) architecture in which sensing information is forwarded to APs for spectrum allocation purposes. APs direct mobile clients to collect such sensing information on their behalf. However, due to the need of a fixed AP infrastructure, such a centralized approach is clearly not scalable. In other work, Zhao et al. propose a distributed coordination approach for spectrum sensing and allocation. Cognitive radios organize into clusters and coordination occurs within clusters. The CORVUS architecture proposes a similar clustering method that can use either a centralized or decentralized approach to manage clusters. Although an improvement over purely centralized approaches, these techniques still require a setup phase to generate the clusters, which not only adds additional delay, but also requires many of the secondary users to be static or quasi-static. In contrast, GUESS does not place such restrictions on secondary users, and can even function in highly mobile environments. 4. BACKGROUND This section provides the background for our approach. We present the FM aggregation scheme that we use to generate spectrum summaries and perform in-network aggregation. We also discuss randomized gossiping techniques for disseminating aggregates in a cognitive radio network. 4.1 FM Aggregation Aggregation is the process where nodes in a distributed network combine data received from neighboring nodes with their local value to generate a combined aggregate. This aggregate is then communicated to other nodes in the network and this process repeats until the aggregate at all nodes has converged to the same value, i.e. the global aggregate. Double-counting is a well known problem in this process, where nodes may contribute more than once to the aggregate, causing inaccuracy in the final result. Intuitively, nodes can tag the aggregate value they transmit with information about which nodes have contributed to it. However, this approach is not scalable. Order and Duplicate Insensitive (ODI) techniques have been proposed in the literature . We adopt the ODI approach pioneered by Flajolet and Martin (FM) for the purposes of aggregation. Next we outline the FM approach; for full details, see . Suppose we want to compute the number of nodes in the network, i.e. the COUNT query. To do so, each node performs a coin toss experiment as follows: toss an unbiased coin, stopping after the first head is seen. The node then sets the ith bit in a bit vector (initially filled with zeros), where i is the number of coin tosses it performed. The intuition is that as the number of nodes doing coin toss experiments increases, the probability of a more significant bit being set in one of the nodes\" bit vectors increases. These bit vectors are then exchanged among nodes. When a node receives a bit vector, it updates its local bit vector by bitwise OR-ing it with the received vector (as shown in Figure 2 which computes AVERAGE). At the end of the aggregation process, every node, with high probability, has the same bit vector. The actual value of the count aggregate is then computed using the following formula, AGGF M = 2j\u22121 /1, where j represents the bit position of the least significant zero in the aggregate bit vector . Although such aggregates are very compact in nature, requiring only O(logN) state space (where N is the number of nodes), they may not be very accurate as they can only approximate values to the closest power of 2, potentially causing errors of up to 50%. More accurate aggregates can be computed by maintaining multiple bit vectors at each node, as explained in . This decreases the error to within O(1/ m), where m is the number of such bit vectors. Queries other than count can also be computed using variants of this basic counting algorithm, as discussed in (and shown in Figure 2). Transmitting FM bit vectors between nodes is done using randomized gossiping, discussed next. 4.2 Gossip Protocols Gossip-based protocols operate in discrete time-steps; a time-step is the required amount of time for all transmissions in that time-step to complete. At every time-step, each node having something to send randomly selects one or more neighboring nodes and transmits its data to them. The randomized propagation of information provides fault-tolerance and resilience to network failures and outages. We emphasize that this characteristic of the protocol also allows it to operate without relying on any underlying network structure. Gossip protocols have been shown to provide exponentially fast convergence2 , on the order of O(log N) , where N is the number of nodes (or radios). These protocols can therefore easily scale to very dense environments. Convergence refers to the state in which all nodes have the most up-to-date view of the network. 14 Two types of gossip protocols are: \u2022 Uniform Gossip: In uniform gossip, at each timestep, each node chooses a random neighbor and sends its data to it. This process repeats for O(log(N)) steps (where N is the number of nodes in the network). Uniform gossip provides exponentially fast convergence, with low network overhead . \u2022 Random Walk: In random walk, only a subset of the nodes (termed designated nodes) communicate in a particular time-step. At startup, k nodes are randomly elected as designated nodes. In each time-step, each designated node sends its data to a random neighbor, which becomes designated for the subsequent timestep (much like passing a token). This process repeats until the aggregate has converged in the network. Random walk has been shown to provide similar convergence bounds as uniform gossip in problems of similar context . 5. INCREMENTAL PROTOCOLS 5.1 Incremental FM Aggregates One limitation of FM aggregation is that it does not support updates. Due to the probabilistic nature of FM, once bit vectors have been ORed together, information cannot simply be removed from them as each node\"s contribution has not been recorded. We propose the use of delete vectors, an extension of FM to support updates. We maintain a separate aggregate delete vector whose value is subtracted from the original aggregate vector\"s value to obtain the resulting value as follows. AGGINC = (2a\u22121 /1) \u2212 (2b\u22121 /1) (1) Here, a and b represent the bit positions of the least significant zero in the original and delete bit vectors respectively. Suppose we wish to compute the average signal level detected in a particular frequency. To compute this, we compute the SUM of all signal level measurements and divide that by the COUNT of the number of measurements. A SUM aggregate is computed similar to COUNT (explained in Section 4.1), except that each node performs s coin toss experiments, where s is the locally measured signal level. Figure 2 illustrates the sequence by which the average signal energy is computed in a particular band using FM aggregation. Now suppose that the measured signal at a node changes from s to s . The vectors are updated as follows. \u2022 s > s: We simply perform (s \u2212 s) more coin toss experiments and bitwise OR the result with the original bit vector. \u2022 s < s: We increase the value of the delete vector by performing (s \u2212 s ) coin toss experiments and bitwise OR the result with the current delete vector. Using delete vectors, we can now support updates to the measured signal level. With the original implementation of FM, the aggregate would need to be discarded and a new one recomputed every time an update occurred. Thus, delete vectors provide a low overhead alternative for applications whose data changes incrementally, such as signal level measurements in a coordinated spectrum sensing environment. Next we discuss how these aggregates can be communicated between devices using incremental routing protocols. 5.2 Incremental Routing Protocol We use the following incremental variants of the routing protocols presented in Section 4.2 to support incremental updates to previously computed aggregates. Update Received OR Local Update Occurs Recovered Susceptible Time-stamp Expires Initial State Additional Update Received Infectious Clean Up Figure 3: State diagram each device passes through as updates proceed in the system \u2022 Incremental Gossip Protocol (IGP): When an update occurs, the updated node initiates the gossiping procedure. Other nodes only begin gossiping once they receive the update. Therefore, nodes receiving the update become active and continue communicating with their neighbors until the update protocol terminates, after O(log(N)) time steps. \u2022 Incremental Random Walk Protocol (IRWP): When an update (or updates) occur in the system, instead of starting random walks at k random nodes in the network, all k random walks are initiated from the updated node(s). The rest of the protocol proceeds in the same fashion as the standard random walk protocol. The allocation of walks to updates is discussed in more detail in , where the authors show that the number of walks has an almost negligible impact on network overhead. 6. PROTOCOL DETAILS Using incremental routing protocols to disseminate incremental FM aggregates is a natural fit for the problem of coordinated spectrum sensing. Here we outline the implementation of such techniques for a cognitive radio network. We continue with the example from Section 5.1, where we wish to perform coordination between a group of wireless devices to compute the average signal level in a particular frequency band. Using either incremental random walk or incremental gossip, each device proceeds through three phases, in order to determine the global average signal level for a particular frequency band. Figure 3 shows a state diagram of these phases. Susceptible: Each device starts in the susceptible state and becomes infectious only when its locally measured signal level changes, or if it receives an update message from a neighboring device. If a local change is observed, the device updates either the original or delete bit vector, as described in Section 5.1, and moves into the infectious state. If it receives an update message, it ORs the received original and delete bit vectors with its local bit vectors and moves into the infectious state. Note, because signal level measurements may change sporadically over time, a smoothing function, such as an exponentially weighted moving average, should be applied to these measurements. Infectious: Once a device is infectious it continues to send its up-to-date bit vectors, using either incremental random walk or incremental gossip, to neighboring nodes. Due to FM\"s order and duplicate insensitive (ODI) properties, simultaneously occurring updates are handled seamlessly by the protocol. Update messages contain a time stamp indicating when the update was generated, 1 10 100 Number of Measured Signal Changes Executiontime(ms) Incremental Gossip Uniform Gossip (a) 1 10 100 Number of Measured Signal Changes ExecutionTime(ms). Incremental Random Walk Random Walk (b) 1 10 100 Number of Measured Signal Changes ExecutionTime(ms). Random Walk Incremental Random Walk (c) Incremental Random Walk and Random Walk on Power-Law Random Graph Figure 4: Execution times of Incremental Protocols 0.9 1.4 1.9 2.4 2.9 1 10 100 Number of Measured Signal Changes OverheadImprovementRatio. (NormalizedtoUniformGossip) Incremental Gossip Uniform Gossip (a) Incremental Gossip and Uniform Gossip on Clique 0.9 1.4 1.9 2.4 2.9 1 10 100 Number of Measured Signal Changes OverheadImprovementRatio. (NormalizedtoRandomWalk) Incremental Random Walk Random Walk (b) Incremental Random Walk and Random Walk on Clique 0.9 1.1 1.3 1.5 1.7 1.9 1 10 100 Number of Measured Signal Changes OverheadImprovementRatio. (NormalizedtoRandomWalk) Random Walk Incremental Random Walk (c) Incremental Random Walk and Random Walk on Power-Law Random Graph Figure 5: Network overhead of Incremental Protocols cal time stamp of when it received the most recent update. Using this information, a device moves into the recovered state once enough time has passed for the most recent update to have converged. As discussed in Section 4.2, this happens after O(log(N)) time steps. Recovered: A recovered device ceases to propagate any update information. At this point, it performs clean-up and prepares for the next infection by entering the susceptible state. Once all devices have entered the recovered state, the system will have converged, and with high probability, all devices will have the up-to-date average signal level. Due to the cumulative nature of FM, even if all devices have not converged, the next update will include all previous updates. Nevertheless, the probability that gossip fails to converge is small, and has been shown to be O(1/N) . For coordinated spectrum sensing, non-incremental routing protocols can be implemented in a similar fashion. Random walk would operate by having devices periodically drop the aggregate and re-run the protocol. Each device would perform a coin toss (biased on the number of walks) to determine whether or not it is a designated node. This is different from the protocol discussed above where only updated nodes initiate random walks. Similar techniques can be used to implement standard gossip. 7. EVALUATION We now provide a preliminary evaluation of GUESS in simulation. A more detailed evaluation of this approach can be found in . Here we focus on how incremental extensions to gossip protocols can lead to further improvements over standard gossiping techniques, for the problem of coordinated spectrum sensing. Simulation Setup: We implemented a custom simulator in C++. We study the improvements of our incremental gossip protocols over standard gossiping in two dimensions: execution time and network overhead. We use two topologies to represent device connectivity: a clique, to eliminate the effects of the underlying topology on protocol performance, and a BRITE-generated power-law random graph (PLRG), to illustrate how our results extend to more realistic scenarios. We simulate a large deployment of 1,000 devices to analyze protocol scalability. In our simulations, we compute the average signal level in a particular band by disseminating FM bit vectors. In each run of the simulation, we induce a change in the measured signal at one or more devices. A run ends when the new average signal level has converged in the network. For each data point, we ran 100 simulations and 95% confidence intervals (error bars) are shown. Simulation Parameters: Each transmission involves sending 70 bits of information to a neighboring node. To compute the AVERAGE aggregate, four bit vectors need to be transmitted: the original SUM vector, the SUM delete vector, the original COUNT vector, and the COUNT delete vector. Non-incremental protocols do not transmit the delete vectors. Each transmission also includes a time stamp of when the update was generated. We assume nodes communicate on a common control channel at 2 Mbps. Therefore, one time-step of protocol execution corresponds to the time required for 1,000 nodes to sequentially send 70 bits at 2 Mbps. Sequential use of the control channel is a worst case for our protocols; in practice, multiple control channels could be used in parallel to reduce execution time. We also assume nodes are loosely time synchronized, the implications of which are discussed further in . Finally, in order to isolate the effect of protocol operation on performance, we do not model the complexities of the wireless channel in our simulations. Incremental Protocols Reduce Execution Time: Figure 4(a) compares the performance of incremental gossip (IGP) with uniform gossip on a clique topology. We observe that both protocols have almost identical execution times. This is expected as IGP operates in a similar fashion to 16 uniform gossip, taking O(log(N)) time-steps to converge. Figure 4(b) compares the execution times of incremental random walk (IRWP) and standard random walk on a clique. IRWP reduces execution time by a factor of 2.7 for a small number of measured signal changes. Although random walk and IRWP both use k random walks (in our simulations k = number of nodes), IRWP initiates walks only from updated nodes (as explained in Section 5.2), resulting in faster information convergence. These improvements carry over to a PLRG topology as well (as shown in Figure 4(c)), where IRWP is 1.33 times faster than random walk. Incremental Protocols Reduce Network Overhead: Figure 5(a) shows the ratio of data transmitted using uniform gossip relative to incremental gossip on a clique. For a small number of signal changes, incremental gossip incurs 2.4 times less overhead than uniform gossip. This is because in the early steps of protocol execution, only devices which detect signal changes communicate. As more signal changes are introduced into the system, gossip and incremental gossip incur approximately the same overhead. Similarly, incremental random walk (IRWP) incurs much less overhead than standard random walk. Figure 5(b) shows a 2.7 fold reduction in overhead for small numbers of signal changes on a clique. Although each protocol uses the same number of random walks, IRWP uses fewer network resources than random walk because it takes less time to converge. This improvement also holds true on more complex PLRG topologies (as shown in Figure 5(c)), where we observe a 33% reduction in network overhead. From these results it is clear that incremental techniques yield significant improvements over standard approaches to gossip, even on complex topologies. Because spectrum utilization is characterized by incremental changes to usage, incremental protocols are ideally suited to solve this problem in an efficient and cost effective manner. 8. DISCUSSION AND FUTURE WORK We have only just scratched the surface in addressing the problem of coordinated spectrum sensing using incremental gossiping. Next, we outline some open areas of research. Spatial Decay: Devices performing coordinated sensing are primarily interested in the spectrum usage of their local neighborhood. Therefore, we recommend the use of spatially decaying aggregates , which limits the impact of an update on more distant nodes. Spatially decaying aggregates work by successively reducing (by means of a decay function) the value of the update as it propagates further from its origin. One challenge with this approach is that propagation distance cannot be determined ahead of time and more importantly, exhibits spatio-temporal variations. Therefore, finding the optimal decay function is non-trivial, and an interesting subject of future work. Significance Threshold: RF spectrum bands continually experience small-scale changes which may not necessarily be significant. Deciding if a change is significant can be done using a significance threshold \u03b2, below which any observed change is not propagated by the node. Choosing an appropriate operating value for \u03b2 is application dependent, and explored further in . Weighted Readings: Although we argued that most devices will likely be equipped with low-cost sensing equipment, there may be situations where there are some special infrastructure nodes that have better sensing abilities than others. Weighting their measurements more heavily could be used to maintain a higher degree of accuracy. Determining how to assign such weights is an open area of research. Implementation Specifics: Finally, implementing gossip for coordinated spectrum sensing is also open. If implemented at the MAC layer, it may be feasible to piggy-back gossip messages over existing management frames (e.g. networking advertisement messages). As well, we also require the use of a control channel to disseminate sensing information. There are a variety of alternatives for implementing such a channel, some of which are outlined in . The trade-offs of different approaches to implementing GUESS is a subject of future work. 9. CONCLUSION Spectrum sensing is a key requirement for dynamic spectrum allocation in cognitive radio networks. The nature of the RF environment necessitates coordination between cognitive radio devices. We propose GUESS, an approximate yet low overhead approach to perform efficient coordination between cognitive radios. The fundamental contributions of GUESS are: (1) an FM aggregation scheme for efficient innetwork aggregation, (2) a randomized gossiping approach which provides exponentially fast convergence and robustness to network alterations, and (3) incremental variations of FM and gossip which we show can reduce the communication time by up to a factor of 2.7 and reduce network overhead by up to a factor of 2.4. Our preliminary simulation results showcase the benefits of this approach and we also outline a set of open problems that make this a new and exciting area of research.", "body1": "There has recently been a huge surge in the growth of wireless technology, driven primarily by the availability of unlicensed spectrum. that can use it when the primary user is absent. However, local sensing is not always adequate, especially in cases where a secondary user is shadowed from a primary user, as illustrated in Figure 1. To realize this vision of dynamic spectrum access, two fundamental problems must be solved: (1) Efficient and coordinated spectrum sensing and (2) Distributed spectrum allocation. We advocate a decentralized coordinated approach, similar in spirit to OSPF link-state routing used in the Internet. This is more effective than uncoordinated approaches because making decisions based only on local information is fallible (as shown in Figure 1). Coordinating sensory data between cognitive radio devices is technically challenging because accurately assessing spectrum usage requires exchanging potentially large amounts of data with many radios at very short time scales. This paper presents GUESS, a novel approach to coordinated spectrum sensing for cognitive radio networks. 2. Moreover, utilization of a specific RF band affects only that band and not the entire spectrum. Based on these observations, GUESS makes the following contributions: 1. 3. The rest of the paper is organized as follows. To estimate the scale of the problem, In-stat predicts that the number of WiFi- . Coordinated sensing among secondary radios is essential due to limited device sensing resolution and physical RF effects such as shadowing. Limited Bandwidth: Due to restrictions of cost and power, most devices will likely not have dedicated hardware for supporting coordination. Short Timescales: Further compounding the problem is the need to immediately propagate updated RF sensory data, in order to allow devices to react to it in a timely fashion. Large Sensory Data: Because cognitive radios can potentially use any part of the RF spectrum, there will be numerous channels that they need to scan. Contrast this to the use of a randomized gossip protocol to disseminate such information, and the use of FM bit vectors to perform in-network aggregation. 13 Figure 2: Using FM aggregation to compute average signal level measured by a group of devices. these incremental extensions can further reduce bandwidth requirements by up to a factor of 2.4 over the standard approaches discussed above. Research in cognitive radio has increased rapidly over the years, and it is being projected as one of the leading enabling technologies for wireless networks of the future . Detecting the presence of a primary user is non-trivial, especially a legacy primary user that is not cognitive radio aware. More recently, some researchers have proposed approaches for radio coordination. Cognitive radios organize into clusters and coordination occurs within clusters. This section provides the background for our approach. 4.1 FM Aggregation Aggregation is the process where nodes in a distributed network combine data received from neighboring nodes with their local value to generate a combined aggregate. Suppose we want to compute the number of nodes in the network, i.e. These bit vectors are then exchanged among nodes. Although such aggregates are very compact in nature, requiring only O(logN) state space (where N is the number of nodes), they may not be very accurate as they can only approximate values to the closest power of 2, potentially causing errors of up to 50%. 4.2 Gossip Protocols Gossip-based protocols operate in discrete time-steps; a time-step is the required amount of time for all transmissions in that time-step to complete. 14 Two types of gossip protocols are: \u2022 Uniform Gossip: In uniform gossip, at each timestep, each node chooses a random neighbor and sends its data to it. Uniform gossip provides exponentially fast convergence, with low network overhead . \u2022 Random Walk: In random walk, only a subset of the nodes (termed designated nodes) communicate in a particular time-step. 5.1 Incremental FM Aggregates One limitation of FM aggregation is that it does not support updates. Suppose we wish to compute the average signal level detected in a particular frequency. Now suppose that the measured signal at a node changes from s to s . \u2022 s > s: We simply perform (s \u2212 s) more coin toss experiments and bitwise OR the result with the original bit vector. \u2022 s < s: We increase the value of the delete vector by performing (s \u2212 s ) coin toss experiments and bitwise OR the result with the current delete vector. Using delete vectors, we can now support updates to the measured signal level. 5.2 Incremental Routing Protocol We use the following incremental variants of the routing protocols presented in Section 4.2 to support incremental updates to previously computed aggregates. Update Received OR Local Update Occurs Recovered Susceptible Time-stamp Expires Initial State Additional Update Received Infectious Clean Up Figure 3: State diagram each device passes through as updates proceed in the system \u2022 Incremental Gossip Protocol (IGP): When an update occurs, the updated node initiates the gossiping procedure. \u2022 Incremental Random Walk Protocol (IRWP): When an update (or updates) occur in the system, instead of starting random walks at k random nodes in the network, all k random walks are initiated from the updated node(s). Using incremental routing protocols to disseminate incremental FM aggregates is a natural fit for the problem of coordinated spectrum sensing. Using either incremental random walk or incremental gossip, each device proceeds through three phases, in order to determine the global average signal level for a particular frequency band. Susceptible: Each device starts in the susceptible state and becomes infectious only when its locally measured signal level changes, or if it receives an update message from a neighboring device. Infectious: Once a device is infectious it continues to send its up-to-date bit vectors, using either incremental random walk or incremental gossip, to neighboring nodes. Update messages contain a time stamp indicating when the update was generated, 1 10 100 Number of Measured Signal Changes Executiontime(ms) Incremental Gossip Uniform Gossip (a) 1 10 100 Number of Measured Signal Changes ExecutionTime(ms). Incremental Random Walk Random Walk (b) 1 10 100 Number of Measured Signal Changes ExecutionTime(ms). Random Walk Incremental Random Walk (c) Incremental Random Walk and Random Walk on Power-Law Random Graph Figure 4: Execution times of Incremental Protocols 0.9 1.4 1.9 2.4 2.9 1 10 100 Number of Measured Signal Changes OverheadImprovementRatio. (NormalizedtoUniformGossip) Incremental Gossip Uniform Gossip (a) Incremental Gossip and Uniform Gossip on Clique 0.9 1.4 1.9 2.4 2.9 1 10 100 Number of Measured Signal Changes OverheadImprovementRatio. (NormalizedtoRandomWalk) Incremental Random Walk Random Walk (b) Incremental Random Walk and Random Walk on Clique 0.9 1.1 1.3 1.5 1.7 1.9 1 10 100 Number of Measured Signal Changes OverheadImprovementRatio. (NormalizedtoRandomWalk) Random Walk Incremental Random Walk (c) Incremental Random Walk and Random Walk on Power-Law Random Graph Figure 5: Network overhead of Incremental Protocols cal time stamp of when it received the most recent update. Using this information, a device moves into the recovered state once enough time has passed for the most recent update to have converged. Recovered: A recovered device ceases to propagate any update information. For coordinated spectrum sensing, non-incremental routing protocols can be implemented in a similar fashion. Random walk would operate by having devices periodically drop the aggregate and re-run the protocol. We now provide a preliminary evaluation of GUESS in simulation. Simulation Setup: We implemented a custom simulator in C++. In our simulations, we compute the average signal level in a particular band by disseminating FM bit vectors. Simulation Parameters: Each transmission involves sending 70 bits of information to a neighboring node. We assume nodes communicate on a common control channel at 2 Mbps. This is expected as IGP operates in a similar fashion to 16 uniform gossip, taking O(log(N)) time-steps to converge. Figure 4(b) compares the execution times of incremental random walk (IRWP) and standard random walk on a clique. Incremental Protocols Reduce Network Overhead: Figure 5(a) shows the ratio of data transmitted using uniform gossip relative to incremental gossip on a clique. Similarly, incremental random walk (IRWP) incurs much less overhead than standard random walk.", "body2": "To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific D1 D2 D5 D3 D4 Primary User Shadowed Secondary Users Secondary Users detect Primary's Signal Shadowed Secondary User Figure 1: Without cooperation, shadowed users are not able to detect the presence of the primary user. This can be done by sensing the environment to detect the presence of primary users. In general, cooperation improves sensing accuracy by an order of magnitude when compared to not cooperating at all . Spectrum sensing techniques for cognitive radio networks are broadly classified into three regimes; (1) centralized coordinated techniques, (2) decentralized coordinated techniques, and (3) decentralized uncoordinated techniques. We advocate a decentralized coordinated approach, similar in spirit to OSPF link-state routing used in the Internet. jamming). This data must also be exchanged between potentially hundreds of neighboring secondary users at short time scales, to account for rapid changes in the RF environment. Many are typically equipped with simple energy detectors that gather only approximate information. RF spectrum changes incrementally: On most bands, RF spectrum utilization changes infrequently. This allows rapid detection of change while saving the overhead of exchanging unnecessary information. radios are power-limited, mobile and have limited bandwidth to support spectrum sensing capabilities. We argue that approximate summaries are adequate for performing accurate radio parameter tuning. Compared to standard gossiping approaches, incremental techniques can further reduce overhead and protocol execution time by requiring fewer radio resources. Section 7 presents simulation results showcasing the benefits of GUESS, and Section 8 presents a discussion and some directions for future work. As a result, distributed spectrum sensing and allocation would become both important and fundamental. However, such coordination is non-trivial due to: (1) the limited bandwidth available for coordination, (2) the need to communicate this information on short timescales, and (3) the large amount of sensory data that needs to be exchanged. Thus, any such coordination must incur minimal network overhead. Coordination which does not support rapid dissemination of information will not be able to account for such RF variations. Exchanging this information among even a small group of 50 devices each second would require (50 time-steps \u00d7 50 devices \u00d7 700 bits per transmission) = 1.67 Mbps of aggregate network bandwidth. As we show in Section 7, Convergence time is correlated with the connectivity topology of the devices, which in turn depends on the environment. 13 Figure 2: Using FM aggregation to compute average signal level measured by a group of devices. these incremental extensions can further reduce bandwidth requirements by up to a factor of 2.4 over the standard approaches discussed above. As mentioned earlier, the FCC has already identified new regimes for spectrum sharing between primary users and secondary users and a variety of systems have been proposed in the literature to support such sharing . This has motivated the need for cooperation among cognitive radios . propose a distributed coordination approach for spectrum sensing and allocation. In contrast, GUESS does not place such restrictions on secondary users, and can even function in highly mobile environments. We also discuss randomized gossiping techniques for disseminating aggregates in a cognitive radio network. Next we outline the FM approach; for full details, see . The intuition is that as the number of nodes doing coin toss experiments increases, the probability of a more significant bit being set in one of the nodes\" bit vectors increases. The actual value of the count aggregate is then computed using the following formula, AGGF M = 2j\u22121 /1, where j represents the bit position of the least significant zero in the aggregate bit vector . Transmitting FM bit vectors between nodes is done using randomized gossiping, discussed next. Convergence refers to the state in which all nodes have the most up-to-date view of the network. This process repeats for O(log(N)) steps (where N is the number of nodes in the network). Uniform gossip provides exponentially fast convergence, with low network overhead . Random walk has been shown to provide similar convergence bounds as uniform gossip in problems of similar context . AGGINC = (2a\u22121 /1) \u2212 (2b\u22121 /1) (1) Here, a and b represent the bit positions of the least significant zero in the original and delete bit vectors respectively. Figure 2 illustrates the sequence by which the average signal energy is computed in a particular band using FM aggregation. The vectors are updated as follows. \u2022 s > s: We simply perform (s \u2212 s) more coin toss experiments and bitwise OR the result with the original bit vector. \u2022 s < s: We increase the value of the delete vector by performing (s \u2212 s ) coin toss experiments and bitwise OR the result with the current delete vector. Next we discuss how these aggregates can be communicated between devices using incremental routing protocols. 5.2 Incremental Routing Protocol We use the following incremental variants of the routing protocols presented in Section 4.2 to support incremental updates to previously computed aggregates. Therefore, nodes receiving the update become active and continue communicating with their neighbors until the update protocol terminates, after O(log(N)) time steps. The allocation of walks to updates is discussed in more detail in , where the authors show that the number of walks has an almost negligible impact on network overhead. We continue with the example from Section 5.1, where we wish to perform coordination between a group of wireless devices to compute the average signal level in a particular frequency band. Figure 3 shows a state diagram of these phases. Note, because signal level measurements may change sporadically over time, a smoothing function, such as an exponentially weighted moving average, should be applied to these measurements. Due to FM\"s order and duplicate insensitive (ODI) properties, simultaneously occurring updates are handled seamlessly by the protocol. Update messages contain a time stamp indicating when the update was generated, 1 10 100 Number of Measured Signal Changes Executiontime(ms) Incremental Gossip Uniform Gossip (a) 1 10 100 Number of Measured Signal Changes ExecutionTime(ms). Incremental Random Walk Random Walk (b) 1 10 100 Number of Measured Signal Changes ExecutionTime(ms). Random Walk Incremental Random Walk (c) Incremental Random Walk and Random Walk on Power-Law Random Graph Figure 4: Execution times of Incremental Protocols 0.9 1.4 1.9 2.4 2.9 1 10 100 Number of Measured Signal Changes OverheadImprovementRatio. (NormalizedtoUniformGossip) Incremental Gossip Uniform Gossip (a) Incremental Gossip and Uniform Gossip on Clique 0.9 1.4 1.9 2.4 2.9 1 10 100 Number of Measured Signal Changes OverheadImprovementRatio. (NormalizedtoRandomWalk) Incremental Random Walk Random Walk (b) Incremental Random Walk and Random Walk on Clique 0.9 1.1 1.3 1.5 1.7 1.9 1 10 100 Number of Measured Signal Changes OverheadImprovementRatio. (NormalizedtoRandomWalk) Random Walk Incremental Random Walk (c) Incremental Random Walk and Random Walk on Power-Law Random Graph Figure 5: Network overhead of Incremental Protocols cal time stamp of when it received the most recent update. As discussed in Section 4.2, this happens after O(log(N)) time steps. Nevertheless, the probability that gossip fails to converge is small, and has been shown to be O(1/N) . For coordinated spectrum sensing, non-incremental routing protocols can be implemented in a similar fashion. Similar techniques can be used to implement standard gossip. Here we focus on how incremental extensions to gossip protocols can lead to further improvements over standard gossiping techniques, for the problem of coordinated spectrum sensing. We simulate a large deployment of 1,000 devices to analyze protocol scalability. For each data point, we ran 100 simulations and 95% confidence intervals (error bars) are shown. Each transmission also includes a time stamp of when the update was generated. We observe that both protocols have almost identical execution times. This is expected as IGP operates in a similar fashion to 16 uniform gossip, taking O(log(N)) time-steps to converge. These improvements carry over to a PLRG topology as well (as shown in Figure 4(c)), where IRWP is 1.33 times faster than random walk. As more signal changes are introduced into the system, gossip and incremental gossip incur approximately the same overhead. Because spectrum utilization is characterized by incremental changes to usage, incremental protocols are ideally suited to solve this problem in an efficient and cost effective manner.", "introduction": "There has recently been a huge surge in the growth of wireless technology, driven primarily by the availability of unlicensed spectrum. However, this has come at the cost of increased RF interference, which has caused the Federal Communications Commission (FCC) in the United States to re-evaluate its strategy on spectrum allocation. Currently, the FCC has licensed RF spectrum to a variety of public and private institutions, termed primary users. New spectrum allocation regimes implemented by the FCC use dynamic spectrum access schemes to either negotiate or opportunistically allocate RF spectrum to unlicensed secondary users Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific D1 D2 D5 D3 D4 Primary User Shadowed Secondary Users Secondary Users detect Primary's Signal Shadowed Secondary User Figure 1: Without cooperation, shadowed users are not able to detect the presence of the primary user. that can use it when the primary user is absent. The second type of allocation scheme is termed opportunistic spectrum sharing. The FCC has already legislated this access method for the 5 GHz band and is also considering the same for TV broadcast bands . As a result, a new wave of intelligent radios, termed cognitive radios (or software defined radios), is emerging that can dynamically re-tune their radio parameters based on interactions with their surrounding environment. Under the new opportunistic allocation strategy, secondary users are obligated not to interfere with primary users (senders or receivers). This can be done by sensing the environment to detect the presence of primary users. However, local sensing is not always adequate, especially in cases where a secondary user is shadowed from a primary user, as illustrated in Figure 1. Here, coordination between secondary users is the only way for shadowed users to detect the primary. In general, cooperation improves sensing accuracy by an order of magnitude when compared to not cooperating at all . To realize this vision of dynamic spectrum access, two fundamental problems must be solved: (1) Efficient and coordinated spectrum sensing and (2) Distributed spectrum allocation. In this paper, we propose strategies for coordinated spectrum sensing that are low cost, operate on timescales comparable to the agility of the RF environment, and are resilient to network failures and alterations. We defer the problem of spectrum allocation to future work. Spectrum sensing techniques for cognitive radio networks are broadly classified into three regimes; (1) centralized coordinated techniques, (2) decentralized coordinated techniques, and (3) decentralized uncoordinated techniques. We advocate a decentralized coordinated approach, similar in spirit to OSPF link-state routing used in the Internet. This is more effective than uncoordinated approaches because making decisions based only on local information is fallible (as shown in Figure 1). Moreover, compared to cen12 tralized approaches, decentralized techniques are more scalable, robust, and resistant to network failures and security attacks (e.g. Coordinating sensory data between cognitive radio devices is technically challenging because accurately assessing spectrum usage requires exchanging potentially large amounts of data with many radios at very short time scales. Data size grows rapidly due to the large number (i.e. thousands) of spectrum bands that must be scanned. This data must also be exchanged between potentially hundreds of neighboring secondary users at short time scales, to account for rapid changes in the RF environment. This paper presents GUESS, a novel approach to coordinated spectrum sensing for cognitive radio networks. Our approach is motivated by the following key observations: 1. Low-cost sensors collect approximate data: Most devices have limited sensing resolution because they are low-cost and low duty-cycle devices and thus cannot perform complex RF signal processing (e.g. Many are typically equipped with simple energy detectors that gather only approximate information. Approximate summaries are sufficient for coordination: Approximate statistical summaries of sensed data are sufficient for correlating sensed information between radios, as relative usage information is more important than absolute usage data. Thus, exchanging exact RF information may not be necessary, and more importantly, too costly for the purposes of spectrum sensing. RF spectrum changes incrementally: On most bands, RF spectrum utilization changes infrequently. Moreover, utilization of a specific RF band affects only that band and not the entire spectrum. Therefore, if the usage pattern of a particular band changes substantially, nodes detecting that change can initiate an update protocol to update the information for that band alone, leaving in place information already collected for other bands. This allows rapid detection of change while saving the overhead of exchanging unnecessary information. Based on these observations, GUESS makes the following contributions: 1. A novel approach that applies randomized gossiping algorithms to the problem of coordinated spectrum sensing. These algorithms are well suited to coordinated spectrum sensing due to the unique characteristics of the problem: i.e. radios are power-limited, mobile and have limited bandwidth to support spectrum sensing capabilities. An application of in-network aggregation for dissemination of spectrum summaries. We argue that approximate summaries are adequate for performing accurate radio parameter tuning. An extension of in-network aggregation and randomized gossiping to support incremental maintenance of spectrum summaries. Compared to standard gossiping approaches, incremental techniques can further reduce overhead and protocol execution time by requiring fewer radio resources. The rest of the paper is organized as follows. Section 2 motivates the need for a low cost and efficient approach to coordinated spectrum sensing. Section 3 discusses related work in the area, while Section 4 provides a background on in-network aggregation and randomized gossiping. Sections 5 and 6 discuss extensions and protocol details of these techniques for coordinated spectrum sensing. Section 7 presents simulation results showcasing the benefits of GUESS, and Section 8 presents a discussion and some directions for future work.", "conclusion": "We have only just scratched the surface in addressing the problem of coordinated spectrum sensing using incremental gossiping.. Next, we outline some open areas of research.. Spatial Decay: Devices performing coordinated sensing are primarily interested in the spectrum usage of their local neighborhood.. Therefore, we recommend the use of spatially decaying aggregates , which limits the impact of an update on more distant nodes.. Spatially decaying aggregates work by successively reducing (by means of a decay function) the value of the update as it propagates further from its origin.. One challenge with this approach is that propagation distance cannot be determined ahead of time and more importantly, exhibits spatio-temporal variations.. Therefore, finding the optimal decay function is non-trivial, and an interesting subject of future work.. Significance Threshold: RF spectrum bands continually experience small-scale changes which may not necessarily be significant.. Deciding if a change is significant can be done using a significance threshold \u03b2, below which any observed change is not propagated by the node.. Choosing an appropriate operating value for \u03b2 is application dependent, and explored further in .. Weighted Readings: Although we argued that most devices will likely be equipped with low-cost sensing equipment, there may be situations where there are some special infrastructure nodes that have better sensing abilities than others.. Weighting their measurements more heavily could be used to maintain a higher degree of accuracy.. Determining how to assign such weights is an open area of research.. Implementation Specifics: Finally, implementing gossip for coordinated spectrum sensing is also open.. If implemented at the MAC layer, it may be feasible to piggy-back gossip messages over existing management frames (e.g.. As well, we also require the use of a control channel to disseminate sensing information.. There are a variety of alternatives for implementing such a channel, some of which are outlined in .. The trade-offs of different approaches to implementing GUESS is a subject of future work.. CONCLUSION Spectrum sensing is a key requirement for dynamic spectrum allocation in cognitive radio networks.. The nature of the RF environment necessitates coordination between cognitive radio devices.. We propose GUESS, an approximate yet low overhead approach to perform efficient coordination between cognitive radios.. The fundamental contributions of GUESS are: (1) an FM aggregation scheme for efficient innetwork aggregation, (2) a randomized gossiping approach which provides exponentially fast convergence and robustness to network alterations, and (3) incremental variations of FM and gossip which we show can reduce the communication time by up to a factor of 2.7 and reduce network overhead by up to a factor of 2.4.. Our preliminary simulation results showcase the benefits of this approach and we also outline a set of open problems that make this a new and exciting area of research."}
{"id": "I-33", "keywords": ["institut", "norm", "logic", "organiz structur"], "title": "A Formal Road from Institutional Norms to Organizational Structures", "abstract": "Up to now, the way institutions and organizations have been used in the development of open systems has not often gone further than a useful heuristics. In order to develop systems actually implementing institutions and organizations, formal methods should take the place of heuristic ones. The paper presents a formal semantics for the notion of institution and its components (abstract and concrete norms, empowerment of agents, roles) and defines a formal relation between institutions and organizational structures. As a result, it is shown how institutional norms can be refined to constructs---organizational structures---which are closer to an implemented system. It is also shown how such a refinement process can be fully formalized and it is therefore amenable to rigorous verification.", "references": ["In Ontologia sociale potere deontico e regole costitutive", "Pushing the EL envelope", "The Description Logic Handbook", "The micro-macro constitution of power", "Dynamic logic", "ISLANDER: an electronic institutions editor", "Ameli: An agent-based middleware for electronic institutions", "An organizational view of distributed systems", "A semantics for abstraction", "Ontological aspects of the implementation of norms in agent-based electronic institutions", "Structural evaluation of agent organizations", "Context in categorization", "Classificatory aspects of counts-as: An analysis in modal logic", "Moise+: Towards a structural functional and deontic model for mas organization", "On the characterization of law and computer systems: The normative systems perspective", "Contextual deontic logics", "On programming karo agents", "Institutions, Institutional Change and Economic Performance", "Coordination artifacts: Environment-based coordination for intelligent agents", "Action theory and social science", "De Jure Naturae et Gentium", "Modeling rational agents within a BDI-architecture", "A basic classification of legal institutions", "Tractable reasoning via approximation", "The Construction of Social Reality", "The role of Norms and Electronic Institutions in Multi-Agent Systems"], "full_text": "1. INTRODUCTION The opportunity of a technology transfer from the field of organizational and social theory to distributed AI and multiagent systems (MASs) has long been advocated . In MASs the application of the organizational and institutional metaphors to system design has proven to be useful for the development of methodologies and tools. In many cases, however, the application of these conceptual apparatuses amounts to mere heuristics guiding the high level design of the systems. It is our thesis that the application of those apparatuses can be pushed further once their key concepts are treated formally, that is, once notions such as norm, role, structure, etc. obtain a formal semantics. This has been the case for agent programming languages after the relevant concepts borrowed from folk psychology (belief, intention, desire, knowledge, etc.) have been addressed in comprehensive formal logical theories such as, for instance, BDICTL and KARO . As a matter of fact, those theories have fostered the production of architectures and programming languages. What is lacking at the moment for the design and development of open MASs is, in our opinion, something that can play the role that BDI-like formalisms have played for the design and development of single-agent architectures. Aim of the present paper is to fill this gap with respect to the notion of institution providing formal foundations for the application of the institutional metaphor and for its relation to the organizational one. The main result of the paper consists in showing how abstract constraints (institutions) can be step by step refined to concrete structural descriptions (organizational structures) of the to-be-implemented system, bridging thus the gap between abstract norms and concrete system specifications. Concretely, in Section 2, a logical framework is presented which provides a formal semantics for the notions of institution, norm, role, and which supports the account of key features of institutions such as the translation of abstract norms into concrete and implementable ones, the institutional empowerment of agents, and some aspects of the design of norm enforcement. In Section 3 the framework is extended to deal with the notion of the infrastructure of an institution. The extended framework is then studied in relation to the formalism for representing organizational structures presented in . In Section 4 some conclusions follow. 2. INSTITUTIONS Social theory usually thinks of institutions as the rules of the game . From an agent perspective institutions are, to paraphrase this quote, the rules of the various games agents can play in order to interact with one another. To assume an institutional perspective on MASs means therefore to think of MASs in normative terms: [. . . ] law, computer systems, and many other kinds of organizational structure may be viewed as instances of normative systems. We use the term to refer to any set of interacting agents whose behavior can usefully be regarded as governed by norms (, p.276). The normative system perspective on institutions is, as such, nothing original and it is already a quite acknowledged position within the community working on electronic institutions, or eInstitutions . What has not been sufficiently investigated and understood with formal methods is, in our view, the question: what does it 628 978-81--7-5 (RPS) IFAAMAS amount to, for a MAS, to be put under a set of norms? Or in other words: what does it mean for a designer of an eInstitution to state a set of norms? We advance a precise thesis on this issue, which is also inspired by work in social theory: Now, as the original manner of producing physical entities is creation, there is hardly a better way to describe the production of moral entities than by the word \u2018imposition\" [impositio]. For moral entities do not arise from the intrinsic substantial principles of things but are superadded to things already existent and physically complete (, pp. 100-101). By ignoring for a second the philosophical jargon of the Seventeenth century we can easily extract an illuminating message from the excerpt: what institutions do is to impose properties on already existing entities. That is to say, institutions provide descriptions of entities by making use of conceptualizations that are not proper of the common descriptions of those entities. For example, that cars have wheels is a common factual property, whereas the fact that cars count as vehicles in some technical legal sense is a property that law imposes on the concept car. To say it with , the fact that cars have wheels is a brute fact, while the fact that cars are vehicles is an institutional fact. Institutions build structured descriptions of institutional properties upon brute descriptions of a given domain. At this point, the step toward eInstitutions is natural. eInstitutions impose properties on the possible states of a MAS: they specify what are the states in which an agent i enacts a role r; what are the states in which a certain agent is violating the norms of the institution, etc. They do this via linking some institutional properties of the possible states and transitions of the system (e.g., agent i enacts role r) to some brute properties of those states and transitions (e.g., agent i performs protocol No.56). An institutional property is therefore a property of system states or system transitions (i.e., a state type or a transition type) that does not belong to a merely technical, or factual, description of the system. To sum up, institution are viewed as sets of norms (normative system perspective), and norms are thought of as the imposition of an institutional description of the system upon its description in terms of brute properties. In a nutshell, institutions are impositions of institutional terminologies upon brute ones. The following sections provide a formal analysis of this thesis and show its explanatory power in delivering a rigorous understanding of key features of institutions. Because of its suitability for representing complex domain descriptions, the formal framework we will make use of is the one of Description Logics (DL). The use of such formalism will also stress the idea of viewing institutions as the impositions of domain descriptions. 2.1 Preliminaries: a very expressive DL The description logic language enabling the necessary expressivity expands the standard description logic language ALC with relational operators ( ,\u25e6,\u00ac,id) to express complex transition types, and relational hierarchies (H) to express inclusion between transition types. Following a notational convention common within DL we denote this language with ALCH( ,\u25e6,\u00ac,id) DEFINITION 1. (Syntax of ALCH( ,\u25e6,\u00ac,id) transition types and state type constructs are defined by the following BNF: \u03b1 := a | \u03b1 \u25e6 \u03b1 | \u03b1 \u03b1 | \u00ac\u03b1 | id(\u03b3) \u03b3 := c | \u22a5 | \u00ac\u03b3 | \u03b3 \u03b3 | \u2200\u03b1.\u03b3 where a and c are atomic transition types and, respectively, atomic state types. It is worth providing the intuitive reading of a couple of the operators and the constructs just introduced. In particular \u2200\u03b1.\u03b3 has to be read as: after all executions of transitions of type \u03b1, states of type \u03b3 are reached. The operator \u25e6 denotes the concatenation of transition types. The operator id applies to a state description \u03b3 and yields a transition description, namely, the transition ending in \u03b3 states. It is the description logic variant of the test operator in Dynamic Logic . Notice that we use the same symbols and \u00ac for denoting the boolean operators of disjunction and negation of both state and transition types. Atomic state types c are often indexed by an agent identifier i in order to express agent properties (e.g., dutch(i)), and atomic transition types a are often indexed by a pair of agent identifiers (i, j) (e.g., PAY(i, j)) denoting the actor and, respectively, the recipient of the transition. By removing the agent identifiers from state types and transition types we obtain state type forms (e.g., dutch or rea(r)) and transition type form (e.g., PAY). A terminological box (henceforth TBox) T = \u0393, A consists of a finite set \u0393 of state type inclusion assertions (\u03b31 \u03b32), and of a finite set A of transition type inclusion assertions (\u03b11 \u03b12). The semantics of ALCH( ,\u25e6,\u00ac,id) is model theoretical and it is given in terms of interpreted transition systems. As usual, state types are interpreted as sets of states and transition types as sets of state pairs. DEFINITION 2. (Semantics of ALCH( ,\u25e6,\u00ac,id) An interpreted transition system m for ALCH( ,\u25e6,\u00ac,id) is a structure S, I where S is a non-empty set of states and I is a function such that: I(c) \u2286 S I(a) \u2286 S \u00d7 S I(\u22a5) = \u2205 I(\u00ac\u03b3) = \u0394m\\ I(\u03b3) I(\u03b31 \u03b32) = I(\u03b31) \u2229 I(\u03b32) I(\u2200\u03b1.\u03b3) = {s \u2208 S | \u2200t, (s, t) \u2208 I(\u03b1) \u21d2 t \u2208 I(\u03b3)} I(\u03b11 \u03b12) = I(\u03b11) \u222a I(\u03b12) I(\u00ac\u03b1) = S \u00d7 S \\ I(\u03b1) I(\u03b11 \u25e6 \u03b12) = {(s, s ) | \u2203s , (s, s ) \u2208 I(\u03b11) & (s , s ) \u2208 I(\u03b12)} I(id(\u03b3)) = {(s, s) | s \u2208 I(\u03b3)} An interpreted transition system m is a model of a state type inclusion assertion \u03b31 \u03b32 if I(\u03b31) \u2286 I(\u03b32). It is a model of a transition type inclusion assertion \u03b11 \u03b12 if I(\u03b11) \u2286 I(\u03b12). An interpreted transition system m is a model of a TBox T = \u0393, A if m is a model of each inclusion assertion in \u0393 and A. REMARK 1. (Derived constructs) The correspondence between description logic and dynamic logic is well-known . In fact, the language presented in Definitions 1 and 2 is a notational variant of the language of Dynamic Logic without the iteration operator of transition types. As a consequence, some key constructs are still definable in ALCH( ,\u25e6,\u00ac,id) . In particular we will make use of the following definition of the if-then-else transition type: if \u03b3 then \u03b11else \u03b12 = (id(\u03b3) \u25e6 \u03b11) (id(\u00ac\u03b3) \u25e6 \u03b12). Boolean operators are defined as usual. We will come back to some complexity features of this logic in Section 2.5. The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 629 2.2 Institutions as terminologies We have upheld that institutions impose new system descriptions which are formulated in terms of sets of norms. The step toward a formal grounding of this view of institutions is now short: norms can be thought of as terminological axioms, and institutions as sets of terminological axioms, i.e., terminological boxes. An institution can be specified as a terminological box Ins = \u0393ins, Ains , where each inclusion statement in \u0393ins and Ains models a norm of the institution. Obviously, not every TBox can be considered to be an institution specification. In particular, an institution specification Ins must have some precise linguistic relationship with the \u2018brute\" descriptions upon which the institution is specified. We denote by Lins the non-logical alphabet containing only institutional state and transition types, and by Lbrute the nonlogical alphabet containing those types taken to talk about, instead, \u2018brute\" states and transitions1 DEFINITION 3. (Institutions as TBoxes) A TBox Ins = \u0393ins, Ains is an institution specification if: 1. The non-logical alphabet on which Ins is specified contains elements of both Lins and Lbrute. In symbols: L(Ins) \u2286 Lins \u222a Lbrute. 2. There exist sets of terminological axioms \u0393bridge \u2286 \u0393ins and Abridge \u2286 Ains such that either the left-hand side of these axioms is always a description expressed in Lbrute and the right-hand side a description expressed in Lins, or those axioms are definitions. In symbols: if \u03b31 \u03b32 \u2208 \u0393bridge then either \u03b31 \u2208 Lbrute and \u03b32 \u2208 Lins or it is the case that also \u03b32 \u03b31 \u2208 \u0393bridge. The clause for Abridge is analogous. 3. The remaining sets of terminological axioms \u0393ins\\\u0393bridge and Ains\\Abridge are all expressed in Lins. In symbols: L(\u0393ins\\\u0393bridge) \u2286 Lins and L(Ains\\Abridge) \u2286 Lins. The definition states that an institution specification needs to be expressed on a language including institutional as well as brute terms (1); that a part of the specification concerns a description of mere institutional terms (3); and that there needs to be a part of the specification which connects institutional terms to brute ones (2). Terminological axioms in \u0393bridge and Abridge formalize in DL the Searlean notion of counts-as conditional , that is, rules stating what kind of meaning an institution gives to certain brute facts and transitions (e.g., checking box No.4 in form No.2 counts as accepting your personal data to be used for research purposes). A formal theory of counts-as statements has been thoroughly developed in a series of papers among which . The technical content of the present paper heavily capitalizes on that work. Notice also that given the semantics presented in Definition 2, if institutions can be specified via TBoxes then the meaning of such specifications is a set of interpreted transition systems, i.e., the models of those TBoxes. These transitions systems can be in turn thought of as all the possible MASs which model the specified institution. REMARK 2. (Lbrute from a designer\"s perspective) From a design perspective language Lbrute has to be thought of as the language on which a designer would specify a system instantiating a given institution2 . Definition 3 shows that for such a design task Symbols from Lins and Lbrute will be indexed (especially with agent identifiers) to add some syntactic sugar. To make a concrete example, the AMELI middleware can be viewed as a specification tool at a Lbrute level. it is needed to formally specify an explicit bridge between the concepts used in the description of the actual system and the institutional \u2018abstract\" concepts. We will come back to this issue in Section 3. 2.3 From abstract to concrete norms To illustrate Definition 3, and show its explanatory power, an example follows which depicts an essential phenomenon of institutions. EXAMPLE 1. (From abstract to concrete norms) Consider an institution supposed to regulate access to a set of public web services. It may contain the following norm: it is forbidden to discriminate access on the basis of citizenship. Suppose now a system has to be built which complies with this norm. The first question is: what does it mean, in concrete, to discriminate on the basis of citizenship? The system designer should make some concrete choices for interpreting the norm and these choices should be kept track of in order to explicitly link the abstract norm to its concrete interpretation. The problem can be represented as follows. The abstract norm is formalized by Formula 1 by making use of a standard reduction technique of deontic notions (see ): the statement it is forbidden to discriminate on the basis of citizenship amounts to the statement after every execution of a transition of type DISCR(i, j) the system always ends up in a violation state. Together with the norm also some intuitive background knowledge about the discrimination action needs to be formalized. Here, as well as in the rest of the examples in the paper, we provide just that part of the formalization which is strictly functional to show how the formalism works in practice. Formulae 2 and 3 express two effect laws: if the requester j is Dutch then after all executions of transitions of type DISCR(i, j) j is accepted by i, whereas if it is not all the executions of the transitions of the same type have as an effect that it is not accepted. All formulae have to be read as schemata determining a finite number of subsumption expressions depending on the number of agents i, j considered. \u2200DISCR(i, j).viol \u2261 (1) dutch(j) \u2200DISCR(i, j).accepted(j) (2) \u00acdutch(j) \u2200DISCR(i, j).\u00acaccepted(j) (3) The rest of the axioms concern the translation of the abstract type DISCR(i, j) to concrete transition types. Formula 4 refines it by making explicit that a precise if-then-else procedure counts as a discriminatory act of agent i. Formulae 5 and 6 specify which messages of i to j count as acceptance and rejection. If the designer uses transition types SEND(msg33, i, j) and SEND(msg38, i, j) for the concrete system specification, then Formulae 5 and 6 can be thought of as bridge axioms connecting notions belonging to the institutional alphabet (to accept, and to reject) to concrete ones (to send specific messages). Finally, Formulae 7 and 8 state two intuitive effect laws concerning the ACCEPT(i, j) and REJECT(i, j) types. if dutch(j)then ACCEPT(i, j) else REJECT(i, j) DISCR(i, j) (4) SEND(msg33, i, j) ACCEPT(i, j) (5) SEND(msg38, i, j) REJECT(i, j) (6) \u2200ACCEPT(i, j).accepted(j) \u2261 (7) \u2200REJECT(i, j).\u00acaccepted(j) \u2261 (8) It is easy to see, on the grounds of the semantics exposed in Definition 2, that the following concrete inclusion statement holds w.r.t. 630 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) the specified institution: if dutch(j) then SEND(msg33, i, j) else SEND(msg38, i, j) DISCR(i, j) (9) This scenario exemplifies a pervasive feature of human institutions which, as extensively argued in , should be incorporated by electronic ones. Current formal approaches to institutions, such as ISLANDER , do not allow for the formal specification of explicit translations of abstract norms into concrete ones, and focus only on norms that can be specified at the concrete system specification level. What Example 1 shows is that the problem of the abstractness of norms in institutions can be formally addressed and can be given a precise formal semantics. The scenario suggests that, just by modifying an appropriate set of terminological axioms, it is possible for the designer to obtain a different institution by just modifying the sets of bridge axioms without touching the terminological axioms expressed only in the institutional language Lins. In fact, it is the case that a same set of abstract norms can be translated to different and even incompatible sets of concrete norms. This translation can nevertheless not be arbitrary . EXAMPLE 2. (Acceptable and unacceptable translations of abstract norms) Reconsider again the scenario sketched in Example 1. The transition type DISCR(i, j) has been translated to a complex procedure composed by concrete transition types. Would any translation do? Consider an alternative institution specification Ins containing Formulae 1-3 and the following translation rule: PAY(j, i, e10) DISCR(i, j) (10) Would this formula be an acceptable translation of the abstract norm expressed in Formula 1? The axiom states that transitions where i receives e10 from j count as transitions of type DISCR(i, j). Needless to say this is not intuitive, because the abstract transition type DISCR(i, j) obeys some intuitive conceptual constraints (Formulae 2 and 3) that all its translations should also obey. In fact, the following inclusions would then hold in Ins : dutch(j) \u2200PAY(j, i, e10).accepted(j) (11) \u00acdutch(j) \u2200PAY(j, i, e10).\u00acaccepted(j) (12) In fact, there properties of the transition type PAY(j, i, e10) look at least awkward: if an agent is Dutch than by paying e10 it would be accepted, while if it was not Dutch the same action would make it not accepted. The problem is that the meaning of \u2018paying\" is not intuitively subsumed by the meaning of \u2018discriminating\". In other words, a transition type PAY(j, i, e10) does not intuitively yield the effects that a sub-type of DISCR(i, j) yields. It is on the contrary perfectly intuitive that Formula 9 obeys the constraints in Formulae 2 and 3, which it does, as it can be easily checked on the grounds of the semantics. It is worth stressing that without providing a model-theoretic semantics for the translation rules linking the institutional notions to the brute ones, it would not be so straightforward to model the logical constraints to which the translations are subjected (Example 2). This is precisely the advantage of viewing translation rules as specific terminological axioms, i.e., \u0393bridge and Abridge, working as a bridge between two languages (Definition 3). In , we have thoroughly compared this approach with approaches such as which conceive of translation rules as inference rules. The two examples have shown how our approach can account for some essential features of institutions. In the next section the same framework is applied to provide a formal analysis of the notion of role. 2.4 Institutional modules and roles Viewing institutions as the impositions of institutional descriptions on systems\" states and transitions allows for analyzing the normative system perspective itself (i.e., institutions are sets of norms) at a finer granularity. We have seen that the terminological axioms specifying an institution concern complex descriptions of new institutional notions. Some of the institutional state types occurring in the institution specification play a key role in structuring the specification of the institution itself. The paradigmatic example in this sense are facts such as agent i enacts role r which will be denoted by state types rea(i, r). By stating how an agent can enact and \u2018deact\" a role r, and what normative consequences follow from the enactment of r, an institution describes expected forms of agents\" behavior while at the same time abstracting from the concrete agents taking part to the system. The sets of norms specifying an institution can be clustered on the grounds of the rea state types. For each relevant institutional state type (e.g., rea(i, r)), the terminological axioms which define an institution, i.e., its norms, can be clustered in (possibly overlapping) sets of three different types: the axioms specifying how states of that institutional type can be reached (e.g., how an agent i can enact the role r); how states of that type can be left (e.g., how an agent i can \u2018deact\" the a role r); and what kind of institutional consequences do those states bear (e.g., what rights and power does agent i acquire by enacting role r). Borrowing the terminology from work in legal and institutional theory , these clusters of norms can be called, respectively, institutive, terminative and status modules. Status modules We call status modules those sets of terminological axioms which specify the institutional consequences of the occurrence of a given institutional state-of-affairs, for instance, the fact that agent i enacts role r. EXAMPLE 3. (A status module for roles) Enacting a role within an institution bears some institutional consequences that are grouped under the notion of status: by playing a role an agent acquires a specific status. Some of these consequences are deontic and concern the obligations, rights, permissions under which the agent puts itself once it enacts the role. An example which pertains to the normative description of the status of both a buyer and a seller roles is the following: rea(i, buyer) rea(j, seller) win bid(i, j, b) \u2200\u00acPAY(i, j, b).viol(i) (13) If agent i enacts the buyer role and j the seller role and i wins bid b then if i does not perform a transition of type PAY (i, j, b), i.e., does not pay to j the price corresponding to bid b, then the system ends up in a state that the institution classifies as a violation state with i being the violator. Notice that Formula 13 formalizes at the same time an obligation pertaining to the role buyer and a right pertaining to the role seller. Of particular interest are then those consequences that attribute powers to agents enacting specific roles: rea(i, buyer) rea(j, seller) \u2200BID(i, j, b).bid(i, j, b) (14) SEND(i, j, msg49) BID(i, j, b) (15) If agent i enacts the buyer role and j the seller role, every time agent i bids b to j this action results in an institutional state testifying that the corresponding bid has been placed by i (Formula 14). Formula 15 states how the bidding action can be executed by sending a specific message to j (SEND(i, j, msg49)). Some observations are in order. As readers acquainted with deontic logic have probably already noticed, our treatment of the notion The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 631 of obligation (Formula 13) makes again use of a standard reduction approach . More interesting is instead how the notion of institutional power is modeled. Essentially, the empowerment phenomenon is analyzed in term of two rules: one specifying the institutional effects of an institutional action (Formula 14), and one translating the institutional transition type in a brute one (Formula 15). Systems of rules of this type empower the agents enacting some relevant role by establishing a connection between the brute actions of the agents and some institutional effect. Whether the agents are actually able to execute the required \u2018brute\" actions is a different issue, since agent i can be in some states (or even all states) unable to effectuate a SEND(i, j, msg49) transition. This is the case also in human societies: priests are empowered to give rise to marriages but if a priest is not in state of performing the required speech acts he is actually unable to marry anybody. There is a difference between being entitled to make a bid and being in state of making a bid . In other words, Formulae 14 and 15 express only that agents playing the buyer role are entitled to make bids. The actual possibility of performing the required \u2018brute\" actions is not an institutional issue, but rather an issue concerning the implementation of an institution in a concrete system. We address this issue extensively in Section 33 Institutive modules We call institutive modules those sets of terminological axioms of an institution specification describing how states with certain institutional properties can be reached, for instance, how an agent i can reach a state in which it enacts role r. They can be seen as procedures that the institution define in order for the agents to bring institutional states of affairs about. EXAMPLE 4. (An institutive module for roles) The fact that an agent i enacts a role r (rea(i, r)) is the effect of a corresponding enactment action ENACT(i, r) performed under certain circumstances (Formula 16), namely that the agent does not already enact the role, and that the agent satisfies given conditions (cond(i, r)), which might for instance pertain the computational capabilities required for an agent to play the chosen role, or its capability to interact with some specific system\"s infrastructures. Formula 17 specifies instead the procedure counting as an action of type ENACT(i, r). Such a procedure is performed through a system infrastructure s, which notifies to i that it has been registered as enacting role r after sending the necessary piece of data d (SEND(i, s, d)), e.g., a valid credit card number. \u00acrea(i, r) cond(i, r) ENACT(i, r).rea(i, r) (16) SEND(i, s, d) \u25e6 NOTIFY(s, i) ENACT(i, r) (17) Terminative modules Analogously, we call terminative modules those sets of terminological axioms stating how a state with certain institutional properties can be left. Rules of this kind state for instance how an agent can stop enacting a certain role. They can be thus thought of as procedures that the institution defines in order for the agent to see to it that certain institutional states stop holding. EXAMPLE 5. (A terminative module for roles) Terminative modules for roles specify, for instance, how a transition type DEACT(i, r) can be executed which has as consequence the reaching of a state of type \u00acrea(i, r): rea(i, r) DEACT(i, r).\u00acrea(i, r) (18) SEND(i, s, msg9) DEACT(i, r) (19) That is to say, i deacting a role r always leads to a state where See in particular Example 6 and Definition 5 i does not enact role r; and i sending message No.9 to a specific interface infrastructure s count as i deacting role r. Examples 3-5 have shown how roles can be formalized in our framework thereby getting a formal semantics: roles are also sets of terminological axioms concerning state types of the sort rea(i, r). It is worth noticing that this modeling option is aligned with work on social theory addressing the concept of role such as . 2.5 Tractable specifications of institutions In the previous sections we fully deployed the expressivity of the language introduced in Section 2.1 and used its semantics to provide a formal understanding of many essential aspects of institutions in terms of transition systems. This section spends a few words about the viability of performing automated reasoning in the logic presented. The satisfiability problem4 in logic ALCH( ,\u25e6,\u00ac,id) is undecidable since transition type inclusion axioms correspond to a version of what in Description Logic are known as role-value maps and logics extending ALC with role-value maps are known to be undecidable . Tractable (i.e., polynomial time decidable) fragments of logic ALCH( ,\u25e6,\u00ac,id) can however be isolated which still exhibit some key expressive features. One of them is logic ELH(\u25e6) . It is obtained from description logic EL, which contains only state types intersection , existential restriction \u2203 and 5 , but extended with the \u22a5 state type and with transition type inclusion axioms of a complex form: a1 \u25e6. . .\u25e6an a (with n finite number). Logic ELH(\u25e6) is also a fragment of the well investigated description logic EL++ whose satisfiability problem has been shown in to be decidable in polynomial time. Despite the very limited expressivity of this fragment, some rudimentary institutional specifications can still be successfully represented. Specifically, institutive and terminative modules can be represented which contain transition types inclusion axioms. Restricted versions of status modules can also be represented enabling two essential deontic notions: it is possible (respectively, impossible) to reach a violation state by performing a transition of a certain type, and it is possible (respectively, impossible) to reach a legal state by performing a transition of a certain type. To this aim language Lins would need to be expanded with a set of state types {legal(i)}0\u2264i\u2264n whose intuitive meaning is to denote legal states as opposed to states of type viol(i). Fragments like ELH(\u25e6) could be used as target logics within theory approximation approaches by aiming at compiling TBoxes expressed in ALCH( ,\u25e6,\u00ac,id) into approximations in those fragments. 3. FROM NORMS TO STRUCTURES 3.1 Infrastructures In discussing Example 3 we observed how being entitled to make a bid does not imply being in state of making a bid. In other words, an institution can empower agents by means of appropriate rules but this empowerment can remain dead letter. Similar This problem amounts to check whether a state description \u03b3 is satisfiable w.r.t. a given TBox T, i.e., to check if there exists a model m of T such that \u2205 \u2282 I(\u03b3). Notice that language ALCH( ,\u25e6,\u00ac,id) contains negation and intersection of arbitrary state types. It is well-known that if these operators are available then all most typical reasoning tasks at the TBox level can be reduced to the satisfiability problem. Notice therefore that EL is a seriously restricted fragment of ALC since it does not contain the negation operator for state types (operators and \u2200 remain thus undefinable). 632 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) observations apply also to deontic notions: agents might be allowed to perform certain transactions under some relevant conditions but they might be unable to do so under those same conditions. We refer to this kind of problems as infrastructural. The implementation of an institution in a concrete system calls therefore for the design of appropriate infrastructures or artifacts . The formal specification of an infrastructure amounts to the formal specification of interaction requirements, that is to say, the specification of which relevant transition types are executable and under what conditions. DEFINITION 4. (Infrastructures as TBoxes) An infrastructure Inf = \u0393inf , Ainf for institution Ins is a TBox on Lbrute such that for all a \u2208 L(Abridge) there exist terminological axioms in \u0393inf of the following form: \u03b3 \u2261 \u2203a. (a is executable exactly in \u03b3 states) and \u03b3 \u2261 \u2203\u00aca. (the negation of a is executable exactly in \u03b3 states). In other words, an infrastructure specification states all and only the conditions under which an atomic brute transition type and its negation are executable, which occur in the brute alphabet of the bridge axioms of Ins. It states what can be in concrete done and under what conditions. EXAMPLE 6. (Infrastructure specification) Consider the institution specified in Example 1. A simple infrastructure Inf for that institution could contain for instance the following terminological axioms for any pair of different agents i, j and message type msg: \u2203SEND(msg33, i, j). (20) The formula states that it is always in the possibilities of agent i to send message No. 33 to agent j. It then follows on the grounds of Example 1 that agent i can always accept agent j. \u2203ACCEPT(i, j). (21) Notice that the executability condition is just . We call a concrete institution specification CIns an institution specification Ins coupled with an infrastructure specification Inf. DEFINITION 5. (Concrete institution) A concrete institution obtained by joining the institution Ins = \u0393ins, Ains and the infrastructure Inf = \u0393inf , Ainf is a TBox CIns = \u0393, A such that \u0393 = \u0393ins \u222a \u0393inf and A = Ains \u222a Ainf . Obviously, different infrastructures can be devised for a same institution giving rise to different concrete institutions which makes precise implementation choices explicit. Of particular relevance are the implementation choices concerning abstract norms like the one represented in Formula 13. A designer can choose to regiment such norm , i.e., make violation states unreachable, via an appropriate infrastructure. EXAMPLE 7. (Regimentation via infrastructure specification) Consider Example 3 and suppose the following translation rule to be also part of the institution: BNK(i, j, b) CC(i, j, b) \u2261 PAY(i, j, b) (22) condition pay(i, j, b) \u2261 rea(i, buyer) rea(j, seller) win bid(i, j, b) (23) The first formula states how the payment can be concretely carried out (via bank transfer or credit card) and the second just provides a concrete label grouping the institutional state types relevant for the norm. In order to specify a regimentation at the infrastructural level it is enough to state that: condition pay(i, j, b) \u2261 \u2203(BNK(i, j, b) CC(i, j, b)). (24) \u00accondition pay(i, j, b) \u2261 \u2203\u00ac(BNK(i, j, b) CC(i, j, b)). (25) In other words, in states of type condition pay(i, j, b) the only executable brute actions are BANK(i, j, b) or CC(i, j, b) and, therefore, PAY(i, j, b) would necessarily be executed. As a result, the following inclusion does not hold with respect to the corresponding concrete institution: condition pay(i, j, b) \u2203\u00acPAY(i).viol(i). 3.2 Organizational Structures This section briefly summarizes and adapts the perspective and results on organizational structures presented in . We refer to that work for a more comprehensive exposition. Organizational structures typically concern the way agents interact within organizations. These interactions can be depicted as the links of a graph defined on the set of roles of the organization. Such links are then to be labeled on the basis of the type of interaction they stand for. First of all, it should be clear whether a link denotes that a certain interaction between two roles can, or ought to, or may etc. take place. Secondly, links should be labeled according to the transition type \u03b1 they refer to and the conditions \u03b3 in which that transition can, ought to, may etc. take place. Links in a formal specification of an organizational structure stand therefore for statements of the kind: role r can (ought to, may) execute \u03b1 w.r.t. role s if \u03b3 is the case. For the sake of simplicity, the following definition will consider only the can and ought-to interaction modalities. State and transition types in Lins \u222aLbrute will be used to label the links of the structure. Interaction modalities can therefore be of an institutional kind or of a brute kind. DEFINITION 6. (Organizational structure) An organizational structure is a multi-graph: OS = Roles, {Cp}p\u2208Mod, {Op}p\u2208Mod where: \u2022 Mod denotes a set of pairs p = \u03b3 : \u03b1, that is, a set of state type (condition) and transition type (action) pairs of Lins\u222aLbrute with \u03b1 being an atomic transition-type indexed with a pair (i, j) denoting placeholders for the actor and the recipient of the transition; \u2022 C (can) denotes links to be interpreted in terms of the executability of the related \u03b1 in \u03b3, whereas O (ought) denotes links to be interpreted in terms of the obligation to execute the related \u03b1 in \u03b3. By the expressions (r, s) \u2208 C\u03b3:\u03b1 and (r, s) \u2208 O\u03b3:\u03b1 we mean therefore: agents enacting role r can and, respectively, ought to interact with agents enacting role s by performing \u03b1 in states of type \u03b3. As shown in such formal representations of organizational structures are of use for investigating the structural properties (robustness, flexibility, etc.) that a given organization exhibits. At this point all the formal means are put in place which allow us to formally represent institutions as well as organizational structures. The next and final step of the work consists in providing a formal relation between the two frameworks. This formal relation will make explicit how institutions are related to organizational structures and vice versa. In particular, it will become clear how a normative conception of the notion of role relates to a structural one, that is, how the view of roles as a sets of norms (specifying how an agent can enact and deact the role, and what social status it obtains by doing that) relates to the view of roles as positions within social structures. 3.3 Relating institutions to organizations The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 633 To translate a given concrete institution into a corresponding organizational structure we need a function t assigning pairs of roles to axioms. Let us denote with Sub the set of all state type inclusion statements \u03b31 \u03b32 that can be expressed on Lins \u222a Lbrute. Function t is a partial function Sub Roles \u00d7 Roles such that, for any x \u2208 Sub if x = rea(i, r) rea(j, s) \u03b3 \u2203\u03b1. (executability) or x = rea(i, r) rea(j, s) \u03b3 \u2200\u00ac\u03b1.viol(i) (obligation) then t(x) = (r, s), where \u03b1 is an atomic transition-type indexed with a pair (i, j). That is to say, executability and obligation laws containing the enactment configuration rea(i, r) rea(j, s) as a premise and concerning transition of types \u03b1, with i actor and j recipient of the \u03b1 transition, are translated into role pairs (r, s). DEFINITION 7. (Correspondence of specifications) A concrete institution CIns = \u0393, A is said to correspond to an organizational structure OS (and vice versa) if, for every x \u2208 \u0393: \u2022 x = rea(i, r) rea(j, s) \u03b3 \u2203\u03b1. iff t(x) \u2208 C\u03b3:\u03b1 \u2022 x = rea(i, r) rea(j, s) \u03b3 \u2200\u00ac\u03b1.viol(i) iff t(x) \u2208 O\u03b3:\u03b1 Intuitively, function t takes axioms from \u0393 (i.e., the set of state type terminological axioms of CIns) and yields pairs of roles. Definition 7 labels the yielded pairs accordingly to the syntactic form of the translated axioms. More concretely, axioms of the form rea(i, r) rea(j, s) \u03b3 \u2203\u03b1. (executability laws) are translated into the pair (r, s) belonging to the executability dimension (i.e., C) of the organizational structure w.r.t. the execution of \u03b1 under circumstances \u03b3. Analogously, axioms of the form rea(i, r) rea(j, s) \u03b3 \u2200\u00ac\u03b1.viol(i) (obligation laws) are translated into the pair (r, s) belonging to the obligation dimension (i.e., O) of the organizational structure w.r.t. the execution of \u03b1 under circumstances \u03b3. Leaving technicalities aside, function t distills thus the terminological and infrastructural constraints of CIns into structural ones. The institutive, terminative and status modules of roles are translated into definitions of positions within a OS. From a design perspective the interpretation of Definition 7 is twofold. On the one hand (from left to right), it can make explicit what the structural consequences are of a given institution supported by a given infrastructure. On the other hand (from right to left), it can make explicit what kind of institution is actually implemented by a given organizational structure. Let us see this in some more details. Given a concrete institution CIns, Definition 7 allows a designer to be aware of the impact that specific terminological choices (in particular, the choice of certain bridge axioms) and infrastructural ones have at a structural level. Notice that Definition 7 supports the inference of links in a structure. By checking whether a given inclusion statement of the relevant syntactic form follows from CIns (i.e., the so-called subsumption problem of DL) it is possible, via t, to add new links to the corresponding organizational structure. This can be recursively done by just adding any new inferred inclusion x to the previous set of axioms \u0393, thus obtaining an updated institutional specification containing \u0393 \u222a {x}. This process can be thought of as the inference of structural links from institutional specifications. In other words, it is possible to use institution specifications as inference tools for structural specifications. For instance, the infrastructural choice formalized in Example 7 implies that for the pair of roles (buyer, seller), it is always the case that (buyer, seller) \u2208 C :PAY(i,j,b). This link follows from links (buyer, seller) \u2208 C :BNK(i,j,b) and (buyer, seller) \u2208 C :CC(i,j,b) on the grounds of the bridge axioms of the institution (Formula 22). Suppose now a designer to be interested in a system which, besides implementing an institution, also incorporates an organizational structure enjoying desirable structural properties such as flexibility, or robustness6 . By relating structural links to state type inclusions it is therefore possible to check whether adding a link in OS results in a stronger institutional specification, that is, if the corresponding inclusion statement is not already implied by Ins. To draw a parallelism with what just said in the previous paragraph, this process can be thought of as the inference of norms and infrastructural constraints from the specification of organizational structures. To give a simple example consider again Example 6 but from a reversed perspective. Suppose a designer wants a fully connected graph in the dimension C :SEND(i,j) of the organizational structure. Exploiting Definition 7, we would obtain a number of executability laws in the fashion of Formula 20 for all roles in Roles (thus 2|Roles| axioms). Definition 7 establishes a correspondence between two essentially different perspectives on the design of open systems allowing for feedbacks between the two to be formally analyzed. One last observation is in order. While given a concrete institution an organizational structure can be in principle fully specified (by checking for all -finitely many- relevant inclusion statements whether they are implied or not by the institution), it is not possible to obtain a full terminological specification from an organizational structure. This lies on the fact that in Definition 6 the strictly terminological information contained in the specification of an institution (eminently, the set of transition type axioms A and therefore the bridge axioms) is lost while moving to a structural description. This shows, in turn, that the added value of the specification of institutions lies precisely in the terminological link they establish between institutional and brute, i.e., system level notions. 4. CONCLUSIONS The paper aimed at providing a comprehensive formal analysis of the institutional metaphor and its relation to the organizational one. The predominant formal tool has been description logic. TBoxes has been used to represent the specifications of institutions (Definition 3) and their infrastructures (Definition 6), providing therefore a transition system semantics for a number of institutional notions (Examples 1-7). Multi-graphs has then been used to represent the specification of organizational structures (Definition 6). The last result presented concerned the definition of a formal correspondence between institution and organization specifications (Definition 7), which provides a formal way for switching between the two paradigms. All in all, these results deliver a way for relating abstract system specifications (i.e., institutions as sets of norms) to specifications that are closer to an implemented system (i.e., organizational structures).", "body1": "The opportunity of a technology transfer from the field of organizational and social theory to distributed AI and multiagent systems (MASs) has long been advocated . What is lacking at the moment for the design and development of open MASs is, in our opinion, something that can play the role that BDI-like formalisms have played for the design and development of single-agent architectures. Concretely, in Section 2, a logical framework is presented which provides a formal semantics for the notions of institution, norm, role, and which supports the account of key features of institutions such as the translation of abstract norms into concrete and implementable ones, the institutional empowerment of agents, and some aspects of the design of norm enforcement. Social theory usually thinks of institutions as the rules of the game . The normative system perspective on institutions is, as such, nothing original and it is already a quite acknowledged position within the community working on electronic institutions, or eInstitutions . By ignoring for a second the philosophical jargon of the Seventeenth century we can easily extract an illuminating message from the excerpt: what institutions do is to impose properties on already existing entities. eInstitutions impose properties on the possible states of a MAS: they specify what are the states in which an agent i enacts a role r; what are the states in which a certain agent is violating the norms of the institution, etc. To sum up, institution are viewed as sets of norms (normative system perspective), and norms are thought of as the imposition of an institutional description of the system upon its description in terms of brute properties. 2.1 Preliminaries: a very expressive DL The description logic language enabling the necessary expressivity expands the standard description logic language ALC with relational operators ( ,\u25e6,\u00ac,id) to express complex transition types, and relational hierarchies (H) to express inclusion between transition types. It is worth providing the intuitive reading of a couple of the operators and the constructs just introduced. The semantics of ALCH( ,\u25e6,\u00ac,id) is model theoretical and it is given in terms of interpreted transition systems. DEFINITION 2. REMARK 1. We will come back to some complexity features of this logic in Section 2.5. The Sixth Intl. An institution can be specified as a terminological box Ins = \u0393ins, Ains , where each inclusion statement in \u0393ins and Ains models a norm of the institution. 2. 3. Terminological axioms in \u0393bridge and Abridge formalize in DL the Searlean notion of counts-as conditional , that is, rules stating what kind of meaning an institution gives to certain brute facts and transitions (e.g., checking box No.4 in form No.2 counts as accepting your personal data to be used for research purposes). REMARK 2. it is needed to formally specify an explicit bridge between the concepts used in the description of the actual system and the institutional \u2018abstract\" concepts. 2.3 From abstract to concrete norms To illustrate Definition 3, and show its explanatory power, an example follows which depicts an essential phenomenon of institutions. EXAMPLE 1. Together with the norm also some intuitive background knowledge about the discrimination action needs to be formalized. \u2200DISCR(i, j).viol \u2261 (1) dutch(j) \u2200DISCR(i, j).accepted(j) (2) \u00acdutch(j) \u2200DISCR(i, j).\u00acaccepted(j) (3) The rest of the axioms concern the translation of the abstract type DISCR(i, j) to concrete transition types. 630 The Sixth Intl. The scenario suggests that, just by modifying an appropriate set of terminological axioms, it is possible for the designer to obtain a different institution by just modifying the sets of bridge axioms without touching the terminological axioms expressed only in the institutional language Lins. EXAMPLE 2. Needless to say this is not intuitive, because the abstract transition type DISCR(i, j) obeys some intuitive conceptual constraints (Formulae 2 and 3) that all its translations should also obey. This is precisely the advantage of viewing translation rules as specific terminological axioms, i.e., \u0393bridge and Abridge, working as a bridge between two languages (Definition 3). The two examples have shown how our approach can account for some essential features of institutions. 2.4 Institutional modules and roles Viewing institutions as the impositions of institutional descriptions on systems\" states and transitions allows for analyzing the normative system perspective itself (i.e., institutions are sets of norms) at a finer granularity. The sets of norms specifying an institution can be clustered on the grounds of the rea state types. EXAMPLE 3. Some observations are in order. EXAMPLE 4. \u00acrea(i, r) cond(i, r) ENACT(i, r).rea(i, r) (16) SEND(i, s, d) \u25e6 NOTIFY(s, i) ENACT(i, r) (17) Terminative modules Analogously, we call terminative modules those sets of terminological axioms stating how a state with certain institutional properties can be left. Examples 3-5 have shown how roles can be formalized in our framework thereby getting a formal semantics: roles are also sets of terminological axioms concerning state types of the sort rea(i, r). It is worth noticing that this modeling option is aligned with work on social theory addressing the concept of role such as . 2.5 Tractable specifications of institutions In the previous sections we fully deployed the expressivity of the language introduced in Section 2.1 and used its semantics to provide a formal understanding of many essential aspects of institutions in terms of transition systems. Tractable (i.e., polynomial time decidable) fragments of logic ALCH( ,\u25e6,\u00ac,id) can however be isolated which still exhibit some key expressive features. 3.1 Infrastructures In discussing Example 3 we observed how being entitled to make a bid does not imply being in state of making a bid. 632 The Sixth Intl. DEFINITION 4. EXAMPLE 6. We call a concrete institution specification CIns an institution specification Ins coupled with an infrastructure specification Inf. DEFINITION 5. Obviously, different infrastructures can be devised for a same institution giving rise to different concrete institutions which makes precise implementation choices explicit. EXAMPLE 7. Organizational structures typically concern the way agents interact within organizations. role s if \u03b3 is the case. By the expressions (r, s) \u2208 C\u03b3:\u03b1 and (r, s) \u2208 O\u03b3:\u03b1 we mean therefore: agents enacting role r can and, respectively, ought to interact with agents enacting role s by performing \u03b1 in states of type \u03b3. As shown in such formal representations of organizational structures are of use for investigating the structural properties (robustness, flexibility, etc.) At this point all the formal means are put in place which allow us to formally represent institutions as well as organizational structures. 3.3 Relating institutions to organizations The Sixth Intl. (executability) or x = rea(i, r) rea(j, s) \u03b3 \u2200\u00ac\u03b1.viol(i) (obligation) then t(x) = (r, s), where \u03b1 is an atomic transition-type indexed with a pair (i, j). DEFINITION 7. Definition 7 labels the yielded pairs accordingly to the syntactic form of the translated axioms. From a design perspective the interpretation of Definition 7 is twofold. Given a concrete institution CIns, Definition 7 allows a designer to be aware of the impact that specific terminological choices (in particular, the choice of certain bridge axioms) and infrastructural ones have at a structural level. This can be recursively done by just adding any new inferred inclusion x to the previous set of axioms \u0393, thus obtaining an updated institutional specification containing \u0393 \u222a {x}. To draw a parallelism with what just said in the previous paragraph, this process can be thought of as the inference of norms and infrastructural constraints from the specification of organizational structures. Definition 7 establishes a correspondence between two essentially different perspectives on the design of open systems allowing for feedbacks between the two to be formally analyzed.", "body2": "As a matter of fact, those theories have fostered the production of architectures and programming languages. The main result of the paper consists in showing how abstract constraints (institutions) can be step by step refined to concrete structural descriptions (organizational structures) of the to-be-implemented system, bridging thus the gap between abstract norms and concrete system specifications. In Section 4 some conclusions follow. We use the term to refer to any set of interacting agents whose behavior can usefully be regarded as governed by norms (, p.276). 100-101). At this point, the step toward eInstitutions is natural. An institutional property is therefore a property of system states or system transitions (i.e., a state type or a transition type) that does not belong to a merely technical, or factual, description of the system. The use of such formalism will also stress the idea of viewing institutions as the impositions of domain descriptions. (Syntax of ALCH( ,\u25e6,\u00ac,id) transition types and state type constructs are defined by the following BNF: \u03b1 := a | \u03b1 \u25e6 \u03b1 | \u03b1 \u03b1 | \u00ac\u03b1 | id(\u03b3) \u03b3 := c | \u22a5 | \u00ac\u03b3 | \u03b3 \u03b3 | \u2200\u03b1.\u03b3 where a and c are atomic transition types and, respectively, atomic state types. A terminological box (henceforth TBox) T = \u0393, A consists of a finite set \u0393 of state type inclusion assertions (\u03b31 \u03b32), and of a finite set A of transition type inclusion assertions (\u03b11 \u03b12). As usual, state types are interpreted as sets of states and transition types as sets of state pairs. An interpreted transition system m is a model of a TBox T = \u0393, A if m is a model of each inclusion assertion in \u0393 and A. Boolean operators are defined as usual. We will come back to some complexity features of this logic in Section 2.5. The step toward a formal grounding of this view of institutions is now short: norms can be thought of as terminological axioms, and institutions as sets of terminological axioms, i.e., terminological boxes. In symbols: L(Ins) \u2286 Lins \u222a Lbrute. The clause for Abridge is analogous. The definition states that an institution specification needs to be expressed on a language including institutional as well as brute terms (1); that a part of the specification concerns a description of mere institutional terms (3); and that there needs to be a part of the specification which connects institutional terms to brute ones (2). These transitions systems can be in turn thought of as all the possible MASs which model the specified institution. To make a concrete example, the AMELI middleware can be viewed as a specification tool at a Lbrute level. We will come back to this issue in Section 3. 2.3 From abstract to concrete norms To illustrate Definition 3, and show its explanatory power, an example follows which depicts an essential phenomenon of institutions. The abstract norm is formalized by Formula 1 by making use of a standard reduction technique of deontic notions (see ): the statement it is forbidden to discriminate on the basis of citizenship amounts to the statement after every execution of a transition of type DISCR(i, j) the system always ends up in a violation state. All formulae have to be read as schemata determining a finite number of subsumption expressions depending on the number of agents i, j considered. if dutch(j)then ACCEPT(i, j) else REJECT(i, j) DISCR(i, j) (4) SEND(msg33, i, j) ACCEPT(i, j) (5) SEND(msg38, i, j) REJECT(i, j) (6) \u2200ACCEPT(i, j).accepted(j) \u2261 (7) \u2200REJECT(i, j).\u00acaccepted(j) \u2261 (8) It is easy to see, on the grounds of the semantics exposed in Definition 2, that the following concrete inclusion statement holds w.r.t. What Example 1 shows is that the problem of the abstractness of norms in institutions can be formally addressed and can be given a precise formal semantics. This translation can nevertheless not be arbitrary . The axiom states that transitions where i receives e10 from j count as transitions of type DISCR(i, j). It is worth stressing that without providing a model-theoretic semantics for the translation rules linking the institutional notions to the brute ones, it would not be so straightforward to model the logical constraints to which the translations are subjected (Example 2). In , we have thoroughly compared this approach with approaches such as which conceive of translation rules as inference rules. In the next section the same framework is applied to provide a formal analysis of the notion of role. By stating how an agent can enact and \u2018deact\" a role r, and what normative consequences follow from the enactment of r, an institution describes expected forms of agents\" behavior while at the same time abstracting from the concrete agents taking part to the system. Status modules We call status modules those sets of terminological axioms which specify the institutional consequences of the occurrence of a given institutional state-of-affairs, for instance, the fact that agent i enacts role r. Formula 15 states how the bidding action can be executed by sending a specific message to j (SEND(i, j, msg49)). We address this issue extensively in Section 33 Institutive modules We call institutive modules those sets of terminological axioms of an institution specification describing how states with certain institutional properties can be reached, for instance, how an agent i can reach a state in which it enacts role r. They can be seen as procedures that the institution define in order for the agents to bring institutional states of affairs about. Such a procedure is performed through a system infrastructure s, which notifies to i that it has been registered as enacting role r after sending the necessary piece of data d (SEND(i, s, d)), e.g., a valid credit card number. (A terminative module for roles) Terminative modules for roles specify, for instance, how a transition type DEACT(i, r) can be executed which has as consequence the reaching of a state of type \u00acrea(i, r): rea(i, r) DEACT(i, r).\u00acrea(i, r) (18) SEND(i, s, msg9) DEACT(i, r) (19) That is to say, i deacting a role r always leads to a state where See in particular Example 6 and Definition 5 i does not enact role r; and i sending message No.9 to a specific interface infrastructure s count as i deacting role r. Examples 3-5 have shown how roles can be formalized in our framework thereby getting a formal semantics: roles are also sets of terminological axioms concerning state types of the sort rea(i, r). It is worth noticing that this modeling option is aligned with work on social theory addressing the concept of role such as . The satisfiability problem4 in logic ALCH( ,\u25e6,\u00ac,id) is undecidable since transition type inclusion axioms correspond to a version of what in Description Logic are known as role-value maps and logics extending ALC with role-value maps are known to be undecidable . Fragments like ELH(\u25e6) could be used as target logics within theory approximation approaches by aiming at compiling TBoxes expressed in ALCH( ,\u25e6,\u00ac,id) into approximations in those fragments. Notice therefore that EL is a seriously restricted fragment of ALC since it does not contain the negation operator for state types (operators and \u2200 remain thus undefinable). The formal specification of an infrastructure amounts to the formal specification of interaction requirements, that is to say, the specification of which relevant transition types are executable and under what conditions. It states what can be in concrete done and under what conditions. (21) Notice that the executability condition is just . We call a concrete institution specification CIns an institution specification Ins coupled with an infrastructure specification Inf. (Concrete institution) A concrete institution obtained by joining the institution Ins = \u0393ins, Ains and the infrastructure Inf = \u0393inf , Ainf is a TBox CIns = \u0393, A such that \u0393 = \u0393ins \u222a \u0393inf and A = Ains \u222a Ainf . A designer can choose to regiment such norm , i.e., make violation states unreachable, via an appropriate infrastructure. We refer to that work for a more comprehensive exposition. Links in a formal specification of an organizational structure stand therefore for statements of the kind: role r can (ought to, may) execute \u03b1 w.r.t. (Organizational structure) An organizational structure is a multi-graph: OS = Roles, {Cp}p\u2208Mod, {Op}p\u2208Mod where: \u2022 Mod denotes a set of pairs p = \u03b3 : \u03b1, that is, a set of state type (condition) and transition type (action) pairs of Lins\u222aLbrute with \u03b1 being an atomic transition-type indexed with a pair (i, j) denoting placeholders for the actor and the recipient of the transition; \u2022 C (can) denotes links to be interpreted in terms of the executability of the related \u03b1 in \u03b3, whereas O (ought) denotes links to be interpreted in terms of the obligation to execute the related \u03b1 in \u03b3. By the expressions (r, s) \u2208 C\u03b3:\u03b1 and (r, s) \u2208 O\u03b3:\u03b1 we mean therefore: agents enacting role r can and, respectively, ought to interact with agents enacting role s by performing \u03b1 in states of type \u03b3. that a given organization exhibits. In particular, it will become clear how a normative conception of the notion of role relates to a structural one, that is, how the view of roles as a sets of norms (specifying how an agent can enact and deact the role, and what social status it obtains by doing that) relates to the view of roles as positions within social structures. Function t is a partial function Sub Roles \u00d7 Roles such that, for any x \u2208 Sub if x = rea(i, r) rea(j, s) \u03b3 \u2203\u03b1. That is to say, executability and obligation laws containing the enactment configuration rea(i, r) rea(j, s) as a premise and concerning transition of types \u03b1, with i actor and j recipient of the \u03b1 transition, are translated into role pairs (r, s). iff t(x) \u2208 C\u03b3:\u03b1 \u2022 x = rea(i, r) rea(j, s) \u03b3 \u2200\u00ac\u03b1.viol(i) iff t(x) \u2208 O\u03b3:\u03b1 Intuitively, function t takes axioms from \u0393 (i.e., the set of state type terminological axioms of CIns) and yields pairs of roles. The institutive, terminative and status modules of roles are translated into definitions of positions within a OS. Let us see this in some more details. By checking whether a given inclusion statement of the relevant syntactic form follows from CIns (i.e., the so-called subsumption problem of DL) it is possible, via t, to add new links to the corresponding organizational structure. By relating structural links to state type inclusions it is therefore possible to check whether adding a link in OS results in a stronger institutional specification, that is, if the corresponding inclusion statement is not already implied by Ins. Exploiting Definition 7, we would obtain a number of executability laws in the fashion of Formula 20 for all roles in Roles (thus 2|Roles| axioms). This shows, in turn, that the added value of the specification of institutions lies precisely in the terminological link they establish between institutional and brute, i.e., system level notions.", "introduction": "The opportunity of a technology transfer from the field of organizational and social theory to distributed AI and multiagent systems (MASs) has long been advocated . In MASs the application of the organizational and institutional metaphors to system design has proven to be useful for the development of methodologies and tools. In many cases, however, the application of these conceptual apparatuses amounts to mere heuristics guiding the high level design of the systems. It is our thesis that the application of those apparatuses can be pushed further once their key concepts are treated formally, that is, once notions such as norm, role, structure, etc. This has been the case for agent programming languages after the relevant concepts borrowed from folk psychology (belief, intention, desire, knowledge, etc.) have been addressed in comprehensive formal logical theories such as, for instance, BDICTL and KARO . As a matter of fact, those theories have fostered the production of architectures and programming languages. What is lacking at the moment for the design and development of open MASs is, in our opinion, something that can play the role that BDI-like formalisms have played for the design and development of single-agent architectures. Aim of the present paper is to fill this gap with respect to the notion of institution providing formal foundations for the application of the institutional metaphor and for its relation to the organizational one. The main result of the paper consists in showing how abstract constraints (institutions) can be step by step refined to concrete structural descriptions (organizational structures) of the to-be-implemented system, bridging thus the gap between abstract norms and concrete system specifications. Concretely, in Section 2, a logical framework is presented which provides a formal semantics for the notions of institution, norm, role, and which supports the account of key features of institutions such as the translation of abstract norms into concrete and implementable ones, the institutional empowerment of agents, and some aspects of the design of norm enforcement. In Section 3 the framework is extended to deal with the notion of the infrastructure of an institution. The extended framework is then studied in relation to the formalism for representing organizational structures presented in . In Section 4 some conclusions follow.", "conclusion": "The paper aimed at providing a comprehensive formal analysis of the institutional metaphor and its relation to the organizational one.. The predominant formal tool has been description logic.. TBoxes has been used to represent the specifications of institutions (Definition 3) and their infrastructures (Definition 6), providing therefore a transition system semantics for a number of institutional notions (Examples 1-7).. Multi-graphs has then been used to represent the specification of organizational structures (Definition 6).. The last result presented concerned the definition of a formal correspondence between institution and organization specifications (Definition 7), which provides a formal way for switching between the two paradigms.. All in all, these results deliver a way for relating abstract system specifications (i.e., institutions as sets of norms) to specifications that are closer to an implemented system (i.e., organizational structures)."}
{"id": "C-46", "keywords": ["wireless sensor network", "archiv storag", "index method"], "title": "TSAR: A Two Tier Sensor Storage Architecture Using Interval Skip Graphs", "abstract": "Archival storage of sensor data is necessary for applications that query, mine, and analyze such data for interesting features and trends. We argue that existing storage systems are designed primarily for flat hierarchies of homogeneous sensor nodes and do not fully exploit the multi-tier nature of emerging sensor networks, where an application can comprise tens of tethered proxies, each managing tens to hundreds of untethered sensors. We present TSAR, a fundamentally different storage architecture that envisions separation of data from metadata by employing local archiving at the sensors and distributed indexing at the proxies. At the proxy tier, TSAR employs a novel multi-resolution ordered distributed index structure, the Interval Skip Graph, for efficiently supporting spatio-temporal and value queries. At the sensor tier,TSAR supports energy-aware adaptive summarization that can trade off the cost of transmitting metadata to the proxies against the overhead of false hits resulting from querying a coarse-grain index. We implement TSAR in a two-tier sensor testbed comprising Stargate-based proxies and Mote-based sensors. Our experiments demonstrate the benefits and feasibility of using our energy-efficient storage architecture in multi-tier sensor networks.", "references": ["Skip graphs", "Multidimensional binary search trees used for associative searching", "Towards sensor database systems", "ZigBee-ready RF transceiver", "Introduction to Algorithms", "Querying Peer-to-Peer Networks Using P-Trees", "ELF: an efficient log-structured flash file system for micro sensor nodes", "PRESTO: A predictive storage architecture for sensor networks", "An evaluation of multi-resolution storage in sensor networks", "A system for simulation, emulation, and deployment of heterogeneous sensor networks", "DIFS: A distributed index for features in sensor networks", "R-trees: a dynamic index structure for spatial searching", "Skipnet: A scalable overlay network with practical locality properties", "System architecture directions for networked sensors", "Samsung Semiconductor", "Directed diffusion: A scalable and robust communication paradigm for sensor networks", "Multi-dimensional range queries in sensor networks", "RP*: A family of order preserving scalable distributed data structures", "TAG: a tiny aggregation service for ad-hoc sensor networks", "High performance, low power sensor platforms featuring gigabyte scale storage", "Versatile low power media access for wireless sensor networks", "Skip lists: a probabilistic alternative to balanced trees", "Data-centric storage in sensornets", "A scalable content addressable network", "GHT - a geographic hash-table for data-centric storage"], "full_text": "1. INTRODUCTION 1.1 MOTIVATION Many different kinds of networked data-centric sensor applications have emerged in recent years. Sensors in these applications sense the environment and generate data that must be processed, filtered, interpreted, and archived in order to provide a useful infrastructure to its users. To achieve its goals, a typical sensor application needs access to both live and past sensor data. Whereas access to live data is necessary in monitoring and surveillance applications, access to past data is necessary for applications such as mining of sensor logs to detect unusual patterns, analysis of historical trends, and post-mortem analysis of particular events. Archival storage of past sensor data requires a storage system, the key attributes of which are: where the data is stored, whether it is indexed, and how the application can access this data in an energy-efficient manner with low latency. There have been a spectrum of approaches for constructing sensor storage systems. In the simplest, sensors stream data or events to a server for long-term archival storage , where the server often indexes the data to permit efficient access at a later time. Since sensors may be several hops from the nearest base station, network costs are incurred; however, once data is indexed and archived, subsequent data accesses can be handled locally at the server without incurring network overhead. In this approach, the storage is centralized, reads are efficient and cheap, while writes are expensive. Further, all data is propagated to the server, regardless of whether it is ever used by the application. An alternate approach is to have each sensor store data or events locally (e.g., in flash memory), so that all writes are local and incur no communication overheads. A read request, such as whether an event was detected by a particular sensor, requires a message to be sent to the sensor for processing. More complex read requests are handled by flooding. For instance, determining if an intruder was detected over a particular time interval requires the request to be flooded to all sensors in the system. Thus, in this approach, the storage is distributed, writes are local and inexpensive, while reads incur significant network overheads. Requests that require flooding, due to the lack of an index, are expensive and may waste precious sensor resources, even if no matching data is stored at those sensors. Research efforts such as Directed Diffusion have attempted to reduce these read costs, however, by intelligent message routing. Between these two extremes lie a number of other sensor storage systems with different trade-offs, summarized in Table 1. The geographic hash table (GHT) approach advocates the use of an in-network index to augment the fully distributed nature of sensor storage. In this approach, each data item has a key associated with it, and a distributed or geographic hash table is used to map keys to nodes that store the corresponding data items. Thus, writes cause data items to be sent to the hashed nodes and also trigger updates to the in-network hash table. A read request requires a lookup in the in-network hash table to locate the node that stores the data 39 item; observe that the presence of an index eliminates the need for flooding in this approach. Most of these approaches assume a flat, homogeneous architecture in which every sensor node is energy-constrained. In this paper, we propose a novel storage architecture called TSAR1 that reflects and exploits the multi-tier nature of emerging sensor networks, where the application is comprised of tens of tethered sensor proxies (or more), each controlling tens or hundreds of untethered sensors. TSAR is a component of our PRESTO predictive storage architecture, which combines archival storage with caching and prediction. We believe that a fundamentally different storage architecture is necessary to address the multi-tier nature of future sensor networks. Specifically, the storage architecture needs to exploit the resource-rich nature of proxies, while respecting resource constraints at the remote sensors. No existing sensor storage architecture explicitly addresses this dichotomy in the resource capabilities of different tiers. Any sensor storage system should also carefully exploit current technology trends, which indicate that the capacities of flash memories continue to rise as per Moore\"s Law, while their costs continue to plummet. Thus it will soon be feasible to equip each sensor with 1 GB of flash storage for a few tens of dollars. An even more compelling argument is the energy cost of flash storage, which can be as much as two orders of magnitude lower than that for communication. Newer NAND flash memories offer very low write and erase energy costs - 802.15.4 wireless radio in Section 6.2 indicates a 1:100 ratio in per-byte energy cost between the two devices, even before accounting for network protocol overheads. These trends, together with the energy-constrained nature of untethered sensors, indicate that local storage offers a viable, energy-efficient alternative to communication in sensor networks. TSAR exploits these trends by storing data or events locally on the energy-efficient flash storage at each sensor. Sensors send concise identifying information, which we term metadata, to a nearby proxy; depending on the representation used, this metadata may be an order of magnitude or more smaller than the data itself, imposing much lower communication costs. The resource-rich proxies interact with one another to construct a distributed index of the metadata reported from all sensors, and thus an index of the associated data stored at the sensors. This index provides a unified, logical view of the distributed data, and enables an application to query and read past data efficiently - the index is used to pinpoint all data that match a read request, followed by messages to retrieve that data from the corresponding sensors. In-network index lookups are eliminated, reducing network overheads for read requests. This separation of data, which is stored at the sensors, and the metadata, which is stored at the proxies, enables TSAR to reduce energy overheads at the sensors, by leveraging resources at tethered proxies. 1.2 CONTRIBUTIONS This paper presents TSAR, a novel two-tier storage architecture for sensor networks. To the best of our knowledge, this is the first sensor storage system that is explicitly tailored for emerging multitier sensor networks. Our design and implementation of TSAR has resulted in four contributions. At the core of the TSAR architecture is a novel distributed index structure based on interval skip graphs that we introduce in this paper. This index structure can store coarse summaries of sensor data and organize them in an ordered manner to be easily search1 TSAR: Tiered Storage ARchitecture for sensor networks. able. This data structure has O(log n) expected search and update complexity. Further, the index provides a logically unified view of all data in the system. Second, at the sensor level, each sensor maintains a local archive that stores data on flash memory. Our storage architecture is fully stateless at each sensor from the perspective of the metadata index; all index structures are maintained at the resource-rich proxies, and only direct requests or simple queries on explicitly identified storage locations are sent to the sensors. Storage at the remote sensor is in effect treated as appendage of the proxy, resulting in low implementation complexity, which makes it ideal for small, resourceconstrained sensor platforms. Further, the local store is optimized for time-series access to archived data, as is typical in many applications. Each sensor periodically sends a summary of its data to a proxy. TSAR employs a novel adaptive summarization technique that adapts the granularity of the data reported in each summary to the ratio of false hits for application queries. More fine grain summaries are sent whenever more false positives are observed, thereby balancing the energy cost of metadata updates and false positives. Third, we have implemented a prototype of TSAR on a multi-tier testbed comprising Stargate-based proxies and Mote-based sensors. Our implementation supports spatio-temporal, value, and rangebased queries on sensor data. Fourth, we conduct a detailed experimental evaluation of TSAR using a combination of EmStar/EmTOS and our prototype. While our EmStar/EmTOS experiments focus on the scalability of TSAR in larger settings, our prototype evaluation involves latency and energy measurements in a real setting. Our results demonstrate the logarithmic scaling property of the sparse skip graph and the low latency of end-to-end queries in a duty-cycled multi-hop network . The remainder of this paper is structured as follows. Section 2 presents key design issues that guide our work. Section 3 and 4 present the proxy-level index and the local archive and summarization at a sensor, respectively. Section 5 discusses our prototype implementation, and Section 6 presents our experimental results. We present related work in Section 7 and our conclusions in Section 8. 2. DESIGN CONSIDERATIONS In this section, we first describe the various components of a multi-tier sensor network assumed in our work. We then present a description of the expected usage models for this system, followed by several principles addressing these factors which guide the design of our storage system. 2.1 SYSTEM MODEL We envision a multi-tier sensor network comprising multiple tiers - a bottom tier of untethered remote sensor nodes, a middle tier of tethered sensor proxies, and an upper tier of applications and user terminals (see Figure 1). The lowest tier is assumed to form a dense deployment of lowpower sensors. A canonical sensor node at this tier is equipped with low-power sensors, a micro-controller, and a radio as well as a significant amount of flash memory (e.g., 1GB). The common constraint for this tier is energy, and the need for a long lifetime in spite of a finite energy constraint. The use of radio, processor, RAM, and the flash memory all consume energy, which needs to be limited. In general, we assume radio communication to be substantially more expensive than accesses to flash memory. The middle tier consists of power-rich sensor proxies that have significant computation, memory and storage resources and can use 40 Table 1: Characteristics of sensor storage systems System Data Index Reads Writes Order preserving Centralized store Centralized Centralized index Handled at store Send to store Yes Local sensor store Fully distributed No index Flooding, diffusion Local No GHT/DCS Fully distributed In-network index Hash to node Send to hashed node No TSAR/PRESTO Fully distributed Distributed index at proxies Proxy lookup + sensor query Local plus index update Yes User Unified Logical Store Queries (time, space, value) Query Response Cache Query forwarding Proxy Remote Sensors Local Data Archive on Flash Memory Interval Skip Graph Query forwarding summaries start index end index linear traversal Query Response Cache-miss triggered query forwarding summaries Figure 1: Architecture of a multi-tier sensor network. these resources continuously. In urban environments, the proxy tier would comprise a tethered base-station class nodes (e.g., Crossbow Stargate), each with with multiple radios-an 802.11 radio that connects it to a wireless mesh network and a low-power radio (e.g. 802.15.4) that connects it to the sensor nodes. In remote sensing applications , this tier could comprise a similar Stargate node with a solar power cell. Each proxy is assumed to manage several tens to hundreds of lower-tier sensors in its vicinity. A typical sensor network deployment will contain multiple geographically distributed proxies. For instance, in a building monitoring application, one sensor proxy might be placed per floor or hallway to monitor temperature, heat and light sensors in their vicinity. At the highest tier of our infrastructure are applications that query the sensor network through a query interface. In this work, we focus on applications that require access to past sensor data. To support such queries, the system needs to archive data on a persistent store. Our goal is to design a storage system that exploits the relative abundance of resources at proxies to mask the scarcity of resources at the sensors. 2.2 USAGE MODELS The design of a storage system such as TSAR is affected by the queries that are likely to be posed to it. A large fraction of queries on sensor data can be expected to be spatio-temporal in nature. Sensors provide information about the physical world; two key attributes of this information are when a particular event or activity occurred and where it occurred. Some instances of such queries include the time and location of target or intruder detections (e.g., security and monitoring applications), notifications of specific types of events such as pressure and humidity values exceeding a threshold (e.g., industrial applications), or simple data collection queries which request data from a particular time or location (e.g., weather or environment monitoring). Expected queries of such data include those requesting ranges of one or more attributes; for instance, a query for all image data from cameras within a specified geographic area for a certain period of time. In addition, it is often desirable to support efficient access to data in a way that maintains spatial and temporal ordering. There are several ways of supporting range queries, such as locality-preserving hashes such as are used in DIMS . However, the most straightforward mechanism, and one which naturally provides efficient ordered access, is via the use of order-preserving data structures. Order-preserving structures such as the well-known B-Tree maintain relationships between indexed values and thus allow natural access to ranges, as well as predecessor and successor operations on their key values. Applications may also pose value-based queries that involve determining if a value v was observed at any sensor; the query returns a list of sensors and the times at which they observed this value. Variants of value queries involve restricting the query to a geographical region, or specifying a range (v1, v2) rather than a single value v. Value queries can be handled by indexing on the values reported in the summaries. Specifically, if a sensor reports a numerical value, then the index is constructed on these values. A search involves finding matching values that are either contained in the search range (v1, v2) or match the search value v exactly. Hybrid value and spatio-temporal queries are also possible. Such queries specify a time interval, a value range and a spatial region and request all records that match these attributes - find all instances where the temperature exceeded 100o F at location R during the month of August. These queries require an index on both time and value. In TSAR our focus is on range queries on value or time, with planned extensions to include spatial scoping. 2.3 DESIGN PRINCIPLES Our design of a sensor storage system for multi-tier networks is based on the following set of principles, which address the issues arising from the system and usage models above. \u2022 Principle 1: Store locally, access globally: Current technology allows local storage to be significantly more energyefficient than network communication, while technology trends show no signs of erasing this gap in the near future. For maximum network life a sensor storage system should leverage the flash memory on sensors to archive data locally, substituting cheap memory operations for expensive radio transmission. But without efficient mechanisms for retrieval, the energy gains of local storage may be outweighed by communication costs incurred by the application in searching for data. We believe that if the data storage system provides the abstraction of a single logical store to applications, as 41 does TSAR, then it will have additional flexibility to optimize communication and storage costs. \u2022 Principle 2: Distinguish data from metadata: Data must be identified so that it may be retrieved by the application without exhaustive search. To do this, we associate metadata with each data record - data fields of known syntax which serve as identifiers and may be queried by the storage system. Examples of this metadata are data attributes such as location and time, or selected or summarized data values. We leverage the presence of resource-rich proxies to index metadata for resource-constrained sensors. The proxies share this metadata index to provide a unified logical view of all data in the system, thereby enabling efficient, low-latency lookups. Such a tier-specific separation of data storage from metadata indexing enables the system to exploit the idiosyncrasies of multi-tier networks, while improving performance and functionality. \u2022 Principle 3: Provide data-centric query support: In a sensor application the specific location (i.e. offset) of a record in a stream is unlikely to be of significance, except if it conveys information concerning the location and/or time at which the information was generated. We thus expect that applications will be best served by a query interface which allows them to locate data by value or attribute (e.g. location and time), rather than a read interface for unstructured data. This in turn implies the need to maintain metadata in the form of an index that provides low cost lookups. 2.4 SYSTEM DESIGN TSAR embodies these design principles by employing local storage at sensors and a distributed index at the proxies. The key features of the system design are as follows: In TSAR, writes occur at sensor nodes, and are assumed to consist of both opaque data as well as application-specific metadata. This metadata is a tuple of known types, which may be used by the application to locate and identify data records, and which may be searched on and compared by TSAR in the course of locating data for the application. In a camera-based sensing application, for instance, this metadata might include coordinates describing the field of view, average luminance, and motion values, in addition to basic information such as time and sensor location. Depending on the application, this metadata may be two or three orders of magnitude smaller than the data itself, for instance if the metadata consists of features extracted from image or acoustic data. In addition to storing data locally, each sensor periodically sends a summary of reported metadata to a nearby proxy. The summary contains information such as the sensor ID, the interval (t1, t2) over which the summary was generated, a handle identifying the corresponding data record (e.g. its location in flash memory), and a coarse-grain representation of the metadata associated with the record. The precise data representation used in the summary is application-specific; for instance, a temperature sensor might choose to report the maximum and minimum temperature values observed in an interval as a coarse-grain representation of the actual time series. The proxy uses the summary to construct an index; the index is global in that it stores information from all sensors in the system and it is distributed across the various proxies in the system. Thus, applications see a unified view of distributed data, and can query the index at any proxy to get access to data stored at any sensor. Specifically, each query triggers lookups in this distributed index and the list of matches is then used to retrieve the corresponding data from the sensors. There are several distributed index and lookup methods which might be used in this system; however, the index structure described in Section 3 is highly suited for the task. Since the index is constructed using a coarse-grain summary, instead of the actual data, index lookups will yield approximate matches. The TSAR summarization mechanism guarantees that index lookups will never yield false negatives - i.e. it will never miss summaries which include the value being searched for. However, index lookups may yield false positives, where a summary matches the query but when queried the remote sensor finds no matching value, wasting network resources. The more coarse-grained the summary, the lower the update overhead and the greater the fraction of false positives, while finer summaries incur update overhead while reducing query overhead due to false positives. Remote sensors may easily distinguish false positives from queries which result in search hits, and calculate the ratio between the two; based on this ratio, TSAR employs a novel adaptive technique that dynamically varies the granularity of sensor summaries to balance the metadata overhead and the overhead of false positives. 3. DATA STRUCTURES At the proxy tier, TSAR employs a novel index structure called the Interval Skip Graph, which is an ordered, distributed data structure for finding all intervals that contain a particular point or range of values. Interval skip graphs combine Interval Trees , an interval-based binary search tree, with Skip Graphs , a ordered, distributed data structure for peer-to-peer systems . The resulting data structure has two properties that make it ideal for sensor networks. First, it has O(log n) search complexity for accessing the first interval that matches a particular value or range, and constant complexity for accessing each successive interval. Second, indexing of intervals rather than individual values makes the data structure ideal for indexing summaries over time or value. Such summary-based indexing is a more natural fit for energyconstrained sensor nodes, since transmitting summaries incurs less energy overhead than transmitting all sensor data. Definitions: We assume that there are Np proxies and Ns sensors in a two-tier sensor network. Each proxy is responsible for multiple sensor nodes, and no assumption is made about the number of sensors per proxy. Each sensor transmits interval summaries of data or events regularly to one or more proxies that it is associated with, where interval i is represented as [lowi, highi]. These intervals can correspond to time or value ranges that are used for indexing sensor data. No assumption is made about the size of an interval or about the amount of overlap between intervals. Range queries on the intervals are posed by users to the network of proxies and sensors; each query q needs to determine all index values that overlap the interval [lowq, highq]. The goal of the interval skip graph is to index all intervals such that the set that overlaps a query interval can be located efficiently. In the rest of this section, we describe the interval skip graph in greater detail. 3.1 SKIP GRAPH OVERVIEW In order to inform the description of the Interval Skip Graph, we first provide a brief overview of the Skip Graph data structure; for a more extensive description the reader is referred to . Figure 2 shows a skip graph which indexes 8 keys; the keys may be seen along the bottom, and above each key are the pointers associated with that key. Each data element, consisting of a key and its associated pointers, may reside on a different node in the network, 42 7 9 13 17 21 25 311 level 0 level 1 level 2 key single skip graph element (each may be on different node) find(21) node-to-node messages Figure 2: Skip Graph of 8 Elements 5 14 14 16 23 23 27 30 [low,high] max contains(13) match no match halt Figure 3: Interval Skip Graph Node 1 Node 2 Node 3 level 2 level 1 level 0 Figure 4: Distributed Interval Skip Graph and pointers therefore identify both a remote node as well as a data element on that node. In this figure we may see the following properties of a skip graph: \u2022 Ordered index: The keys are members of an ordered data type, for instance integers. Lookups make use of ordered comparisons between the search key and existing index entries. In addition, the pointers at the lowest level point directly to the successor of each item in the index. \u2022 In-place indexing: Data elements remain on the nodes where they were inserted, and messages are sent between nodes to establish links between those elements and others in the index. \u2022 Log n height: There are log2 n pointers associated with each element, where n is the number of data elements indexed. Each pointer belongs to a level l in [0... log2 n \u2212 1], and together with some other pointers at that level forms a chain of n/2l elements. \u2022 Probabilistic balance: Rather than relying on re-balancing operations which may be triggered at insert or delete, skip graphs implement a simple random balancing mechanism which maintains close to perfect balance on average, with an extremely low probability of significant imbalance. \u2022 Redundancy and resiliency: Each data element forms an independent search tree root, so searches may begin at any node in the network, eliminating hot spots at a single search root. In addition the index is resilient against node failure; data on the failed node will not be accessible, but remaining data elements will be accessible through search trees rooted on other nodes. In Figure 2 we see the process of searching for a particular value in a skip graph. The pointers reachable from a single data element form a binary tree: a pointer traversal at the highest level skips over n/2 elements, n/4 at the next level, and so on. Search consists of descending the tree from the highest level to level 0, at each level comparing the target key with the next element at that level and deciding whether or not to traverse. In the perfectly balanced case shown here there are log2 n levels of pointers, and search will traverse 0 or 1 pointers at each level. We assume that each data element resides on a different node, and measure search cost by the number messages sent (i.e. the number of pointers traversed); this will clearly be O(log n). Tree update proceeds from the bottom, as in a B-Tree, with the root(s) being promoted in level as the tree grows. In this way, for instance, the two chains at level 1 always contain n/2 entries each, and there is never a need to split chains as the structure grows. The update process then consists of choosing which of the 2l chains to insert an element into at each level l, and inserting it in the proper place in each chain. Maintaining a perfectly balanced skip graph as shown in Figure 2 would be quite complex; instead, the probabilistic balancing method introduced in Skip Lists is used, which trades off a small amount of overhead in the expected case in return for simple update and deletion. The basis for this method is the observation that any element which belongs to a particular chain at level l can only belong to one of two chains at level l+1. To insert an element we ascend levels starting at 0, randomly choosing one of the two possible chains at each level, an stopping when we reach an empty chain. One means of implementation (e.g. as described in ) is to assign each element an arbitrarily long random bit string. Each chain at level l is then constructed from those elements whose bit strings match in the first l bits, thus creating 2l possible chains at each level and ensuring that each chain splits into exactly two chains at the next level. Although the resulting structure is not perfectly balanced, following the analysis in we can show that the probability of it being significantly out of balance is extremely small; in addition, since the structure is determined by the random number stream, input data patterns cannot cause the tree to become imbalanced. 3.2 INTERVAL SKIP GRAPH A skip graph is designed to store single-valued entries. In this section, we introduce a novel data structure that extends skip graphs to store intervals [lowi, highi] and allows efficient searches for all intervals covering a value v, i.e. {i : lowi \u2264 v \u2264 highi}. Our data structure can be extended to range searches in a straightforward manner. The interval skip graph is constructed by applying the method of augmented search trees, as described by Cormen, Leiserson, and Rivest and applied to binary search trees to create an Interval Tree. The method is based on the observation that a search structure based on comparison of ordered keys, such as a binary tree, may also be used to search on a secondary key which is non-decreasing in the first key. Given a set of intervals sorted by lower bound - lowi \u2264 lowi+1 - we define the secondary key as the cumulative maximum, maxi = maxk=0...i (highk). The set of intervals intersecting a value v may then be found by searching for the first interval (and thus the interval with least lowi) such that maxi \u2265 v. We then 43 traverse intervals in increasing order lower bound, until we find the first interval with lowi > v, selecting those intervals which intersect v. Using this approach we augment the skip graph data structure, as shown in Figure 3, so that each entry stores a range (lower bound and upper bound) and a secondary key (cumulative maximum of upper bound). To efficiently calculate the secondary key maxi for an entry i, we take the greatest of highi and the maximum values reported by each of i\"s left-hand neighbors. To search for those intervals containing the value v, we first search for v on the secondary index, maxi, and locate the first entry with maxi \u2265 v. (by the definition of maxi, for this data element maxi = highi.) If lowi > v, then this interval does not contain v, and no other intervals will, either, so we are done. Otherwise we traverse the index in increasing order of mini, returning matching intervals, until we reach an entry with mini > v and we are done. Searches for all intervals which overlap a query range, or which completely contain a query range, are straightforward extensions of this mechanism. Lookup Complexity: Lookup for the first interval that matches a given value is performed in a manner very similar to an interval tree. The complexity of search is O(log n). The number of intervals that match a range query can vary depending on the amount of overlap in the intervals being indexed, as well as the range specified in the query. Insert Complexity: In an interval tree or interval skip list, the maximum value for an entry need only be calculated over the subtree rooted at that entry, as this value will be examined only when searching within the subtree rooted at that entry. For a simple interval skip graph, however, this maximum value for an entry must be computed over all entries preceding it in the index, as searches may begin anywhere in the data structure, rather than at a distinguished root element. It may be easily seen that in the worse case the insertion of a single interval (one that covers all existing intervals in the index) will trigger the update of all entries in the index, for a worst-case insertion cost of O(n). 3.3 SPARSE INTERVAL SKIP GRAPH The final extensions we propose take advantage of the difference between the number of items indexed in a skip graph and the number of systems on which these items are distributed. The cost in network messages of an operation may be reduced by arranging the data structure so that most structure traversals occur locally on a single node, and thus incur zero network cost. In addition, since both congestion and failure occur on a per-node basis, we may eliminate links without adverse consequences if those links only contribute to load distribution and/or resiliency within a single node. These two modifications allow us to achieve reductions in asymptotic complexity of both update and search. As may be in Section 3.2, insert and delete cost on an interval skip graph has a worst case complexity of O(n), compared to O(log n) for an interval tree. The main reason for the difference is that skip graphs have a full search structure rooted at each element, in order to distribute load and provide resilience to system failures in a distributed setting. However, in order to provide load distribution and failure resilience it is only necessary to provide a full search structure for each system. If as in TSAR the number of nodes (proxies) is much smaller than the number of data elements (data summaries indexed), then this will result in significant savings. Implementation: To construct a sparse interval skip graph, we ensure that there is a single distinguished element on each system, the root element for that system; all searches will start at one of these root elements. When adding a new element, rather than splitting lists at increasing levels l until the element is in a list with no others, we stop when we find that the element would be in a list containing no root elements, thus ensuring that the element is reachable from all root elements. An example of applying this optimization may be seen in Figure 5. (In practice, rather than designating existing data elements as roots, as shown, it may be preferable to insert null values at startup.) When using the technique of membership vectors as in , this may be done by broadcasting the membership vectors of each root element to all other systems, and stopping insertion of an element at level l when it does not share an l-bit prefix with any of the Np root elements. The expected number of roots sharing a log2Np-bit prefix is 1, giving an expected expected height for each element of log2Np +O(1). An alternate implementation, which distributes information concerning root elements at pointer establishment time, is omitted due to space constraints; this method eliminates the need for additional messages. Performance: In a (non-interval) sparse skip graph, since the expected height of an inserted element is now log2 Np + O(1), expected insertion complexity is O(log Np), rather than O(log n), where Np is the number of root elements and thus the number of separate systems in the network. (In the degenerate case of a single system we have a skip list; with splitting probability 0.5 the expected height of an individual element is 1.) Note that since searches are started at root elements of expected height log2 n, search complexity is not improved. For an interval sparse skip graph, update performance is improved considerably compared to the O(n) worst case for the nonsparse case. In an augmented search structure such as this, an element only stores information for nodes which may be reached from that element-e.g. the subtree rooted at that element, in the case of a tree. Thus, when updating the maximum value in an interval tree, the update is only propagated towards the root. In a sparse interval skip graph, updates to a node only propagate towards the Np root elements, for a worst-case cost of Np log2 n. Shortcut search: When beginning a search for a value v, rather than beginning at the root on that proxy, we can find the element that is closest to v (e.g. using a secondary local index), and then begin the search at that element. The expected distance between this element and the search terminus is log2 Np, and the search will now take on average log2 Np + O(1) steps. To illustrate this optimization, in Figure 4 depending on the choice of search root, a search for beginning at node 2 may take 3 network hops, traversing to node 1, then back to node 2, and finally to node 3 where the destination is located, for a cost of 3 messages. The shortcut search, however, locates the intermediate data element on node 2, and then proceeds directly to node 3 for a cost of 1 message. Performance: This technique may be applied to the primary key search which is the first of two insertion steps in an interval skip graph. By combining the short-cut optimization with sparse interval skip graphs, the expected cost of insertion is now O(log Np), independent of the size of the index or the degree of overlap of the inserted intervals. 3.4 ALTERNATIVE DATA STRUCTURES Thus far we have only compared the sparse interval skip graph with similar structures from which it is derived. A comparison with several other data structures which meet at least some of the requirements for the TSAR index is shown in Table 2. 44 Table 2: Comparison of Distributed Index Structures Range Query Support Interval Representation Re-balancing Resilience Small Networks Large Networks DHT, GHT no no no yes good good Local index, flood query yes yes no yes good bad P-tree, RP* (distributed B-Trees) yes possible yes no good good DIMS yes no yes yes yes yes Interval Skipgraph yes yes no yes good good Roots Node 1 Node 2 Figure 5: Sparse Interval Skip Graph The hash-based systems, DHT and GHT , lack the ability to perform range queries and are thus not well-suited to indexing spatio-temporal data. Indexing locally using an appropriate singlenode structure and then flooding queries to all proxies is a competitive alternative for small networks; for large networks the linear dependence on the number of proxies becomes an issue. Two distributed B-Trees were examined - P-Trees and RP* . Each of these supports range queries, and in theory could be modified to support indexing of intervals; however, they both require complex re-balancing, and do not provide the resilience characteristics of the other structures. DIMS provides the ability to perform spatio-temporal range queries, and has the necessary resilience to failures; however, it cannot be used index intervals, which are used by TSAR\"s data summarization algorithm. 4. DATA STORAGE AND SUMMARIZATION Having described the proxy-level index structure, we turn to the mechanisms at the sensor tier. TSAR implements two key mechanisms at the sensor tier. The first is a local archival store at each sensor node that is optimized for resource-constrained devices. The second is an adaptive summarization technique that enables each sensor to adapt to changing data and query characteristics. The rest of this section describes these mechanisms in detail. 4.1 LOCAL STORAGE AT SENSORS Interval skip graphs provide an efficient mechanism to lookup sensor nodes containing data relevant to a query. These queries are then routed to the sensors, which locate the relevant data records in the local archive and respond back to the proxy. To enable such lookups, each sensor node in TSAR maintains an archival store of sensor data. While the implementation of such an archival store is straightforward on resource-rich devices that can run a database, sensors are often power and resource-constrained. Consequently, the sensor archiving subsystem in TSAR is explicitly designed to exploit characteristics of sensor data in a resource-constrained setting. Timestamp Calibration Parameters Opaque DataData/Event Attributes size Figure 6: Single storage record Sensor data has very distinct characteristics that inform our design of the TSAR archival store. Sensors produce time-series data streams, and therefore, temporal ordering of data is a natural and simple way of storing archived sensor data. In addition to simplicity, a temporally ordered store is often suitable for many sensor data processing tasks since they involve time-series data processing. Examples include signal processing operations such as FFT, wavelet transforms, clustering, similarity matching, and target detection. Consequently, the local archival store is a collection of records, designed as an append-only circular buffer, where new records are appended to the tail of the buffer. The format of each data record is shown in Figure 6. Each record has a metadata field which includes a timestamp, sensor settings, calibration parameters, etc. Raw sensor data is stored in the data field of the record. The data field is opaque and application-specific-the storage system does not know or care about interpreting this field. A camera-based sensor, for instance, may store binary images in this data field. In order to support a variety of applications, TSAR supports variable-length data fields; as a result, record sizes can vary from one record to another. Our archival store supports three operations on records: create, read, and delete. Due to the append-only nature of the store, creation of records is simple and efficient. The create operation simply creates a new record and appends it to the tail of the store. Since records are always written at the tail, the store need not maintain a free space list. All fields of the record need to be specified at creation time; thus, the size of the record is known a priori and the store simply allocates the the corresponding number of bytes at the tail to store the record. Since writes are immutable, the size of a record does not change once it is created. proxy proxy proxy record 3 record summary local archive in flash memory data summary start,end offset time interval sensor summary sent to proxy Insert summaries into interval skip graph Figure 7: Sensor Summarization 45 The read operation enables stored records to be retrieved in order to answer queries. In a traditional database system, efficient lookups are enabled by maintaining a structure such as a B-tree that indexes certain keys of the records. However, this can be quite complex for a small sensor node with limited resources. Consequently, TSAR sensors do not maintain any index for the data stored in their archive. Instead, they rely on the proxies to maintain this metadata index-sensors periodically send the proxy information summarizing the data contained in a contiguous sequence of records, as well as a handle indicating the location of these records in flash memory. The mechanism works as follows: In addition to the summary of sensor data, each node sends metadata to the proxy containing the time interval corresponding to the summary, as well as the start and end offsets of the flash memory location where the raw data corresponding is stored (as shown in Figure 7). Thus, random access is enabled at granularity of a summary-the start offset of each chunk of records represented by a summary is known to the proxy. Within this collection, records are accessed sequentially. When a query matches a summary in the index, the sensor uses these offsets to access the relevant records on its local flash by sequentially reading data from the start address until the end address. Any queryspecific operation can then be performed on this data. Thus, no index needs to be maintained at the sensor, in line with our goal of simplifying sensor state management. The state of the archive is captured in the metadata associated with the summaries, and is stored and maintained at the proxy. While we anticipate local storage capacity to be large, eventually there might be a need to overwrite older data, especially in high data rate applications. This may be done via techniques such as multi-resolution storage of data , or just simply by overwriting older data. When older data is overwritten, a delete operation is performed, where an index entry is deleted from the interval skip graph at the proxy and the corresponding storage space in flash memory at the sensor is freed. 4.2 ADAPTIVE SUMMARIZATION The data summaries serve as glue between the storage at the remote sensor and the index at the proxy. Each update from a sensor to the proxy includes three pieces of information: the summary, a time period corresponding to the summary, and the start and end offsets for the flash archive. In general, the proxy can index the time interval representing a summary or the value range reported in the summary (or both). The former index enables quick lookups on all records seen during a certain interval, while the latter index enables quick lookups on all records matching a certain value. As described in Section 2.4, there is a trade-off between the energy used in sending summaries (and thus the frequency and resolution of those summaries) and the cost of false hits during queries. The coarser and less frequent the summary information, the less energy required, while false query hits in turn waste energy on requests for non-existent data. TSAR employs an adaptive summarization technique that balances the cost of sending updates against the cost of false positives. The key intuition is that each sensor can independently identify the fraction of false hits and true hits for queries that access its local archive. If most queries result in true hits, then the sensor determines that the summary can be coarsened further to reduce update costs without adversely impacting the hit ratio. If many queries result in false hits, then the sensor makes the granularity of each summary finer to reduce the number and overhead of false hits. The resolution of the summary depends on two parametersthe interval over which summaries of the data are constructed and transmitted to the proxy, as well as the size of the applicationspecific summary. Our focus in this paper is on the interval over which the summary is constructed. Changing the size of the data summary can be performed in an application-specific manner (e.g. using wavelet compression techniques as in ) and is beyond the scope of this paper. Currently, TSAR employs a simple summarization scheme that computes the ratio of false and true hits and decreases (increases) the interval between summaries whenever this ratio increases (decreases) beyond a threshold. 5. TSAR IMPLEMENTATION We have implemented a prototype of TSAR on a multi-tier sensor network testbed. Our prototype employs Crossbow Stargate nodes to implement the proxy tier. Each Stargate node employs a 400MHz Intel XScale processor with 64MB RAM and runs the Linux 2.4.19 kernel and EmStar release 2.1. The proxy nodes are equipped with two wireless radios, a Cisco Aironet 340-based 802.11b radio and a hostmote bridge to the Mica2 sensor nodes using the EmStar transceiver. The 802.11b wireless network is used for inter-proxy communication within the proxy tier, while the wireless bridge enables sensor-proxy communication. The sensor tier consists of Crossbow Mica2s and Mica2dots, radio, a BMAC protocol stack, a 4 Mb on-board flash memory and an ATMega 128L processor. The sensor nodes run TinyOS 1.1.8. In addition to the on-board flash, the sensor nodes can be equipped with external MMC/SD flash cards using a custom connector. The proxy nodes can be equipped with external storage such as high-capacity compact flash (up to 4GB), 6GB micro-drives, or up to 60GB 1.8inch mobile disk drives. Since sensor nodes may be several hops away from the nearest proxy, the sensor tier employs multi-hop routing to communicate with the proxy tier. In addition, to reduce the power consumption of the radio while still making the sensor node available for queries, low power listening is enabled, in which the radio receiver is periodically powered up for a short interval to sense the channel for transmissions, and the packet preamble is extended to account for the latency until the next interval when the receiving radio wakes up. Our prototype employs the MultiHopLEPSM routing protocol with the BMAC layer configured in the low-power mode with a 11% duty cycle (one of the default BMAC parameters) Our TSAR implementation on the Mote involves a data gathering task that periodically obtains sensor readings and logs these reading to flash memory. The flash memory is assumed to be a circular append-only store and the format of the logged data is depicted in Figure 6. The Mote sends a report to the proxy every N readings, summarizing the observed data. The report contains: (i) the address of the Mote, (ii) a handle that contains an offset and the length of the region in flash memory containing data referred to by the summary, (iii) an interval (t1, t2) over which this report is generated, (iv) a tuple (low, high) representing the minimum and the maximum values observed at the sensor in the interval, and (v) a sequence number. The sensor updates are used to construct a sparse interval skip graph that is distributed across proxies, via network messages between proxies over the 802.11b wireless network. Our current implementation supports queries that request records matching a time interval (t1, t2) or a value range (v1, v2). Spatial constraints are specified using sensor IDs. Given a list of matching intervals from the skip graph, TSAR supports two types of messages to query the sensor: lookup and fetch. A lookup message triggers a search within the corresponding region in flash memory and returns the number of matching records in that memory region (but does not retrieve data). In contrast, NumberofMessages Index size (entries) Insert (skipgraph) Insert (sparse skipgraph) Initial lookup (a) NumberofMessages Index size (entries) Insert (skipgraph) Insert (sparse skipgraph) Initial lookup (b) Synthetic Data Figure 8: Skip Graph Insert Performance triggers a search but also returns all matching data records to the proxy. Lookup messages are useful for polling a sensor, for instance, to determine if a query matches too many records. 6. EXPERIMENTAL EVALUATION In this section, we evaluate the efficacy of TSAR using our prototype and simulations. The testbed for our experiments consists of four Stargate proxies and twelve Mica2 and Mica2dot sensors; three sensors each are assigned to each proxy. Given the limited size of our testbed, we employ simulations to evaluate the behavior of TSAR in larger settings. Our simulation employs the EmTOS emulator , which enables us to run the same code in simulation and the hardware platform. Rather than using live data from a real sensor, to ensure repeatable experiments, we seed each sensor node with a dataset (i.e., a trace) that dictates the values reported by that node to the proxy. One section of the flash memory on each sensor node is programmed with data points from the trace; these observations are then replayed during an experiment, logged to the local archive (located in flash memory, as well), and reported to the proxy. The first dataset used to evaluate TSAR is a temperature dataset from James Reserve that includes data from eleven temperature sensor nodes over a period of 34 days. The second dataset is synthetically generated; the trace for each sensor is generated using a uniformly distributed random walk though the value space. Our experimental evaluation has four parts. First, we run EmTOS simulations to evaluate the lookup, update and delete overhead for sparse interval skip graphs using the real and synthetic datasets. Second, we provide summary results from micro-benchmarks of the storage component of TSAR, which include empirical characterization of the energy costs and latency of reads and writes for the flash memory chip as well as the whole mote platform, and comparisons to published numbers for other storage and communication technologies. These micro-benchmarks form the basis for our full-scale evaluation of TSAR on a testbed of four Stargate proxies and twelve Motes. We measure the end-to-end query latency in our multi-hop testbed as well as the query processing overhead at the mote tier. Finally, we demonstrate the adaptive summarization capability at each sensor node. The remainder of this section presents our experimental results. 6.1 SPARSE INTERVAL SKIP GRAPH PERFORMANCE This section evaluates the performance of sparse interval skip graphs by quantifying insert, lookup and delete overheads. We assume a proxy tier with 32 proxies and construct sparse interval skip graphs of various sizes using our datasets. NumberofMessages Index size (entries) Initial lookup Traversal (a) NumberofMessages Index size (entries) Initial lookup Traversal (b) Synthetic Data Figure 9: Skip Graph Lookup Performance 10 20 30 40 50 60 70 1 4 8 16 24 32 48 Numberofmessages Number of proxies Skipgraph insert Sparse skipgraph insert Initial lookup (a) NumberofMessages Index size (entries) Insert (redundant) Insert (non-redundant) Lookup (redundant) Lookup (non-redundant) (b) Impact of Redundant Summaries Figure 10: Skip Graph Overheads graph, we evaluate the cost of inserting a new value into the index. Each entry was deleted after its insertion, enabling us to quantify the delete overhead as well. Figure 8(a) and (b) quantify the insert overhead for our two datasets: each insert entails an initial traversal that incurs log n messages, followed by neighbor pointer update at increasing levels, incurring a cost of 4 log n messages. Our results demonstrate this behavior, and show as well that performance of delete-which also involves an initial traversal followed by pointer updates at each level-incurs a similar cost. Next, we evaluate the lookup performance of the index structure. Again, we construct skip graphs of various sizes using our datasets and evaluate the cost of a lookup on the index structure. Figures 9(a) and (b) depict our results. There are two components for each lookup-the lookup of the first interval that matches the query and, in the case of overlapping intervals, the subsequent linear traversal to identify all matching intervals. The initial lookup can be seen to takes log n messages, as expected. The costs of the subsequent linear traversal, however, are highly data dependent. For instance, temperature values for the James Reserve data exhibit significant spatial correlations, resulting in significant overlap between different intervals and variable, high traversal cost (see Figure 9(a)). The synthetic data, however, has less overlap and incurs lower traversal overhead as shown in Figure 9(b). Since the previous experiments assumed 32 proxies, we evaluate the impact of the number of proxies on skip graph performance. entries among these proxies. We construct regular interval skip graphs as well as sparse interval skip graphs using these entries and measure the overhead of inserts and lookups. Thus, the experiment also seeks to demonstrate the benefits of sparse skip graphs over regular skip graphs. Figure 10(a) depicts our results. In regular skip graphs, the complexity of insert is O(log2n) in the 47 expected case (and O(n) in the worst case) where n is the number of elements. This complexity is unaffected by changing the number of proxies, as indicated by the flat line in the figure. Sparse skip graphs require fewer pointer updates; however, their overhead is dependent on the number of proxies, and is O(log2Np) in the expected case, independent of n. This can be seen to result in significant reduction in overhead when the number of proxies is small, which decreases as the number of proxies increases. Failure handling is an important issue in a multi-tier sensor architecture since it relies on many components-proxies, sensor nodes and routing nodes can fail, and wireless links can fade. Handling of many of these failure modes is outside the scope of this paper; however, we consider the case of resilience of skip graphs to proxy failures. In this case, skip graph search (and subsequent repair operations) can follow any one of the other links from a root element. Since a sparse skip graph has search trees rooted at each node, searching can then resume once the lookup request has routed around the failure. Together, these two properties ensure that even if a proxy fails, the remaining entries in the skip graph will be reachable with high probability-only the entries on the failed proxy and the corresponding data at the sensors becomes inaccessible. To ensure that all data on sensors remains accessible, even in the event of failure of a proxy holding index entries for that data, we incorporate redundant index entries. TSAR employs a simple redundancy scheme where additional coarse-grain summaries are used to protect regular summaries. Each sensor sends summary data periodically to its local proxy, but less frequently sends a lowerresolution summary to a backup proxy-the backup summary represents all of the data represented by the finer-grained summaries, but in a lossier fashion, thus resulting in higher read overhead (due to false hits) if the backup summary is used. The cost of implementing this in our system is low - Figure 10(b) shows the overhead of such a redundancy scheme, where a single coarse summary is send to a backup for every two summaries sent to the primary proxy. Since a redundant summary is sent for every two summaries, the insert cost is 1.5 times the cost in the normal case. However, these redundant entries result in only a negligible increase in lookup overhead, due the logarithmic dependence of lookup cost on the index size, while providing full resilience to any single proxy failure. 6.2 STORAGE MICROBENCHMARKS Since sensors are resource-constrained, the energy consumption and the latency at this tier are important measures for evaluating the performance of a storage architecture. Before performing an endto-end evaluation of our system, we provide more detailed information on the energy consumption of the storage component used to implement the TSAR local archive, based on empirical measurements. In addition we compare these figures to those for other local storage technologies, as well as to the energy consumption of wireless communication, using information from the literature. For empirical measurements we measure energy usage for the storage component itself (i.e. current drawn by the flash chip), as well as for the entire Mica2 mote. The power measurements in Table 3 were performed for the AT45DB041 flash memory on a Mica2 mote, which is an older NOR flash device. The most promising technology for low-energy storage on sensing devices is NAND flash, such as the Samsung K9K4G08U0M device ; published power numbers for this device are provided in the table. radio (used in MicaZ and Telos motes) are provided for comparison, assuming Energy Energy/byte Mote flash Read 256 byte page 58\u00b5J* / 136\u00b5J* total 0.23\u00b5J* Write 256 byte page 926\u00b5J* /\u00b5J* total 3.6\u00b5J* NAND Flash Read 512 byte page 2.7\u00b5J 1.8nJ Write 512 byte page 7.8\u00b5J 15nJ Erase 16K byte sector 60\u00b5J 3. radio Transmit 8 bits (-25dBm) 0.8\u00b5J 0.8\u00b5J Receive 8 bits 1.9\u00b5J 1.9\u00b5J Mote AVR processor In-memory search, 256 bytes 1.8\u00b5J 6.9nJ Table 3: Storage and Communication Energy Costs (*measured values) 1 2 3 Latency(ms) Number of hops (a) Multi- Latency(ms) Index size (entries) Sensor communication Proxy communication Sensor lookup, processing (b) Query Performance Figure 11: Query Processing Latency zero network and protocol overhead. Comparing the total energy cost for writing flash (erase + write) to the total cost for communication (transmit + receive), we find that the NAND flash is almost 150 times more efficient than radio communication, even assuming perfect network protocols. 6.3 PROTOTYPE EVALUATION This section reports results from an end-to-end evaluation of the TSAR prototype involving both tiers. In our setup, there are four proxies connected via 802.11 links and three sensors per proxy. The multi-hop topology was preconfigured such that sensor nodes were connected in a line to each proxy, 0 20 40 60 80 100 120 140 160 Retrievallatency(ms) Archived data retrieved (bytes) (a) Data Query and Fetch Time 10 12 4 8 16 32 Latency(ms) Number of 34-byte records searched (b) Sensor query processing delay Figure 12: Query Latency Components 48 3. Due to resource constraints we were unable to perform experiments with dozens of sensor nodes, however this topology ensured that the network diameter was as large as for a typical network of significantly larger size. Our evaluation metric is the end-to-end latency of query processing. A query posed on TSAR first incurs the latency of a sparse skip graph lookup, followed by routing to the appropriate sensor node(s). The sensor node reads the required page(s) from its local archive, processes the query on the page that is read, and transmits the response to the proxy, which then forwards it to the user. We first measure query latency for different sensors in our multi-hop topology. Depending on which of the sensors is queried, the total latency increases almost linearly from about 400ms to 1 second, as the number of hops increases from 1 to 3 (see Figure 11(a)). Figure 11(b) provides a breakdown of the various components of the end-to-end latency. The dominant component of the total latency is the communication over one or more hops. The typical time to communicate over one hop is approximately 300ms. This large latency is primarily due to the use of a duty-cycled MAC layer; the latency will be larger if the duty cycle is reduced (e.g. the 2% setting as opposed to the 11.5% setting used in this experiment), and will conversely decrease if the duty cycle is increased. The figure also shows the latency for varying index sizes; as expected, the latency of inter-proxy communication and skip graph lookups increases logarithmically with index size. Not surprisingly, the overhead seen at the sensor is independent of the index size. The latency also depends on the number of packets transmitted in response to a query-the larger the amount of data retrieved by a query, the greater the latency. This result is shown in Figure 12(a). The step function is due to packetization in TinyOS; TinyOS sends one packet so long as the payload is smaller than 30 bytes and splits the response into multiple packets for larger payloads. As the data retrieved by a query is increased, the latency increases in steps, where each step denotes the overhead of an additional packet. Finally, Figure 12(b) shows the impact of searching and processing flash memory regions of increasing sizes on a sensor. Each summary represents a collection of records in flash memory, and all of these records need to be retrieved and processed if that summary matches a query. The coarser the summary, the larger the memory region that needs to be accessed. For the search sizes examined, amortization of overhead when searching multiple flash pages and archival records, as well as within the flash chip and its associated driver, results in the appearance of sub-linear increase in latency with search size. In addition, the operation can be seen to have very low latency, in part due to the simplicity of our query processing, requiring only a compare operation with each stored element. More complex operations, however, will of course incur greater latency. 6.4 ADAPTIVE SUMMARIZATION When data is summarized by the sensor before being reported to the proxy, information is lost. With the interval summarization method we are using, this information loss will never cause the proxy to believe that a sensor node does not hold a value which it in fact does, as all archived values will be contained within the interval reported. However, it does cause the proxy to believe that the sensor may hold values which it does not, and forward query messages to the sensor for these values. These false positives constitute the cost of the summarization mechanism, and need to be balanced against the savings achieved by reducing the number of reports. The goal of adaptive summarization is to dynamically vary the summary size so that these two costs are balanced. 0.1 0.2 0.3 0.4 0.5 0 5 10 15 20 25 30 Fractionoftruehits Summary size (number of records) (a) Summarizationsize(num.records) Normalized time (units) query rate 0.2 query rate 0.03 query rate 0.1 (b) Adaptation to query rate Figure 13: Impact of Summarization Granularity Figure 13(a) demonstrates the impact of summary granularity on false hits. As the number of records included in a summary is increased, the fraction of queries forwarded to the sensor which match data held on that sensor (true positives) decreases. Next, in Figure 13(b) we run the a EmTOS simulation with our adaptive summarization algorithm enabled. The adaptive algorithm increases the summary granularity (defined as the number of records per summary) when Cost(updates) Cost(falsehits) > 1 + and reduces it if Cost(updates) Cost(falsehits) > 1 \u2212 , where is a small constant. To demonstrate the adaptive nature of our technique, we plot a time series of the summarization granularity. We begin with a query rate of 1 query per 5 samples, decrease it to 1 every 30 samples, and then increase it again to 1 query every 10 samples. As shown in Figure 13(b), the adaptive technique adjusts accordingly by sending more fine-grain summaries at higher query rates (in response to the higher false hit rate), and fewer, coarse-grain summaries at lower query rates. 7. RELATED WORK In this section, we review prior work on storage and indexing techniques for sensor networks. While our work addresses both problems jointly, much prior work has considered them in isolation. The problem of archival storage of sensor data has received limited attention in sensor network literature. ELF is a logstructured file system for local storage on flash memory that provides load leveling and Matchbox is a simple file system that is packaged with the TinyOS distribution . Both these systems focus on local storage, whereas our focus is both on storage at the remote sensors as well as providing a unified view of distributed data across all such local archives. Multi-resolution storage is intended for in-network storage and search in systems where there is significant data in comparison to storage resources. In contrast, TSAR addresses the problem of archival storage in two-tier systems where sufficient resources can be placed at the edge sensors. The RISE platform being developed as part of the NODE project at UCR addresses the issues of hardware platform support for large amounts of storage in remote sensor nodes, but not the indexing and querying of this data. In order to efficiently access a distributed sensor store, an index needs to be constructed of the data. Early work on sensor networks such as Directed Diffusion assumes a system where all useful sensor data was stored locally at each sensor, and spatially scoped queries are routed using geographic co-ordinates to locations where the data is stored. Sources publish the events that they detect, and sinks with interest in specific events can subscribe to these events. The Directed Diffusion substrate routes queries to specific locations 49 if the query has geographic information embedded in it (e.g.: find temperature in the south-west quadrant), and if not, the query is flooded throughout the network. These schemes had the drawback that for queries that are not geographically scoped, search cost (O(n) for a network of n nodes) may be prohibitive in large networks with frequent queries. Local storage with in-network indexing approaches address this issue by constructing indexes using frameworks such as Geographic Hash Tables and Quad Trees . Recent research has seen a growing body of work on data indexing schemes for sensor networks. One such scheme is DCS , which provides a hash function for mapping from event name to location. DCS constructs a distributed structure that groups events together spatially by their named type. Distributed Index of Features in Sensornets (DIFS ) and Multi-dimensional Range Queries in Sensor Networks (DIM ) extend the data-centric storage approach to provide spatially distributed hierarchies of indexes to data. While these approaches advocate in-network indexing for sensor networks, we believe that indexing is a task that is far too complicated to be performed at the remote sensor nodes since it involves maintaining significant state and large tables. TSAR provides a better match between resource requirements of storage and indexing and the availability of resources at different tiers. Thus complex operations such as indexing and managing metadata are performed at the proxies, while storage at the sensor remains simple. In addition to storage and indexing techniques specific to sensor networks, many distributed, peer-to-peer and spatio-temporal index structures are relevant to our work. DHTs can be used for indexing events based on their type, quad-tree variants such as Rtrees can be used for optimizing spatial searches, and K-D trees can be used for multi-attribute search. While this paper focuses on building an ordered index structure for range queries, we will explore the use of other index structures for alternate queries over sensor data. 8. CONCLUSIONS In this paper, we argued that existing sensor storage systems are designed primarily for flat hierarchies of homogeneous sensor nodes and do not fully exploit the multi-tier nature of emerging sensor networks. We presented the design of TSAR, a fundamentally different storage architecture that envisions separation of data from metadata by employing local storage at the sensors and distributed indexing at the proxies. At the proxy tier, TSAR employs a novel multi-resolution ordered distributed index structure, the Sparse Interval Skip Graph, for efficiently supporting spatio-temporal and range queries. At the sensor tier, TSAR supports energy-aware adaptive summarization that can trade-off the energy cost of transmitting metadata to the proxies against the overhead of false hits resulting from querying a coarser resolution index structure. We implemented TSAR in a two-tier sensor testbed comprising Stargatebased proxies and Mote-based sensors. Our experimental evaluation of TSAR demonstrated the benefits and feasibility of employing our energy-efficient low-latency distributed storage architecture in multi-tier sensor networks.", "body1": "1.1 MOTIVATION Many different kinds of networked data-centric sensor applications have emerged in recent years. There have been a spectrum of approaches for constructing sensor storage systems. An alternate approach is to have each sensor store data or events locally (e.g., in flash memory), so that all writes are local and incur no communication overheads. Between these two extremes lie a number of other sensor storage systems with different trade-offs, summarized in Table 1. Most of these approaches assume a flat, homogeneous architecture in which every sensor node is energy-constrained. Any sensor storage system should also carefully exploit current technology trends, which indicate that the capacities of flash memories continue to rise as per Moore\"s Law, while their costs continue to plummet. TSAR exploits these trends by storing data or events locally on the energy-efficient flash storage at each sensor. 1.2 CONTRIBUTIONS This paper presents TSAR, a novel two-tier storage architecture for sensor networks. able. Second, at the sensor level, each sensor maintains a local archive that stores data on flash memory. Our implementation supports spatio-temporal, value, and rangebased queries on sensor data. Fourth, we conduct a detailed experimental evaluation of TSAR using a combination of EmStar/EmTOS and our prototype. While our EmStar/EmTOS experiments focus on the scalability of TSAR in larger settings, our prototype evaluation involves latency and energy measurements in a real setting. The remainder of this paper is structured as follows. In this section, we first describe the various components of a multi-tier sensor network assumed in our work. We envision a multi-tier sensor network comprising multiple tiers - a bottom tier of untethered remote sensor nodes, a middle tier of tethered sensor proxies, and an upper tier of applications and user terminals (see Figure 1). The lowest tier is assumed to form a dense deployment of lowpower sensors. these resources continuously. 802.15.4) that connects it to the sensor nodes. At the highest tier of our infrastructure are applications that query the sensor network through a query interface. The design of a storage system such as TSAR is affected by the queries that are likely to be posed to it. Expected queries of such data include those requesting ranges of one or more attributes; for instance, a query for all image data from cameras within a specified geographic area for a certain period of time. Applications may also pose value-based queries that involve determining if a value v was observed at any sensor; the query returns a list of sensors and the times at which they observed this value. Hybrid value and spatio-temporal queries are also possible. Our design of a sensor storage system for multi-tier networks is based on the following set of principles, which address the issues arising from the system and usage models above. For maximum network life a sensor storage system should leverage the flash memory on sensors to archive data locally, substituting cheap memory operations for expensive radio transmission. \u2022 Principle 2: Distinguish data from metadata: Data must be identified so that it may be retrieved by the application without exhaustive search. \u2022 Principle 3: Provide data-centric query support: In a sensor application the specific location (i.e. TSAR embodies these design principles by employing local storage at sensors and a distributed index at the proxies. This metadata is a tuple of known types, which may be used by the application to locate and identify data records, and which may be searched on and compared by TSAR in the course of locating data for the application. In addition to storing data locally, each sensor periodically sends a summary of reported metadata to a nearby proxy. Thus, applications see a unified view of distributed data, and can query the index at any proxy to get access to data stored at any sensor. Since the index is constructed using a coarse-grain summary, instead of the actual data, index lookups will yield approximate matches. At the proxy tier, TSAR employs a novel index structure called the Interval Skip Graph, which is an ordered, distributed data structure for finding all intervals that contain a particular point or range of values. Such summary-based indexing is a more natural fit for energyconstrained sensor nodes, since transmitting summaries incurs less energy overhead than transmitting all sensor data. Definitions: We assume that there are Np proxies and Ns sensors in a two-tier sensor network. Range queries on the intervals are posed by users to the network of proxies and sensors; each query q needs to determine all index values that overlap the interval [lowq, highq]. In order to inform the description of the Interval Skip Graph, we first provide a brief overview of the Skip Graph data structure; for a more extensive description the reader is referred to . \u2022 Log n height: There are log2 n pointers associated with each element, where n is the number of data elements indexed. Each pointer belongs to a level l in [0... log2 n \u2212 1], and together with some other pointers at that level forms a chain of n/2l elements. \u2022 Probabilistic balance: Rather than relying on re-balancing operations which may be triggered at insert or delete, skip graphs implement a simple random balancing mechanism which maintains close to perfect balance on average, with an extremely low probability of significant imbalance. \u2022 Redundancy and resiliency: Each data element forms an independent search tree root, so searches may begin at any node in the network, eliminating hot spots at a single search root. In Figure 2 we see the process of searching for a particular value in a skip graph. Tree update proceeds from the bottom, as in a B-Tree, with the root(s) being promoted in level as the tree grows. Maintaining a perfectly balanced skip graph as shown in Figure 2 would be quite complex; instead, the probabilistic balancing method introduced in Skip Lists is used, which trades off a small amount of overhead in the expected case in return for simple update and deletion. One means of implementation (e.g. A skip graph is designed to store single-valued entries. Given a set of intervals sorted by lower bound - lowi \u2264 lowi+1 - we define the secondary key as the cumulative maximum, maxi = maxk=0...i (highk). Using this approach we augment the skip graph data structure, as shown in Figure 3, so that each entry stores a range (lower bound and upper bound) and a secondary key (cumulative maximum of upper bound). To search for those intervals containing the value v, we first search for v on the secondary index, maxi, and locate the first entry with maxi \u2265 v. (by the definition of maxi, for this data element maxi = highi.) Lookup Complexity: Lookup for the first interval that matches a given value is performed in a manner very similar to an interval tree. Insert Complexity: In an interval tree or interval skip list, the maximum value for an entry need only be calculated over the subtree rooted at that entry, as this value will be examined only when searching within the subtree rooted at that entry. The final extensions we propose take advantage of the difference between the number of items indexed in a skip graph and the number of systems on which these items are distributed. As may be in Section 3.2, insert and delete cost on an interval skip graph has a worst case complexity of O(n), compared to O(log n) for an interval tree. Implementation: To construct a sparse interval skip graph, we ensure that there is a single distinguished element on each system, the root element for that system; all searches will start at one of these root elements. Performance: In a (non-interval) sparse skip graph, since the expected height of an inserted element is now log2 Np + O(1), expected insertion complexity is O(log Np), rather than O(log n), where Np is the number of root elements and thus the number of separate systems in the network. For an interval sparse skip graph, update performance is improved considerably compared to the O(n) worst case for the nonsparse case. Shortcut search: When beginning a search for a value v, rather than beginning at the root on that proxy, we can find the element that is closest to v (e.g. Thus far we have only compared the sparse interval skip graph with similar structures from which it is derived. 44 Table 2: Comparison of Distributed Index Structures Range Query Support Interval Representation Re-balancing Resilience Small Networks Large Networks DHT, GHT no no no yes good good Local index, flood query yes yes no yes good bad P-tree, RP* (distributed B-Trees) yes possible yes no good good DIMS yes no yes yes yes yes Interval Skipgraph yes yes no yes good good Roots Node 1 Node 2 Figure 5: Sparse Interval Skip Graph The hash-based systems, DHT and GHT , lack the ability to perform range queries and are thus not well-suited to indexing spatio-temporal data. Having described the proxy-level index structure, we turn to the mechanisms at the sensor tier. Interval skip graphs provide an efficient mechanism to lookup sensor nodes containing data relevant to a query. Timestamp Calibration Parameters Opaque DataData/Event Attributes size Figure 6: Single storage record Sensor data has very distinct characteristics that inform our design of the TSAR archival store. Consequently, the local archival store is a collection of records, designed as an append-only circular buffer, where new records are appended to the tail of the buffer. Our archival store supports three operations on records: create, read, and delete. proxy proxy proxy record 3 record summary local archive in flash memory data summary start,end offset time interval sensor summary sent to proxy Insert summaries into interval skip graph Figure 7: Sensor Summarization 45 The read operation enables stored records to be retrieved in order to answer queries. Within this collection, records are accessed sequentially. While we anticipate local storage capacity to be large, eventually there might be a need to overwrite older data, especially in high data rate applications. The data summaries serve as glue between the storage at the remote sensor and the index at the proxy. The coarser and less frequent the summary information, the less energy required, while false query hits in turn waste energy on requests for non-existent data. TSAR employs an adaptive summarization technique that balances the cost of sending updates against the cost of false positives. The key intuition is that each sensor can independently identify the fraction of false hits and true hits for queries that access its local archive. The resolution of the summary depends on two parametersthe interval over which summaries of the data are constructed and transmitted to the proxy, as well as the size of the applicationspecific summary. We have implemented a prototype of TSAR on a multi-tier sensor network testbed. Since sensor nodes may be several hops away from the nearest proxy, the sensor tier employs multi-hop routing to communicate with the proxy tier. Our current implementation supports queries that request records matching a time interval (t1, t2) or a value range (v1, v2). In this section, we evaluate the efficacy of TSAR using our prototype and simulations. Rather than using live data from a real sensor, to ensure repeatable experiments, we seed each sensor node with a dataset (i.e., a trace) that dictates the values reported by that node to the proxy. Second, we provide summary results from micro-benchmarks of the storage component of TSAR, which include empirical characterization of the energy costs and latency of reads and writes for the flash memory chip as well as the whole mote platform, and comparisons to published numbers for other storage and communication technologies. This section evaluates the performance of sparse interval skip graphs by quantifying insert, lookup and delete overheads. Each entry was deleted after its insertion, enabling us to quantify the delete overhead as well. Figures 9(a) and (b) depict our results. Since the previous experiments assumed 32 proxies, we evaluate the impact of the number of proxies on skip graph performance. In regular skip graphs, the complexity of insert is O(log2n) in the 47 expected case (and O(n) in the worst case) where n is the number of elements. Failure handling is an important issue in a multi-tier sensor architecture since it relies on many components-proxies, sensor nodes and routing nodes can fail, and wireless links can fade. To ensure that all data on sensors remains accessible, even in the event of failure of a proxy holding index entries for that data, we incorporate redundant index entries. Since sensors are resource-constrained, the energy consumption and the latency at this tier are important measures for evaluating the performance of a storage architecture. The power measurements in Table 3 were performed for the AT45DB041 flash memory on a Mica2 mote, which is an older NOR flash device. This section reports results from an end-to-end evaluation of the TSAR prototype involving both tiers. Our evaluation metric is the end-to-end latency of query processing. Figure 11(b) provides a breakdown of the various components of the end-to-end latency. the 2% setting as opposed to the 11.5% setting used in this experiment), and will conversely decrease if the duty cycle is increased. The figure also shows the latency for varying index sizes; as expected, the latency of inter-proxy communication and skip graph lookups increases logarithmically with index size. The latency also depends on the number of packets transmitted in response to a query-the larger the amount of data retrieved by a query, the greater the latency. The step function is due to packetization in TinyOS; TinyOS sends one packet so long as the payload is smaller than 30 bytes and splits the response into multiple packets for larger payloads. Finally, Figure 12(b) shows the impact of searching and processing flash memory regions of increasing sizes on a sensor. When data is summarized by the sensor before being reported to the proxy, information is lost. 0.1 0.2 0.3 0.4 0.5 0 5 10 15 20 25 30 Fractionoftruehits Summary size (number of records) (a) Summarizationsize(num.records) Normalized time (units) query rate 0.2 query rate 0.03 query rate 0.1 (b) Adaptation to query rate Figure 13: Impact of Summarization Granularity Figure 13(a) demonstrates the impact of summary granularity on false hits. In this section, we review prior work on storage and indexing techniques for sensor networks. The problem of archival storage of sensor data has received limited attention in sensor network literature. In order to efficiently access a distributed sensor store, an index needs to be constructed of the data. These schemes had the drawback that for queries that are not geographically scoped, search cost (O(n) for a network of n nodes) may be prohibitive in large networks with frequent queries. Local storage with in-network indexing approaches address this issue by constructing indexes using frameworks such as Geographic Hash Tables and Quad Trees . While these approaches advocate in-network indexing for sensor networks, we believe that indexing is a task that is far too complicated to be performed at the remote sensor nodes since it involves maintaining significant state and large tables. In addition to storage and indexing techniques specific to sensor networks, many distributed, peer-to-peer and spatio-temporal index structures are relevant to our work.", "body2": "Archival storage of past sensor data requires a storage system, the key attributes of which are: where the data is stored, whether it is indexed, and how the application can access this data in an energy-efficient manner with low latency. Further, all data is propagated to the server, regardless of whether it is ever used by the application. Research efforts such as Directed Diffusion have attempted to reduce these read costs, however, by intelligent message routing. A read request requires a lookup in the in-network hash table to locate the node that stores the data 39 item; observe that the presence of an index eliminates the need for flooding in this approach. No existing sensor storage architecture explicitly addresses this dichotomy in the resource capabilities of different tiers. These trends, together with the energy-constrained nature of untethered sensors, indicate that local storage offers a viable, energy-efficient alternative to communication in sensor networks. This separation of data, which is stored at the sensors, and the metadata, which is stored at the proxies, enables TSAR to reduce energy overheads at the sensors, by leveraging resources at tethered proxies. This index structure can store coarse summaries of sensor data and organize them in an ordered manner to be easily search1 TSAR: Tiered Storage ARchitecture for sensor networks. Further, the index provides a logically unified view of all data in the system. Third, we have implemented a prototype of TSAR on a multi-tier testbed comprising Stargate-based proxies and Mote-based sensors. Our implementation supports spatio-temporal, value, and rangebased queries on sensor data. Fourth, we conduct a detailed experimental evaluation of TSAR using a combination of EmStar/EmTOS and our prototype. Our results demonstrate the logarithmic scaling property of the sparse skip graph and the low latency of end-to-end queries in a duty-cycled multi-hop network . We present related work in Section 7 and our conclusions in Section 8. We then present a description of the expected usage models for this system, followed by several principles addressing these factors which guide the design of our storage system. We envision a multi-tier sensor network comprising multiple tiers - a bottom tier of untethered remote sensor nodes, a middle tier of tethered sensor proxies, and an upper tier of applications and user terminals (see Figure 1). The middle tier consists of power-rich sensor proxies that have significant computation, memory and storage resources and can use 40 Table 1: Characteristics of sensor storage systems System Data Index Reads Writes Order preserving Centralized store Centralized Centralized index Handled at store Send to store Yes Local sensor store Fully distributed No index Flooding, diffusion Local No GHT/DCS Fully distributed In-network index Hash to node Send to hashed node No TSAR/PRESTO Fully distributed Distributed index at proxies Proxy lookup + sensor query Local plus index update Yes User Unified Logical Store Queries (time, space, value) Query Response Cache Query forwarding Proxy Remote Sensors Local Data Archive on Flash Memory Interval Skip Graph Query forwarding summaries start index end index linear traversal Query Response Cache-miss triggered query forwarding summaries Figure 1: Architecture of a multi-tier sensor network. In urban environments, the proxy tier would comprise a tethered base-station class nodes (e.g., Crossbow Stargate), each with with multiple radios-an 802.11 radio that connects it to a wireless mesh network and a low-power radio (e.g. For instance, in a building monitoring application, one sensor proxy might be placed per floor or hallway to monitor temperature, heat and light sensors in their vicinity. Our goal is to design a storage system that exploits the relative abundance of resources at proxies to mask the scarcity of resources at the sensors. Some instances of such queries include the time and location of target or intruder detections (e.g., security and monitoring applications), notifications of specific types of events such as pressure and humidity values exceeding a threshold (e.g., industrial applications), or simple data collection queries which request data from a particular time or location (e.g., weather or environment monitoring). Order-preserving structures such as the well-known B-Tree maintain relationships between indexed values and thus allow natural access to ranges, as well as predecessor and successor operations on their key values. A search involves finding matching values that are either contained in the search range (v1, v2) or match the search value v exactly. In TSAR our focus is on range queries on value or time, with planned extensions to include spatial scoping. \u2022 Principle 1: Store locally, access globally: Current technology allows local storage to be significantly more energyefficient than network communication, while technology trends show no signs of erasing this gap in the near future. We believe that if the data storage system provides the abstraction of a single logical store to applications, as 41 does TSAR, then it will have additional flexibility to optimize communication and storage costs. Such a tier-specific separation of data storage from metadata indexing enables the system to exploit the idiosyncrasies of multi-tier networks, while improving performance and functionality. This in turn implies the need to maintain metadata in the form of an index that provides low cost lookups. The key features of the system design are as follows: In TSAR, writes occur at sensor nodes, and are assumed to consist of both opaque data as well as application-specific metadata. Depending on the application, this metadata may be two or three orders of magnitude smaller than the data itself, for instance if the metadata consists of features extracted from image or acoustic data. The proxy uses the summary to construct an index; the index is global in that it stores information from all sensors in the system and it is distributed across the various proxies in the system. There are several distributed index and lookup methods which might be used in this system; however, the index structure described in Section 3 is highly suited for the task. Remote sensors may easily distinguish false positives from queries which result in search hits, and calculate the ratio between the two; based on this ratio, TSAR employs a novel adaptive technique that dynamically varies the granularity of sensor summaries to balance the metadata overhead and the overhead of false positives. Second, indexing of intervals rather than individual values makes the data structure ideal for indexing summaries over time or value. Such summary-based indexing is a more natural fit for energyconstrained sensor nodes, since transmitting summaries incurs less energy overhead than transmitting all sensor data. No assumption is made about the size of an interval or about the amount of overlap between intervals. In the rest of this section, we describe the interval skip graph in greater detail. \u2022 In-place indexing: Data elements remain on the nodes where they were inserted, and messages are sent between nodes to establish links between those elements and others in the index. \u2022 Log n height: There are log2 n pointers associated with each element, where n is the number of data elements indexed. Each pointer belongs to a level l in [0... log2 n \u2212 1], and together with some other pointers at that level forms a chain of n/2l elements. \u2022 Probabilistic balance: Rather than relying on re-balancing operations which may be triggered at insert or delete, skip graphs implement a simple random balancing mechanism which maintains close to perfect balance on average, with an extremely low probability of significant imbalance. In addition the index is resilient against node failure; data on the failed node will not be accessible, but remaining data elements will be accessible through search trees rooted on other nodes. the number of pointers traversed); this will clearly be O(log n). The update process then consists of choosing which of the 2l chains to insert an element into at each level l, and inserting it in the proper place in each chain. To insert an element we ascend levels starting at 0, randomly choosing one of the two possible chains at each level, an stopping when we reach an empty chain. Although the resulting structure is not perfectly balanced, following the analysis in we can show that the probability of it being significantly out of balance is extremely small; in addition, since the structure is determined by the random number stream, input data patterns cannot cause the tree to become imbalanced. The method is based on the observation that a search structure based on comparison of ordered keys, such as a binary tree, may also be used to search on a secondary key which is non-decreasing in the first key. The set of intervals intersecting a value v may then be found by searching for the first interval (and thus the interval with least lowi) such that maxi \u2265 v. We then 43 traverse intervals in increasing order lower bound, until we find the first interval with lowi > v, selecting those intervals which intersect v. To efficiently calculate the secondary key maxi for an entry i, we take the greatest of highi and the maximum values reported by each of i\"s left-hand neighbors. Searches for all intervals which overlap a query range, or which completely contain a query range, are straightforward extensions of this mechanism. The number of intervals that match a range query can vary depending on the amount of overlap in the intervals being indexed, as well as the range specified in the query. It may be easily seen that in the worse case the insertion of a single interval (one that covers all existing intervals in the index) will trigger the update of all entries in the index, for a worst-case insertion cost of O(n). These two modifications allow us to achieve reductions in asymptotic complexity of both update and search. If as in TSAR the number of nodes (proxies) is much smaller than the number of data elements (data summaries indexed), then this will result in significant savings. An alternate implementation, which distributes information concerning root elements at pointer establishment time, is omitted due to space constraints; this method eliminates the need for additional messages. Note that since searches are started at root elements of expected height log2 n, search complexity is not improved. In a sparse interval skip graph, updates to a node only propagate towards the Np root elements, for a worst-case cost of Np log2 n. By combining the short-cut optimization with sparse interval skip graphs, the expected cost of insertion is now O(log Np), independent of the size of the index or the degree of overlap of the inserted intervals. A comparison with several other data structures which meet at least some of the requirements for the TSAR index is shown in Table 2. DIMS provides the ability to perform spatio-temporal range queries, and has the necessary resilience to failures; however, it cannot be used index intervals, which are used by TSAR\"s data summarization algorithm. The rest of this section describes these mechanisms in detail. Consequently, the sensor archiving subsystem in TSAR is explicitly designed to exploit characteristics of sensor data in a resource-constrained setting. Examples include signal processing operations such as FFT, wavelet transforms, clustering, similarity matching, and target detection. In order to support a variety of applications, TSAR supports variable-length data fields; as a result, record sizes can vary from one record to another. Since writes are immutable, the size of a record does not change once it is created. Thus, random access is enabled at granularity of a summary-the start offset of each chunk of records represented by a summary is known to the proxy. The state of the archive is captured in the metadata associated with the summaries, and is stored and maintained at the proxy. When older data is overwritten, a delete operation is performed, where an index entry is deleted from the interval skip graph at the proxy and the corresponding storage space in flash memory at the sensor is freed. As described in Section 2.4, there is a trade-off between the energy used in sending summaries (and thus the frequency and resolution of those summaries) and the cost of false hits during queries. The coarser and less frequent the summary information, the less energy required, while false query hits in turn waste energy on requests for non-existent data. TSAR employs an adaptive summarization technique that balances the cost of sending updates against the cost of false positives. If many queries result in false hits, then the sensor makes the granularity of each summary finer to reduce the number and overhead of false hits. Currently, TSAR employs a simple summarization scheme that computes the ratio of false and true hits and decreases (increases) the interval between summaries whenever this ratio increases (decreases) beyond a threshold. The proxy nodes can be equipped with external storage such as high-capacity compact flash (up to 4GB), 6GB micro-drives, or up to 60GB 1.8inch mobile disk drives. The sensor updates are used to construct a sparse interval skip graph that is distributed across proxies, via network messages between proxies over the 802.11b wireless network. Lookup messages are useful for polling a sensor, for instance, to determine if a query matches too many records. Our simulation employs the EmTOS emulator , which enables us to run the same code in simulation and the hardware platform. First, we run EmTOS simulations to evaluate the lookup, update and delete overhead for sparse interval skip graphs using the real and synthetic datasets. The remainder of this section presents our experimental results. NumberofMessages Index size (entries) Initial lookup Traversal (a) NumberofMessages Index size (entries) Initial lookup Traversal (b) Synthetic Data Figure 9: Skip Graph Lookup Performance 10 20 30 40 50 60 70 1 4 8 16 24 32 48 Numberofmessages Number of proxies Skipgraph insert Sparse skipgraph insert Initial lookup (a) NumberofMessages Index size (entries) Insert (redundant) Insert (non-redundant) Lookup (redundant) Lookup (non-redundant) (b) Impact of Redundant Summaries Figure 10: Skip Graph Overheads graph, we evaluate the cost of inserting a new value into the index. Again, we construct skip graphs of various sizes using our datasets and evaluate the cost of a lookup on the index structure. The synthetic data, however, has less overlap and incurs lower traversal overhead as shown in Figure 9(b). Figure 10(a) depicts our results. Sparse skip graphs require fewer pointer updates; however, their overhead is dependent on the number of proxies, and is O(log2Np) in the expected case, independent of n. This can be seen to result in significant reduction in overhead when the number of proxies is small, which decreases as the number of proxies increases. Together, these two properties ensure that even if a proxy fails, the remaining entries in the skip graph will be reachable with high probability-only the entries on the failed proxy and the corresponding data at the sensors becomes inaccessible. However, these redundant entries result in only a negligible increase in lookup overhead, due the logarithmic dependence of lookup cost on the index size, while providing full resilience to any single proxy failure. current drawn by the flash chip), as well as for the entire Mica2 mote. Comparing the total energy cost for writing flash (erase + write) to the total cost for communication (transmit + receive), we find that the NAND flash is almost 150 times more efficient than radio communication, even assuming perfect network protocols. Due to resource constraints we were unable to perform experiments with dozens of sensor nodes, however this topology ensured that the network diameter was as large as for a typical network of significantly larger size. Depending on which of the sensors is queried, the total latency increases almost linearly from about 400ms to 1 second, as the number of hops increases from 1 to 3 (see Figure 11(a)). This large latency is primarily due to the use of a duty-cycled MAC layer; the latency will be larger if the duty cycle is reduced (e.g. the 2% setting as opposed to the 11.5% setting used in this experiment), and will conversely decrease if the duty cycle is increased. Not surprisingly, the overhead seen at the sensor is independent of the index size. This result is shown in Figure 12(a). As the data retrieved by a query is increased, the latency increases in steps, where each step denotes the overhead of an additional packet. More complex operations, however, will of course incur greater latency. The goal of adaptive summarization is to dynamically vary the summary size so that these two costs are balanced. As shown in Figure 13(b), the adaptive technique adjusts accordingly by sending more fine-grain summaries at higher query rates (in response to the higher false hit rate), and fewer, coarse-grain summaries at lower query rates. While our work addresses both problems jointly, much prior work has considered them in isolation. The RISE platform being developed as part of the NODE project at UCR addresses the issues of hardware platform support for large amounts of storage in remote sensor nodes, but not the indexing and querying of this data. : find temperature in the south-west quadrant), and if not, the query is flooded throughout the network. These schemes had the drawback that for queries that are not geographically scoped, search cost (O(n) for a network of n nodes) may be prohibitive in large networks with frequent queries. Distributed Index of Features in Sensornets (DIFS ) and Multi-dimensional Range Queries in Sensor Networks (DIM ) extend the data-centric storage approach to provide spatially distributed hierarchies of indexes to data. Thus complex operations such as indexing and managing metadata are performed at the proxies, while storage at the sensor remains simple. While this paper focuses on building an ordered index structure for range queries, we will explore the use of other index structures for alternate queries over sensor data.", "introduction": "1.1 MOTIVATION Many different kinds of networked data-centric sensor applications have emerged in recent years. Sensors in these applications sense the environment and generate data that must be processed, filtered, interpreted, and archived in order to provide a useful infrastructure to its users. To achieve its goals, a typical sensor application needs access to both live and past sensor data. Whereas access to live data is necessary in monitoring and surveillance applications, access to past data is necessary for applications such as mining of sensor logs to detect unusual patterns, analysis of historical trends, and post-mortem analysis of particular events. Archival storage of past sensor data requires a storage system, the key attributes of which are: where the data is stored, whether it is indexed, and how the application can access this data in an energy-efficient manner with low latency. There have been a spectrum of approaches for constructing sensor storage systems. In the simplest, sensors stream data or events to a server for long-term archival storage , where the server often indexes the data to permit efficient access at a later time. Since sensors may be several hops from the nearest base station, network costs are incurred; however, once data is indexed and archived, subsequent data accesses can be handled locally at the server without incurring network overhead. In this approach, the storage is centralized, reads are efficient and cheap, while writes are expensive. Further, all data is propagated to the server, regardless of whether it is ever used by the application. An alternate approach is to have each sensor store data or events locally (e.g., in flash memory), so that all writes are local and incur no communication overheads. A read request, such as whether an event was detected by a particular sensor, requires a message to be sent to the sensor for processing. More complex read requests are handled by flooding. For instance, determining if an intruder was detected over a particular time interval requires the request to be flooded to all sensors in the system. Thus, in this approach, the storage is distributed, writes are local and inexpensive, while reads incur significant network overheads. Requests that require flooding, due to the lack of an index, are expensive and may waste precious sensor resources, even if no matching data is stored at those sensors. Research efforts such as Directed Diffusion have attempted to reduce these read costs, however, by intelligent message routing. Between these two extremes lie a number of other sensor storage systems with different trade-offs, summarized in Table 1. The geographic hash table (GHT) approach advocates the use of an in-network index to augment the fully distributed nature of sensor storage. In this approach, each data item has a key associated with it, and a distributed or geographic hash table is used to map keys to nodes that store the corresponding data items. Thus, writes cause data items to be sent to the hashed nodes and also trigger updates to the in-network hash table. A read request requires a lookup in the in-network hash table to locate the node that stores the data 39 item; observe that the presence of an index eliminates the need for flooding in this approach. Most of these approaches assume a flat, homogeneous architecture in which every sensor node is energy-constrained. In this paper, we propose a novel storage architecture called TSAR1 that reflects and exploits the multi-tier nature of emerging sensor networks, where the application is comprised of tens of tethered sensor proxies (or more), each controlling tens or hundreds of untethered sensors. TSAR is a component of our PRESTO predictive storage architecture, which combines archival storage with caching and prediction. We believe that a fundamentally different storage architecture is necessary to address the multi-tier nature of future sensor networks. Specifically, the storage architecture needs to exploit the resource-rich nature of proxies, while respecting resource constraints at the remote sensors. No existing sensor storage architecture explicitly addresses this dichotomy in the resource capabilities of different tiers. Any sensor storage system should also carefully exploit current technology trends, which indicate that the capacities of flash memories continue to rise as per Moore\"s Law, while their costs continue to plummet. Thus it will soon be feasible to equip each sensor with 1 GB of flash storage for a few tens of dollars. An even more compelling argument is the energy cost of flash storage, which can be as much as two orders of magnitude lower than that for communication. Newer NAND flash memories offer very low write and erase energy costs - 802.15.4 wireless radio in Section 6.2 indicates a 1:100 ratio in per-byte energy cost between the two devices, even before accounting for network protocol overheads. These trends, together with the energy-constrained nature of untethered sensors, indicate that local storage offers a viable, energy-efficient alternative to communication in sensor networks. TSAR exploits these trends by storing data or events locally on the energy-efficient flash storage at each sensor. Sensors send concise identifying information, which we term metadata, to a nearby proxy; depending on the representation used, this metadata may be an order of magnitude or more smaller than the data itself, imposing much lower communication costs. The resource-rich proxies interact with one another to construct a distributed index of the metadata reported from all sensors, and thus an index of the associated data stored at the sensors. This index provides a unified, logical view of the distributed data, and enables an application to query and read past data efficiently - the index is used to pinpoint all data that match a read request, followed by messages to retrieve that data from the corresponding sensors. In-network index lookups are eliminated, reducing network overheads for read requests. This separation of data, which is stored at the sensors, and the metadata, which is stored at the proxies, enables TSAR to reduce energy overheads at the sensors, by leveraging resources at tethered proxies. 1.2 CONTRIBUTIONS This paper presents TSAR, a novel two-tier storage architecture for sensor networks. To the best of our knowledge, this is the first sensor storage system that is explicitly tailored for emerging multitier sensor networks. Our design and implementation of TSAR has resulted in four contributions. At the core of the TSAR architecture is a novel distributed index structure based on interval skip graphs that we introduce in this paper. This index structure can store coarse summaries of sensor data and organize them in an ordered manner to be easily search1 TSAR: Tiered Storage ARchitecture for sensor networks. This data structure has O(log n) expected search and update complexity. Further, the index provides a logically unified view of all data in the system. Second, at the sensor level, each sensor maintains a local archive that stores data on flash memory. Our storage architecture is fully stateless at each sensor from the perspective of the metadata index; all index structures are maintained at the resource-rich proxies, and only direct requests or simple queries on explicitly identified storage locations are sent to the sensors. Storage at the remote sensor is in effect treated as appendage of the proxy, resulting in low implementation complexity, which makes it ideal for small, resourceconstrained sensor platforms. Further, the local store is optimized for time-series access to archived data, as is typical in many applications. Each sensor periodically sends a summary of its data to a proxy. TSAR employs a novel adaptive summarization technique that adapts the granularity of the data reported in each summary to the ratio of false hits for application queries. More fine grain summaries are sent whenever more false positives are observed, thereby balancing the energy cost of metadata updates and false positives. Third, we have implemented a prototype of TSAR on a multi-tier testbed comprising Stargate-based proxies and Mote-based sensors. Our implementation supports spatio-temporal, value, and rangebased queries on sensor data. Fourth, we conduct a detailed experimental evaluation of TSAR using a combination of EmStar/EmTOS and our prototype. While our EmStar/EmTOS experiments focus on the scalability of TSAR in larger settings, our prototype evaluation involves latency and energy measurements in a real setting. Our results demonstrate the logarithmic scaling property of the sparse skip graph and the low latency of end-to-end queries in a duty-cycled multi-hop network . The remainder of this paper is structured as follows. Section 2 presents key design issues that guide our work. Section 3 and 4 present the proxy-level index and the local archive and summarization at a sensor, respectively. Section 5 discusses our prototype implementation, and Section 6 presents our experimental results. We present related work in Section 7 and our conclusions in Section 8.", "conclusion": "In this paper, we argued that existing sensor storage systems are designed primarily for flat hierarchies of homogeneous sensor nodes and do not fully exploit the multi-tier nature of emerging sensor networks.. We presented the design of TSAR, a fundamentally different storage architecture that envisions separation of data from metadata by employing local storage at the sensors and distributed indexing at the proxies.. At the proxy tier, TSAR employs a novel multi-resolution ordered distributed index structure, the Sparse Interval Skip Graph, for efficiently supporting spatio-temporal and range queries.. At the sensor tier, TSAR supports energy-aware adaptive summarization that can trade-off the energy cost of transmitting metadata to the proxies against the overhead of false hits resulting from querying a coarser resolution index structure.. We implemented TSAR in a two-tier sensor testbed comprising Stargatebased proxies and Mote-based sensors.. Our experimental evaluation of TSAR demonstrated the benefits and feasibility of employing our energy-efficient low-latency distributed storage architecture in multi-tier sensor networks."}
{"id": "H-84", "keywords": ["event", "depend", "thread", "cluster"], "title": "Event Threading within News Topics", "abstract": "With the overwhelming volume of online news available today, there is an increasing need for automatic techniques to analyze and present news to the user in a meaningful and efficient manner. Previous research focused only on organizing news stories by their topics into a flat hierarchy. We believe viewing a news topic as a flat collection of stories is too restrictive and inefficient for a user to understand the topic quickly. In this work, we attempt to capture the rich structure of events and their dependencies in a news topic through our event models. We call the process of recognizing events and their dependencies  event threading. We believe our perspective of modeling the structure of a topic is more effective in capturing its semantics than a flat list of on-topic stories. We formally define the novel problem, suggest evaluation metrics and present a few techniques for solving the problem. Besides the standard word based features, our approaches take into account novel features such as temporal locality of stories for event recognition and time-ordering for capturing dependencies. Our experiments on a manually labeled data sets show that our models effectively identify the events and capture dependencies among them.", "references": ["Topic detection and tracking pilot study: Final report", "Flexible intrinsic evaluation of hierarchical clustering for tdt", "Topic Detection and Tracking: Event based Information Organization", "Temporal summaries of new topics", "Catching the drift: Probabilistic content models, with applications to generation and summarization", "Discovering and comparing topic hierarchies", "Threading electronic mail: a preliminary study", "Investigations on event evolution in tdt", "Hierarchical text classification and evaluation", "Learning approaches for detecting and tracking news events"], "full_text": "1. INTRODUCTION News forms a major portion of information disseminated in the world everyday. Common people and news analysts alike are very interested in keeping abreast of new things that happen in the news, but it is becoming very difficult to cope with the huge volumes of information that arrives each day. Hence there is an increasing need for automatic techniques to organize news stories in a way that helps users interpret and analyze them quickly. This problem is addressed by a research program called Topic Detection and Tracking (TDT) that runs an open annual competition on standardized tasks of news organization. One of the shortcomings of current TDT evaluation is its view of news topics as flat collection of stories. For example, the detection task of TDT is to arrange a collection of news stories into clusters of topics. However, a topic in news is more than a mere collection of stories: it is characterized by a definite structure of inter-related events. This is indeed recognized by TDT which defines a topic as \u2018a set of news stories that are strongly related by some seminal realworld event\" where an event is defined as \u2018something that happens at a specific time and location\" . For example, when a bomb explodes in a building, that is the seminal event that triggers the topic. Other events in the topic may include the rescue attempts, the search for perpetrators, arrests and trials and so on. We see that there is a pattern of dependencies between pairs of events in the topic. In the above example, the event of rescue attempts is \u2018influenced\" by the event of bombing and so is the event of search for perpetrators. In this work we investigate methods for modeling the structure of a topic in terms of its events. By structure, we mean not only identifying the events that make up a topic, but also establishing dependencies-generally causal-among them. We call the process of recognizing events and identifying dependencies among them event threading, an analogy to email threading that shows connections between related email messages. We refer to the resulting interconnected structure of events as the event model of the topic. Although this paper focuses on threading events within an existing news topic, we expect that such event based dependency structure more accurately reflects the structure of news than strictly bounded topics do. From a user\"s perspective, we believe that our view of a news topic as a set of interconnected events helps him/her get a quick overview of the topic and also allows him/her navigate through the topic faster. The rest of the paper is organized as follows. In section 2, we discuss related work. In section 3, we define the problem and use an example to illustrate threading of events within a news topic. In section 4, we describe how we built the corpus for our problem. Section 5 presents our evaluation techniques while section 6 describes the techniques we use for modeling event structure. In section 7 we present our experiments and results. Section 8 concludes the paper with a few observations on our results and comments on future work. 446 2. RELATED WORK The process of threading events together is related to threading of electronic mail only by name for the most part. Email usually incorporates a strong structure of referenced messages and consistently formatted subject headings-though information retrieval techniques are useful when the structure breaks down . Email threading captures reference dependencies between messages and does not attempt to reflect any underlying real-world structure of the matter under discussion. Another area of research that looks at the structure within a topic is hierarchical text classification of topics . The hierarchy within a topic does impose a structure on the topic, but we do not know of an effort to explore the extent to which that structure reflects the underlying event relationships. Barzilay and Lee proposed a content structure modeling technique where topics within text are learnt using unsupervised methods, and a linear order of these topics is modeled using hidden Markov models. Our work differs from theirs in that we do not constrain the dependency to be linear. Also their algorithms are tuned to work on specific genres of topics such as earthquakes, accidents, etc., while we expect our algorithms to generalize over any topic. In TDT, researchers have traditionally considered topics as flatclusters . However, in TDT- a hierarchical structure of topic detection has been proposed and made useful attempts to adopt the new structure. However this structure still did not explicitly model any dependencies between events. In a work closest to ours, Makkonen suggested modeling news topics in terms of its evolving events. However, the paper stopped short of proposing any models to the problem. Other related work that dealt with analysis within a news topic includes temporal summarization of news topics . 3. PROBLEM DEFINITION AND NOTATION In this work, we have adhered to the definition of event and topic as defined in TDT. We present some definitions (in italics) and our interpretations (regular-faced) below for clarity. 1. Story: A story is a news article delivering some information to users. In TDT, a story is assumed to refer to only a single topic. In this work, we also assume that each story discusses a single event. In other words, a story is the smallest atomic unit in the hierarchy (topic event story). Clearly, both the assumptions are not necessarily true in reality, but we accept them for simplicity in modeling. 2. Event: An event is something that happens at some specific time and place . In our work, we represent an event by a set of stories that discuss it. Following the assumption of atomicity of a story, this means that any set of distinct events can be represented by a set of non-overlapping clusters of news stories. 3. Topic: A set of news stories strongly connected by a seminal event. We expand on this definition and interpret a topic as a series of related events. Thus a topic can be represented by clusters of stories each representing an event and a set of (directed or undirected) edges between pairs of these clusters representing the dependencies between these events. We will describe this representation of a topic in more detail in the next section. 4. Topic detection and tracking (TDT) :Topic detection detects clusters of stories that discuss the same topic; Topic tracking detects stories that discuss a previously known topic . Thus TDT concerns itself mainly with clustering stories into topics that discuss them. 5. Event threading: Event threading detects events within in a topic, and also captures the dependencies among the events. Thus the main difference between event threading and TDT is that we focus our modeling effort on microscopic events rather than larger topics. Additionally event threading models the relatedness or dependencies between pairs of events in a topic while TDT models topics as unrelated clusters of stories. We first define our problem and representation of our model formally and then illustrate with the help of an example. We are given a set of \u00d2 news stories \u00cb \u00d7\u00bd \u00a1 \u00a1 \u00a1 \u00d7\u00d2 on a given topic \u00cc and their time of publication. We define a set of events \u00bd \u00a1 \u00a1 \u00a1 \u00d1 with the following constraints: \u00be \u00be \u00cb (1) \u00d7 \u00d8 (2) \u00d7 \u00be \u00d7 \u00d8 \u00d7 \u00be (3) While the first constraint says that each event is an element in the power set of S, the second constraint ensures that each story can belong to at most one event. The last constraint tells us that every story belongs to one of the events in . In fact this allows us to define a mapping function from stories to events as follows: \u00b4\u00d7 \u00b5 iff \u00d7 \u00be (4) Further, we also define a set of directed edges \u00b4 \u00b5 which denote dependencies between events. It is important to explain what we mean by this directional dependency: While the existence of an edge itself represents relatedness of two events, the direction could imply causality or temporal-ordering. By causal dependency we mean that the occurrence of event B is related to and is a consequence of the occurrence of event A. By temporal ordering, we mean that event B happened after event A and is related to A but is not necessarily a consequence of A. For example, consider the following two events: \u2018plane crash\" (event A) and \u2018subsequent investigations\" (event B) in a topic on a plane crash incident. Clearly, the investigations are a result of the crash. Hence an arrow from A to B falls under the category of causal dependency. Now consider the pair of events \u2018Pope arrives in Cuba\"(event A) and \u2018Pope meets Castro\"(event B) in a topic that discusses Pope\"s visit to Cuba. Now events A and B are closely related through their association with the Pope and Cuba but event B is not necessarily a consequence of the occurrence of event A. An arrow in such scenario captures what we call time ordering. In this work, we do not make an attempt to distinguish between these two kinds of dependencies and our models treats them as identical. A simpler (and hence less controversial) choice would be to ignore direction in the dependencies altogether and consider only undirected edges. This choice definitely makes sense as a first step but we chose the former since we believe directional edges make more sense to the user as they provide a more illustrative flow-chart perspective to the topic. To make the idea of event threading more concrete, titled \u2018Osama bin Laden\"s Indictment\" ( news). This topic has 23 stories which form 5 events. An event model of this topic can be represented as in figure 1. Each box in the figure indicates an event in the topic of Osama\"s indictment. The occurrence of event 2, namely \u2018Trial and Indictment of Osama\" is dependent on the event of \u2018evidence gathered by CIA\", i.e., event 1. Similarly, event 2 influences the occurrences of events 3, 4 and 5, namely \u2018Threats from Militants\", \u2018Reactions 447 from Muslim World\" and \u2018announcement of reward\". Thus all the dependencies in the example are causal. Extending our notation further, we call an event A a parent of B and B the child of A, if \u00b4 \u00b5 \u00be . We define an event model \u00c5 \u00b4 \u00b5 to be a tuple of the set of events and set of dependencies. Trial and (5) (3) (4) CIA announces reward Muslim world Reactions from Islamic militants Threats from (2) (1) Osama Indictment of CIA gathered by Evidence Figure 1: An event model of TDT topic \u2018Osama bin Laden\"s indictment\". Event threading is strongly related to topic detection and tracking, but also different from it significantly. It goes beyond topics, and models the relationships between events. Thus, event threading can be considered as a further extension of topic detection and tracking and is more challenging due to at least the following difficulties. 1. The number of events is unknown. 2. The granularity of events is hard to define. 3. The dependencies among events are hard to model. 4. Since it is a brand new research area, no standard evaluation metrics and benchmark data is available. In the next few sections, we will describe our attempts to tackle these problems. 4. LABELED DATA We picked 28 topics from the TDT2 corpus and 25 topics from the TDT3 corpus. The criterion we used for selecting a topic is that it should contain at least 15 on-topic stories from CNN headline news. If the topic contained more than 30 CNN stories, we picked only the first 30 stories to keep the topic short enough for annotators. The reason for choosing only CNN as the source is that the stories from this source tend to be short and precise and do not tend to digress or drift too far away from the central theme. We believe modeling such stories would be a useful first step before dealing with more complex data sets. We hired an annotator to create truth data. Annotation includes defining the event membership for each story and also the dependencies. We supervised the annotator on a set of three topics that we did our own annotations on and then asked her to annotate the 28 topics from TDT2 and 25 topics from TDT3. In identifying events in a topic, the annotator was asked to broadly follow the TDT definition of an event, i.e., \u2018something that happens at a specific time and location\". The annotator was encouraged to merge two events A and B into a single event C if any of the stories discusses both A and B. This is to satisfy our assumption that each story corresponds to a unique event. The annotator was also encouraged to avoid singleton events, events that contain a single news story, if possible. We realized from our own experience that people differ in their perception of an event especially when the number of stories in that event is small. As part of the guidelines, we instructed the annotator to assign titles to all the events in each topic. We believe that this would help make her understanding of the events more concrete. We however, do not use or model these titles in our algorithms. In defining dependencies between events, we imposed no restrictions on the graph structure. Each event could have single, multiple or no parents. Further, the graph could have cycles or orphannodes. The annotator was however instructed to assign a dependency from event A to event B if and only if the occurrence of B is \u2018either causally influenced by A or is closely related to A and follows A in time\". From the annotated topics, we created a training set of 26 topics and a test set of 27 topics by merging the 28 topics from TDT2 and 25 from TDT3 and splitting them randomly. Table 1 shows that the training and test sets have fairly similar statistics. Feature Training set Test set Num. topics 26 27 Avg. Num. Stories/Topic 28.69 26.74 Avg. Doc. Len. 64.60 64.04 Avg. Num. Stories/Event 5.65 6.22 Avg. Num. Events/Topic 5.07 4.29 Avg. Num. Dependencies/Topic 3.07 2.92 Avg. Num. Dependencies/Event 0.61 0.68 Avg. Num. Days/Topic 30.65 34.48 Table 1: Statistics of annotated data 5. EVALUATION A system can generate some event model \u00c5\u00bc \u00b4 \u00bc \u00bc\u00b5 using certain algorithms, which is usually different from the truth model \u00c5 \u00b4 \u00b5 (we assume the annotator did not make any mistake). Comparing a system event model \u00c5\u00bc with the true model \u00c5 requires comparing the entire event models including their dependency structure. And different event granularities may bring huge discrepancy between \u00c5\u00bc and \u00c5. This is certainly non-trivial as even testing whether two graphs are isomorphic has no known polynomial time solution. Hence instead of comparing the actual structure we examine a pair of stories at a time and verify if the system and true labels agree on their event-memberships and dependencies. Specifically, we compare two kinds of story pairs: \u00af Cluster pairs ( \u00b4\u00c5\u00b5): These are the complete set of unordered pairs \u00b4\u00d7 \u00d7 \u00b5 of stories \u00d7 and \u00d7 that fall within the same event given a model \u00c5. Formally, \u00b4\u00c5\u00b5 \u00b4\u00d7 \u00d7 \u00b5 \u00d7 \u00d7 \u00be \u00cb \u00b4\u00d7 \u00b5 \u00b4\u00d7 \u00b5 (5) where is the function in \u00c5 that maps stories to events as defined in equation 4. \u00af Dependency pairs ( \u00b4\u00c5\u00b5): These are the set of all ordered pairs of stories \u00b4\u00d7 \u00d7 \u00b5 such that there is a dependency from the event of \u00d7 to the event of \u00d7 in the model \u00c5. \u00b4\u00c5\u00b5 \u00b4\u00d7 \u00d7 \u00b5 \u00b4 \u00b4\u00d7 \u00b5 \u00b4\u00d7 \u00b5\u00b5 \u00be (6) Note the story pair is ordered here, so \u00b4\u00d7 \u00d7 \u00b5 is not equivalent to \u00b4\u00d7 \u00d7 \u00b5. In our evaluation, a correct pair with wrong 448 (B->D) Cluster pairs (A,C) Dependency pairs (A->B) (C->B) (B->D) D,E D,E (D,E) (D,E) (A->C) (A->E) (B->C) (B->E) (B->E) Cluster precision: 1/2 Cluster Recall: 1/2 Dependency Recall: 2/6 Dependency Precision: 2/4 (A->D) True event model System event model A,B A,C B Cluster pairs (A,B) Dependency pairs Figure 2: Evaluation measures direction will be considered a mistake. As we mentioned earlier in section 3, ignoring the direction may make the problem simpler, but we will lose the expressiveness of our representation. Given these two sets of story pairs corresponding to the true event model \u00c5 and the system event model \u00c5\u00bc, we define recall and precision for each category as follows. \u00af Cluster Precision (CP): It is the probability that two randomly selected stories \u00d7 and \u00d7 are in the same true-event given that they are in the same system event. \u00c8 \u00c8\u00b4 \u00b4\u00d7 \u00b5 \u00b4\u00d7 \u00b5 \u00bc\u00b4\u00d7 \u00b5 \u00bc\u00b4\u00d7 \u00b5\u00b5 \u00b4\u00c5\u00b5 \u00b4\u00c5\u00bc\u00b5 \u00b4\u00c5\u00bc\u00b5 (7) where \u00bc is the story-event mapping function corresponding to the model \u00c5\u00bc. \u00af Cluster Recall(CR): It is the probability that two randomly selected stories \u00d7 and \u00d7 are in the same system-event given that they are in the same true event. \u00ca \u00c8\u00b4 \u00bc\u00b4\u00d7 \u00b5 \u00bc\u00b4\u00d7 \u00b5 \u00b4\u00d7 \u00b5 \u00b4\u00d7 \u00b5\u00b5 \u00b4\u00c5\u00b5 \u00b4\u00c5\u00bc\u00b5 \u00b4\u00c5\u00b5 (8) \u00af Dependency Precision(DP): It is the probability that there is a dependency between the events of two randomly selected stories \u00d7 and \u00d7 in the true model \u00c5 given that they have a dependency in the system model \u00c5\u00bc. Note that the direction of dependency is important in comparison. \u00c8 \u00c8\u00b4\u00b4 \u00b4\u00d7 \u00b5 \u00b4\u00d7 \u00b5\u00b5 \u00be \u00b4 \u00bc\u00b4\u00d7 \u00b5 \u00bc\u00b4\u00d7 \u00b5\u00b5 \u00be \u00bc\u00b5 \u00b4\u00c5\u00b5 \u00b4\u00c5\u00bc\u00b5 \u00b4\u00c5\u00bc\u00b5 (9) \u00af Dependency Recall(DR): It is the probability that there is a dependency between the events of two randomly selected stories \u00d7 and \u00d7 in the system model \u00c5\u00bc given that they have a dependency in the true model \u00c5. Again, the direction of dependency is taken into consideration. \u00ca \u00c8\u00b4\u00b4 \u00bc\u00b4\u00d7 \u00b5 \u00bc\u00b4\u00d7 \u00b5\u00b5 \u00be \u00bc \u00b4 \u00b4\u00d7 \u00b5 \u00b4\u00d7 \u00b5\u00b5 \u00be \u00b5 \u00b4\u00c5\u00b5 \u00b4\u00c5\u00bc\u00b5 \u00b4\u00c5\u00b5 (10) The measures are illustrated by an example in figure 2. We also combine these measures using the well known F1-measure commonly used in text classification and other research areas as shown below. \u00be \u00a2 \u00c8 \u00a2 \u00ca \u00c8 \u00b7 \u00ca \u00be \u00a2 \u00c8 \u00a2 \u00ca \u00c8 \u00b7 \u00ca \u00c2 \u00be \u00a2 \u00a2 (11) where and are the cluster and dependency F1-measures respectively and \u00c2 is the Joint F1-measure (\u00c2 ) that we use to measure the overall performance. 6. TECHNIQUES The task of event modeling can be split into two parts: clustering the stories into unique events in the topic and constructing dependencies among them. In the following subsections, we describe techniques we developed for each of these sub-tasks. 6.1 Clustering Each topic is composed of multiple events, so stories must be clustered into events before we can model the dependencies among them. For simplicity, all stories in the same topic are assumed to be available at one time, rather than coming in a text stream. This task is similar to traditional clustering but features other than word distributions may also be critical in our application. In many text clustering systems, the similarity between two stories is the inner product of their tf-idf vectors, hence we use it as one of our features. Stories in the same event tend to follow temporal locality, so the time stamp of each story can be a useful feature. Additionally, named-entities such as person and location names are another obvious feature when forming events. Stories in the same event tend to be related to the same person(s) and locations(s). In this subsection, we present an agglomerative clustering algorithm that combines all these features. In our experiments, however, we study the effect of each feature on the performance separately using modified versions of this algorithm. 6.1.1 Agglomerative clustering with time decay (ACDT) We initialize our events to singleton events (clusters), i.e., each cluster contains exactly one story. So the similarity between two events, to start with, is exactly the similarity between the corresponding stories. The similarity \u00db\u00d7\u00d9\u00d1\u00b4\u00d7\u00bd \u00d7\u00be\u00b5 between two stories \u00d7\u00bd and \u00d7\u00be is given by the following formula: \u00db\u00d7\u00d9\u00d1\u00b4\u00d7\u00bd \u00d7\u00be\u00b5 \u00bd \u00d3\u00d7\u00b4\u00d7\u00bd \u00d7\u00be\u00b5 \u00b7 \u00be\u00c4\u00d3 \u00b4\u00d7\u00bd \u00d7\u00be\u00b5 \u00b7 \u00bf\u00c8 \u00d6\u00b4\u00d7\u00bd \u00d7\u00be\u00b5 (12) Here \u00bd, \u00be, \u00bf are the weights on different features. In this work, we determined them empirically, but in the future, one can consider more sophisticated learning techniques to determine them. \u00d3\u00d7\u00b4\u00d7\u00bd \u00d7\u00be\u00b5 is the cosine similarity of the term vectors. \u00c4\u00d3 \u00b4\u00d7\u00bd \u00d7\u00be\u00b5 is 1 if there is some location that appears in both stories, otherwise it is 0. \u00c8 \u00d6\u00b4\u00d7\u00bd \u00d7\u00be\u00b5 is similarly defined for person name. We use time decay when calculating similarity of story pairs, i.e., the larger time difference between two stories, the smaller their similarities. The time period of each topic differs a lot, from a few days to a few months. So we normalize the time difference using the whole duration of that topic. The time decay adjusted similarity 449 \u00d7 \u00d1\u00b4\u00d7\u00bd \u00d7\u00be\u00b5 is given by \u00d7 \u00d1\u00b4\u00d7\u00bd \u00d7\u00be\u00b5 \u00db\u00d7\u00d9\u00d1\u00b4\u00d7\u00bd \u00d7\u00be\u00b5 \u00ab \u00d8\u00bd\u00a0\u00d8\u00be \u00cc (13) where \u00d8\u00bd and \u00d8\u00be are the time stamps for story 1 and 2 respectively. T is the time difference between the earliest and the latest story in the given topic. \u00ab is the time decay factor. In each iteration, we find the most similar event pair and merge them. We have three different ways to compute the similarity between two events \u00d9 and \u00da: \u00af Average link: In this case the similarity is the average of the similarities of all pairs of stories between \u00d9 and \u00da as shown below: \u00d7 \u00d1\u00b4 \u00d9 \u00da \u00b5 \u00c8\u00d7\u00d9\u00be \u00d9 \u00c8\u00d7\u00da\u00be \u00da \u00d7 \u00d1\u00b4\u00d7\u00d9 \u00d7\u00da \u00b5 \u00d9 \u00da (14) \u00af Complete link: The similarity between two events is given by the smallest of the pair-wise similarities. \u00d7 \u00d1\u00b4 \u00d9 \u00da \u00b5 \u00d1 \u00d2 \u00d7\u00d9\u00be \u00d9 \u00d7\u00da\u00be \u00da \u00d7 \u00d1\u00b4\u00d7\u00d9 \u00d7\u00da \u00b5 (15) \u00af Single link: Here the similarity is given by the best similarity between all pairs of stories. \u00d7 \u00d1\u00b4 \u00d9 \u00da \u00b5 \u00d1 \u00dc \u00d7\u00d9\u00be \u00d9 \u00d7\u00da\u00be \u00da \u00d7 \u00d1\u00b4\u00d7\u00d9 \u00d7\u00da \u00b5 (16) This process continues until the maximum similarity falls below the threshold or the number of clusters is smaller than a given number. 6.2 Dependency modeling Capturing dependencies is an extremely hard problem because it may require a \u2018deeper understanding\" of the events in question. A human annotator decides on dependencies not just based on the information in the events but also based on his/her vast repertoire of domain-knowledge and general understanding of how things operate in the world. For example, in Figure 1 a human knows \u2018Trial and indictment of Osama\" is influenced by \u2018Evidence gathered by CIA\" because he/she understands the process of law in general. We believe a robust model should incorporate such domain knowledge in capturing dependencies, but in this work, as a first step, we will rely on surface-features such as time-ordering of news stories and word distributions to model them. Our experiments in later sections demonstrate that such features are indeed useful in capturing dependencies to a large extent. In this subsection, we describe the models we considered for capturing dependencies. In the rest of the discussion in this subsection, we assume that we are already given the mapping \u00bc \u00cb and we focus only on modeling the edges \u00bc. First we define a couple of features that the following models will employ. First we define a 1-1 time-ordering function \u00d8 \u00cb \u00bd \u00a1 \u00a1 \u00a1 \u00d2 that sorts stories in ascending order by their time of publication. Now, the event-time-ordering function \u00d8 is defined as follows. \u00d8 \u00bd \u00a1 \u00a1 \u00a1 \u00d1 \u00d7 \u00d8 \u00d9 \u00da \u00be \u00d8 \u00b4 \u00d9\u00b5 \u00d8 \u00b4 \u00da\u00b5 \u00b4\u00b5 \u00d1 \u00d2 \u00d7\u00d9\u00be \u00d9 \u00d8\u00b4\u00d7\u00d9\u00b5 \u00d1 \u00d2 \u00d7\u00da\u00be \u00da \u00d8\u00b4\u00d7\u00da\u00b5 (17) In other words, \u00d8 time-orders events based on the time-ordering of their respective first stories. We will also use average cosine similarity between two events as a feature and it is defined as follows. \u00da \u00cb \u00d1\u00b4 \u00d9 \u00da \u00b5 \u00c8\u00d7\u00d9\u00be \u00d9 \u00c8\u00d7\u00da\u00be \u00da \u00d3\u00d7\u00b4\u00d7\u00d9 \u00d7\u00da \u00b5 \u00d9 \u00da (18) 6.2.1 Complete-Link model In this model, we assume that there are dependencies between all pairs of events. The direction of dependency is determined by the time-ordering of the first stories in the respective events. Formally, the system edges are defined as follows. \u00bc \u00b4 \u00d9 \u00da \u00b5 \u00d8 \u00b4 \u00d9\u00b5 \u00d8 \u00b4 \u00da \u00b5 (19) where \u00d8 is the event-time-ordering function. In other words, the dependency edge is directed from event \u00d9 to event \u00da , if the first story in event \u00d9 is earlier than the first story in event \u00da . We point out that this is not to be confused with the complete-link algorithm in clustering. Although we use the same names, it will be clear from the context which one we refer to. 6.2.2 Simple Thresholding This model is an extension of the complete link model with an additional constraint that there is a dependency between any two events \u00d9 and \u00da only if the average cosine similarity between event \u00d9 and event \u00da is greater than a threshold \u00cc. Formally, \u00bc \u00b4 \u00d9 \u00da\u00b5 \u00da \u00cb \u00d1\u00b4 \u00d9 \u00da \u00b5 \u00cc \u00d8 \u00b4 \u00d9\u00b5 \u00d8 \u00b4 \u00da \u00b5 (20) 6.2.3 Nearest Parent Model In this model, we assume that each event can have at most one parent. We define the set of dependencies as follows. \u00bc \u00b4 \u00d9 \u00da\u00b5 \u00da \u00cb \u00d1\u00b4 \u00d9 \u00da \u00b5 \u00cc \u00d8 \u00b4 \u00da\u00b5 \u00d8 \u00b4 \u00d9\u00b5 \u00b7 \u00bd (21) Thus, for each event \u00da , the nearest parent model considers only the event preceding it as defined by \u00d8 as a potential candidate. The candidate is assigned as the parent only if the average similarity exceeds a pre-defined threshold \u00cc. 6.2.4 Best Similarity Model This model also assumes that each event can have at most one parent. An event \u00da is assigned a parent \u00d9 if and only if \u00d9 is the most similar earlier event to \u00da and the similarity exceeds a threshold \u00cc. Mathematically, this can be expressed as: \u00bc \u00b4 \u00d9 \u00da \u00b5 \u00da \u00cb \u00d1\u00b4 \u00d9 \u00da\u00b5 \u00cc \u00d9 \u00d6 \u00d1 \u00dc \u00db \u00d8 \u00b4 \u00db\u00b5 \u00d8 \u00b4 \u00da\u00b5 \u00da \u00cb \u00d1\u00b4 \u00db \u00da \u00b5 (22) 6.2.5 Maximum Spanning Tree model In this model, we first build a maximum spanning tree (MST) using a greedy algorithm on the following fully connected weighted, undirected graph whose vertices are the events and whose edges are defined as follows: \u00b4 \u00d9 \u00da \u00b5 \u00db\u00b4 \u00d9 \u00da \u00b5 \u00da \u00cb \u00d1\u00b4 \u00d9 \u00da\u00b5 (23) Let \u00c5\u00cb\u00cc\u00b4 \u00b5 be the set of edges in the maximum spanning tree of \u00bc. Now our directed dependency edges are defined as follows. \u00bc \u00b4 \u00d9 \u00da \u00b5 \u00b4 \u00d9 \u00da \u00b5 \u00be \u00c5\u00cb\u00cc\u00b4 \u00b5 \u00d8 \u00b4 \u00d9\u00b5 \u00d8 \u00b4 \u00da\u00b5 \u00da \u00cb \u00d1\u00b4 \u00d9 \u00da \u00b5 \u00cc (24) 450 Thus in this model, we assign dependencies between the most similar events in the topic. 7. EXPERIMENTS Our experiments consists of three parts. First we modeled only the event clustering part (defining the mapping function \u00bc) using clustering algorithms described in section 6.1. Then we modeled only the dependencies by providing to the system the true clusters and running only the dependency algorithms of section 6.2. Finally, we experimented with combinations of clustering and dependency algorithms to produce the complete event model. This way of experimentation allows us to compare the performance of our algorithms in isolation and in association with other components. The following subsections present the three parts of our experimentation. 7.1 Clustering We have tried several variations of the \u00cc algorithm to study the effects of various features on the clustering performance. All the parameters are learned by tuning on the training set. We also tested the algorithms on the test set with parameters fixed at their optimal values learned from training. We used agglomerative clusModel best T CP CR CF P-value cos+1-lnk 0.15 0.41 0.56 0.43cos+all-lnk 0.00 0.40 0.62 0.45cos+Loc+avg-lnk 0.07 0.37 0.74 0.45cos+Per+avg-lnk 0.07 0.39 0.70 0.46cos+TD+avg-lnk 0.04 0.45 0.70 0.53 2.9e-4* cos+N(T)+avg-lnk - 0.41 0.62 0.48 7.5e-2 cos+N(T)+T+avg-lnk 0.03 0.42 0.62 0.49 2.4e-2* cos+TD+N(T)+avg-lnk - 0.44 0.66 0.52 7.0e-3* cos+TD+N(T)+T+avg-lnk 0.03 0.47 0.64 0.53 1.1e-3* Baseline(cos+avg-lnk) 0.05 0.39 0.67 0.46Table 2: Comparison of agglomerative clustering algorithms (training set) tering based on only cosine similarity as our clustering baseline. The results on the training and test sets are in Table 2 and 3 respectively. We use the Cluster F1-measure (CF) averaged over all topics as our evaluation criterion. Model CP CR CF P-value cos+1-lnk 0.43 0.49 0.39cos+all-lnk 0.43 0.62 0.47cos+Loc+avg-lnk 0.37 0.73 0.45cos+Per+avg-lnk 0.44 0.62 0.45cos+TD+avg-lnk 0.48 0.70 0.54 0.014* cos+N(T)+avg-lnk 0.41 0.71 0.51 0.31 cos+N(T)+T+avg-lnk 0.43 0.69* 0.52 0.14 cos+TD+N(T)+avg-lnk 0.43 0.76 0.54 0.025* cos+TD+N(T)+T+avg-lnk 0.47 0.69 0.* Baseline(cos+avg-lnk) 0.44 0.67 0.50Table 3: Comparison of agglomerative clustering algorithms (test set) P-value marked with a \u00a3 means that it is a statistically significant improvement over the baseline (95% confidence level, one tailed T-test). The methods shown in table 2 and 3 are: \u00af Baseline: tf-idf vector weight, cosine similarity, average link in clustering. In equation 12, \u00bd \u00bd, \u00be \u00bf \u00bc. And \u00ab \u00bc in equation 13. This F-value is the maximum obtained by tuning the threshold. \u00af cos+1-lnk: Single link comparison (see equation 16) is used where similarity of two clusters is the maximum of all story pairs, other configurations are the same as the baseline run. \u00af cos+all-lnk: Complete link algorithm of equation 15 is used. Similar to single link but it takes the minimum similarity of all story pairs. \u00af cos+Loc+avg-lnk: Location names are used when calculating similarity. \u00be \u00bc \u00bc in equation 12. All algorithms starting from this one use average link (equation 14), since single link and complete link do not show any improvement of performance. \u00af cos+Per+avg-lnk: \u00bf \u00bc \u00bc in equation 12, i.e., we put some weight on person names in the similarity. \u00af cos+TD+avg-lnk: Time Decay coefficient \u00ab \u00bd in equation 13, which means the similarity between two stories will be decayed to \u00bd if they are at different ends of the topic. \u00af cos+N(T)+avg-lnk: Use the number of true events to control the agglomerative clustering algorithm. When the number of clusters is fewer than that of truth events, stop merging clusters. \u00af cos+N(T)+T+avg-lnk: similar to N(T) but also stop agglomeration if the maximal similarity is below the threshold \u00cc. \u00af cos+TD:+N(T)+avg-lnk: similar to N(T) but the similarities are decayed, \u00ab \u00bd in equation 13. \u00af cos+TD+N(T)+T+avg-lnk: similar to TD+N(Truth) but calculation halts when the maximal similarity is smaller than the threshold \u00cc. Our experiments demonstrate that single link and complete link similarities perform worse than average link, which is reasonable since average link is less sensitive to one or two story pairs. We had expected locations and person names to improve the result, but it is not the case. Analysis of topics shows that many on-topic stories share the same locations or persons irrespective of the event they belong to, so these features may be more useful in identifying topics rather than events. Time decay is successful because events are temporally localized, i.e., stories discussing the same event tend to be adjacent to each other in terms of time. Also we noticed that providing the number of true events improves the performance since it guides the clustering algorithm to get correct granularity. However, for most applications, it is not available. We used it only as a cheat experiment for comparison with other algorithms. On the whole, time decay proved to the most powerful feature besides cosine similarity on both training and test sets. 7.2 Dependencies In this subsection, our goal is to model only dependencies. We use the true mapping function and by implication the true events \u00ce . We build our dependency structure \u00bc using all the five models described in section 6.2. We first train our models on the 26 training topics. Training involves learning the best threshold \u00cc for each of the models. We then test the performances of all the trained models on the 27 test topics. We evaluate our performance 451 using the average values of Dependency Precision (DP), Dependency Recall (DR) and Dependency F-measure (DF). We consider the complete-link model to be our baseline since for each event, it trivially considers all earlier events to be parents. Table 4 lists the results on the training set. We see that while all the algorithms except MST outperform the baseline complete-link algorithm , the nearest Parent algorithm is statistically significant from the baseline in terms of its DF-value using a one-tailed paired T-test at 95% confidence level. Model best \u00cc DP DR DF P-value Nearest Parent 0.025 0.55 0.62 0.56 0.04* Best Similarity 0.02 0.51 0.62 0.53 0.24 MST 0.0 0.46 0.58 0.48Simple Thresh. 0.045 0.45 0.76 0.52 0.14 Complete-link - 0.36 0.93 0.48Table 4: Results on the training set: Best \u00cc is the optimal value of the threshold \u00cc. * indicates the corresponding model is statistically significant compared to the baseline using a one-tailed, paired T-test at 95% confidence level. In table 5 we present the comparison of the models on the test set. Here, we do not use any tuning but set the threshold to the corresponding optimal values learned from the training set. The results throw some surprises: The nearest parent model, which was significantly better than the baseline on training set, turns out to be worse than the baseline on the test set. However all the other models are better than the baseline including the best similarity which is statistically significant. Notice that all the models that perform better than the baseline in terms of DF, actually sacrifice their recall performance compared to the baseline, but improve on their precision substantially thereby improving their performance on the DF-measure. We notice that both simple-thresholding and best similarity are better than the baseline on both training and test sets although the improvement is not significant. On the whole, we observe that the surface-level features we used capture the dependencies to a reasonable level achieving a best value of 0.72 DF on the test set. Although there is a lot of room for improvement, we believe this is a good first step. Model DP DR DF P-value Nearest Parent 0.61 0.60 0.60Best Similarity 0.71 0.74 0.72 0.04* MST 0.70 0.68 0.69 0.22 Simple Thresh. 0.57 0.75 0.64 0.24 Baseline (Complete-link) 0.50 0.94 0.63Table 5: Results on the test set 7.3 Combining Clustering and Dependencies Now that we have studied the clustering and dependency algorithms in isolation, we combine the best performing algorithms and build the entire event model. Since none of the dependency algorithms has been shown to be consistently and significantly better than the others, we use all of them in our experimentation. From the clustering techniques, we choose the best performing Cos+TD. As a baseline, we use a combination of the baselines in each components, i.e., cos for clustering and complete-link for dependencies. Note that we need to retrain all the algorithms on the training set because our objective function to optimize is now JF, the joint F-measure. For each algorithm, we need to optimize both the clustering threshold and the dependency threshold. We did this empirically on the training set and the optimal values are listed in table 6. The results on the training set, also presented in table 6, indicate that cos+TD+Simple-Thresholding is significantly better than the baseline in terms of the joint F-value JF, using a one-tailed paired Ttest at 95% confidence level. On the whole, we notice that while the clustering performance is comparable to the experiments in section 7.1, the overall performance is undermined by the low dependency performance. Unlike our experiments in section 7.2 where we had provided the true clusters to the system, in this case, the system has to deal with deterioration in the cluster quality. Hence the performance of the dependency algorithms has suffered substantially thereby lowering the overall performance. The results on the test set present a very similar story as shown in table 7. We also notice a fair amount of consistency in the performance of the combination algorithms. cos+TD+Simple-Thresholding outperforms the baseline significantly. The test set results also point to the fact that the clustering component remains a bottleneck in achieving an overall good performance. 8. DISCUSSION AND CONCLUSIONS In this paper, we have presented a new perspective of modeling news topics. Contrary to the TDT view of topics as flat collection of news stories, we view a news topic as a relational structure of events interconnected by dependencies. In this paper, we also proposed a few approaches for both clustering stories into events and constructing dependencies among them. We developed a timedecay based clustering approach that takes advantage of temporallocalization of news stories on the same event and showed that it performs significantly better than the baseline approach based on cosine similarity. Our experiments also show that we can do fairly well on dependencies using only surface-features such as cosinesimilarity and time-stamps of news stories as long as true events are provided to the system. However, the performance deteriorates rapidly if the system has to discover the events by itself. Despite that discouraging result, we have shown that our combined algorithms perform significantly better than the baselines. Our results indicate modeling dependencies can be a very hard problem especially when the clustering performance is below ideal level. Errors in clustering have a magnifying effect on errors in dependencies as we have seen in our experiments. Hence, we should focus not only on improving dependencies but also on clustering at the same time. As part of our future work, we plan to investigate further into the data and discover new features that influence clustering as well as dependencies. And for modeling dependencies, a probabilistic framework should be a better choice since there is no definite answer of yes/no for the causal relations among some events. We also hope to devise an iterative algorithm which can improve clustering and dependency performance alternately as suggested by one of the reviewers. We also hope to expand our labeled corpus further to include more diverse news sources and larger and more complex event structures. Acknowledgments We would like to thank the three anonymous reviewers for their valuable comments. This work was supported in part by the Center 452 Model Cluster T Dep. T CP CR CF DP DR DF JF P-value cos+TD+Nearest-Parent 0.055 0.02 0.51 0.53 0.49 0.21 0.19 0.19 0.27cos+TD+Best-Similarity 0.04 0.02 0.45 0.70 0.53 0.21 0.33 0.23 0.32cos+TD+MST 0.04 0.00 0.45 0.70 0.53 0.22 0.35 0.25 0.33cos+TD+Simple-Thresholding 0.065 0.02 0.56 0.47 0.48 0.23 0.61 0.32 0.* Baseline (cos+Complete-link) 0.10 - 0.58 0.31 0.38 0.20 0.67 0.30 0.33Table 6: Combined results on the training set Model CP CR CF DP DR DF JF P-value cos+TD+Nearest Parent 0.57 0.50 0.50 0.27 0.19 0.21 0.30cos+TD+Best Similarity 0.48 0.70 0.54 0.31 0.27 0.26 0.35cos+TD+MST 0.48 0.70 0.54 0.31 0.30 0.28 0.37cos+TD+Simple Thresholding 0.60 0.39 0.44 0.32 0.66 0.42 0.* Baseline (cos+Complete-link) 0.66 0.27 0.36 0.30 0.72 0.43 0.39Table 7:-02-1-. Any opinions, findings and conclusions or recommendations expressed in this material are the authors\" and do not necessarily reflect those of the sponsor.", "body1": "News forms a major portion of information disseminated in the world everyday. One of the shortcomings of current TDT evaluation is its view of news topics as flat collection of stories. In this work we investigate methods for modeling the structure of a topic in terms of its events. The rest of the paper is organized as follows. Section 5 presents our evaluation techniques while section 6 describes the techniques we use for modeling event structure. The process of threading events together is related to threading of electronic mail only by name for the most part. Barzilay and Lee proposed a content structure modeling technique where topics within text are learnt using unsupervised methods, and a linear order of these topics is modeled using hidden Markov models. In TDT, researchers have traditionally considered topics as flatclusters . In a work closest to ours, Makkonen suggested modeling news topics in terms of its evolving events. In this work, we have adhered to the definition of event and topic as defined in TDT. 1. 2. 3. Thus TDT concerns itself mainly with clustering stories into topics that discuss them. 5. Thus the main difference between event threading and TDT is that we focus our modeling effort on microscopic events rather than larger topics. We first define our problem and representation of our model formally and then illustrate with the help of an example. Now consider the pair of events \u2018Pope arrives in Cuba\"(event A) and \u2018Pope meets Castro\"(event B) in a topic that discusses Pope\"s visit to Cuba. To make the idea of event threading more concrete, titled \u2018Osama bin Laden\"s Indictment\" ( news). Trial and (5) (3) (4) CIA announces reward Muslim world Reactions from Islamic militants Threats from (2) (1) Osama Indictment of CIA gathered by Evidence Figure 1: An event model of TDT topic \u2018Osama bin Laden\"s indictment\". Event threading is strongly related to topic detection and tracking, but also different from it significantly. 2. 3. 4. In the next few sections, we will describe our attempts to tackle these problems. We picked 28 topics from the TDT2 corpus and 25 topics from the TDT3 corpus. We hired an annotator to create truth data. In identifying events in a topic, the annotator was asked to broadly follow the TDT definition of an event, i.e., \u2018something that happens at a specific time and location\". In defining dependencies between events, we imposed no restrictions on the graph structure. A system can generate some event model \u00c5\u00bc \u00b4 \u00bc \u00bc\u00b5 using certain algorithms, which is usually different from the truth model \u00c5 \u00b4 \u00b5 (we assume the annotator did not make any mistake). \u00b4\u00c5\u00b5 \u00b4\u00d7 \u00d7 \u00b5 \u00b4 \u00b4\u00d7 \u00b5 \u00b4\u00d7 \u00b5\u00b5 \u00be (6) Note the story pair is ordered here, so \u00b4\u00d7 \u00d7 \u00b5 is not equivalent to \u00b4\u00d7 \u00d7 \u00b5. \u00af Cluster Precision (CP): It is the probability that two randomly selected stories \u00d7 and \u00d7 are in the same true-event given that they are in the same system event. \u00c8 \u00c8\u00b4 \u00b4\u00d7 \u00b5 \u00b4\u00d7 \u00b5 \u00bc\u00b4\u00d7 \u00b5 \u00bc\u00b4\u00d7 \u00b5\u00b5 \u00b4\u00c5\u00b5 \u00b4\u00c5\u00bc\u00b5 \u00b4\u00c5\u00bc\u00b5 (7) where \u00bc is the story-event mapping function corresponding to the model \u00c5\u00bc. \u00af Cluster Recall(CR): It is the probability that two randomly selected stories \u00d7 and \u00d7 are in the same system-event given that they are in the same true event. \u00ca \u00c8\u00b4 \u00bc\u00b4\u00d7 \u00b5 \u00bc\u00b4\u00d7 \u00b5 \u00b4\u00d7 \u00b5 \u00b4\u00d7 \u00b5\u00b5 \u00b4\u00c5\u00b5 \u00b4\u00c5\u00bc\u00b5 \u00b4\u00c5\u00b5 (8) \u00af Dependency Precision(DP): It is the probability that there is a dependency between the events of two randomly selected stories \u00d7 and \u00d7 in the true model \u00c5 given that they have a dependency in the system model \u00c5\u00bc. \u00c8 \u00c8\u00b4\u00b4 \u00b4\u00d7 \u00b5 \u00b4\u00d7 \u00b5\u00b5 \u00be \u00b4 \u00bc\u00b4\u00d7 \u00b5 \u00bc\u00b4\u00d7 \u00b5\u00b5 \u00be \u00bc\u00b5 \u00b4\u00c5\u00b5 \u00b4\u00c5\u00bc\u00b5 \u00b4\u00c5\u00bc\u00b5 (9) \u00af Dependency Recall(DR): It is the probability that there is a dependency between the events of two randomly selected stories \u00d7 and \u00d7 in the system model \u00c5\u00bc given that they have a dependency in the true model \u00c5. \u00ca \u00c8\u00b4\u00b4 \u00bc\u00b4\u00d7 \u00b5 \u00bc\u00b4\u00d7 \u00b5\u00b5 \u00be \u00bc \u00b4 \u00b4\u00d7 \u00b5 \u00b4\u00d7 \u00b5\u00b5 \u00be \u00b5 \u00b4\u00c5\u00b5 \u00b4\u00c5\u00bc\u00b5 \u00b4\u00c5\u00b5 (10) The measures are illustrated by an example in figure 2. \u00be \u00a2 \u00c8 \u00a2 \u00ca \u00c8 \u00b7 \u00ca \u00be \u00a2 \u00c8 \u00a2 \u00ca \u00c8 \u00b7 \u00ca \u00c2 \u00be \u00a2 \u00a2 (11) where and are the cluster and dependency F1-measures respectively and \u00c2 is the Joint F1-measure (\u00c2 ) that we use to measure the overall performance. The task of event modeling can be split into two parts: clustering the stories into unique events in the topic and constructing dependencies among them. 6.1 Clustering Each topic is composed of multiple events, so stories must be clustered into events before we can model the dependencies among them. Additionally, named-entities such as person and location names are another obvious feature when forming events. In this subsection, we present an agglomerative clustering algorithm that combines all these features. 6.1.1 Agglomerative clustering with time decay (ACDT) We initialize our events to singleton events (clusters), i.e., each cluster contains exactly one story. \u00d3\u00d7\u00b4\u00d7\u00bd \u00d7\u00be\u00b5 is the cosine similarity of the term vectors. We use time decay when calculating similarity of story pairs, i.e., the larger time difference between two stories, the smaller their similarities. In each iteration, we find the most similar event pair and merge them. \u00d7 \u00d1\u00b4 \u00d9 \u00da \u00b5 \u00d1 \u00d2 \u00d7\u00d9\u00be \u00d9 \u00d7\u00da\u00be \u00da \u00d7 \u00d1\u00b4\u00d7\u00d9 \u00d7\u00da \u00b5 (15) \u00af Single link: Here the similarity is given by the best similarity between all pairs of stories. \u00d7 \u00d1\u00b4 \u00d9 \u00da \u00b5 \u00d1 \u00dc \u00d7\u00d9\u00be \u00d9 \u00d7\u00da\u00be \u00da \u00d7 \u00d1\u00b4\u00d7\u00d9 \u00d7\u00da \u00b5 (16) This process continues until the maximum similarity falls below the threshold or the number of clusters is smaller than a given number. 6.2 Dependency modeling Capturing dependencies is an extremely hard problem because it may require a \u2018deeper understanding\" of the events in question. A human annotator decides on dependencies not just based on the information in the events but also based on his/her vast repertoire of domain-knowledge and general understanding of how things operate in the world. We believe a robust model should incorporate such domain knowledge in capturing dependencies, but in this work, as a first step, we will rely on surface-features such as time-ordering of news stories and word distributions to model them. In this subsection, we describe the models we considered for capturing dependencies. Now, the event-time-ordering function \u00d8 is defined as follows. \u00d8 \u00bd \u00a1 \u00a1 \u00a1 \u00d1 \u00d7 \u00d8 \u00d9 \u00da \u00be \u00d8 \u00b4 \u00d9\u00b5 \u00d8 \u00b4 \u00da\u00b5 \u00b4\u00b5 \u00d1 \u00d2 \u00d7\u00d9\u00be \u00d9 \u00d8\u00b4\u00d7\u00d9\u00b5 \u00d1 \u00d2 \u00d7\u00da\u00be \u00da \u00d8\u00b4\u00d7\u00da\u00b5 (17) In other words, \u00d8 time-orders events based on the time-ordering of their respective first stories. We will also use average cosine similarity between two events as a feature and it is defined as follows. \u00da \u00cb \u00d1\u00b4 \u00d9 \u00da \u00b5 \u00c8\u00d7\u00d9\u00be \u00d9 \u00c8\u00d7\u00da\u00be \u00da \u00d3\u00d7\u00b4\u00d7\u00d9 \u00d7\u00da \u00b5 \u00d9 \u00da (18) 6.2.1 Complete-Link model In this model, we assume that there are dependencies between all pairs of events. \u00bc \u00b4 \u00d9 \u00da \u00b5 \u00d8 \u00b4 \u00d9\u00b5 \u00d8 \u00b4 \u00da \u00b5 (19) where \u00d8 is the event-time-ordering function. 6.2.2 Simple Thresholding This model is an extension of the complete link model with an additional constraint that there is a dependency between any two events \u00d9 and \u00da only if the average cosine similarity between event \u00d9 and event \u00da is greater than a threshold \u00cc. 6.2.4 Best Similarity Model This model also assumes that each event can have at most one parent. Our experiments consists of three parts. 7.1 Clustering We have tried several variations of the \u00cc algorithm to study the effects of various features on the clustering performance. Model CP CR CF P-value cos+1-lnk 0.43 0.49 0.39cos+all-lnk 0.43 0.62 0.47cos+Loc+avg-lnk 0.37 0.73 0.45cos+Per+avg-lnk 0.44 0.62 0.45cos+TD+avg-lnk 0.48 0.70 0.54 0.014* cos+N(T)+avg-lnk 0.41 0.71 0.51 0.31 cos+N(T)+T+avg-lnk 0.43 0.69* 0.52 0.14 cos+TD+N(T)+avg-lnk 0.43 0.76 0.54 0.025* cos+TD+N(T)+T+avg-lnk 0.47 0.69 0.* Baseline(cos+avg-lnk) 0.44 0.67 0.50Table 3: Comparison of agglomerative clustering algorithms (test set) P-value marked with a \u00a3 means that it is a statistically significant improvement over the baseline (95% confidence level, one tailed T-test). \u00af cos+all-lnk: Complete link algorithm of equation 15 is used. Similar to single link but it takes the minimum similarity of all story pairs. \u00af cos+Loc+avg-lnk: Location names are used when calculating similarity. \u00af cos+TD+avg-lnk: Time Decay coefficient \u00ab \u00bd in equation 13, which means the similarity between two stories will be decayed to \u00bd if they are at different ends of the topic. \u00af cos+N(T)+avg-lnk: Use the number of true events to control the agglomerative clustering algorithm. \u00af cos+N(T)+T+avg-lnk: similar to N(T) but also stop agglomeration if the maximal similarity is below the threshold \u00cc. \u00af cos+TD:+N(T)+avg-lnk: similar to N(T) but the similarities are decayed, \u00ab \u00bd in equation 13. \u00af cos+TD+N(T)+T+avg-lnk: similar to TD+N(Truth) but calculation halts when the maximal similarity is smaller than the threshold \u00cc. Our experiments demonstrate that single link and complete link similarities perform worse than average link, which is reasonable since average link is less sensitive to one or two story pairs. However, for most applications, it is not available. 7.2 Dependencies In this subsection, our goal is to model only dependencies. Model best \u00cc DP DR DF P-value Nearest Parent 0.025 0.55 0.62 0.56 0.04* Best Similarity 0.02 0.51 0.62 0.53 0.24 MST 0.0 0.46 0.58 0.48Simple Thresh. In table 5 we present the comparison of the models on the test set. Although there is a lot of room for improvement, we believe this is a good first step. Model DP DR DF P-value Nearest Parent 0.61 0.60 0.60Best Similarity 0.71 0.74 0.72 0.04* MST 0.70 0.68 0.69 0.22 Simple Thresh. Note that we need to retrain all the algorithms on the training set because our objective function to optimize is now JF, the joint F-measure. The results on the training set, also presented in table 6, indicate that cos+TD+Simple-Thresholding is significantly better than the baseline in terms of the joint F-value JF, using a one-tailed paired Ttest at 95% confidence level. The results on the test set present a very similar story as shown in table 7.", "body2": "This problem is addressed by a research program called Topic Detection and Tracking (TDT) that runs an open annual competition on standardized tasks of news organization. In the above example, the event of rescue attempts is \u2018influenced\" by the event of bombing and so is the event of search for perpetrators. From a user\"s perspective, we believe that our view of a news topic as a set of interconnected events helps him/her get a quick overview of the topic and also allows him/her navigate through the topic faster. In section 4, we describe how we built the corpus for our problem. Section 8 concludes the paper with a few observations on our results and comments on future work. The hierarchy within a topic does impose a structure on the topic, but we do not know of an effort to explore the extent to which that structure reflects the underlying event relationships. Also their algorithms are tuned to work on specific genres of topics such as earthquakes, accidents, etc., while we expect our algorithms to generalize over any topic. However this structure still did not explicitly model any dependencies between events. Other related work that dealt with analysis within a news topic includes temporal summarization of news topics . We present some definitions (in italics) and our interpretations (regular-faced) below for clarity. Clearly, both the assumptions are not necessarily true in reality, but we accept them for simplicity in modeling. Following the assumption of atomicity of a story, this means that any set of distinct events can be represented by a set of non-overlapping clusters of news stories. Topic detection and tracking (TDT) :Topic detection detects clusters of stories that discuss the same topic; Topic tracking detects stories that discuss a previously known topic . Thus TDT concerns itself mainly with clustering stories into topics that discuss them. Event threading: Event threading detects events within in a topic, and also captures the dependencies among the events. Additionally event threading models the relatedness or dependencies between pairs of events in a topic while TDT models topics as unrelated clusters of stories. Hence an arrow from A to B falls under the category of causal dependency. This choice definitely makes sense as a first step but we chose the former since we believe directional edges make more sense to the user as they provide a more illustrative flow-chart perspective to the topic. We define an event model \u00c5 \u00b4 \u00b5 to be a tuple of the set of events and set of dependencies. Trial and (5) (3) (4) CIA announces reward Muslim world Reactions from Islamic militants Threats from (2) (1) Osama Indictment of CIA gathered by Evidence Figure 1: An event model of TDT topic \u2018Osama bin Laden\"s indictment\". The number of events is unknown. The granularity of events is hard to define. The dependencies among events are hard to model. Since it is a brand new research area, no standard evaluation metrics and benchmark data is available. In the next few sections, we will describe our attempts to tackle these problems. We believe modeling such stories would be a useful first step before dealing with more complex data sets. We supervised the annotator on a set of three topics that we did our own annotations on and then asked her to annotate the 28 topics from TDT2 and 25 topics from TDT3. We however, do not use or model these titles in our algorithms. Table 1 shows that the training and test sets have fairly similar statistics. \u00af Dependency pairs ( \u00b4\u00c5\u00b5): These are the set of all ordered pairs of stories \u00b4\u00d7 \u00d7 \u00b5 such that there is a dependency from the event of \u00d7 to the event of \u00d7 in the model \u00c5. Given these two sets of story pairs corresponding to the true event model \u00c5 and the system event model \u00c5\u00bc, we define recall and precision for each category as follows. \u00af Cluster Precision (CP): It is the probability that two randomly selected stories \u00d7 and \u00d7 are in the same true-event given that they are in the same system event. \u00c8 \u00c8\u00b4 \u00b4\u00d7 \u00b5 \u00b4\u00d7 \u00b5 \u00bc\u00b4\u00d7 \u00b5 \u00bc\u00b4\u00d7 \u00b5\u00b5 \u00b4\u00c5\u00b5 \u00b4\u00c5\u00bc\u00b5 \u00b4\u00c5\u00bc\u00b5 (7) where \u00bc is the story-event mapping function corresponding to the model \u00c5\u00bc. \u00af Cluster Recall(CR): It is the probability that two randomly selected stories \u00d7 and \u00d7 are in the same system-event given that they are in the same true event. Note that the direction of dependency is important in comparison. Again, the direction of dependency is taken into consideration. We also combine these measures using the well known F1-measure commonly used in text classification and other research areas as shown below. \u00be \u00a2 \u00c8 \u00a2 \u00ca \u00c8 \u00b7 \u00ca \u00be \u00a2 \u00c8 \u00a2 \u00ca \u00c8 \u00b7 \u00ca \u00c2 \u00be \u00a2 \u00a2 (11) where and are the cluster and dependency F1-measures respectively and \u00c2 is the Joint F1-measure (\u00c2 ) that we use to measure the overall performance. In the following subsections, we describe techniques we developed for each of these sub-tasks. Stories in the same event tend to follow temporal locality, so the time stamp of each story can be a useful feature. Stories in the same event tend to be related to the same person(s) and locations(s). In our experiments, however, we study the effect of each feature on the performance separately using modified versions of this algorithm. In this work, we determined them empirically, but in the future, one can consider more sophisticated learning techniques to determine them. \u00c8 \u00d6\u00b4\u00d7\u00bd \u00d7\u00be\u00b5 is similarly defined for person name. \u00ab is the time decay factor. We have three different ways to compute the similarity between two events \u00d9 and \u00da: \u00af Average link: In this case the similarity is the average of the similarities of all pairs of stories between \u00d9 and \u00da as shown below: \u00d7 \u00d1\u00b4 \u00d9 \u00da \u00b5 \u00c8\u00d7\u00d9\u00be \u00d9 \u00c8\u00d7\u00da\u00be \u00da \u00d7 \u00d1\u00b4\u00d7\u00d9 \u00d7\u00da \u00b5 \u00d9 \u00da (14) \u00af Complete link: The similarity between two events is given by the smallest of the pair-wise similarities. \u00d7 \u00d1\u00b4 \u00d9 \u00da \u00b5 \u00d1 \u00d2 \u00d7\u00d9\u00be \u00d9 \u00d7\u00da\u00be \u00da \u00d7 \u00d1\u00b4\u00d7\u00d9 \u00d7\u00da \u00b5 (15) \u00af Single link: Here the similarity is given by the best similarity between all pairs of stories. \u00d7 \u00d1\u00b4 \u00d9 \u00da \u00b5 \u00d1 \u00dc \u00d7\u00d9\u00be \u00d9 \u00d7\u00da\u00be \u00da \u00d7 \u00d1\u00b4\u00d7\u00d9 \u00d7\u00da \u00b5 (16) This process continues until the maximum similarity falls below the threshold or the number of clusters is smaller than a given number. 6.2 Dependency modeling Capturing dependencies is an extremely hard problem because it may require a \u2018deeper understanding\" of the events in question. For example, in Figure 1 a human knows \u2018Trial and indictment of Osama\" is influenced by \u2018Evidence gathered by CIA\" because he/she understands the process of law in general. Our experiments in later sections demonstrate that such features are indeed useful in capturing dependencies to a large extent. First we define a 1-1 time-ordering function \u00d8 \u00cb \u00bd \u00a1 \u00a1 \u00a1 \u00d2 that sorts stories in ascending order by their time of publication. Now, the event-time-ordering function \u00d8 is defined as follows. \u00d8 \u00bd \u00a1 \u00a1 \u00a1 \u00d1 \u00d7 \u00d8 \u00d9 \u00da \u00be \u00d8 \u00b4 \u00d9\u00b5 \u00d8 \u00b4 \u00da\u00b5 \u00b4\u00b5 \u00d1 \u00d2 \u00d7\u00d9\u00be \u00d9 \u00d8\u00b4\u00d7\u00d9\u00b5 \u00d1 \u00d2 \u00d7\u00da\u00be \u00da \u00d8\u00b4\u00d7\u00da\u00b5 (17) In other words, \u00d8 time-orders events based on the time-ordering of their respective first stories. We will also use average cosine similarity between two events as a feature and it is defined as follows. Formally, the system edges are defined as follows. Although we use the same names, it will be clear from the context which one we refer to. The candidate is assigned as the parent only if the average similarity exceeds a pre-defined threshold \u00cc. \u00bc \u00b4 \u00d9 \u00da \u00b5 \u00b4 \u00d9 \u00da \u00b5 \u00be \u00c5\u00cb\u00cc\u00b4 \u00b5 \u00d8 \u00b4 \u00d9\u00b5 \u00d8 \u00b4 \u00da\u00b5 \u00da \u00cb \u00d1\u00b4 \u00d9 \u00da \u00b5 \u00cc (24) 450 Thus in this model, we assign dependencies between the most similar events in the topic. The following subsections present the three parts of our experimentation. We use the Cluster F1-measure (CF) averaged over all topics as our evaluation criterion. \u00af cos+1-lnk: Single link comparison (see equation 16) is used where similarity of two clusters is the maximum of all story pairs, other configurations are the same as the baseline run. \u00af cos+all-lnk: Complete link algorithm of equation 15 is used. Similar to single link but it takes the minimum similarity of all story pairs. \u00af cos+Per+avg-lnk: \u00bf \u00bc \u00bc in equation 12, i.e., we put some weight on person names in the similarity. \u00af cos+TD+avg-lnk: Time Decay coefficient \u00ab \u00bd in equation 13, which means the similarity between two stories will be decayed to \u00bd if they are at different ends of the topic. When the number of clusters is fewer than that of truth events, stop merging clusters. \u00af cos+N(T)+T+avg-lnk: similar to N(T) but also stop agglomeration if the maximal similarity is below the threshold \u00cc. \u00af cos+TD:+N(T)+avg-lnk: similar to N(T) but the similarities are decayed, \u00ab \u00bd in equation 13. \u00af cos+TD+N(T)+T+avg-lnk: similar to TD+N(Truth) but calculation halts when the maximal similarity is smaller than the threshold \u00cc. Also we noticed that providing the number of true events improves the performance since it guides the clustering algorithm to get correct granularity. On the whole, time decay proved to the most powerful feature besides cosine similarity on both training and test sets. We see that while all the algorithms except MST outperform the baseline complete-link algorithm , the nearest Parent algorithm is statistically significant from the baseline in terms of its DF-value using a one-tailed paired T-test at 95% confidence level. * indicates the corresponding model is statistically significant compared to the baseline using a one-tailed, paired T-test at 95% confidence level. On the whole, we observe that the surface-level features we used capture the dependencies to a reasonable level achieving a best value of 0.72 DF on the test set. Although there is a lot of room for improvement, we believe this is a good first step. As a baseline, we use a combination of the baselines in each components, i.e., cos for clustering and complete-link for dependencies. We did this empirically on the training set and the optimal values are listed in table 6. Hence the performance of the dependency algorithms has suffered substantially thereby lowering the overall performance. The test set results also point to the fact that the clustering component remains a bottleneck in achieving an overall good performance.", "introduction": "News forms a major portion of information disseminated in the world everyday. Common people and news analysts alike are very interested in keeping abreast of new things that happen in the news, but it is becoming very difficult to cope with the huge volumes of information that arrives each day. Hence there is an increasing need for automatic techniques to organize news stories in a way that helps users interpret and analyze them quickly. This problem is addressed by a research program called Topic Detection and Tracking (TDT) that runs an open annual competition on standardized tasks of news organization. One of the shortcomings of current TDT evaluation is its view of news topics as flat collection of stories. For example, the detection task of TDT is to arrange a collection of news stories into clusters of topics. However, a topic in news is more than a mere collection of stories: it is characterized by a definite structure of inter-related events. This is indeed recognized by TDT which defines a topic as \u2018a set of news stories that are strongly related by some seminal realworld event\" where an event is defined as \u2018something that happens at a specific time and location\" . For example, when a bomb explodes in a building, that is the seminal event that triggers the topic. Other events in the topic may include the rescue attempts, the search for perpetrators, arrests and trials and so on. We see that there is a pattern of dependencies between pairs of events in the topic. In the above example, the event of rescue attempts is \u2018influenced\" by the event of bombing and so is the event of search for perpetrators. In this work we investigate methods for modeling the structure of a topic in terms of its events. By structure, we mean not only identifying the events that make up a topic, but also establishing dependencies-generally causal-among them. We call the process of recognizing events and identifying dependencies among them event threading, an analogy to email threading that shows connections between related email messages. We refer to the resulting interconnected structure of events as the event model of the topic. Although this paper focuses on threading events within an existing news topic, we expect that such event based dependency structure more accurately reflects the structure of news than strictly bounded topics do. From a user\"s perspective, we believe that our view of a news topic as a set of interconnected events helps him/her get a quick overview of the topic and also allows him/her navigate through the topic faster. The rest of the paper is organized as follows. In section 2, we discuss related work. In section 3, we define the problem and use an example to illustrate threading of events within a news topic. In section 4, we describe how we built the corpus for our problem. Section 5 presents our evaluation techniques while section 6 describes the techniques we use for modeling event structure. In section 7 we present our experiments and results. Section 8 concludes the paper with a few observations on our results and comments on future work.", "conclusion": "In this paper, we have presented a new perspective of modeling news topics.. Contrary to the TDT view of topics as flat collection of news stories, we view a news topic as a relational structure of events interconnected by dependencies.. In this paper, we also proposed a few approaches for both clustering stories into events and constructing dependencies among them.. We developed a timedecay based clustering approach that takes advantage of temporallocalization of news stories on the same event and showed that it performs significantly better than the baseline approach based on cosine similarity.. Our experiments also show that we can do fairly well on dependencies using only surface-features such as cosinesimilarity and time-stamps of news stories as long as true events are provided to the system.. However, the performance deteriorates rapidly if the system has to discover the events by itself.. Despite that discouraging result, we have shown that our combined algorithms perform significantly better than the baselines.. Our results indicate modeling dependencies can be a very hard problem especially when the clustering performance is below ideal level.. Errors in clustering have a magnifying effect on errors in dependencies as we have seen in our experiments.. Hence, we should focus not only on improving dependencies but also on clustering at the same time.. As part of our future work, we plan to investigate further into the data and discover new features that influence clustering as well as dependencies.. And for modeling dependencies, a probabilistic framework should be a better choice since there is no definite answer of yes/no for the causal relations among some events.. We also hope to devise an iterative algorithm which can improve clustering and dependency performance alternately as suggested by one of the reviewers.. We also hope to expand our labeled corpus further to include more diverse news sources and larger and more complex event structures.. Acknowledgments We would like to thank the three anonymous reviewers for their valuable comments.. This work was supported in part by the Center 452 Model Cluster T Dep.. T CP CR CF DP DR DF JF P-value cos+TD+Nearest-Parent 0.055 0.02 0.51 0.53 0.49 0.21 0.19 0.19 0.27cos+TD+Best-Similarity 0.04 0.02 0.45 0.70 0.53 0.21 0.33 0.23 0.32cos+TD+MST 0.04 0.00 0.45 0.70 0.53 0.22 0.35 0.25 0.33cos+TD+Simple-Thresholding 0.065 0.02 0.56 0.47 0.48 0.23 0.61 0.32 0.* Baseline (cos+Complete-link) 0.10 - 0.58 0.31 0.38 0.20 0.67 0.30 0.33Table 6: Combined results on the training set Model CP CR CF DP DR DF JF P-value cos+TD+Nearest Parent 0.57 0.50 0.50 0.27 0.19 0.21 0.30cos+TD+Best Similarity 0.48 0.70 0.54 0.31 0.27 0.26 0.35cos+TD+MST 0.48 0.70 0.54 0.31 0.30 0.28 0.37cos+TD+Simple Thresholding 0.60 0.39 0.44 0.32 0.66 0.42 0.* Baseline (cos+Complete-link) 0.66 0.27 0.36 0.30 0.72 0.43 0.39Table 7:-02-1-.. Any opinions, findings and conclusions or recommendations expressed in this material are the authors\" and do not necessarily reflect those of the sponsor."}
{"id": "C-31", "keywords": ["peer-to-peer", "p2p", "file share", "jxta", "apocrita"], "title": "Apocrita: A Distributed Peer-to-Peer File Sharing System for Intranets", "abstract": "Many organizations are required to author documents for various purposes, and such documents may need to be accessible by all member of the organization. This access may be needed for editing or simply viewing a document. In some cases these documents are shared between authors, via email, to be edited. This can easily cause incorrect version to be sent or conflicts created between multiple users trying to make amendments to a document. There may even be multiple different documents in the process of being edited. The user may be required to search for a particular document, which some search tools such as Google Desktop may be a solution for local documents but will not find a document on another user's machine. Another problem arises when a document is made available on a user's machine and that user is offline, in which case the document is no longer accessible. In this paper we present Apocrita, a revolutionary distributed P2P file sharing system for Intranets.", "references": ["The Design of a Robust Peer-to-Peer System", "Making Gnutella-like P2P Systems Scalable", "Harvest: A Distributed Search System", "Majestic-12: Distributed Search Engine", "YaCy: Distributed P2P-based Web Indexing", "Lucene Search Engine Library", "Test Collections (Time Magazine and NPL)"], "full_text": "1. INTRODUCTION The Peer-to-Peer (P2P) computing paradigm is becoming a completely new form of mutual resource sharing over the Internet. With the increasingly common place broadband Internet access, P2P technology has finally become a viable way to share documents and media files. There are already programs on the market that enable P2P file sharing. These programs enable millions of users to share files among themselves. While the utilization of P2P clients is already a gigantic step forward compared to downloading files off websites, using such programs are not without their problems. The downloaded files still require a lot of manual management by the user. The user still needs to put the files in the proper directory, manage files with multiple versions, delete the files when they are no longer wanted. We strive to make the process of sharing documents within an Intranet easier. Many organizations are required to author documents for various purposes, and such documents may need to be accessible by all members of the organization. This access may be needed for editing or simply viewing a document. In some cases these documents are sent between authors, via email, to be edited. This can easily cause incorrect version to be sent or conflicts created between multiple users trying to make amendments to a document. There may even be multiple different documents in the process of being edited. The user may be required to search for a particular document, which some search tools such as Google Desktop may be a solution for local documents but will not find a document on another user\"s machine. Furthermore, some organizations do not have a file sharing server or the necessary network infrastructure to enable one. In this paper we present Apocrita, which is a cost-effective distributed P2P file sharing system for such organizations. The rest of this paper is organized as follows. In section 2, we present Apocrita. The distributed indexing mechanism and protocol are presented in Section 3. Section 4 presents the peer-topeer distribution model. A proof of concept prototype is presented in Section 5, and performance evaluations are discussed in Section 6. Related work is presented is Section 7, and finally conclusions and future work are discussed in Section 8. 2. APOCRITA Apocrita is a distributed peer-to-peer file sharing system, and has been designed to make finding documents easier in an Intranet environment. Currently, it is possible for documents to be located on a user's machine or on a remote machine. It is even possible that different revisions could reside on each node on the Intranet. This means there must be a manual process to maintain document versions. Apocrita solves this problem using two approaches. First, due to the inherent nature of Apocrita, the document will only reside on a single logical location. Second, Apocrita provides a method of reverting to previous document versions. Apocrita Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. ACMSE\"07, MARCH 23- WINSTON-SALEM, NC, USA. ACM 978-1--629-5/07/ \u2026$5.00. 174 will also distribute documents across multiple machines to ensure high availability of important documents. For example, if a machine contains an important document and the machine is currently inaccessible, the system is capable of maintaining availability of the document through this distribution mechanism. It provides a simple interface for searching and accessing files that may exist either locally or remotely. The distributed nature of the documents is transparent to the user. Apocrita supports a decentralized network model where the peers use a discovery protocol to determine peers. Apocrita is intended for network users on an Intranet. The main focus is organizations that may not have a network large enough to require a file server and supporting infrastructure. It eliminates the need for documents to be manually shared between users while being edited and reduces the possibility of conflicting versions being distributed. The system also provides some redundancy and in the event of a single machine failure, no important documents will be lost. It is operating system independent, and easy to access through a web browser or through a standalone application. To decrease the time required for indexing a large number of documents, the indexing process is distributed across available idle nodes. Local and remote files should be easily accessible through a virtual mountable file system, providing transparency for users. 3. DISTRIBUTED INDEXING Apocrita uses a distributed index for all the documents that are available on the Intranet. Each node will contain part of the full index, and be aware of what part of the index each other node has. A node will be able to contact each node that contains a unique portion of the index. In addition, each node has a separate local index of its own documents. But as discussed later, in the current implementation, each node has a copy of the entire index. Indexing of the documents is distributed. Therefore, if a node is in the process of indexing many documents, it will break up the work over the nodes. Once a node\"s local index is updated with the new documents, the distributed index will then be updated. The current distributed indexing system consists of three separate modules: NodeController, FileSender, and NodeIndexer. The responsibility of each module is discussed later in this section. 3.1 Indexing Protocol The protocol we have designed for the distributed indexing is depicted in Figure 1. Figure 1. Apocrita distributed indexing protocol. IDLE QUERY: The IDLE QUERY is sent out from the initiating node to determine which other nodes may be able to help with the overall indexing process. There are no parameters sent with the command. The receiving node will respond with either a BUSY or IDLE command. If the IDLE command is received, the initiating node will add the responding node to a list of available distributed indexing helpers. In the case of a BUSY command being received, the responding node is ignored. BUSY: Once a node received an IDL QUERY, it will determine whether it can be considered a candidate for distributed indexing. This determination is based on the overall CPU usage of the node. If the node is using most of its CPU for other processes, the node will respond to the IDLE QUERY with a BUSY command. IDLE: As with the case of the BUSY response, the node receiving the IDLE QUERY will determine its eligibility for distributed indexing. To be considered a candidate for distributed indexing, the overall CPU usage must be at a minimum to all for dedicated indexing of the distributed documents. If this is the case, the node will respond with an IDLE command. INCOMING FILE: Once the initiating node assembles a set of idle nodes to assist with the distributed indexing, it will divide the documents to be sent to the nodes. To do this, it sends an INCOMING FILE message, which contains the name of the file as well as the size in bytes. After the INCOMING FILE command has been sent, the initiating node will begin to stream the file to the other node. The initiating node will loop through the files that are to be sent to the other node; each file stream being preceded by the INCOMING FILE command with the appropriate parameters. INDEX FILE: Once the indexing node has completed the indexing process of the set of files, it must send the resultant index back to the initiating node. The index is comprised of multiple files, which exist on the file system of the indexing node. As with the INCOMING FILE command, the indexing node streams each index file after sending an INDEX FILE command. The INDEX FILE command has two parameters: the first being the name of the index, and the second is the size of the file in bytes. SEND COMPLETE: When sending the sets of files for both the index and the files to be indexed, the node must notify the corresponding node when the process is complete. Once the initiating node is finished sending the set of documents to be indexed, it will then send a SEND COMPLETE command indicating to the indexing node that there are no more files and the node can proceed with indexing the files. In the case of the initiating node sending the index files, the indexing node will complete the transfer with the SEND COMPLETE command indicating to the initiating node that there are no more index files to be sent and the initiating node can then assemble those index files into the main index. The NodeController is responsible for setting up connections with nodes in the idle state to distribute the indexing process. Using JXTA , the node controller will obtain a set of nodes. This set of nodes is iterated and each one is sent the IDLE QUERY command. The nodes that respond with idle are then collected. The set of idle nodes includes the node initiating the distributed indexing process, referred to as the local node. Once the collection of idle nodes is obtained, the node updates the set of controllers and evenly divides the set of documents that are to be indexed. For example, if there are 100 documents and 10 nodes (including the local node) then each node will have 10 documents to index. For each indexing node an instance of the FileSender object is created. The FileSender is aware of the set of documents that node is responsible for. Once a FileSender object has been created for each node, the NodeController waits for each FileSender to complete. When the FileSender objects have completed the NodeController will take the resultant indexes from 175 each node and pass them to an instance of the IndexCompiler, which maintains the index and the list of FileSenders. Once the IndexCompiler has completed it will return to the idle state and activate the directory scanner to monitor the locally owned set of documents for changes that may require reindexing. The NodeIndexer is responsible for receiving documents sent to it by the initiating node and then indexing them using the Lucene engine . Once the indexing is complete the resulting index is streamed back to the initiating node as well as compiled in the indexer nodes own local index. Before initiating the indexing process it must be sent an IDLE QUERY message. This is the first command that sets off the indexing process. The indexer node will determine whether it is considered idle based on the current CPU usage. As outlined in the protocol section if the node is not being used and has a low overall CPU usage percentage it will return IDLE to the IDLE QUERY command. If the indexer nodes CPU usage is above 50% for a specified amount of time it is then considered to be busy and will respond to the IDLE QUERY command with BUSY. If a node is determined busy it returns to its listening state waiting for another IDLE QUERY from another initiating node. If the node is determined to be idle it will enter the state where it will receive files from the initiating node that it is responsible for indexing. Once all of the files are received by the initiating node, indicated by a SEND COMPLETE message, it starts an instance of the Lucene indexing engine. The files are stored in a temporary directory separate from the nodes local documents that it is responsible for maintaining an index of. The Lucene index writer then indexes all of the transferred files. The index is stored on the drive within a temporary directory separate from the current index. After the indexing of the files completes the indexer node enters the state where the index files are sent back to the initiating node. The indexer node loops through all of the files created by Lucene\"s IndexWriter and streams them to the initiating node. Once these files are sent back that index is then merged into the indexer nodes own full index of the existing files. It then enters the idle state where it will then listen for any other nodes that required distributing the indexing process. The FileSender object is the initiating node equivalent of the indexer node. It initiates the communication between the initiating node and the node that will assist in the distributed indexing. The initiating node runs many instances of the FileSender node one for each other node it has determined to be idle. Upon instantiation of the FileSender it is passed the node that it is responsible for contacting and the set of files that must be sent. The FileSender\"s first job is to send the files that are to be indexed by the other idle node. The files are streamed one at a time to the other node. It sends each file using the INCOMING FILE command. With that command it sends the name of the file being sent and the size in bytes. Once all files have been sent the FileSender sends the SEND COMPLETE command. The FileSender creates an instance of Lucene\"s IndexWriter and prepares to create the index in a temporary directory on the file system. The FileSender will begin to receive the files that are to be saved within the index. It receives an INDEX FILE command with the name of the files and the size in bytes. This file is then streamed into the temporary index directory on the FileSender node. After the transfer of the index files has been completed the FileSender notifies the instance of the index compiler that it is ready to combine the index. Each instance of the FileSender has its own unique section of temporary space to store the index that has been transferred back from the indexing node. When notifying the IndexCompiler it will also pass the location of the particular FileSenders directory location of that index. 4. PEER-TO-PEER DISTRIBUTION Apocrita uses a peer-to-peer distribution model in order to distribute files. Files are distributed solely from a serving node to a client node without regard for the availability of file pieces from other clients in the network. This means that the file transfers will be fast and efficient and should not severely affect the usability of serving nodes from the point of view of a local user. The JXTA framework is used in order to implement peer-to-peer functionality. This has been decided due to the extremely shorttimeline of the project which allows us to take advantage of over five years of testing and development and support from many large organizations employing JXTA in their own products. We are not concerned with any potential quality problems because JXTA is considered to be the most mature and stable peer-to-peer framework available. Using JXTA terminology, there are three types of peers used in node classification. Edge peers are typically low-bandwidth, non-dedicated nodes. Due to these characteristics, edge peers are not used with Apocrita. Relay peers are typically higher-bandwidth, dedicated nodes. This is the classification of all nodes in the Apocrita network, and, as such, are the default classification used. Rendezvous peers are used to coordinate message passing between nodes in the Apocrita network. This means that a minimum of one rendezvous peer per subnet is required. 4.1 Peer Discovery The Apocrita server subsystem uses the JXTA Peer Discovery Protocol (PDP) in order to find participating peers within the network as shown in Figure 2. Figure 2. Apocrita peer discovery process. 176 The PDP listens for peer advertisements from other nodes in the Apocrita swarm. If a peer advertisement is detected, the server will attempt to join the peer group and start actively contributing to the network. If no peers are found by the discovery service, the server will create a new peer group and start advertising this peer group. This new peer group will be periodically advertised on the network; any new peers joining the network will attach to this peer group. A distinct advantage of using the JXTA PDP is that Apocrita does not have to be sensitive to particular networking nuances such as Maximum Transmission Unit (MTU). In addition, Apocrita does not have to support one-to-many packet delivery methods such as multicast and instead can rely on JXTA for this support. 4.2 Index Query Operation All nodes in the Apocrita swarm have a complete and up-to-date copy of the network index stored locally. This makes querying the index for search results trivial. Unlike the Gnutella protocol, a query does not have to propagate throughout the network. This also means that the time to return query results is very fast - much faster than protocols that rely on nodes in the network to pass the query throughout the network and then wait for results. This is demonstrated in Figure 3. Figure 3. Apocrita query operation. Each document in the swarm has a unique document identification number (ID). A node will query the index and a result will be returned with both the document ID number as well as a list of peers with a copy of the matched document ID. It is then the responsibility of the searching peer to contact the peers in the list to negotiate file transfer between the client and server. 5. PROTOTYPE IMPLEMENTATION Apocrita uses the Lucene framework , which is a project under development by the Apache Software Foundation. Apache Lucene is a high-performance, full-featured text search engine library written entirely in Java. In the current implementation, Apocrita is only capable of indexing plain text documents. Apocrita uses the JXTA framework as a peer-to-peer transport library between nodes. JXTA is used to pass both messages and files between nodes in the search network. By using JXTA, Apocrita takes advantage of a reliable, and proven peer-to-peer transport mechanism. It uses the pipe facility in order to pass messages and files between nodes. The pipe facility provides many different types of pipe advertisements. This includes an unsecured unicast pipe, a secured unicast pipe, and a propagated unsecured pipe. Message passing is used to pass status messages between nodes in order to aid in indexing, searching, and retrieval. For example, a node attempting to find an idle node to participate in indexing will query nodes via the message facility. Idle nodes will reply with a status message to indicate they are available to start indexing. File passing is used within Apocrita for file transfer. After a file has been searched for and located within the peer group, a JXTA socket will be opened and file transfer will take place. A JXTA socket is similar to a standard Java socket, however a JXTA socket uses JXTA pipes in underlying network transport. File passing uses an unsecured unicast pipe in order to transfer data. File passing is also used within Apocrita for index transfer. Index transfer works exactly like a file transfer. In fact, the index transfer actually passes the index as a file. However, there is one key difference between file transfer and index transfer. In the case of file transfer, a socket is created between only two nodes. In the case of index transfer, a socket must be created between all nodes in the network in order to pass the index, which allows for all nodes to have a full and complete index of the entire network. In order to facilitate this transfer efficiently, index transfer will use an unsecured propagated pipe to communicate with all nodes in the Apocrita network. 6. PERFORMANCE EVALUATION It is difficult to objectively benchmark the results obtained through Apocrita because there is no other system currently available with the same goals as Apocrita. We have, however, evaluated the performance of the critical sections of the system. The critical sections were determined to be the processes that are the most time intensive. The evaluation was completed on standard lab computers on a 100Mb/s Ethernet LAN; the machines run Windows XP with a Pentium 4 CPU running at 2.4GHz with 512 MB of RAM. The indexing time has been run against both: the Time Magazine collection , which contains 432 documents and 83 queries and their most relevant results, and the NPL collection that has a total of 11,429 documents and 93 queries with expected results. Each document ranges in size between 4KB and 8KB. As Figure 4 demonstrates, the number of nodes involved in the indexing process affects the time taken to complete the indexing processsometimes even drastically. Figure 4. Node vs. index time. The difference in going from one indexing node to two indexing nodes is the most drastic and equates to an indexing time 37% faster than a single indexing node. The different between two 177 indexing nodes and three indexing nodes is still significant and represents a 16% faster time than two indexing nodes. As the number of indexing nodes increases the results are less dramatic. This can be attributed to the time overhead associated with having many nodes perform indexing. The time needed to communicate with a node is constant, so as the number of nodes increases, this constant becomes more prevalent. Also, the complexity of joining the indexing results is a complex operation and is complicated further as the number of indexing nodes increases. Socket performance is also a very important part of Apocrita. Benchmarks were performed using a 65MB file on a system with both the client and server running locally. This was done to isolate possible network issues. Although less drastic, similar results were shown when the client and server run on independent hardware. In order to mitigate possible unexpected errors, each test was run 10 times. Figure 5. Java sockets vs. JXTA sockets. As Figure 5 demonstrates, the performance of JXTA sockets is abysmal as compared to the performance of standard Java sockets. The minimum transfer rate obtained using Java sockets is 81,945KB/s while the minimum transfer rater obtained using JXTA sockets is much lower at 3, 805KB/s. The maximum transfer rater obtain using Java sockets is 97,412KB/s while the maximum transfer rate obtained using JXTA sockets is 5,530KB/s. Finally, the average transfer rate using Java sockets is 87,540KB/s while the average transfer rate using JXTA sockets is 4,293KB/s. The major problem found in these benchmarks is that the underlying network transport mechanism does not perform as quickly or efficiently as expected. In order to garner a performance increase, the JXTA framework needs to be substituted with a more traditional approach. The indexing time is also a bottleneck and will need to be improved for the overall quality of Apocrita to be improved. 7. RELATED WORK Several decentralized P2P systems exist today that Apocrita features some of their functionality. However, Apocrita also has unique novel searching and indexing features that make this system unique. For example, Majestic-12 is a distributed search and indexing project designed for searching the Internet. Each user would install a client, which is responsible for indexing a portion of the web. A central area for querying the index is available on the Majestic-12 web page. The index itself is not distributed, only the act of indexing is distributed. The distributed indexing aspect of this project most closely relates Apocrita goals. YaCy is a peer-to-peer web search application. YaCy consists of a web crawler, an indexer, a built-in database engine, and a p2p index exchange protocol. YaCy is designed to maintain a distributed index of the Internet. It used a distributed hash table (DHT) to maintain the index. The local node is used to query but all results that are returned are accessible on the Internet. YaCy used many peers and DHT to maintain a distributed index. Apocrita will also use a distributed index in future implementations and may benefit from using an implementation of a DHT. YaCy however, is designed as a web search engine and, as such solves a much different problem than Apocrita. 8. CONCLUSIONS AND FUTURE WORK We presented Apocrita, a distributed P2P searching and indexing system intended for network users on an Intranet. It can help organizations with no network file server or necessary network infrastructure to share documents. It eliminates the need for documents to be manually shared among users while being edited and reduce the possibility of conflicting versions being distributed. A proof of concept prototype has been constructed, but the results from measuring the network transport mechanism and the indexing time were not as impressive as initially envisioned. Despite these shortcomings, the experience gained from the design and implementation of Apocrita has given us more insight into building challenging distributed systems. For future work, Apocrita will have a smart content distribution model in which a single instance of a file can intelligently and transparently replicate throughout the network to ensure a copy of every important file will always be available regardless of the availability of specific nodes in the network. In addition, we plan to integrate a revision control system into the content distribution portion of Apocrita so that users could have the ability to update an existing file that they found and have the old revision maintained and the new revision propagated. Finally, the current implementation has some overhead and redundancy due to the fact that the entire index is maintained on each individual node, we plan to design a distributed index.", "body1": "The Peer-to-Peer (P2P) computing paradigm is becoming a completely new form of mutual resource sharing over the Internet. There are already programs on the market that enable P2P file sharing. The downloaded files still require a lot of manual management by the user. Many organizations are required to author documents for various purposes, and such documents may need to be accessible by all members of the organization. The rest of this paper is organized as follows. Apocrita is a distributed peer-to-peer file sharing system, and has been designed to make finding documents easier in an Intranet environment. First, due to the inherent nature of Apocrita, the document will only reside on a single logical location. ACM 978-1--629-5/07/ \u2026$5.00. 174 will also distribute documents across multiple machines to ensure high availability of important documents. It provides a simple interface for searching and accessing files that may exist either locally or remotely. Apocrita is intended for network users on an Intranet. Apocrita uses a distributed index for all the documents that are available on the Intranet. A node will be able to contact each node that contains a unique portion of the index. Indexing of the documents is distributed. 3.1 Indexing Protocol The protocol we have designed for the distributed indexing is depicted in Figure 1. Figure 1. IDLE QUERY: The IDLE QUERY is sent out from the initiating node to determine which other nodes may be able to help with the overall indexing process. This determination is based on the overall CPU usage of the node. If the node is using most of its CPU for other processes, the node will respond to the IDLE QUERY with a BUSY command. IDLE: As with the case of the BUSY response, the node receiving the IDLE QUERY will determine its eligibility for distributed indexing. INCOMING FILE: Once the initiating node assembles a set of idle nodes to assist with the distributed indexing, it will divide the documents to be sent to the nodes. As with the INCOMING FILE command, the indexing node streams each index file after sending an INDEX FILE command. The INDEX FILE command has two parameters: the first being the name of the index, and the second is the size of the file in bytes. SEND COMPLETE: When sending the sets of files for both the index and the files to be indexed, the node must notify the corresponding node when the process is complete. The NodeController is responsible for setting up connections with nodes in the idle state to distribute the indexing process. The set of idle nodes includes the node initiating the distributed indexing process, referred to as the local node. The NodeIndexer is responsible for receiving documents sent to it by the initiating node and then indexing them using the Lucene engine . The FileSender object is the initiating node equivalent of the indexer node. The FileSender\"s first job is to send the files that are to be indexed by the other idle node. Apocrita uses a peer-to-peer distribution model in order to distribute files. Edge peers are typically low-bandwidth, non-dedicated nodes. Due to these characteristics, edge peers are not used with Apocrita. Relay peers are typically higher-bandwidth, dedicated nodes. This is the classification of all nodes in the Apocrita network, and, as such, are the default classification used. Rendezvous peers are used to coordinate message passing between nodes in the Apocrita network. 4.1 Peer Discovery The Apocrita server subsystem uses the JXTA Peer Discovery Protocol (PDP) in order to find participating peers within the network as shown in Figure 2. Figure 2. 176 The PDP listens for peer advertisements from other nodes in the Apocrita swarm. 4.2 Index Query Operation All nodes in the Apocrita swarm have a complete and up-to-date copy of the network index stored locally. Each document in the swarm has a unique document identification number (ID). Apocrita uses the Lucene framework , which is a project under development by the Apache Software Foundation. Apocrita uses the JXTA framework as a peer-to-peer transport library between nodes. Message passing is used to pass status messages between nodes in order to aid in indexing, searching, and retrieval. File passing is used within Apocrita for file transfer. File passing is also used within Apocrita for index transfer. It is difficult to objectively benchmark the results obtained through Apocrita because there is no other system currently available with the same goals as Apocrita. The indexing time has been run against both: the Time Magazine collection , which contains 432 documents and 83 queries and their most relevant results, and the NPL collection that has a total of 11,429 documents and 93 queries with expected results. Each document ranges in size between 4KB and 8KB. Figure 4. The difference in going from one indexing node to two indexing nodes is the most drastic and equates to an indexing time 37% faster than a single indexing node. This can be attributed to the time overhead associated with having many nodes perform indexing. Benchmarks were performed using a 65MB file on a system with both the client and server running locally. As Figure 5 demonstrates, the performance of JXTA sockets is abysmal as compared to the performance of standard Java sockets. The minimum transfer rate obtained using Java sockets is 81,945KB/s while the minimum transfer rater obtained using JXTA sockets is much lower at 3, 805KB/s. The major problem found in these benchmarks is that the underlying network transport mechanism does not perform as quickly or efficiently as expected. Several decentralized P2P systems exist today that Apocrita features some of their functionality. Each user would install a client, which is responsible for indexing a portion of the web. YaCy is a peer-to-peer web search application.", "body2": "With the increasingly common place broadband Internet access, P2P technology has finally become a viable way to share documents and media files. While the utilization of P2P clients is already a gigantic step forward compared to downloading files off websites, using such programs are not without their problems. We strive to make the process of sharing documents within an Intranet easier. In this paper we present Apocrita, which is a cost-effective distributed P2P file sharing system for such organizations. Related work is presented is Section 7, and finally conclusions and future work are discussed in Section 8. Apocrita solves this problem using two approaches. ACMSE\"07, MARCH 23- WINSTON-SALEM, NC, USA. ACM 978-1--629-5/07/ \u2026$5.00. For example, if a machine contains an important document and the machine is currently inaccessible, the system is capable of maintaining availability of the document through this distribution mechanism. Apocrita supports a decentralized network model where the peers use a discovery protocol to determine peers. Local and remote files should be easily accessible through a virtual mountable file system, providing transparency for users. Each node will contain part of the full index, and be aware of what part of the index each other node has. But as discussed later, in the current implementation, each node has a copy of the entire index. The responsibility of each module is discussed later in this section. 3.1 Indexing Protocol The protocol we have designed for the distributed indexing is depicted in Figure 1. Apocrita distributed indexing protocol. BUSY: Once a node received an IDL QUERY, it will determine whether it can be considered a candidate for distributed indexing. This determination is based on the overall CPU usage of the node. If the node is using most of its CPU for other processes, the node will respond to the IDLE QUERY with a BUSY command. If this is the case, the node will respond with an IDLE command. The index is comprised of multiple files, which exist on the file system of the indexing node. As with the INCOMING FILE command, the indexing node streams each index file after sending an INDEX FILE command. The INDEX FILE command has two parameters: the first being the name of the index, and the second is the size of the file in bytes. In the case of the initiating node sending the index files, the indexing node will complete the transfer with the SEND COMPLETE command indicating to the initiating node that there are no more index files to be sent and the initiating node can then assemble those index files into the main index. The nodes that respond with idle are then collected. Once the IndexCompiler has completed it will return to the idle state and activate the directory scanner to monitor the locally owned set of documents for changes that may require reindexing. It then enters the idle state where it will then listen for any other nodes that required distributing the indexing process. Upon instantiation of the FileSender it is passed the node that it is responsible for contacting and the set of files that must be sent. When notifying the IndexCompiler it will also pass the location of the particular FileSenders directory location of that index. Using JXTA terminology, there are three types of peers used in node classification. Edge peers are typically low-bandwidth, non-dedicated nodes. Due to these characteristics, edge peers are not used with Apocrita. Relay peers are typically higher-bandwidth, dedicated nodes. This is the classification of all nodes in the Apocrita network, and, as such, are the default classification used. This means that a minimum of one rendezvous peer per subnet is required. 4.1 Peer Discovery The Apocrita server subsystem uses the JXTA Peer Discovery Protocol (PDP) in order to find participating peers within the network as shown in Figure 2. Apocrita peer discovery process. In addition, Apocrita does not have to support one-to-many packet delivery methods such as multicast and instead can rely on JXTA for this support. Apocrita query operation. It is then the responsibility of the searching peer to contact the peers in the list to negotiate file transfer between the client and server. In the current implementation, Apocrita is only capable of indexing plain text documents. This includes an unsecured unicast pipe, a secured unicast pipe, and a propagated unsecured pipe. Idle nodes will reply with a status message to indicate they are available to start indexing. File passing uses an unsecured unicast pipe in order to transfer data. In order to facilitate this transfer efficiently, index transfer will use an unsecured propagated pipe to communicate with all nodes in the Apocrita network. The evaluation was completed on standard lab computers on a 100Mb/s Ethernet LAN; the machines run Windows XP with a Pentium 4 CPU running at 2.4GHz with 512 MB of RAM. The indexing time has been run against both: the Time Magazine collection , which contains 432 documents and 83 queries and their most relevant results, and the NPL collection that has a total of 11,429 documents and 93 queries with expected results. As Figure 4 demonstrates, the number of nodes involved in the indexing process affects the time taken to complete the indexing processsometimes even drastically. Node vs. index time. As the number of indexing nodes increases the results are less dramatic. Socket performance is also a very important part of Apocrita. Java sockets vs. JXTA sockets. As Figure 5 demonstrates, the performance of JXTA sockets is abysmal as compared to the performance of standard Java sockets. Finally, the average transfer rate using Java sockets is 87,540KB/s while the average transfer rate using JXTA sockets is 4,293KB/s. The indexing time is also a bottleneck and will need to be improved for the overall quality of Apocrita to be improved. For example, Majestic-12 is a distributed search and indexing project designed for searching the Internet. The distributed indexing aspect of this project most closely relates Apocrita goals. YaCy however, is designed as a web search engine and, as such solves a much different problem than Apocrita.", "introduction": "The Peer-to-Peer (P2P) computing paradigm is becoming a completely new form of mutual resource sharing over the Internet. With the increasingly common place broadband Internet access, P2P technology has finally become a viable way to share documents and media files. There are already programs on the market that enable P2P file sharing. These programs enable millions of users to share files among themselves. While the utilization of P2P clients is already a gigantic step forward compared to downloading files off websites, using such programs are not without their problems. The downloaded files still require a lot of manual management by the user. The user still needs to put the files in the proper directory, manage files with multiple versions, delete the files when they are no longer wanted. We strive to make the process of sharing documents within an Intranet easier. Many organizations are required to author documents for various purposes, and such documents may need to be accessible by all members of the organization. This access may be needed for editing or simply viewing a document. In some cases these documents are sent between authors, via email, to be edited. This can easily cause incorrect version to be sent or conflicts created between multiple users trying to make amendments to a document. There may even be multiple different documents in the process of being edited. The user may be required to search for a particular document, which some search tools such as Google Desktop may be a solution for local documents but will not find a document on another user\"s machine. Furthermore, some organizations do not have a file sharing server or the necessary network infrastructure to enable one. In this paper we present Apocrita, which is a cost-effective distributed P2P file sharing system for such organizations. The rest of this paper is organized as follows. In section 2, we present Apocrita. The distributed indexing mechanism and protocol are presented in Section 3. Section 4 presents the peer-topeer distribution model. A proof of concept prototype is presented in Section 5, and performance evaluations are discussed in Section 6. Related work is presented is Section 7, and finally conclusions and future work are discussed in Section 8.", "conclusion": "We presented Apocrita, a distributed P2P searching and indexing system intended for network users on an Intranet.. It can help organizations with no network file server or necessary network infrastructure to share documents.. It eliminates the need for documents to be manually shared among users while being edited and reduce the possibility of conflicting versions being distributed.. A proof of concept prototype has been constructed, but the results from measuring the network transport mechanism and the indexing time were not as impressive as initially envisioned.. Despite these shortcomings, the experience gained from the design and implementation of Apocrita has given us more insight into building challenging distributed systems.. For future work, Apocrita will have a smart content distribution model in which a single instance of a file can intelligently and transparently replicate throughout the network to ensure a copy of every important file will always be available regardless of the availability of specific nodes in the network.. In addition, we plan to integrate a revision control system into the content distribution portion of Apocrita so that users could have the ability to update an existing file that they found and have the old revision maintained and the new revision propagated.. Finally, the current implementation has some overhead and redundancy due to the fact that the entire index is maintained on each individual node, we plan to design a distributed index."}
{"id": "J-55", "keywords": ["auction", "mechan design", "competit analysi"], "title": "From Optimal Limited To Unlimited Supply Auctions", "abstract": "We investigate the class of single-round, sealed-bid auctions for a set of identical items to bidders who each desire one unit. We adopt the worst-case competitive framework defined by [9, 5] that compares the profit of an auction to that of an optimal single-price sale of least two items. In this paper, we first derive an optimal auction for three items, answering an open question from [8]. Second, we show that the form of this auction is independent of the competitive framework used. Third, we propose a schema for converting a given limited-supply auction into an unlimited supply auction. Applying this technique to our optimal auction for three items, we achieve an auction with a competitive ratio of 3.25, which improves upon the previously best-known competitive ratio of 3.39 from [7]. Finally, we generalize a result from [8] and extend our understanding of the nature of the optimal competitive auction by showing that the optimal competitive auction occasionally offers prices that are higher than all bid values.", "references": ["Truthful mechanisms for one-parameter agents", "Market research and market design", "Online Computation and Competitive Analysis", "The Simple Economics of Optimal Auctions", "Competitive generalized auctions", "Competitive auctions and digital goods", "Competitiveness via consensus", "A lower bound on the competitive ratio of truthful auctions", "Competitive auctions and digital goods", "Truth Revelation in Approximately Efficient Combinatorial Auctions", "Strategyproof Sharing of Submodular Costs: Budget Balance Versus Efficiency", "Optimal Auction Design", "Algorithmic Mechanism Design", "Optimal pricing mechanisms with unknown demand", "Counterspeculation, Auctions, and Competitive Sealed Tenders"], "full_text": "1. INTRODUCTION The research area of optimal mechanism design looks at designing a mechanism to produce the most desirable outcome for the entity running the mechanism. This problem is well studied for the auction design problem where the optimal mechanism is the one that brings the seller the most profit. Here, the classical approach is to design such a mechanism given the prior distribution from which the bidders\" preferences are drawn (See e.g., ). Recently Goldberg et al. introduced the use of worst-case competitive analysis (See e.g., ) to analyze the performance of auctions that have no knowledge of the prior distribution. The goal of such work is to design an auction that achieves a large constant fraction of the profit attainable by an optimal mechanism that knows the prior distribution in advance. Positive results in this direction are fueled by the observation that in auctions for a number of identical units, much of the distribution from which the bidders are drawn can be deduced on the fly by the auction as it is being run . The performance of an auction in such a worst-case competitive analysis is measured by its competitive ratio, the ratio between a benchmark performance and the auction\"s performance on the input distribution that maximizes this ratio. The holy grail of the worstcase competitive analysis of auctions is the auction that achieves the optimal competitive ratio (as small as possible). Since this search has led to improved understanding of the nature of the optimal auction, the techniques for on-the-fly pricing in these scenarios, and the competitive ratio of the optimal auction . In this paper we continue this line of research by improving in all of these directions. Furthermore, we give evidence corroborating the conjecture that the form of the optimal auction is independent of the benchmark used in the auction\"s competitive analysis. This result further validates the use of competitive analysis in gauging auction performance. We consider the single item, multi-unit, unit-demand auction problem. In such an auction there are many units of a single item available for sale to bidders who each desire only one unit. Each bidder has a valuation representing how much the item is worth to him. The auction is performed by soliciting a sealed bid from each of the bidders and deciding on the allocation of units to bidders and the prices to be paid by the bidders. The bidders are assumed to bid so as to maximize their personal utility, the difference between their valuation and the price they pay. To handle the problem of designing and analyzing auctions where bidders may falsely declare their valuations to get a better deal, we will adopt the solution concept of truthful mechanism design (see, e.g., ). In a truthful auction, revealing one\"s true valuation as one\"s bid is an optimal strategy for each bidder regardless of the bids of the other bidders. In this paper, we will restrict our attention to truthful (a.k.a., incentive compatible or strategyproof) auctions. A particularly interesting special case of the auction problem is the unlimited supply case. In this case the number of units for sale is at least the number of bidders in the auction. This is natural for the sale of digital goods where there is negligible cost for duplicating 175 and distributing the good. Pay-per-view television and downloadable audio files are examples of such goods. The competitive framework introduced in and further refined in uses the profit of the optimal omniscient single priced mechanism that sells at least two units as the benchmark for competitive analysis. The assumption that two or more units are sold is necessary because in the worst case it is impossible to obtain a constant fraction of the profit of the optimal mechanism when it sells only one unit . In this framework for competitive analysis, an auction is said to be \u03b2-competitive if it achieves a profit that is within a factor of \u03b2 \u2265 1 of the benchmark profit on every input. The optimal auction is the one which is \u03b2-competitive with the minimum value of \u03b2. Previous to this work, the best known auction for the unlimited supply case had a competitive ratio of 3.39 and the best lower bound known was 2.42 . For the limited supply case, auctions can achieve substantially better competitive ratios. When there are only two units for sale, the optimal auction gives a competitive ratio of 2, which matches the lower bound for two units. When there are three units for sale, the best previously known auction had a competitive ratio of 2.3, compared with a lower bound of 13/6 \u2248 2.17 . The results of this paper are as follows: \u2022 We give the auction for three units that is optimally competitive against the profit of the omniscient single priced mechanism that sells at least two units. This auction achieves a competitive ratio of 13/6, matching the lower bound from (Section 3). \u2022 We show that the form of the optimal auction is independent of the benchmark used in competitive analysis. In doing so, we give an optimal three bidder auction for generalized benchmarks (Section 4). \u2022 We give a general technique for converting a limited supply auction into an unlimited supply auction where it is possible to use the competitive ratio of the limited supply auction to obtain a bound on the competitive ratio of the unlimited supply auction. We refer to auctions derived from this framework as aggregation auctions (Section 5). \u2022 We improve on the best known competitive ratio by proving that the aggregation auction constructed from our optimal three-unit auction is 3.25-competitive (Section 5.1). \u2022 Assuming that the conjecture that the optimal -unit auction has a competitive ratio that matches the lower bound proved in , we show that this optimal auction for \u2265 3 on some inputs will occasionally offer prices that are higher than any bid in that input (Section 6). For the three-unit case where we have shown that the lower bound of is tight, this observation led to our construction of the optimal three-unit auction. 2. DEFINITIONS AND BACKGROUND We consider single-round, sealed-bid auctions for a set of identical units of an item to bidders who each desire one unit. As mentioned in the introduction, we adopt the game-theoretic solution concept of truthful mechanism design. A useful simplification of the problem of designing truthful auctions is obtained through the following algorithmic characterization . Related formulations to this one have appeared in numerous places in recent literature (e.g., ). DEFINITION 1. Given a bid vector of n bids, b = (b1, . . . , bn), let b-i denote the vector of with bi replaced with a \u2018?\", i.e., b-i = (b1, . . . , bi\u22121, ?, bi+1, . . . , bn). DEFINITION 2. Let f be a function from bid vectors (with a \u2018?\") to prices (non-negative real numbers). The deterministic bidindependent auction defined by f, BIf , works as follows. For each bidder i: 1. Set ti = f(b-i). 2. If ti < bi, bidder i wins at price ti. 3. If ti > bi, bidder i loses. 4. Otherwise, (ti = bi) the auction can either accept the bid at price ti or reject it. A randomized bid-independent auction is a distribution over deterministic bid-independent auctions. The proof of the following theorem can be found, for example, in . THEOREM 1. An auction is truthful if and only if it is equivalent to a bid-independent auction. Given this equivalence, we will use the the terminology bidindependent and truthful interchangeably. For a randomized bid-independent auction, f(b-i) is a random variable. We denote the probability density of f(b-i) at z by \u03c1b-i (z). We denote the profit of a truthful auction A on input b as A(b). The expected profit of the auction, E[A(b)], is the sum of the expected payments made by each bidder, which we denote by pi(b) for bidder i. Clearly, the expected payment of each bid satisfies pi(b) = bi x\u03c1b-i (x)dx. 2.1 Competitive Framework We now review the competitive framework from . In order to evaluate the performance of auctions with respect to the goal of profit maximization, we introduce the optimal single price omniscient auction F and the related omniscient auction F(2) DEFINITION 3. Give a vector b = (b1, . . . , bn), let b(i) represent the i-th largest value in b. The optimal single price omniscient auction, F, is defined as follows. Auction F on input b determines the value k such that kb(k) is maximized. All bidders with bi \u2265 b(k) win at price b(k); all remaining bidders lose. The profit of F on input b is thus F(b) = max1\u2264k\u2264n kb(k). In the competitive framework of and subsequent papers, the performance of a truthful auction is gauged in comparison to F(2) the optimal singled priced auction that sells at least two units. The profit of F(2) is max2\u2264k\u2264n kb(k) There are a number of reasons to choose this benchmark for comparison, interested readers should see or for a more detailed discussion. Let A be a truthful auction. We say that A is \u03b2-competitive against F(2) (or just \u03b2-competitive) if for all bid vectors b, the expected profit of A on b satisfies E[A(b)] \u2265 F(2) (b) In Section 4 we generalize this framework to other profit benchmarks. 176 2.2 Scale Invariant and Symmetric Auctions A symmetric auction is one where the auction outcome is unchanged when the input bids arrive in a different permutation. Goldberg et al. show that a symmetric auction achieves the optimal competitive ratio. This is natural as the profit benchmark we consider is symmetric, and it allows us to consider only symmetric auctions when looking for the one with the optimal competitive ratio. An auction defined by bid-independent function f is scale invariant if, for all i and all z, Pr[f(b-i) \u2265 z] = Pr[f(cb-i) \u2265 cz]. It is conjectured that the assumption of scale invariance is without loss of generality. Thus, we are motivated to consider symmetric scale-invariant auctions. When specifying a symmetric scaleinvariant auction we can assume that f is only a function of the relative magnitudes of the n \u2212 1 bids in b-i and that one of the bids, bj = 1. It will be convenient to specify such auctions via the density function of f(b-i), \u03c1b-i (z). It is enough to specify such a density function of the form \u03c11,z1,...,zn\u22121 (z) with 1 \u2264 zi \u2264 zi+1. 2.3 Limited Supply Versus Unlimited Supply Following , throughout the remainder of this paper we will be making the assumption that n = , i.e., the number of bidders is equal to the number of units for sale. This is without loss of generality as (a) any lower bound that applies to the n = case also extends to the case where n \u2265 , and (b) there is a reduction from the unlimited supply auction problem to the limited supply auction problem that takes an unlimited supply auction that is \u03b2-competitive with F(2) and constructs a limited supply auction parameterized by that is \u03b2-competitive with F(2, ) , the optimal omniscient auction that sells between 2 and units . Henceforth, we will assume that we are in the unlimited supply case, and we will examine lower bounds for limited supply problems by placing a restriction on the number of bidders in the auction. 2.4 Lower Bounds and Optimal Auctions Frequently in this paper, we will refer to the best known lower bound on the competitive ratio of truthful auctions: THEOREM 2. The competitive ratio of any auction on n bidders is at least 1 \u2212 i=2 \u22121 i\u22121 i \u2212 1 n \u2212 1 i \u2212 1 DEFINITION 4. Let \u03a5n denote the n-bidder auction that achieves the optimal competitive ratio. This bound is derived by analyzing the performance of any auction on the following distribution B. In each random bid vector B, each bid Bi is drawn i.i.d. from the distribution such that Pr[Bi \u2265 s] \u2264 1/s for all s \u2208 S. In the two-bidder case, this lower bound is 2. This is achieved by \u03a52 which is the 1-unit Vickrey auction.1 In the three-bidder case, this lower bound is 13/6. In the next section, we define the auction \u03a53 which matches this lower bound. In the four-bidder case, this lower bound is 96/215. In the limit as the number of bidders grows, this lower bound approaches a number which is approximately 2.42. It is conjectured that this lower bound is tight for any number of bidders and the optimal auction, \u03a5n, matches it. The 1-unit Vickrey auction sells to the highest bidder at the second highest bid value. 2.5 Profit Extraction In this section we review the truthful profit extraction mechanism ProfitExtractR. This mechanism is a special case of a general cost-sharing schema due to Moulin and Shenker . The goal of profit extraction is, given bids b, to extract a target value R of profit from some subset of the bidders. ProfitExtractR: Given bids b, find the largest k such that the highest k bidders can equally share the cost R. Charge each of these bidders R/k. If no subset of bidders can cover the cost, the mechanism has no winners. Important properties of this auction are as follows: \u2022 ProfitExtractR is truthful. \u2022 If R \u2264 F(b), ProfitExtractR(b) = R; otherwise it has no winners and no revenue. We will use this profit extraction mechanism in Section 5 with the following intuition. Such a profit extractor makes it possible to treat this subset of bidders as a single bid with value F(S). Note that given a single bid, b, a truthful mechanism might offer it price t and if t \u2264 b then the bidder wins and pays t; otherwise the bidder pays nothing (and loses). Likewise, a mechanism can offer the set of bidders S a target revenue R. If R \u2264 F(2) (S), then ProfitExtractR raises R from S; otherwise, the it raises no revenue from S. 3. AN OPTIMAL AUCTION FOR THREE BIDDERS In this section we define the optimal auction for three bidders, \u03a53, and prove that it indeed matches the known lower bound of 13/6. We follow the definition and proof with a discussion of how this auction was derived. DEFINITION 5. \u03a53 is scale-invariant and symmetric and given by the bid-independent function with density function \u03c11,x(z) = \u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23a8 \u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23a9 For x \u2264 3/2 1 with probability 9/13 z with probability density g(z) for z > 3/2 For x > 3/2\u23a7 \u23aa\u23a8 \u23aa\u23a9 1 with probability 9/13 \u2212 3/2 zg(z)dz x with probability 3/2 (z + 1)g(z)dz z with probability density g(z) for z > x where g(x) = 2/13 (x\u22121)3 . THEOREM 3. The \u03a53 auction has a competitive ratio of 13/6 \u2248 2.17, which is optimal. Furthermore, the auction raises exactly 13 F(2) on every input with non-identical bids. PROOF. Consider the bids 1, x, y, with 1 < x < y. There are three cases. CASE 1 (x < y \u2264 3/2): F(2) = 3. The auction must raise expected revenue of at least 18/13 on these bids. The bidder with valuation x will pay 1 with 9/13, and the bidder with valuation y will pay 1 with probability 9/13. Therefore \u03a53 raises 18/13 on these bids. CASE 2 (x \u2264 3/2 < y): F(2) = 3. The auction must raise expected revenue of at least 18/13 on these bids. The bidder with 177 valuation x will pay 9/13 \u2212 3/2 zg(z)dz in expectation. The bidder with valuation y will pay 9/13 + 3/2 zg(z)dz in expectation. Therefore \u03a53 raises 18/13 on these bids. CASE 3 (3/2 < x \u2264 y): F(2) = 2x. The auction must raise expected revenue of at least 12x/13 on these bids. Consider the revenue raised from all three bidders: E[\u03a53(b)] = p(1, x, y) + p(x, 1, y) + p(y, 1, x) = 0 + 9/13 \u2212 3/2 zg(z)dz + 9/13 \u2212 3/2 zg(z)dz + x 3/2 (z + 1)g(z)dz + zg(z)dz = 18/13 + (x \u2212 2) 3/2 zg(z)dz + x 3/2 g(z)dz = 12x/13. The final equation comes from substituting in g(x) = 2/13 (x\u22121)3 and expanding the integrals. Note that the fraction of F(2) raised on every input is identical. If any of the inequalities 1 \u2264 x \u2264 y are not strict, the same proof applies giving a lower bound on the auction\"s profit; however, this bound may no longer be tight. Motivation for \u03a53 In this section, we will conjecture that a particular input distribution is worst-case, and show, as a consequence, that all inputs are worstcase in the optimal auction. By applying this consequence, we will derive an optimal auction for three bidders. A truthful, randomized auction on n bidders can be represented by a randomized function f : Rn\u22121 \u00d7 n \u2192 R that maps masked bid vectors to prices in R. By normalization, we can assume that the lowest possible bid is 1. Recall that \u03c1b-i (z) = Pr[f(b-i) = z]. The optimal auction for the finite auction problem can be found by the following optimization problem in which the variables are \u03c1b-i (z): maximize r subject to i=1 bi z=1 z\u03c1b-i (z)dz \u2265 rF(2) (b) z=1 \u03c1b-i (z)dz = 1 \u03c1b-i (z) \u2265 0 This set of integral inequalities is difficult to maximize over. However, by guessing which constraints are tight and which are slack at the optimum, we will be able to derive a set of differential equations for which any feasible solution is an optimal auction. As we discuss in Section 2.4, in , the authors define a distribution and use it to find a lower bound on the competitive ratio of the optimal auction. For two bidders, this bid distribution is the worst-case input distribution. We guess (and later verify) that this distribution is the worst-case input distribution for three bidders as well. Since this distribution has full support over the set of all bid vectors and a worst-case distribution puts positive probability only on worst-case inputs, we can therefore assume that all but a measure zero set of inputs is worst-case for the optimal auction. In the optimal two-bidder auction, all inputs with non-identical bids are worst-case, so we will assume the same for three bidders. The guess that these constraints are tight allows us to transform the optimization problem into a feasibility problem constrained by differential equations. If the solution to these equations has value matching the lower bound obtained from the worst-case distribution, then this solution is the optimal auction and that our conjectured choice of worst-case distribution is correct. In Section 6 we show that the optimal auction must sometimes place probability mass on sale prices above the highest bid. This motivates considering symmetric scale-invariant auctions for three bidders with probability density function, \u03c11,x(z), of the following form: \u03c11,x(z) = \u23aa\u23a8 \u23aa\u23a9 1 with discrete probability a(x) x with discrete probability b(x) z with probability density g(z) for z > x In this auction, the sale price for the first bidder is either one of the latter two bids, or higher than either bid with a probability density which is independent of the input. The feasibility problem which arises from the linear optimization problem by assuming the constraints are tight is as follows: a(y) + a(x) + xb(x) + zg(z)dz = r max(3, 2x) \u2200x < y a(x) + b(x) + g(z)dz = 1 a(x) \u2265 0 b(x) \u2265 0 g(z) \u2265 0 Solving this feasibility problem gives the auction \u03a53 proposed above. The proof of its optimality validates its proposed form. Finding a simple restriction on the form of n-bidder auctions for n > 3 under which the optimal auction can be found analytically as above remains an open problem. 4. GENERALIZED PROFIT BENCHMARKS In this section, we widen our focus beyond auctions that compete with F(2) to consider other benchmarks for an auction\"s profit. We will show that, for three bidders, the form of the optimal auction is essentially independent of the benchmark profit used. This results strongly corroborates the worst-case competitive analysis of auctions by showing that our techniques allow us to derive auctions which are competitive against a broad variety of reasonable benchmarks rather than simply against F(2) Previous work in competitive analysis of auctions has focused on the question of designing the auction with the best competitive ratio against F(2) , the profit of the optimal omniscient single-priced mechanism that sells at least two items. However, it is reasonable to consider other benchmarks. For instance, one might wish to compete against V\u2217 , the profit of the k-Vickrey auction with optimal-inhindsight choice of k.2 Alternatively, if an auction is being used as a subroutine in a larger mechanism, one might wish to choose the auction which is optimally competitive with a benchmark specific to that purpose. Recall that F(2) (b) = max2\u2265k\u2265n kb(k). We can generalize this definition to Gs, parameterized by s = (s2, . . . , sn) and defined as: Gs(b) = max 2\u2264k\u2264n skb(k). When considering Gs we assume without loss of generality that si < si+1 as otherwise the constraint imposed by si+1 is irrelevant. Note that F(2) is the special case of Gs with si = i, and that V\u2217 Gs with si = i \u2212 1. Recall that the k-Vickrey auction sells a unit to each of the highest k bidders at a price equal to the k + 1st highest bid, b(k+1), achieving a profit of kb(k+1). 178 Competing with Gs We will now design a three-bidder auction \u03a5s,t 3 that achieves the optimal competitive ratio against Gs,t. As before, we will first find a lower bound on the competitive ratio and then design an auction to meet that bound. We can lower bound the competitive ratio of \u03a5s,t 3 using the same worst-case distribution from that we used against F(2) . Evaluating the performance of any auction competing against Gs,t on this distribution will yield the following theorem. We denote the optimal auction for three bidders against Gs,t as \u03a5s,t 3 . THEOREM 4. The optimal three-bidder auction, \u03a5s,t 3 , competing against Gs,t(b) = max(sb(2), tb(3)) has a competitive ratio of at least s2 +t2 2t The proof can be found in the appendix. Similarly, we can find the optimal auction against Gs,t using the same technique we used to solve for the three bidder auction with the best competitive ratio against F(2) DEFINITION 6. \u03a5s,t 3 is scale-invariant and symmetric and given by the bid-independent function with density function \u03c11,x(z) = \u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23a8 \u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23a9 For x \u2264 t 1 with probability t2 s2+t2 z with probability density g(z) for z > t For x > t s\u23a7 \u23aa\u23aa\u23a8 \u23aa\u23aa\u23a9 1 with probability t2 s2+t2 \u2212 zg(z)dz x with probability (z + 1)g(z)dz z with probability density g(z) for z > x where g(x) = 2(t\u2212s)2 /(s2 +t2 (x\u22121)3 . THEOREM 5. \u03a5s,t 3 is s2 +t2 2t -competitive with Gs,t. This auction, like \u03a53, can be derived by reducing the optimization problem to a feasibility problem, guessing that the optimal solution has the same form as \u03a5s,t 3 , and solving. The auction is optimal because it matches the lower bound found above. Note that the form of \u03a5s,t 3 is essentially the same as for \u03a53, but that the probability of each price is scaled depending on the values of s and t. That our auction for three bidders matches the lower bound computed by the input distribution used in is strong evidence that this input distribution is the worst-case input distribution for any number of bidders and any generalized profit benchmark. Furthermore, we strongly suspect that for any number of bidders, the form of the optimal auction will be independent of the benchmark used. 5. AGGREGATION AUCTIONS We have seen that optimal auctions for small cases of the limitedsupply model can be found analytically. In this section, we will construct a schema for turning limited supply auctions into unlimited supply auctions with a good competitive ratio. As discussed in Section 2.5, the existence of a profit extractor, ProfitExtractR, allows an auction to treat a set of bids S as a single bid with value F(S). Given n bidders and an auction, Am, for m < n bidders, we can convert the m-bidder auction into an n-bidder auction by randomly partitioning the bidders into m subsets and then treating each subset as a single bidder (via ProfitExtractR) and running the m-bidder auction. DEFINITION 7. Given a truthful m-bidder auction, Am, the m-aggregation auction for Am, AggAm , works as follows: 1. Cast each bid uniformly at random into one of m bins, resulting in bid vectors b(1) , . . . , b(m) 2. For each bin j, compute the aggregate bid Bj = F(b(j) ). Let B be the vector of aggregate bids, and B\u2212j be the aggregate bids for all bins other than j. 3. Compute the aggregate price Tj = f(B\u2212j), where f is the bid-independent function for Am. 4. For each bin j, run ProfitExtractTj on b(j) Since Am and ProfitExtractR are truthful, Tj is computed independently of any bid in bin j and thus the price offered any bidder in b(j) is independent of his bid; therefore, THEOREM 6. If Am is truthful, the m-aggregation auction for Am, AggAm , is truthful. Note that this schema yields a new way of understanding the Random Sampling Profit Extraction (RSPE) auction as the simplest case of an aggregation auction. It is the 2-aggregation auction for \u03a52, the 1-unit Vickrey auction. To analyze AggAm , consider throwing k balls into m labeled bins. Let k represent a configuration of balls in bins, so that ki is equal to the number of balls in bin i, and k(i) is equal to the number of balls in the ith largest bin. Let Km,k represent the set of all possible configurations of k balls in m bins. We write the multinomial coefficient of k as k . The probability that a particular configuration k arises by throwing balls into bins uniformly at random is m\u2212k THEOREM 7. Let Am be an auction with competitive ratio \u03b2. Then the m-aggregation auction for Am, AggAm , raises the following fraction of the optimal revenue F(2) (b): E AggAm (b) F(2) \u2265 min k\u22652 k\u2208Km,k F(2) (k) k \u03b2kmk PROOF. By definition, F(2) sells to k \u2265 2 bidders at a single price p. Let kj be the number of such bidders in b(j) . Clearly, F(b(j) ) \u2265 pkj. Therefore, F(2) (F(b(1) ), . . . , F(b(n) )) F(2)(b) F(2) (pk1, . . . , pkn) pk F(2) (k1, . . . , kn) The inequality follows from the monotonicity of F(2) , and the equality from the homogeneity of F(2) ProfitExtractTj will raise Tj if Tj \u2264 Bj , and no profit otherwise. Thus, E AggAm (b) \u2265 E F(2) (B)/\u03b2 . The theorem follows by rewriting this expectation as a sum over all k in Km,k. 5.1 A 3.25 Competitive Auction We apply the aggregation auction schema to \u03a53, our optimal auction for three bidders, to achieve an auction with competitive ratio 3.25. This improves on the previously best known auction which is 3.39-competitive . THEOREM 8. The aggregation auction for \u03a53 has competitive ratio 3.25. 179 PROOF. By theorem 7, E Agg\u03a53 (b) F(2)(b) \u2265 min k\u22652 i=1 k\u2212i j=1 F(2) (i, j, k \u2212 i \u2212 j) k i,j,k\u2212i\u2212j \u03b2k3k For k = 2 and k = 3, E Agg\u03a53 (b) = 2 k/\u03b2. As k increases, E Agg\u03a53 (b) /F(2) increases as well. Since we do not expect to find a closed-form formula for the revenue, we lower bound F(2) (b) by 3b(3). Using large deviation bounds, one can show that this lower bound is greater than 2 k/\u03b2 for large-enough k, and the remainder can be shown by explicit calculation. Plugging in \u03b2 = 13/6, the competitive ratio is 13/4. As k increases, the competitive ratio approaches 13/6. Note that the above bound on the competitive ratio of Agg\u03a53 is tight. To see this, consider the bid vector with two very large and non-identical bids of h and h + with the remaining bids 1. Given that the competitive ratio of \u03a53 is tight on this example, the expected revenue of this auction on this input will be exactly 13/4. 5.2 A Gs,t-based Aggregation Auction In this section we show that \u03a53 is not the optimal auction to use in an aggregation auction. One can do better by choosing the auction that is optimally competitive against a specially tailored benchmark. To see why this might be the case, notice (Table 1) that the fraction of F(2) (b) raised for when there are k = 2 and k = 3 winning bidders in F(2) (b) is substantially smaller than the fraction of F(2) (b) raised when there are more winners. This occurs because the expected ratio between F(2) (B) and F(2) (b) is lower in this case while the competitive ratio of \u03a53 is constant. If we chose a three bidder auction that performed better when F(2) has smaller numbers of winners, our aggregation auction would perform better in the worst case. One approach is to compete against a different benchmark that puts more weight than F(2) on solutions with a small number of winners. Recall that F(2) is the instance of Gs,t with s = 2 and t = 3. By using the auction that competes optimally against Gs,t with s > 2, while holding t = 3, we will raise a higher fraction of revenue on smaller numbers of winning bidders and a lower fraction of revenue on large numbers of winning bidders. We can numerically optimize the values of s and t in Gs,t(b) in order to achieve the best competitive ratio for the aggregation auction. In fact, this will allow us to improve our competitive ratio slightly. THEOREM 9. For an optimal choice of s and t, the aggregation auction for \u03a5s,t 3 is 3.243-competitive. The proof follows the outline of Theorem 7 and 8 with the optimal choice of s = 2.162 (while t is held constant at 3). 5.3 Further Reducing the Competitive Ratio There are a number of ways we might attempt to use this aggregation auction schema to continue to push the competitive ratio down. In this section, we give a brief discussion of several attempts. 5.3.1 Agg\u03a5m for m > 3 If the aggregation auction for \u03a52 has a competitive ratio of 4 and the aggregation auction for \u03a53 has a competitive ratio of 3.25, can we improve the competitive ratio by aggregating \u03a54 or \u03a5m for larger m? We conjecture in the negative: for m > 3, the aggregation auction for \u03a5m has a larger competitive ratio than the aggregation auction for \u03a53. The primary difficulty in proving this k m = 2 m = 3 m = 4 m = 5 m = 6 m = 7 2 0. 3 0. 0. 0.311 0. 0. 0.292 10 0. 0. 11 0. 0. 0. 0. 0. 0. Table 1: E A(b)/F(2) (b) for Agg\u03a5m as a function of k, the optimal number of winners in F(2) (b). The lowest value for each column is printed in bold. conjecture lies in the difficulty of finding a closed-form solution for the formula of Theorem 7. We can, however, evaluate this formula numerically for different values of m and k, assuming that the competitive ratio for \u03a5m matches the lower bound for m given by Theorem 2. Table 1 shows, for each value of m and k, the fraction of F(2) raised by the aggregation auction for Agg\u03a5m when there are k winning bidders, assuming the lower bound of Theorem 2 is tight. 5.3.2 Convex combinations of Agg\u03a5m As can be seen in Table 1, when m > 3, the worst-case value of k is no longer 2 and 3, but instead an increasing function of m. An aggregation auction for \u03a5m outperforms the aggregation auction for \u03a53 when there are two or three winning bidders, while the aggregation auction for \u03a53 outperforms the other when there are at least six winning bidders. Thus, for instance, an auction which randomizes between aggregation auctions for \u03a53 and \u03a54 will have a worst-case which is better than that of either auction alone. Larger combinations of auctions will allow more room to optimize the worst-case. However, we suspect that no convex combination of aggregation auctions will have a competitive ratio lower than 3. Furthermore, note that we cannot yet claim the existence of a good auction via this technique as the optimal auction \u03a5n for n > 3 is not known and it is only conjectured that the bound given by Theorem 2 and represented in Table 1 is correct for \u03a5n. 6. A LOWER BOUND FOR CONSERVATIVE AUCTIONS In this section, we define a class of auctions that never offer a sale price which is higher than any bid in the input and prove a lower bound on the competitive ratio of these auctions. As this 180 lower bound is stronger than the lower bound of Theorem 2 for n \u2265 3, it shows that the optimal auction must occasionally charge a sales price higher than any bid in the input. Specifically, this result partially explains the form of the optimal three bidder auction. DEFINITION 8. We say an auction BIf is conservative if its bidindependent function f satisfies f(b-i) \u2264 max(b-i). We can now state our lower bound for conservative auctions. THEOREM 10. Let A be a conservative auction for n bidders. Then the competitive ratio of A is at least 3n\u22122 COROLLARY 1. The competitive ratio of any conservative auction for an arbitrary number of bidders is at least three. For a two-bidder auction, this restriction does not prevent optimality. \u03a52, the 1-unit Vickrey auction, is conservative. For larger numbers of bidders, however, the restriction to conservative auctions does affect the competitive ratio. For the three-bidder case, \u03a53 has competitive ratio 2.17, while the best conservative auction is no better than 2.33-competitive. The k-Vickrey auction and the Random Sampling Optimal Price auction are conservative auctions. The Random Sampling Profit Extraction auction and the CORE auction , on the other hand, use the ProfitExtractR mechanism as a subroutine and thus sometimes offer a sale price which is higher than the highest input bid value. In , the authors define a restricted auction as one on which, for any input, the sale prices are drawn from the set of input bid values. The class of conservative auctions can be viewed as a generalization of the class of restricted auctions and therefore our result below gives lower bounds on the performance of a broader class of auctions. We will prove Theorem 10 with the aid of the following lemma: LEMMA 1. Let A be a conservative auction with competitive ratio 1/r for n bidders. Let h n. Let h0 = 1 and hk = kh otherwise. Then, for all k and H \u2265 kh, Pr[f(1, 1, . . . , 1, H) \u2264 hk] \u2265 nr\u22121 n\u22121 + k(3nr\u22122r\u2212n n\u22121 ). PROOF. The lemma is proved by strong induction on k. First some notation that will be convenient. For any particular k and H we will be considering the bid vector b = (1, . . . , 1, hk, H) and placing bounds on \u03c1b-i (z). Since we can assume without loss of generality that the auction is symmetric, we will notate b-1 as b with any one of the 1-valued bids masked. Similarly we notate b-hk (resp. b-H ) as b with the hk-valued bid (resp. H-valued bid) masked. We will also let p1(b), phk (b), and pH (b) represent the expected payment of a 1-valued, hk-valued, and H-valued bidder in A on b, respectively (note by symmetry the expected payment for all 1-valued bidders is the same). Base case (k = 0, hk = 1): A must raise revenue of at least rn on b = (1, . . . , 1, 1, H): rn \u2264 pH (b) + (n \u2212 1)p1(b) = 1 + (n \u2212 1) x\u03c1b-1 (x)dx \u2264 1 + (n \u2212 1) \u03c1b-1 (x)dx The second inequality follows from the conservatism of the underlying auction. The base case follows trivially from the final inequality. Inductive case (k > 0, hk = kh): Let b = (1, . . . , 1, hk, H). First, we will find an upper bound on pH(b) pH (b) = x\u03c1b-H (x)dx + i=1 hi hi\u22121 x\u03c1b-H (x)dx (1) \u2264 1 + i=1 hi hi hi\u22121 \u03c1b-H (x)dx \u2264 1 + 3nr \u2212 2r \u2212 n n \u2212 1 k\u22121 i=1 ih + kh 1 \u2212 nr \u2212 1 n \u2212 1 \u2212 (k \u2212 1) 3nr \u2212 2r \u2212 n n \u2212 1 (2) = kh n(1 \u2212 r) n \u2212 1 (k \u2212 1) 3nr \u2212 2r \u2212 n n \u2212 1 + 1. (3) Equation (1) follows from the conservatism of A and (2) is from invoking the strong inductive hypothesis with H = kh and the observation that the maximum possible revenue will be found by placing exactly enough probability at each multiple of h to satisfy the constraints of the inductive hypothesis and placing the remaining probability at kh. Next, we will find a lower bound on phk (b) by considering the revenue raised by the bids b. Recall that A must obtain a profit of at least rF(2) (b) = 2rkh. Given upper-bounds on the profit from the H-valued, equation bid (3), and the 1-valued bids, the profit from the hk-valued bid must be at least: phk (b) \u2265 2rkh \u2212 (n \u2212 2)p1(b) \u2212 pH(b) \u2265 kh 2r \u2212 n(1 \u2212 r) n \u2212 1 (k \u2212 1) 3nr \u2212 2r \u2212 n n \u2212 1 \u2212 O(n). (4) In order to lower bound Pr[f(b-hk ) \u2264 kh], consider the auction that minimizes it and is consistent with the lower bounds obtained by the strong inductive hypothesis on Pr[f(b-hk ) \u2264 ih]. To minimize the constraints implied by the strong inductive hypothesis, we place the minimal amount of probability mass required each price level. This gives \u03c1hk (b) with nr\u22121 n\u22121 probability at 1 and exactly 3nr\u22122r\u2212n n\u22121 at each hi for 1 \u2264 i < k. Thus, the profit from offering prices at most hk\u22121 is nr\u22121 n\u22121 \u2212kh(k\u22121)3nr\u22122r\u2212n n\u22121 . In order to satisfy our lower bound, (4), on phk (b), it must put at least 3nr\u22122r\u2212n n\u22121 on hk. Therefore, the probability that the sale price will be no more than kh on masked bid vector on bid vector b = (1, . . . , 1, kh, H) must be at least nr\u22121 n\u22121 + k(3nr\u22122r\u2212n n\u22121 ). Given Lemma 1, Theorem 10 is simple to prove. PROOF. Let A be a conservative auction. Suppose 3nr\u22122r\u2212n n\u22121 > 0. Let k = 1/ , H \u2265 kh, and h n. By Lemma 1, Pr[f(1, . . . , 1, kh, H) \u2264 hk] \u2265 nr\u22121 n\u22121 + k > 1. But this is a contradiction, so 3nr\u22122r\u2212n n\u22121 \u2264 0. Thus, r \u2264 n 3n\u22122 . The theorem follows. 7. CONCLUSIONS AND FUTURE WORK We have found the optimal auction for the three-unit limitedsupply case, and shown that its structure is essentially independent of the benchmark used in its competitive analysis. We have then used this auction to derive the best known auction for the unlimited supply case. Our work leaves many interesting open questions. We found that the lower bound of is matched by an auction for three bidders, 181 even when competing against generalized benchmarks. The most interesting open question from our work is whether the lower bound from Theorem 2 can be matched by an auction for more than three bidders. We conjecture that it can. Second, we consider whether our techniques can be extended to find optimal auctions for greater numbers of bidders. The use of our analytic solution method requires knowledge of a restricted class of auctions which is large enough to contain an optimal auction but small enough that the optimal auction in this class can be found explicitly through analytic methods. No class of auctions which meets these criteria is known even for the four bidder case. Also, when the number of bidders is greater than three, it might be the case that the optimal auction is not expressible in terms of elementary functions. Another interesting set of open questions concerns aggregation auctions. As we show, the aggregation auction for \u03a53 outperforms the aggregation auction for \u03a52 and it appears that the aggregation auction for \u03a53 is better than \u03a5m for m > 3. We leave verification of this conjecture for future work. We also show that \u03a53 is not the best three-bidder auction for use in an aggregation auction, but the auction that beats it is able to reduce the competitive ratio of the overall auction only a little bit. It would be interesting to know whether for any m there is an m-aggregation auction that substantially improves on the competitive ratio of Agg\u03a5m Finally, we remark that very little is known about the structure of the optimal competitive auction. In our auction \u03a53, the sales price for a given bidder is restricted either to be one of the other bid values or to be higher than all other bid values. The optimal auction for two bidders, the 1-unit Vickrey auction, also falls within this class of auctions, as its sales prices are restricted to bid values. We conjecture that an optimal auction for any number of bidders lies within this class. Our paper provides partial evidence for this conjecture: the lower bound of Section 6 on conservative auctions shows that the optimal auction must offer sales prices higher than any bid value if the lower bound of Theorem 2 is tight, as is conjectured. It remains to show that optimal auctions otherwise only offer sales prices at bid values.", "body1": "The research area of optimal mechanism design looks at designing a mechanism to produce the most desirable outcome for the entity running the mechanism. Recently Goldberg et al. The performance of an auction in such a worst-case competitive analysis is measured by its competitive ratio, the ratio between a benchmark performance and the auction\"s performance on the input distribution that maximizes this ratio. We consider the single item, multi-unit, unit-demand auction problem. The auction is performed by soliciting a sealed bid from each of the bidders and deciding on the allocation of units to bidders and the prices to be paid by the bidders. A particularly interesting special case of the auction problem is the unlimited supply case. The competitive framework introduced in and further refined in uses the profit of the optimal omniscient single priced mechanism that sells at least two units as the benchmark for competitive analysis. Previous to this work, the best known auction for the unlimited supply case had a competitive ratio of 3.39 and the best lower bound known was 2.42 . \u2022 We show that the form of the optimal auction is independent of the benchmark used in competitive analysis. \u2022 We give a general technique for converting a limited supply auction into an unlimited supply auction where it is possible to use the competitive ratio of the limited supply auction to obtain a bound on the competitive ratio of the unlimited supply auction. \u2022 We improve on the best known competitive ratio by proving that the aggregation auction constructed from our optimal three-unit auction is 3.25-competitive (Section 5.1). \u2022 Assuming that the conjecture that the optimal -unit auction has a competitive ratio that matches the lower bound proved in , we show that this optimal auction for \u2265 3 on some inputs will occasionally offer prices that are higher than any bid in that input (Section 6). We consider single-round, sealed-bid auctions for a set of identical units of an item to bidders who each desire one unit. DEFINITION 1. DEFINITION 2. 3. 4. A randomized bid-independent auction is a distribution over deterministic bid-independent auctions. The proof of the following theorem can be found, for example, in . THEOREM 1. Given this equivalence, we will use the the terminology bidindependent and truthful interchangeably. For a randomized bid-independent auction, f(b-i) is a random variable. We denote the profit of a truthful auction A on input b as A(b). The expected profit of the auction, E[A(b)], is the sum of the expected payments made by each bidder, which we denote by pi(b) for bidder i. 2.1 Competitive Framework We now review the competitive framework from . The optimal single price omniscient auction, F, is defined as follows. Let A be a truthful auction. 176 2.2 Scale Invariant and Symmetric Auctions A symmetric auction is one where the auction outcome is unchanged when the input bids arrive in a different permutation. Goldberg et al. It is conjectured that the assumption of scale invariance is without loss of generality. Henceforth, we will assume that we are in the unlimited supply case, and we will examine lower bounds for limited supply problems by placing a restriction on the number of bidders in the auction. 2.4 Lower Bounds and Optimal Auctions Frequently in this paper, we will refer to the best known lower bound on the competitive ratio of truthful auctions: THEOREM 2. This bound is derived by analyzing the performance of any auction on the following distribution B. In the two-bidder case, this lower bound is 2. The 1-unit Vickrey auction sells to the highest bidder at the second highest bid value. 2.5 Profit Extraction In this section we review the truthful profit extraction mechanism ProfitExtractR. The goal of profit extraction is, given bids b, to extract a target value R of profit from some subset of the bidders. ProfitExtractR: Given bids b, find the largest k such that the highest k bidders can equally share the cost R. Charge each of these bidders R/k. Important properties of this auction are as follows: \u2022 ProfitExtractR is truthful. \u2022 If R \u2264 F(b), ProfitExtractR(b) = R; otherwise it has no winners and no revenue. We will use this profit extraction mechanism in Section 5 with the following intuition. BIDDERS In this section we define the optimal auction for three bidders, \u03a53, and prove that it indeed matches the known lower bound of 13/6. THEOREM 3. PROOF. CASE 1 (x < y \u2264 3/2): F(2) = 3. CASE 2 (x \u2264 3/2 < y): F(2) = 3. CASE 3 (3/2 < x \u2264 y): F(2) = 2x. The final equation comes from substituting in g(x) = 2/13 (x\u22121)3 and expanding the integrals. A truthful, randomized auction on n bidders can be represented by a randomized function f : Rn\u22121 \u00d7 n \u2192 R that maps masked bid vectors to prices in R. By normalization, we can assume that the lowest possible bid is 1. The optimal auction for the finite auction problem can be found by the following optimization problem in which the variables are \u03c1b-i (z): maximize r subject to i=1 bi z=1 z\u03c1b-i (z)dz \u2265 rF(2) (b) z=1 \u03c1b-i (z)dz = 1 \u03c1b-i (z) \u2265 0 This set of integral inequalities is difficult to maximize over. However, by guessing which constraints are tight and which are slack at the optimum, we will be able to derive a set of differential equations for which any feasible solution is an optimal auction. As we discuss in Section 2.4, in , the authors define a distribution and use it to find a lower bound on the competitive ratio of the optimal auction. In Section 6 we show that the optimal auction must sometimes place probability mass on sale prices above the highest bid. The feasibility problem which arises from the linear optimization problem by assuming the constraints are tight is as follows: a(y) + a(x) + xb(x) + zg(z)dz = r max(3, 2x) \u2200x < y a(x) + b(x) + g(z)dz = 1 a(x) \u2265 0 b(x) \u2265 0 g(z) \u2265 0 Solving this feasibility problem gives the auction \u03a53 proposed above. Finding a simple restriction on the form of n-bidder auctions for n > 3 under which the optimal auction can be found analytically as above remains an open problem. In this section, we widen our focus beyond auctions that compete with F(2) to consider other benchmarks for an auction\"s profit. Recall that F(2) (b) = max2\u2265k\u2265n kb(k). Note that F(2) is the special case of Gs with si = i, and that V\u2217 Gs with si = i \u2212 1. Recall that the k-Vickrey auction sells a unit to each of the highest k bidders at a price equal to the k + 1st highest bid, b(k+1), achieving a profit of kb(k+1). 178 Competing with Gs We will now design a three-bidder auction \u03a5s,t 3 that achieves the optimal competitive ratio against Gs,t. We can lower bound the competitive ratio of \u03a5s,t 3 using the same worst-case distribution from that we used against F(2) . Evaluating the performance of any auction competing against Gs,t on this distribution will yield the following theorem. THEOREM 4. Similarly, we can find the optimal auction against Gs,t using the same technique we used to solve for the three bidder auction with the best competitive ratio against F(2) DEFINITION 6. THEOREM 5. This auction, like \u03a53, can be derived by reducing the optimization problem to a feasibility problem, guessing that the optimal solution has the same form as \u03a5s,t 3 , and solving. Furthermore, we strongly suspect that for any number of bidders, the form of the optimal auction will be independent of the benchmark used. We have seen that optimal auctions for small cases of the limitedsupply model can be found analytically. DEFINITION 7. 4. To analyze AggAm , consider throwing k balls into m labeled bins. Then the m-aggregation auction for Am, AggAm , raises the following fraction of the optimal revenue F(2) (b): E AggAm (b) F(2) \u2265 min k\u22652 k\u2208Km,k F(2) (k) k \u03b2kmk PROOF. THEOREM 8. 179 PROOF. Note that the above bound on the competitive ratio of Agg\u03a53 is tight. Given that the competitive ratio of \u03a53 is tight on this example, the expected revenue of this auction on this input will be exactly 13/4. 5.2 A Gs,t-based Aggregation Auction In this section we show that \u03a53 is not the optimal auction to use in an aggregation auction. To see why this might be the case, notice (Table 1) that the fraction of F(2) (b) raised for when there are k = 2 and k = 3 winning bidders in F(2) (b) is substantially smaller than the fraction of F(2) (b) raised when there are more winners. One approach is to compete against a different benchmark that puts more weight than F(2) on solutions with a small number of winners. The proof follows the outline of Theorem 7 and 8 with the optimal choice of s = 2.162 (while t is held constant at 3). 5.3 Further Reducing the Competitive Ratio There are a number of ways we might attempt to use this aggregation auction schema to continue to push the competitive ratio down. 5.3.1 Agg\u03a5m for m > 3 If the aggregation auction for \u03a52 has a competitive ratio of 4 and the aggregation auction for \u03a53 has a competitive ratio of 3.25, can we improve the competitive ratio by aggregating \u03a54 or \u03a5m for larger m? conjecture lies in the difficulty of finding a closed-form solution for the formula of Theorem 7. 5.3.2 Convex combinations of Agg\u03a5m As can be seen in Table 1, when m > 3, the worst-case value of k is no longer 2 and 3, but instead an increasing function of m. An aggregation auction for \u03a5m outperforms the aggregation auction for \u03a53 when there are two or three winning bidders, while the aggregation auction for \u03a53 outperforms the other when there are at least six winning bidders. AUCTIONS In this section, we define a class of auctions that never offer a sale price which is higher than any bid in the input and prove a lower bound on the competitive ratio of these auctions. We can now state our lower bound for conservative auctions. THEOREM 10. Then the competitive ratio of A is at least 3n\u22122 COROLLARY 1. For a two-bidder auction, this restriction does not prevent optimality. In , the authors define a restricted auction as one on which, for any input, the sale prices are drawn from the set of input bid values. We will prove Theorem 10 with the aid of the following lemma: LEMMA 1. Base case (k = 0, hk = 1): A must raise revenue of at least rn on b = (1, . Inductive case (k > 0, hk = kh): Let b = (1, . First, we will find an upper bound on pH(b) pH (b) = x\u03c1b-H (x)dx + i=1 hi hi\u22121 x\u03c1b-H (x)dx (1) \u2264 1 + i=1 hi hi hi\u22121 \u03c1b-H (x)dx \u2264 1 + 3nr \u2212 2r \u2212 n n \u2212 1 k\u22121 i=1 ih + kh 1 \u2212 nr \u2212 1 n \u2212 1 \u2212 (k \u2212 1) 3nr \u2212 2r \u2212 n n \u2212 1 (2) = kh n(1 \u2212 r) n \u2212 1 (k \u2212 1) 3nr \u2212 2r \u2212 n n \u2212 1 + 1. (4) In order to lower bound Pr[f(b-hk ) \u2264 kh], consider the auction that minimizes it and is consistent with the lower bounds obtained by the strong inductive hypothesis on Pr[f(b-hk ) \u2264 ih]. Therefore, the probability that the sale price will be no more than kh on masked bid vector on bid vector b = (1, .", "body2": "Here, the classical approach is to design such a mechanism given the prior distribution from which the bidders\" preferences are drawn (See e.g., ). Positive results in this direction are fueled by the observation that in auctions for a number of identical units, much of the distribution from which the bidders are drawn can be deduced on the fly by the auction as it is being run . This result further validates the use of competitive analysis in gauging auction performance. Each bidder has a valuation representing how much the item is worth to him. In this paper, we will restrict our attention to truthful (a.k.a., incentive compatible or strategyproof) auctions. Pay-per-view television and downloadable audio files are examples of such goods. The optimal auction is the one which is \u03b2-competitive with the minimum value of \u03b2. This auction achieves a competitive ratio of 13/6, matching the lower bound from (Section 3). In doing so, we give an optimal three bidder auction for generalized benchmarks (Section 4). We refer to auctions derived from this framework as aggregation auctions (Section 5). \u2022 We improve on the best known competitive ratio by proving that the aggregation auction constructed from our optimal three-unit auction is 3.25-competitive (Section 5.1). For the three-unit case where we have shown that the lower bound of is tight, this observation led to our construction of the optimal three-unit auction. Related formulations to this one have appeared in numerous places in recent literature (e.g., ). , bn). If ti < bi, bidder i wins at price ti. If ti > bi, bidder i loses. Otherwise, (ti = bi) the auction can either accept the bid at price ti or reject it. A randomized bid-independent auction is a distribution over deterministic bid-independent auctions. The proof of the following theorem can be found, for example, in . An auction is truthful if and only if it is equivalent to a bid-independent auction. Given this equivalence, we will use the the terminology bidindependent and truthful interchangeably. We denote the probability density of f(b-i) at z by \u03c1b-i (z). We denote the profit of a truthful auction A on input b as A(b). Clearly, the expected payment of each bid satisfies pi(b) = bi x\u03c1b-i (x)dx. , bn), let b(i) represent the i-th largest value in b. The profit of F(2) is max2\u2264k\u2264n kb(k) There are a number of reasons to choose this benchmark for comparison, interested readers should see or for a more detailed discussion. We say that A is \u03b2-competitive against F(2) (or just \u03b2-competitive) if for all bid vectors b, the expected profit of A on b satisfies E[A(b)] \u2265 F(2) (b) In Section 4 we generalize this framework to other profit benchmarks. 176 2.2 Scale Invariant and Symmetric Auctions A symmetric auction is one where the auction outcome is unchanged when the input bids arrive in a different permutation. An auction defined by bid-independent function f is scale invariant if, for all i and all z, Pr[f(b-i) \u2265 z] = Pr[f(cb-i) \u2265 cz]. This is without loss of generality as (a) any lower bound that applies to the n = case also extends to the case where n \u2265 , and (b) there is a reduction from the unlimited supply auction problem to the limited supply auction problem that takes an unlimited supply auction that is \u03b2-competitive with F(2) and constructs a limited supply auction parameterized by that is \u03b2-competitive with F(2, ) , the optimal omniscient auction that sells between 2 and units . Henceforth, we will assume that we are in the unlimited supply case, and we will examine lower bounds for limited supply problems by placing a restriction on the number of bidders in the auction. Let \u03a5n denote the n-bidder auction that achieves the optimal competitive ratio. from the distribution such that Pr[Bi \u2265 s] \u2264 1/s for all s \u2208 S. It is conjectured that this lower bound is tight for any number of bidders and the optimal auction, \u03a5n, matches it. The 1-unit Vickrey auction sells to the highest bidder at the second highest bid value. This mechanism is a special case of a general cost-sharing schema due to Moulin and Shenker . The goal of profit extraction is, given bids b, to extract a target value R of profit from some subset of the bidders. If no subset of bidders can cover the cost, the mechanism has no winners. Important properties of this auction are as follows: \u2022 ProfitExtractR is truthful. \u2022 If R \u2264 F(b), ProfitExtractR(b) = R; otherwise it has no winners and no revenue. Likewise, a mechanism can offer the set of bidders S a target revenue R. If R \u2264 F(2) (S), then ProfitExtractR raises R from S; otherwise, the it raises no revenue from S. \u03a53 is scale-invariant and symmetric and given by the bid-independent function with density function \u03c11,x(z) = \u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23a8 \u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23a9 For x \u2264 3/2 1 with probability 9/13 z with probability density g(z) for z > 3/2 For x > 3/2\u23a7 \u23aa\u23a8 \u23aa\u23a9 1 with probability 9/13 \u2212 3/2 zg(z)dz x with probability 3/2 (z + 1)g(z)dz z with probability density g(z) for z > x where g(x) = 2/13 (x\u22121)3 . Furthermore, the auction raises exactly 13 F(2) on every input with non-identical bids. There are three cases. Therefore \u03a53 raises 18/13 on these bids. Consider the revenue raised from all three bidders: E[\u03a53(b)] = p(1, x, y) + p(x, 1, y) + p(y, 1, x) = 0 + 9/13 \u2212 3/2 zg(z)dz + 9/13 \u2212 3/2 zg(z)dz + x 3/2 (z + 1)g(z)dz + zg(z)dz = 18/13 + (x \u2212 2) 3/2 zg(z)dz + x 3/2 g(z)dz = 12x/13. By applying this consequence, we will derive an optimal auction for three bidders. Recall that \u03c1b-i (z) = Pr[f(b-i) = z]. The optimal auction for the finite auction problem can be found by the following optimization problem in which the variables are \u03c1b-i (z): maximize r subject to i=1 bi z=1 z\u03c1b-i (z)dz \u2265 rF(2) (b) z=1 \u03c1b-i (z)dz = 1 \u03c1b-i (z) \u2265 0 This set of integral inequalities is difficult to maximize over. However, by guessing which constraints are tight and which are slack at the optimum, we will be able to derive a set of differential equations for which any feasible solution is an optimal auction. If the solution to these equations has value matching the lower bound obtained from the worst-case distribution, then this solution is the optimal auction and that our conjectured choice of worst-case distribution is correct. This motivates considering symmetric scale-invariant auctions for three bidders with probability density function, \u03c11,x(z), of the following form: \u03c11,x(z) = \u23aa\u23a8 \u23aa\u23a9 1 with discrete probability a(x) x with discrete probability b(x) z with probability density g(z) for z > x In this auction, the sale price for the first bidder is either one of the latter two bids, or higher than either bid with a probability density which is independent of the input. The proof of its optimality validates its proposed form. Finding a simple restriction on the form of n-bidder auctions for n > 3 under which the optimal auction can be found analytically as above remains an open problem. For instance, one might wish to compete against V\u2217 , the profit of the k-Vickrey auction with optimal-inhindsight choice of k.2 Alternatively, if an auction is being used as a subroutine in a larger mechanism, one might wish to choose the auction which is optimally competitive with a benchmark specific to that purpose. When considering Gs we assume without loss of generality that si < si+1 as otherwise the constraint imposed by si+1 is irrelevant. Note that F(2) is the special case of Gs with si = i, and that V\u2217 Gs with si = i \u2212 1. Recall that the k-Vickrey auction sells a unit to each of the highest k bidders at a price equal to the k + 1st highest bid, b(k+1), achieving a profit of kb(k+1). As before, we will first find a lower bound on the competitive ratio and then design an auction to meet that bound. We can lower bound the competitive ratio of \u03a5s,t 3 using the same worst-case distribution from that we used against F(2) . We denote the optimal auction for three bidders against Gs,t as \u03a5s,t 3 . The optimal three-bidder auction, \u03a5s,t 3 , competing against Gs,t(b) = max(sb(2), tb(3)) has a competitive ratio of at least s2 +t2 2t The proof can be found in the appendix. \u03a5s,t 3 is scale-invariant and symmetric and given by the bid-independent function with density function \u03c11,x(z) = \u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23a8 \u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23a9 For x \u2264 t 1 with probability t2 s2+t2 z with probability density g(z) for z > t For x > t s\u23a7 \u23aa\u23aa\u23a8 \u23aa\u23aa\u23a9 1 with probability t2 s2+t2 \u2212 zg(z)dz x with probability (z + 1)g(z)dz z with probability density g(z) for z > x where g(x) = 2(t\u2212s)2 /(s2 +t2 (x\u22121)3 . \u03a5s,t 3 is s2 +t2 2t -competitive with Gs,t. Note that the form of \u03a5s,t 3 is essentially the same as for \u03a53, but that the probability of each price is scaled depending on the values of s and t. That our auction for three bidders matches the lower bound computed by the input distribution used in is strong evidence that this input distribution is the worst-case input distribution for any number of bidders and any generalized profit benchmark. Furthermore, we strongly suspect that for any number of bidders, the form of the optimal auction will be independent of the benchmark used. Given n bidders and an auction, Am, for m < n bidders, we can convert the m-bidder auction into an n-bidder auction by randomly partitioning the bidders into m subsets and then treating each subset as a single bidder (via ProfitExtractR) and running the m-bidder auction. Let B be the vector of aggregate bids, and B\u2212j be the aggregate bids for all bins other than j. Compute the aggregate price Tj = f(B\u2212j), where f is the bid-independent function for Am. It is the 2-aggregation auction for \u03a52, the 1-unit Vickrey auction. Let Am be an auction with competitive ratio \u03b2. This improves on the previously best known auction which is 3.39-competitive . The aggregation auction for \u03a53 has competitive ratio 3.25. As k increases, the competitive ratio approaches 13/6. To see this, consider the bid vector with two very large and non-identical bids of h and h + with the remaining bids 1. Given that the competitive ratio of \u03a53 is tight on this example, the expected revenue of this auction on this input will be exactly 13/4. One can do better by choosing the auction that is optimally competitive against a specially tailored benchmark. If we chose a three bidder auction that performed better when F(2) has smaller numbers of winners, our aggregation auction would perform better in the worst case. For an optimal choice of s and t, the aggregation auction for \u03a5s,t 3 is 3.243-competitive. The proof follows the outline of Theorem 7 and 8 with the optimal choice of s = 2.162 (while t is held constant at 3). In this section, we give a brief discussion of several attempts. The lowest value for each column is printed in bold. Table 1 shows, for each value of m and k, the fraction of F(2) raised by the aggregation auction for Agg\u03a5m when there are k winning bidders, assuming the lower bound of Theorem 2 is tight. Furthermore, note that we cannot yet claim the existence of a good auction via this technique as the optimal auction \u03a5n for n > 3 is not known and it is only conjectured that the bound given by Theorem 2 and represented in Table 1 is correct for \u03a5n. We say an auction BIf is conservative if its bidindependent function f satisfies f(b-i) \u2264 max(b-i). We can now state our lower bound for conservative auctions. Let A be a conservative auction for n bidders. The competitive ratio of any conservative auction for an arbitrary number of bidders is at least three. The Random Sampling Profit Extraction auction and the CORE auction , on the other hand, use the ProfitExtractR mechanism as a subroutine and thus sometimes offer a sale price which is higher than the highest input bid value. The class of conservative auctions can be viewed as a generalization of the class of restricted auctions and therefore our result below gives lower bounds on the performance of a broader class of auctions. , 1, H) \u2264 hk] \u2265 nr\u22121 n\u22121 + k(3nr\u22122r\u2212n n\u22121 ). We will also let p1(b), phk (b), and pH (b) represent the expected payment of a 1-valued, hk-valued, and H-valued bidder in A on b, respectively (note by symmetry the expected payment for all 1-valued bidders is the same). The base case follows trivially from the final inequality. , 1, hk, H). Given upper-bounds on the profit from the H-valued, equation bid (3), and the 1-valued bids, the profit from the hk-valued bid must be at least: phk (b) \u2265 2rkh \u2212 (n \u2212 2)p1(b) \u2212 pH(b) \u2265 kh 2r \u2212 n(1 \u2212 r) n \u2212 1 (k \u2212 1) 3nr \u2212 2r \u2212 n n \u2212 1 \u2212 O(n). In order to satisfy our lower bound, (4), on phk (b), it must put at least 3nr\u22122r\u2212n n\u22121 on hk. Given Lemma 1, Theorem 10 is simple to prove. The theorem follows.", "introduction": "The research area of optimal mechanism design looks at designing a mechanism to produce the most desirable outcome for the entity running the mechanism. This problem is well studied for the auction design problem where the optimal mechanism is the one that brings the seller the most profit. Here, the classical approach is to design such a mechanism given the prior distribution from which the bidders\" preferences are drawn (See e.g., ). introduced the use of worst-case competitive analysis (See e.g., ) to analyze the performance of auctions that have no knowledge of the prior distribution. The goal of such work is to design an auction that achieves a large constant fraction of the profit attainable by an optimal mechanism that knows the prior distribution in advance. Positive results in this direction are fueled by the observation that in auctions for a number of identical units, much of the distribution from which the bidders are drawn can be deduced on the fly by the auction as it is being run . The performance of an auction in such a worst-case competitive analysis is measured by its competitive ratio, the ratio between a benchmark performance and the auction\"s performance on the input distribution that maximizes this ratio. The holy grail of the worstcase competitive analysis of auctions is the auction that achieves the optimal competitive ratio (as small as possible). Since this search has led to improved understanding of the nature of the optimal auction, the techniques for on-the-fly pricing in these scenarios, and the competitive ratio of the optimal auction . In this paper we continue this line of research by improving in all of these directions. Furthermore, we give evidence corroborating the conjecture that the form of the optimal auction is independent of the benchmark used in the auction\"s competitive analysis. This result further validates the use of competitive analysis in gauging auction performance. We consider the single item, multi-unit, unit-demand auction problem. In such an auction there are many units of a single item available for sale to bidders who each desire only one unit. Each bidder has a valuation representing how much the item is worth to him. The auction is performed by soliciting a sealed bid from each of the bidders and deciding on the allocation of units to bidders and the prices to be paid by the bidders. The bidders are assumed to bid so as to maximize their personal utility, the difference between their valuation and the price they pay. To handle the problem of designing and analyzing auctions where bidders may falsely declare their valuations to get a better deal, we will adopt the solution concept of truthful mechanism design (see, e.g., ). In a truthful auction, revealing one\"s true valuation as one\"s bid is an optimal strategy for each bidder regardless of the bids of the other bidders. In this paper, we will restrict our attention to truthful (a.k.a., incentive compatible or strategyproof) auctions. A particularly interesting special case of the auction problem is the unlimited supply case. In this case the number of units for sale is at least the number of bidders in the auction. This is natural for the sale of digital goods where there is negligible cost for duplicating 175 and distributing the good. Pay-per-view television and downloadable audio files are examples of such goods. The competitive framework introduced in and further refined in uses the profit of the optimal omniscient single priced mechanism that sells at least two units as the benchmark for competitive analysis. The assumption that two or more units are sold is necessary because in the worst case it is impossible to obtain a constant fraction of the profit of the optimal mechanism when it sells only one unit . In this framework for competitive analysis, an auction is said to be \u03b2-competitive if it achieves a profit that is within a factor of \u03b2 \u2265 1 of the benchmark profit on every input. The optimal auction is the one which is \u03b2-competitive with the minimum value of \u03b2. Previous to this work, the best known auction for the unlimited supply case had a competitive ratio of 3.39 and the best lower bound known was 2.42 . For the limited supply case, auctions can achieve substantially better competitive ratios. When there are only two units for sale, the optimal auction gives a competitive ratio of 2, which matches the lower bound for two units. When there are three units for sale, the best previously known auction had a competitive ratio of 2.3, compared with a lower bound of 13/6 \u2248 2.17 . The results of this paper are as follows: \u2022 We give the auction for three units that is optimally competitive against the profit of the omniscient single priced mechanism that sells at least two units. This auction achieves a competitive ratio of 13/6, matching the lower bound from (Section 3). \u2022 We show that the form of the optimal auction is independent of the benchmark used in competitive analysis. In doing so, we give an optimal three bidder auction for generalized benchmarks (Section 4). \u2022 We give a general technique for converting a limited supply auction into an unlimited supply auction where it is possible to use the competitive ratio of the limited supply auction to obtain a bound on the competitive ratio of the unlimited supply auction. We refer to auctions derived from this framework as aggregation auctions (Section 5). \u2022 We improve on the best known competitive ratio by proving that the aggregation auction constructed from our optimal three-unit auction is 3.25-competitive (Section 5.1). \u2022 Assuming that the conjecture that the optimal -unit auction has a competitive ratio that matches the lower bound proved in , we show that this optimal auction for \u2265 3 on some inputs will occasionally offer prices that are higher than any bid in that input (Section 6). For the three-unit case where we have shown that the lower bound of is tight, this observation led to our construction of the optimal three-unit auction.", "conclusion": "We have found the optimal auction for the three-unit limitedsupply case, and shown that its structure is essentially independent of the benchmark used in its competitive analysis.. We have then used this auction to derive the best known auction for the unlimited supply case.. Our work leaves many interesting open questions.. We found that the lower bound of is matched by an auction for three bidders, 181 even when competing against generalized benchmarks.. The most interesting open question from our work is whether the lower bound from Theorem 2 can be matched by an auction for more than three bidders.. Second, we consider whether our techniques can be extended to find optimal auctions for greater numbers of bidders.. The use of our analytic solution method requires knowledge of a restricted class of auctions which is large enough to contain an optimal auction but small enough that the optimal auction in this class can be found explicitly through analytic methods.. No class of auctions which meets these criteria is known even for the four bidder case.. Also, when the number of bidders is greater than three, it might be the case that the optimal auction is not expressible in terms of elementary functions.. Another interesting set of open questions concerns aggregation auctions.. As we show, the aggregation auction for \u03a53 outperforms the aggregation auction for \u03a52 and it appears that the aggregation auction for \u03a53 is better than \u03a5m for m > 3.. We leave verification of this conjecture for future work.. We also show that \u03a53 is not the best three-bidder auction for use in an aggregation auction, but the auction that beats it is able to reduce the competitive ratio of the overall auction only a little bit.. It would be interesting to know whether for any m there is an m-aggregation auction that substantially improves on the competitive ratio of Agg\u03a5m Finally, we remark that very little is known about the structure of the optimal competitive auction.. In our auction \u03a53, the sales price for a given bidder is restricted either to be one of the other bid values or to be higher than all other bid values.. The optimal auction for two bidders, the 1-unit Vickrey auction, also falls within this class of auctions, as its sales prices are restricted to bid values.. We conjecture that an optimal auction for any number of bidders lies within this class.. Our paper provides partial evidence for this conjecture: the lower bound of Section 6 on conservative auctions shows that the optimal auction must offer sales prices higher than any bid value if the lower bound of Theorem 2 is tight, as is conjectured.. It remains to show that optimal auctions otherwise only offer sales prices at bid values."}
{"id": "H-29", "keywords": ["queri expans", "pseudo-relev feedback"], "title": "Estimation and Use of Uncertainty in Pseudo-relevance Feedback", "abstract": "Existing pseudo-relevance feedback methods typically per- form averaging over the top-retrieved documents, but ig- nore an important statistical dimension: the risk or variance associated with either the individual document models, or their combination. Treating the baseline feedback method as a black box, and the output feedback model as a random variable, we estimate a posterior distribution for the feed- back model by resampling a given query's top-retrieved doc- uments, using the posterior mean or mode as the enhanced feedback model. We then perform model combination over several enhanced models, each based on a slightly modified query sampled from the original query. We find that resam- pling documents helps increase individual feedback model precision by removing noise terms, while sampling from the query improves robustness (worst-case performance) by em- phasizing terms related to multiple query aspects. The re- sult is a meta-feedback algorithm that is both more robust and more precise than the original strong baseline method.", "references": ["The Lemur toolkit for language modeling and retrieval", "Query difficulty, robustness, and selective application of query expansion", "A high-performance semi-supervised learning method for text chunking", "Improving retrieval feedback with multiple term-ranking function combination", "Initial results with structured queries and language models on half a terabyte of text", "Pattern Classification", "The role of variance in term weighting for probabilistic information retrieval", "SOMPAK: The self-organizing map program package", "A Generative Theory of Relevance", "Using query-specific variance estimates to combine Bayesian classifiers", "Combining the language model and inference network approaches to retrieval", "Estimating a Dirichlet distribution", "Advances in Information Retrieval, chapter Language models for relevance feedback", "A language modeling approach to information retrieval", "The SMART Retrieval System, chapter Relevance Feedback in Information Retrieval", "Flexible pseudo-relevance feedback via selective sampling", "Regularized estimation of mixture models for robust pseudo-relevance feedback", "Improving the effectiveness of information retrieval with local context analysis", "Learning to estimate query difficulty", "Ranking robustness: a novel framework to predict query performance"], "full_text": "1. INTRODUCTION Uncertainty is an inherent feature of information retrieval. Not only do we not know the queries that will be presented to our retrieval algorithm ahead of time, but the user\"s information need may be vague or incompletely specified by these queries. Even if the query were perfectly specified, language in the collection documents is inherently complex and ambiguous and matching such language effectively is a formidable problem by itself. With this in mind, we wish to treat many important quantities calculated by the retrieval system, whether a relevance score for a document, or a weight for a query expansion term, as random variables whose true value is uncertain but where the uncertainty about the true value may be quantified by replacing the fixed value with a probability distribution over possible values. In this way, retrieval algorithms may attempt to quantify the risk or uncertainty associated with their output rankings, or improve the stability or precision of their internal calculations. Current algorithms for pseudo-relevance feedback (PRF) tend to follow the same basic method whether we use vector space-based algorithms such as Rocchio\"s formula , or more recent language modeling approaches such as Relevance Models . First, a set of top-retrieved documents is obtained from an initial query and assumed to approximate a set of relevant documents. Next, a single feedback model vector is computed according to some sort of average, centroid, or expectation over the set of possibly-relevant document models. For example, the document vectors may be combined with equal weighting, as in Rocchio, or by query likelihood, as may be done using the Relevance Model1 . The use of an expectation is reasonable for practical and theoretical reasons, but by itself ignores potentially valuable information about the risk of the feedback model. Our main hypothesis in this paper is that estimating the uncertainty in feedback is useful and leads to better individual feedback models and more robust combined models. Therefore, we propose a method for estimating uncertainty associated with an individual feedback model in terms of a posterior distribution over language models. To do this, we systematically vary the inputs to the baseline feedback method and fit a Dirichlet distribution to the output. We use the posterior mean or mode as the improved feedback model estimate. This process is shown in Figure 1. As we show later, the mean and mode may vary significantly from the single feedback model proposed by the baseline method. We also perform model combination using several improved feedback language models obtained by a small number of new queries sampled from the original query. A model\"s weight combines two complementary factors: the model\"s probability of generating the query, and the variance of the model, with high-variance models getting lower weight. For example, an expected parameter vector conditioned on the query observation is formed from top-retrieved documents, which are treated as training strings (see , p. 62). Figure 1: Estimating the uncertainty of the feedback model for a single query. 2. SAMPLING-BASED FEEDBACK In Sections 2.1-2.5 we describe a general method for estimating a probability distribution over the set of possible language models. In Sections 2.6 and 2.7 we summarize how different query samples are used to generate multiple feedback models, which are then combined. 2.1 Modeling Feedback Uncertainty Given a query Q and a collection C, we assume a probabilistic retrieval system that assigns a real-valued document score f(D, Q) to each document D in C, such that the score is proportional to the estimated probability of relevance. We make no other assumptions about f(D, Q). The nature of f(D, Q) may be complex: for example, if the retrieval system supports structured query languages , then f(D, Q) may represent the output of an arbitrarily complex inference network defined by the structured query operators. In theory, the scoring function can vary from query to query, although in this study for simplicity we keep the scoring function the same for all queries. Our specific query method is given in Section 3. We treat the feedback algorithm as a black box and assume that the inputs to the feedback algorithm are the original query and the corresponding top-retrieved documents, with a score being given to each document. We assume that the output of the feedback algorithm is a vector of term weights to be used to add or reweight the terms in the representation of the original query, with the vector normalized to form a probability distribution. We view the the inputs to the feedback black box as random variables, and analyze the feedback model as a random variable that changes in response to changes in the inputs. Like the document scoring function f(D, Q), the feedback algorithm may implement a complex, non-linear scoring formula, and so as its inputs vary, the resulting feedback models may have a complex distribution over the space of feedback models (the sample space). Because of this potential complexity, we do not attempt to derive a posterior distribution in closed form, but instead use simulation. We call this distribution over possible feedback models the feedback model distribution. Our goal in this section is to estimate a useful approximation to the feedback model distribution. For a specific framework for experiments, we use the language modeling (LM) approach for information retrieval . The score of a document D with respect to a query Q and collection C is given by p(Q|D) with respect to language models \u02c6\u03b8Q and \u02c6\u03b8D estimated for the query and document respectively. We denote the set of k top-retrieved documents from collection C in response to Q by DQ(k, C). For simplicity, we assume that queries and documents are generated by multinomial distributions whose parameters are represented by unigram language models. To incorporate feedback in the LM approach, we assume a model-based scheme in which our goal is take the query and resulting ranked documents DQ(k, C) as input, and output an expansion language model \u02c6\u03b8E, which is then interpolated with the original query model \u02c6\u03b8Q: \u02c6\u03b8New = (1 \u2212 \u03b1) \u00b7 \u02c6\u03b8Q + \u03b1 \u00b7 \u02c6\u03b8E (1) This includes the possibility of \u03b1 = 1 where the original query mode is completely replaced by the feedback model. Our sample space is the set of all possible language models LF that may be output as feedback models. Our approach is to take samples from this space and then fit a distribution to the samples using maximum likelihood. For simplicity, we start by assuming the latent feedback distribution has the form of a Dirichlet distribution. Although the Dirichlet is a unimodal distribution, and in general quite limited in its expressiveness in the sample space, it is a natural match for the multinomial language model, can be estimated quickly, and can capture the most salient features of confident and uncertain feedback models, such as the overall spread of the distibution. 2.2 Resampling document models We would like an approximation to the posterior distribution of the feedback model LF . To accomplish this, we apply a widely-used simulation technique called bootstrap sampling (, p. 474) on the input parameters, namely, the set of top-retrieved documents. Bootstrap sampling allows us to simulate the approximate effect of perturbing the parameters within the black box feedback algorithm by perturbing the inputs to that algorithm in a systematic way, while making no assumptions about the nature of the feedback algorithm. Specifically, we sample k documents with replacement from DQ(k, C), and calculate an expansion language model \u03b8b using the black box feedback method. We repeat this process B times to obtain a set of B feedback language models, to which we then fit a Dirichlet distribution. Typically B is in the range of 20 to 50 samples, with performance being relatively stable in this range. Note that instead of treating each top document as equally likely, we sample according to the estimated probabilities of relevance of each document in DQ(k, C). Thus, a document is more likely to be chosen the higher it is in the ranking. 2.3 Justification for a sampling approach The rationale for our sampling approach has two parts. First, we want to improve the quality of individual feedback models by smoothing out variation when the baseline feedback model is unstable. In this respect, our approach resembles bagging , an ensemble approach which generates multiple versions of a predictor by making bootstrap copies of the training set, and then averages the (numerical) predictors. In our application, top-retrieved documents can be seen as a kind of noisy training set for relevance. Second, sampling is an effective way to estimate basic properties of the feedback posterior distribution, which can then be used for improved model combination. For example, a model may be weighted by its prediction confidence, estimated as a function of the variability of the posterior around the model. foo2-401.map-Dim:Size:12*12units,gaussianneighborhood (a) Topic 401 Foreign minorities, Germany foo2-402.map-Dim:Size:12*12units,gaussianneighborhood (b) Topic 402 Behavioral genetics foo2-459.map-Dim:Size:12*12units,gaussianneighborhood (c) Topic 459 When can a lender foreclose on property Figure 2: Visualization of expansion language model variance using self-organizing maps, showing the distribution of language models that results from resampling the inputs to the baseline expansion method. The language model that would have been chosen by the baseline expansion is at the center of each map. The similarity function is JensenShannon divergence. 2.4 Visualizing feedback distributions Before describing how we fit and use the Dirichlet distribution over feedback models, it is instructive to view some examples of actual feedback model distributions that result from bootstrap sampling the top-retrieved documents from different TREC topics. Each point in our sample space is a language model, which typically has several thousand dimensions. To help analyze the behavior of our method we used a Self-Organizing Map (via the SOM-PAK package ), to \u2018flatten\" and visualize the high-dimensional density function2 The density maps for three TREC topics are shown in Figure 2 above. The dark areas represent regions of high similarity between language models. The light areas represent regions of low similarity - the \u2018valleys\" between clusters. Each diagram is centered on the language model that would have been chosen by the baseline expansion. A single peak (mode) is evident in some examples, but more complex structure appears in others. Also, while the distribution is usually close to the baseline feedback model, for some topics they are a significant distance apart (as measured by JensenShannon divergence), as in Subfigure 2c. In such cases, the mode or mean of the feedback distribution often performs significantly better than the baseline (and in a smaller proportion of cases, significantly worse). 2.5 Fitting a posterior feedback distribution After obtaining feedback model samples by resampling the feedback model inputs, we estimate the feedback distribution. We assume that the multinomial feedback models {\u02c6\u03b81, . . . , \u02c6\u03b8B} were generated by a latent Dirichlet distribution with parameters {\u03b11, . . . , \u03b1N }. To estimate the {\u03b11, . . . , \u03b1N }, we fit the Dirichlet parameters to the B language model samples according to maximum likelihood using a generalized Newton procedure, details of which are given in Minka . We assume a simple Dirichlet prior over the {\u03b11, . . . , \u03b1N }, setting each to \u03b1i = \u03bc \u00b7 p(wi | C), where \u03bc is a parameter and p(\u00b7 | C) is the collection language model estimated from a set of documents from collection C. The parameter fitting converges very quickly - typically just 2 or Because our points are language models in the multinomial simplex, we extended SOM-PAK to support JensenShannon divergence, a widely-used similarity measure between probability distributions. 3 iterations are enough - so that it is practical to apply at query-time when computational overhead must be small. In practice, we can restrict the calculation to the vocabulary of the top-retrieved documents, instead of the entire collection. Note that for this step we are re-using the existing retrieved documents and not performing additional queries. Given the parameters of an N-dimensional Dirichlet distribution Dir(\u03b1) the mean \u03bc and mode x vectors are easy to calculate and are given respectively by \u03bci = \u03b1iP \u03b1i (2) and xi = \u03b1i\u22121P \u03b1i\u2212N . (3) We can then choose the language model at the mean or the mode of the posterior as the final enhanced feedback model. (We found the mode to give slightly better performance.) For information retrieval, the number of samples we will have available is likely to be quite small for performance reasons - usually less than ten. Moreover, while random sampling is useful in certain cases, it is perfectly acceptable to allow deterministic sampling distributions, but these must be designed carefully in order to approximate an accurate output variance. We leave this for future study. 2.6 Query variants We use the following methods for generating variants of the original query. Each variant corresponds to a different assumption about which aspects of the original query may be important. This is a form of deterministic sampling. We selected three simple methods that cover complimentary assumptions about the query. No-expansion Use only the original query. The assumption is that the given terms are a complete description of the information need. Leave-one-out A single term is left out of the original query. The assumption is that one of the query terms is a noise term. Single-term A single term is chosen from the original query. This assumes that only one aspect of the query, namely, that represented by the term, is most important. After generating a variant of the original query, we combine it with the original query using a weight \u03b1SUB so that we do not stray too \u2018far\". In this study, we set \u03b1SUB = 0.5. For example, using the Indri query language, a leave-oneout variant of the initial query that omits the term \u2018ireland\" for TREC topic 404 is: #weight(0.5 #combine(ireland peace talks) 0.5 #combine(peace talks)) 2.7 Combining enhanced feedback models from multiple query variants When using multiple query variants, the resulting enhanced feedback models are combined using Bayesian model combination. To do this, we treat each word as an item to be classified as belonging to a relevant or non-relevant class, and derive a class probability for each word by combining the scores from each query variant. Each score is given by that term\"s probability in the Dirichlet distribution. The term scores are weighted by the inverse of the variance of the term in the enhanced feedback model\"s Dirichlet distribution. The prior probability of a word\"s membership in the relevant class is given by the probability of the original query in the entire enhanced expansion model. 3. EVALUATION In this section we present results confirming the usefulness of estimating a feedback model distribution from weighted resampling of top-ranked documents, and of combining the feedback models obtained from different small changes in the original query. 3.1 General method We evaluated performance on a total of 350 queries derived from four sets of TREC topics: 51-200 (TREC-1&2), 351-400 (TREC-7), 401-450 (TREC-8), and 451-550 (wt10g, TREC-9&10). We chose these for their varied content and document properties. For example, wt10g documents are Web pages with a wide variety of subjects and styles while TREC-1&2 documents are more homogeneous news articles. Indexing and retrieval was performed using the Indri system in the Lemur toolkit . Our queries were derived from the words in the title field of the TREC topics. Phrases were not used. To generate the baseline queries passed to Indri, we wrapped the query terms with Indri\"s #combine operator. For example, the initial query for topic 404 is: #combine(ireland peace talks) We performed Krovetz stemming for all experiments. Because we found that the baseline (Indri) expansion method performed better using a stopword list with the feedback model, all experiments used a stoplist of 419 common English words. However, an interesting side-effect of our resampling approach is that it tends to remove many stopwords from the feedback model, making a stoplist less critical. This is discussed further in Section 3.6. 3.2 Baseline feedback method For our baseline expansion method, we use an algorithm included in Indri 1.0 as the default expansion method. This method first selects terms using a log-odds calculation described by Ponte , but assigns final term weights using Lavrenko\"s relevance model. We chose the Indri method because it gives a consistently strong baseline, is based on a language modeling approach, and is simple to experiment with. In a TREC evaluation using the GOV2 corpus , the method was one of the topperforming runs, achieving a 19.8% gain in MAP compared to using unexpanded queries. In this study, it achieves an average gain in MAP of 17.25% over the four collections. Indri\"s expansion method first calculates a log-odds ratio o(v) for each potential expansion term v given by o(v) = log p(v|D) p(v|C) (4) over all documents D containing v, in collection C. Then, the expansion term candidates are sorted by descending o(v), and the top m are chosen. Finally, the term weights r(v) used in the expanded query are calculated based on the relevance model r(v) = p(q|D)p(v|D) p(v) p(D) (5) The quantity p(q|D) is the probability score assigned to the document in the initial retrieval. We use Dirichlet smoothing of p(v|D) with \u03bc =. This relevance model is then combined with the original query using linear interpolation, weighted by a parameter \u03b1. By default we used the top 50 documents for feedback and the top 20 expansion terms, with the feedback interpolation parameter \u03b1 = 0.5 unless otherwise stated. For example, the baseline expanded query for topic 404 is: #weight(0.5 #combine(ireland peace talks) 0.5 #weight(0.10 ireland 0.08 peace 0.08 northern ...) 3.3 Expansion performance We measure our feedback algorithm\"s effectiveness by two main criteria: precision, and robustness. Robustness, and the tradeoff between precision and robustness, is analyzed in Section 3.4. In this section, we examine average precision and precision in the top 10 documents (P10). We also include recall at 1,000 documents. For each query, we obtained a set of B feedback models using the Indri baseline. Each feedback model was obtained from a random sample of the top k documents taken with replacement. For these experiments, B = 30 and k = 50. Each feedback model contained 20 terms. On the query side, we used leave-one-out (LOO) sampling to create the query variants. Single-term query sampling had consistently worse performance across all collections and so our results here focus on LOO sampling. We used the methods described in Section 2 to estimate an enhanced feedback model from the Dirichlet posterior distribution for each query variant, and to combine the feedback models from all the query variants. We call our method \u2018resampling expansion\" and denote it as RS-FB here. We denote the Indri baseline feedback method as Base-FB. Results from applying both the baseline expansion method (Base-FB) and resampling expansion (RS-FB) are shown in Table 1. We observe several trends in this table. First, the average precision of RS-FB was comparable to Base-FB, achieving an average gain of 17.6% compared to using no expansion across the four collections. The Indri baseline expansion gain was 17.25%. Also, the RS-FB method achieved consistent improvements in P10 over Base-FB for every topic set, with an average improvement of 6.89% over Base-FB for all 350 topics. The lowest P10 gain over Base-FB was +3.82% for TREC-7 and the highest was +11.95% for wt10g. Finally, both Base-FB and RS-FB also consistently improved recall over using no expansion, with Base-FB achieving better recall than RS-FB for all topic sets. 3.4 Retrieval robustness We use the term robustness to mean the worst-case average precision performance of a feedback algorithm. Ideally, a robust feedback method would never perform worse than using the original query, while often performing better using the expansion. To evaluate robustness in this study, we use a very simple measure called the robustness index (RI)3 . For a set of queries Q, the RI measure is defined as: RI(Q) = n+ \u2212 n\u2212 |Q| (6) where n+ is the number of queries helped by the feedback method and n\u2212 is the number of queries hurt. Here, by \u2018helped\" we mean obtaining a higher average precision as a result of feedback. The value of RI ranges from a minimum This is sometimes also called the reliability of improvement index and was used in Sakai et al. . Collection NoExp Base-FB RS-FB TREC 1& (+33.04%) (+32.24%) (+10.57%) (+17.83%)/// (+15.07%) (+14.75%) (+2.85%) (+6.67%)/// (+16.25%) (+11.70%) (+5.05%) (+9.59%)/// (+5.06%) (+11.78%) (-4.71%) (+7.24%)/// Table 1: Comparison of baseline (Base-FB) feedback and feedback using re-sampling (RS-FB). Improvement shown for BaseFB and RS-FB is relative to using no expansion. (a) TREC 1&2 (upper curve); TREC 8 (lower curve) (b) TREC 7 (upper curve); wt10g (lower curve) Figure 3: The trade-off between robustness and average precision for different corpora. The x-axis gives the change in MAP over using baseline expansion with \u03b1 = 0.5. The yaxis gives the Robustness Index (RI). Each curve through uncircled points shows the RI/MAP tradeoff using the simple small-\u03b1 strategy (see text) as \u03b1 decreases from 0.5 to zero in the direction of the arrow. Circled points represent the tradeoffs obtained by resampling feedback for \u03b1 = 0.5. Collection N Base-FB RS-FB n\u2212 RI n\u2212 RI TREC 1&2 103 26 +0.495 15 +0.709 TREC 7 46 14 +0.391 10 +0.565 TREC 8 44 12 +0.455 12 +0.455 wt10g 91 48 -0.055 39 +0.143 Combined 284 100 +0.296 76 +0.465 Table 2: Comparison of robustness index (RI) for baseline feedback (Base-FB) vs. resampling feedback (RS-FB). Also shown are the actual number of queries hurt by feedback (n\u2212) for each method and collection. Queries for which initial average precision was negligible (\u2264 0.01) were ignored, giving the remaining query count in column N. of \u22121.0, when all queries are hurt by the feedback method, to +1.0 when all queries are helped. The RI measure does not take into account the magnitude or distribution of the amount of change across the set Q. However, it is easy to understand as a general indication of robustness. One obvious way to improve the worst-case performance of feedback is simply to use a smaller fixed \u03b1 interpolation parameter, such as \u03b1 = 0.3, placing less weight on the (possibly risky) feedback model and more on the original query. We call this the \u2018small-\u03b1\" strategy. Since we are also reducing the potential gains when the feedback model is \u2018right\", however, we would expect some trade-off between average precision and robustness. We therefore compared the precision/robustness trade-off between our resampling feedback algorithm, and the simple small-\u03b1 method. The results are summarized in Figure 3. In the figure, the curve for each topic set interpolates between trade-off points, beginning at x=0, where \u03b1 = 0.5, and continuing in the direction of the arrow as \u03b1 decreases and the original query is given more and more weight. As expected, robustness continuously increases as we move along the curve, but mean average precision generally drops as the gains from feedback are eliminated. For comparison, the performance of resampling feedback at \u03b1 = 0.5 is shown for each collection as the circled point. Higher and to the right is better. This figure shows that resampling feedback gives a somewhat better trade-off than the small-\u03b1 approach for 3 of the 4 collections. Figure 4: Histogram showing improved robustness of resampling feedback (RS-FB) over baseline feedback (Base-FB) for all datasets combined. Queries are binned by % change in AP compared to the unexpanded query. Collection DS + QV DS + No QV TREC 1& (+5.86%) (+1.88%) (-) (+1.43%) (-4.02%) (-) (-0.49%) (-3.23%) (-) (-4.16%) (-9.46%) (-) Table 3: Comparison of resampling feedback using document sampling (DS) with (QV) and without (No QV) combining feedback models from multiple query variants. Table 2 gives the Robustness Index scores for Base-FB and RS-FB. The RS-FB feedback method obtained higher robustness than Base-FB on three of the four topic sets, with only slightly worse performance on TREC-8. A more detailed view showing the distribution over relative changes in AP is given by the histogram in Figure 4. Compared to Base-FB, the RS-FB method achieves a noticable reduction in the number of queries significantly hurt by expansion (i.e. where AP is hurt by 25% or more), while preserving positive gains in AP. 3.5 Effect of query and document sampling methods Given our algorithm\"s improved robustness seen in Section 3.4, an important question is what component of our system is responsible. Is it the use of document re-sampling, the use of multiple query variants, or some other factor? The results in Table 3 suggest that the model combination based on query variants may be largely account for the improved robustness. When query variants are turned off and the original query is used by itself with document sampling, there is little net change in average precision, a small decrease in P10 for 3 out of the 4 topic sets, but a significant drop in robustness for all topic sets. In two cases, the RI measure drops by more than 50%. We also examined the effect of the document sampling method on retrieval effectiveness, using two different strategies. The \u2018uniform weighting\" strategy ignored the relevance scores from the initial retrieval and gave each document in the top k the same probability of selection. In contrast, the \u2018relevance-score weighting\" strategy chose documents with probability proportional to their relevance scores. In this way, documents that were more highly ranked were more likely to be selected. Results are shown in Table 4. The relevance-score weighting strategy performs better overall, with significantly higher RI and P10 scores on 3 of the 4 topic sets. The difference in average precision between the methods, however, is less marked. This suggests that uniform weighting acts to increase variance in retrieval results: when initial average precision is high, there are many relevant documents in the top k and uniform sampling may give a more representative relevance model than focusing on the highly-ranked items. On the other hand, when initial precision is low, there are few relevant documents in the bottom ranks and uniform sampling mixes in more of the non-relevant documents. For space reasons we only summarize our findings on sample size here. The number of samples has some effect on precision when less than 10, but performance stabilizes at around 15 to 20 samples. We used 30 samples for our experiments. Much beyond this level, the additional benefits of more samples decrease as the initial score distribution is more closely fit and the processing time increases. 3.6 The effect of resampling on expansion term quality Ideally, a retrieval model should not require a stopword list when estimating a model of relevance: a robust statistical model should down-weight stopwords automatically depending on context. Stopwords can harm feedback if selected as feedback terms, because they are typically poor discriminators and waste valuable term slots. In practice, however, because most term selection methods resemble a tf \u00b7 idf type of weighting, terms with low idf but very high tf can sometimes be selected as expansion term candidates. This happens, for example, even with the Relevance Model approach that is part of our baseline feedback. To ensure as strong a baseline as possible, we use a stoplist for all experiments reported here. If we turn off the stopword list, however, we obtain results such as those shown in Table 5 where four of the top ten baseline feedback terms for TREC topic 60 (said, but, their, not) are stopwords using the BaseFB method. (The top 100 expansion terms were selected to generate this example.) Indri\"s method attempts to address the stopword problem by applying an initial step based on Ponte to select less-common terms that have high log-odds of being in the top-ranked documents compared to the whole collection. Nevertheless, this does not overcome the stopword problem completely, especially as the number of feedback terms grows. Using resampling feedback, however, appears to mitigate Collection QV + Uniform QV + Relevance-score weighting weighting TREC 1& (-5.46%) (-1.97%) (+14.09%) (-0.23%) (+3.70%) (+18.17%) (+0.04%) (+5.34%) (+0.00%) (+7.63%) (+10.45%) (+399.5%) Table 4: Comparison of uniform and relevance-weighted document sampling. The percentage change compared to uniform sampling is shown in parentheses. QV indicates that query variants were used in both runs. Baseline FB p(wi|R) Resampling FB p(wi|R) said 0.055 court 0.026 court 0.055 pay 0.018 pay 0.034 federal 0.012 but 0.026 education 0.011 employees 0.024 teachers 0.010 their 0.024 employees 0.010 not 0.023 case 0.010 federal 0.021 their 0.009 workers 0.020 appeals 0.008 education 0.020 union 0.007 Table 5: Feedback term quality when a stoplist is not used. Feedback terms for TREC topic 60: merit pay vs seniority. the effect of stopwords automatically. In the example of Table 5, resampling feedback leaves only one stopword (their) in the top ten. We observed similar feedback term behavior across many other topics. The reason for this effect appears to be the interaction of the term selection score with the top-m term cutoff. While the presence and even proportion of particular stopwords is fairly stable across different document samples, their relative position in the top-m list is not, as sets of documents with varying numbers of better, lower-frequency term candidates are examined for each sample. As a result, while some number of stopwords may appear in each sampled document set, any given stopword tends to fall below the cutoff for multiple samples, leading to its classification as a high-variance, low-weight feature. 4. RELATED WORK Our approach is related to previous work from several areas of information retrieval and machine learning. Our use of query variation was inspired by the work of YomTov et al. , Carpineto et al. , and Amati et al. , among others. These studies use the idea of creating multiple subqueries and then examining the nature of the overlap in the documents and/or expansion terms that result from each subquery. Model combination is performed using heuristics. In particular, the studies of Amati et al. and Carpineto et al. investigated combining terms from individual distributional methods using a term-reranking combination heuristic. In a set of TREC topics they found wide average variation in the rank-distance of terms from different expansion methods. Their combination method gave modest positive improvements in average precision. The idea of examining the overlap between lists of suggested terms has also been used in early query expansion approaches. Xu and Croft\"s method of Local Context Analysis (LCA) includes a factor in the empirically-derived weighting formula that causes expansion terms to be preferred that have connections to multiple query terms. On the document side, recent work by Zhou & Croft explored the idea of adding noise to documents, re-scoring them, and using the stability of the resulting rankings as an estimate of query difficulty. This is related to our use of document sampling to estimate the risk of the feedback model built from the different sets of top-retrieved documents. Sakai et al. proposed an approach to improving the robustness of pseudo-relevance feedback using a method they call selective sampling. The essence of their method is that they allow skipping of some top-ranked documents, based on a clustering criterion, in order to select a more varied and novel set of documents later in the ranking for use by a traditional pseudo-feedback method. Their study did not find significant improvements in either robustness (RI) or MAP on their corpora. Greiff, Morgan and Ponte explored the role of variance in term weighting. In a series of simulations that simplified the problem to 2-feature documents, they found that average precision degrades as term frequency variance - high noiseincreases. Downweighting terms with high variance resulted in improved average precision. This seems in accord with our own findings for individual feedback models. Estimates of output variance have recently been used for improved text classification. Lee et al. used queryspecific variance estimates of classifier outputs to perform improved model combination. Instead of using sampling, they were able to derive closed-form expressions for classifier variance by assuming base classifiers using simple types of inference networks. Ando and Zhang proposed a method that they call structural feedback and showed how to apply it to query expansion for the TREC Genomics Track. They used r query variations to obtain R different sets Sr of top-ranked documents that have been intersected with the top-ranked documents obtained from the original query qorig. For each Si, the normalized centroid vector \u02c6wi of the documents is calculated. Principal component analysis (PCA) is then applied to the \u02c6wi to obtain the matrix \u03a6 of H left singular vectors \u03c6h that are used to obtain the new, expanded query qexp = qorig + \u03a6T \u03a6qorig. (7) In the case H = 1, we have a single left singular vector \u03c6: qexp = qorig + (\u03c6T qorig)\u03c6 so that the dot product \u03c6T qorig is a type of dynamic weight on the expanded query that is based on the similarity of the original query to the expanded query. The use of variance as a feedback model quality measure occurs indirectly through the application of PCA. It would be interesting to study the connections between this approach and our own modelfitting method. Finally, in language modeling approaches to feedback, Tao and Zhai describe a method for more robust feedback that allows each document to have a different feedback \u03b1. The feedback weights are derived automatically using regularized EM. A roughly equal balance of query and expansion model is implied by their EM stopping condition. They propose tailoring the stopping parameter \u03b7 based on a function of some quality measure of feedback documents. 5. CONCLUSIONS We have presented a new approach to pseudo-relevance feedback based on document and query sampling. The use of sampling is a very flexible and powerful device and is motivated by our general desire to extend current models of retrieval by estimating the risk or variance associated with the parameters or output of retrieval processes. Such variance estimates, for example, may be naturally used in a Bayesian framework for improved model estimation and combination. Applications such as selective expansion may then be implemented in a principled way. While our study uses the language modeling approach as a framework for experiments, we make few assumptions about the actual workings of the feedback algorithm. We believe it is likely that any reasonably effective baseline feedback algorithm would benefit from our approach. Our results on standard TREC collections show that our framework improves the robustness of a strong baseline feedback method across a variety of collections, without sacrificing average precision. It also gives small but consistent gains in top10 precision. In future work, we envision an investigation into how varying the set of sampling methods used and the number of samples controls the trade-off between robustness, accuracy, and efficiency.", "body1": "Uncertainty is an inherent feature of information retrieval. Not only do we not know the queries that will be presented to our retrieval algorithm ahead of time, but the user\"s information need may be vague or incompletely specified by these queries. Current algorithms for pseudo-relevance feedback (PRF) tend to follow the same basic method whether we use vector space-based algorithms such as Rocchio\"s formula , or more recent language modeling approaches such as Relevance Models . Therefore, we propose a method for estimating uncertainty associated with an individual feedback model in terms of a posterior distribution over language models. For example, an expected parameter vector conditioned on the query observation is formed from top-retrieved documents, which are treated as training strings (see , p. 62). Figure 1: Estimating the uncertainty of the feedback model for a single query. In Sections 2.1-2.5 we describe a general method for estimating a probability distribution over the set of possible language models. 2.1 Modeling Feedback Uncertainty Given a query Q and a collection C, we assume a probabilistic retrieval system that assigns a real-valued document score f(D, Q) to each document D in C, such that the score is proportional to the estimated probability of relevance. We treat the feedback algorithm as a black box and assume that the inputs to the feedback algorithm are the original query and the corresponding top-retrieved documents, with a score being given to each document. The score of a document D with respect to a query Q and collection C is given by p(Q|D) with respect to language models \u02c6\u03b8Q and \u02c6\u03b8D estimated for the query and document respectively. Our sample space is the set of all possible language models LF that may be output as feedback models. Bootstrap sampling allows us to simulate the approximate effect of perturbing the parameters within the black box feedback algorithm by perturbing the inputs to that algorithm in a systematic way, while making no assumptions about the nature of the feedback algorithm. Specifically, we sample k documents with replacement from DQ(k, C), and calculate an expansion language model \u03b8b using the black box feedback method. First, we want to improve the quality of individual feedback models by smoothing out variation when the baseline feedback model is unstable. foo2-401.map-Dim:Size:12*12units,gaussianneighborhood (a) Topic 401 Foreign minorities, Germany foo2-402.map-Dim:Size:12*12units,gaussianneighborhood (b) Topic 402 Behavioral genetics foo2-459.map-Dim:Size:12*12units,gaussianneighborhood (c) Topic 459 When can a lender foreclose on property Figure 2: Visualization of expansion language model variance using self-organizing maps, showing the distribution of language models that results from resampling the inputs to the baseline expansion method. Each point in our sample space is a language model, which typically has several thousand dimensions. 2.5 Fitting a posterior feedback distribution After obtaining feedback model samples by resampling the feedback model inputs, we estimate the feedback distribution. Note that for this step we are re-using the existing retrieved documents and not performing additional queries. Given the parameters of an N-dimensional Dirichlet distribution Dir(\u03b1) the mean \u03bc and mode x vectors are easy to calculate and are given respectively by \u03bci = \u03b1iP \u03b1i (2) and xi = \u03b1i\u22121P \u03b1i\u2212N . (We found the mode to give slightly better performance.) 2.6 Query variants We use the following methods for generating variants of the original query. No-expansion Use only the original query. Leave-one-out A single term is left out of the original query. Single-term A single term is chosen from the original query. This assumes that only one aspect of the query, namely, that represented by the term, is most important. After generating a variant of the original query, we combine it with the original query using a weight \u03b1SUB so that we do not stray too \u2018far\". In this section we present results confirming the usefulness of estimating a feedback model distribution from weighted resampling of top-ranked documents, and of combining the feedback models obtained from different small changes in the original query. 3.1 General method We evaluated performance on a total of 350 queries derived from four sets of TREC topics: 51-200 (TREC-1&2), 351-400 (TREC-7), 401-450 (TREC-8), and 451-550 (wt10g, TREC-9&10). Indexing and retrieval was performed using the Indri system in the Lemur toolkit . Because we found that the baseline (Indri) expansion method performed better using a stopword list with the feedback model, all experiments used a stoplist of 419 common English words. We chose the Indri method because it gives a consistently strong baseline, is based on a language modeling approach, and is simple to experiment with. Indri\"s expansion method first calculates a log-odds ratio o(v) for each potential expansion term v given by o(v) = log p(v|D) p(v|C) (4) over all documents D containing v, in collection C. Then, the expansion term candidates are sorted by descending o(v), and the top m are chosen. By default we used the top 50 documents for feedback and the top 20 expansion terms, with the feedback interpolation parameter \u03b1 = 0.5 unless otherwise stated. For each query, we obtained a set of B feedback models using the Indri baseline. Each feedback model contained 20 terms. We call our method \u2018resampling expansion\" and denote it as RS-FB here. We observe several trends in this table. 3.4 Retrieval robustness We use the term robustness to mean the worst-case average precision performance of a feedback algorithm. To evaluate robustness in this study, we use a very simple measure called the robustness index (RI)3 . (a) TREC 1&2 (upper curve); TREC 8 (lower curve) (b) TREC 7 (upper curve); wt10g (lower curve) Figure 3: The trade-off between robustness and average precision for different corpora. Collection N Base-FB RS-FB n\u2212 RI n\u2212 RI TREC 1&2 103 26 +0.495 15 +0.709 TREC 7 46 14 +0.391 10 +0.565 TREC 8 44 12 +0.455 12 +0.455 wt10g 91 48 -0.055 39 +0.143 Combined 284 100 +0.296 76 +0.465 Table 2: Comparison of robustness index (RI) for baseline feedback (Base-FB) vs. resampling feedback (RS-FB). of \u22121.0, when all queries are hurt by the feedback method, to +1.0 when all queries are helped. We call this the \u2018small-\u03b1\" strategy. Collection DS + QV DS + No QV TREC 1& (+5.86%) (+1.88%) (-) (+1.43%) (-4.02%) (-) (-0.49%) (-3.23%) (-) (-4.16%) (-9.46%) (-) Table 3: Comparison of resampling feedback using document sampling (DS) with (QV) and without (No QV) combining feedback models from multiple query variants. Table 2 gives the Robustness Index scores for Base-FB and RS-FB. A more detailed view showing the distribution over relative changes in AP is given by the histogram in Figure 4. Compared to Base-FB, the RS-FB method achieves a noticable reduction in the number of queries significantly hurt by expansion (i.e. 3.5 Effect of query and document sampling methods Given our algorithm\"s improved robustness seen in Section 3.4, an important question is what component of our system is responsible. We also examined the effect of the document sampling method on retrieval effectiveness, using two different strategies. The relevance-score weighting strategy performs better overall, with significantly higher RI and P10 scores on 3 of the 4 topic sets. For space reasons we only summarize our findings on sample size here. 3.6 The effect of resampling on expansion term quality Ideally, a retrieval model should not require a stopword list when estimating a model of relevance: a robust statistical model should down-weight stopwords automatically depending on context. This happens, for example, even with the Relevance Model approach that is part of our baseline feedback. Using resampling feedback, however, appears to mitigate Collection QV + Uniform QV + Relevance-score weighting weighting TREC 1& (-5.46%) (-1.97%) (+14.09%) (-0.23%) (+3.70%) (+18.17%) (+0.04%) (+5.34%) (+0.00%) (+7.63%) (+10.45%) (+399.5%) Table 4: Comparison of uniform and relevance-weighted document sampling. Feedback terms for TREC topic 60: merit pay vs seniority. the effect of stopwords automatically. Our approach is related to previous work from several areas of information retrieval and machine learning. investigated combining terms from individual distributional methods using a term-reranking combination heuristic. On the document side, recent work by Zhou & Croft explored the idea of adding noise to documents, re-scoring them, and using the stability of the resulting rankings as an estimate of query difficulty. Greiff, Morgan and Ponte explored the role of variance in term weighting. Estimates of output variance have recently been used for improved text classification. Ando and Zhang proposed a method that they call structural feedback and showed how to apply it to query expansion for the TREC Genomics Track. The feedback weights are derived automatically using regularized EM.", "body2": "Uncertainty is an inherent feature of information retrieval. In this way, retrieval algorithms may attempt to quantify the risk or uncertainty associated with their output rankings, or improve the stability or precision of their internal calculations. Our main hypothesis in this paper is that estimating the uncertainty in feedback is useful and leads to better individual feedback models and more robust combined models. A model\"s weight combines two complementary factors: the model\"s probability of generating the query, and the variance of the model, with high-variance models getting lower weight. For example, an expected parameter vector conditioned on the query observation is formed from top-retrieved documents, which are treated as training strings (see , p. 62). Figure 1: Estimating the uncertainty of the feedback model for a single query. In Sections 2.6 and 2.7 we summarize how different query samples are used to generate multiple feedback models, which are then combined. Our specific query method is given in Section 3. For a specific framework for experiments, we use the language modeling (LM) approach for information retrieval . To incorporate feedback in the LM approach, we assume a model-based scheme in which our goal is take the query and resulting ranked documents DQ(k, C) as input, and output an expansion language model \u02c6\u03b8E, which is then interpolated with the original query model \u02c6\u03b8Q: \u02c6\u03b8New = (1 \u2212 \u03b1) \u00b7 \u02c6\u03b8Q + \u03b1 \u00b7 \u02c6\u03b8E (1) This includes the possibility of \u03b1 = 1 where the original query mode is completely replaced by the feedback model. To accomplish this, we apply a widely-used simulation technique called bootstrap sampling (, p. 474) on the input parameters, namely, the set of top-retrieved documents. Bootstrap sampling allows us to simulate the approximate effect of perturbing the parameters within the black box feedback algorithm by perturbing the inputs to that algorithm in a systematic way, while making no assumptions about the nature of the feedback algorithm. 2.3 Justification for a sampling approach The rationale for our sampling approach has two parts. For example, a model may be weighted by its prediction confidence, estimated as a function of the variability of the posterior around the model. 2.4 Visualizing feedback distributions Before describing how we fit and use the Dirichlet distribution over feedback models, it is instructive to view some examples of actual feedback model distributions that result from bootstrap sampling the top-retrieved documents from different TREC topics. In such cases, the mode or mean of the feedback distribution often performs significantly better than the baseline (and in a smaller proportion of cases, significantly worse). In practice, we can restrict the calculation to the vocabulary of the top-retrieved documents, instead of the entire collection. Note that for this step we are re-using the existing retrieved documents and not performing additional queries. (3) We can then choose the language model at the mean or the mode of the posterior as the final enhanced feedback model. We leave this for future study. We selected three simple methods that cover complimentary assumptions about the query. The assumption is that the given terms are a complete description of the information need. The assumption is that one of the query terms is a noise term. Single-term A single term is chosen from the original query. This assumes that only one aspect of the query, namely, that represented by the term, is most important. The prior probability of a word\"s membership in the relevant class is given by the probability of the original query in the entire enhanced expansion model. In this section we present results confirming the usefulness of estimating a feedback model distribution from weighted resampling of top-ranked documents, and of combining the feedback models obtained from different small changes in the original query. For example, wt10g documents are Web pages with a wide variety of subjects and styles while TREC-1&2 documents are more homogeneous news articles. For example, the initial query for topic 404 is: #combine(ireland peace talks) We performed Krovetz stemming for all experiments. This method first selects terms using a log-odds calculation described by Ponte , but assigns final term weights using Lavrenko\"s relevance model. In this study, it achieves an average gain in MAP of 17.25% over the four collections. This relevance model is then combined with the original query using linear interpolation, weighted by a parameter \u03b1. We also include recall at 1,000 documents. For these experiments, B = 30 and k = 50. We used the methods described in Section 2 to estimate an enhanced feedback model from the Dirichlet posterior distribution for each query variant, and to combine the feedback models from all the query variants. Results from applying both the baseline expansion method (Base-FB) and resampling expansion (RS-FB) are shown in Table 1. Finally, both Base-FB and RS-FB also consistently improved recall over using no expansion, with Base-FB achieving better recall than RS-FB for all topic sets. Ideally, a robust feedback method would never perform worse than using the original query, while often performing better using the expansion. Improvement shown for BaseFB and RS-FB is relative to using no expansion. Circled points represent the tradeoffs obtained by resampling feedback for \u03b1 = 0.5. Queries for which initial average precision was negligible (\u2264 0.01) were ignored, giving the remaining query count in column N. One obvious way to improve the worst-case performance of feedback is simply to use a smaller fixed \u03b1 interpolation parameter, such as \u03b1 = 0.3, placing less weight on the (possibly risky) feedback model and more on the original query. Queries are binned by % change in AP compared to the unexpanded query. Collection DS + QV DS + No QV TREC 1& (+5.86%) (+1.88%) (-) (+1.43%) (-4.02%) (-) (-0.49%) (-3.23%) (-) (-4.16%) (-9.46%) (-) Table 3: Comparison of resampling feedback using document sampling (DS) with (QV) and without (No QV) combining feedback models from multiple query variants. The RS-FB feedback method obtained higher robustness than Base-FB on three of the four topic sets, with only slightly worse performance on TREC-8. A more detailed view showing the distribution over relative changes in AP is given by the histogram in Figure 4. where AP is hurt by 25% or more), while preserving positive gains in AP. In two cases, the RI measure drops by more than 50%. Results are shown in Table 4. On the other hand, when initial precision is low, there are few relevant documents in the bottom ranks and uniform sampling mixes in more of the non-relevant documents. Much beyond this level, the additional benefits of more samples decrease as the initial score distribution is more closely fit and the processing time increases. In practice, however, because most term selection methods resemble a tf \u00b7 idf type of weighting, terms with low idf but very high tf can sometimes be selected as expansion term candidates. Nevertheless, this does not overcome the stopword problem completely, especially as the number of feedback terms grows. Baseline FB p(wi|R) Resampling FB p(wi|R) said 0.055 court 0.026 court 0.055 pay 0.018 pay 0.034 federal 0.012 but 0.026 education 0.011 employees 0.024 teachers 0.010 their 0.024 employees 0.010 not 0.023 case 0.010 federal 0.021 their 0.009 workers 0.020 appeals 0.008 education 0.020 union 0.007 Table 5: Feedback term quality when a stoplist is not used. Feedback terms for TREC topic 60: merit pay vs seniority. As a result, while some number of stopwords may appear in each sampled document set, any given stopword tends to fall below the cutoff for multiple samples, leading to its classification as a high-variance, low-weight feature. and Carpineto et al. Xu and Croft\"s method of Local Context Analysis (LCA) includes a factor in the empirically-derived weighting formula that causes expansion terms to be preferred that have connections to multiple query terms. Their study did not find significant improvements in either robustness (RI) or MAP on their corpora. This seems in accord with our own findings for individual feedback models. Instead of using sampling, they were able to derive closed-form expressions for classifier variance by assuming base classifiers using simple types of inference networks. Finally, in language modeling approaches to feedback, Tao and Zhai describe a method for more robust feedback that allows each document to have a different feedback \u03b1. They propose tailoring the stopping parameter \u03b7 based on a function of some quality measure of feedback documents.", "introduction": "Uncertainty is an inherent feature of information retrieval. Not only do we not know the queries that will be presented to our retrieval algorithm ahead of time, but the user\"s information need may be vague or incompletely specified by these queries. Even if the query were perfectly specified, language in the collection documents is inherently complex and ambiguous and matching such language effectively is a formidable problem by itself. With this in mind, we wish to treat many important quantities calculated by the retrieval system, whether a relevance score for a document, or a weight for a query expansion term, as random variables whose true value is uncertain but where the uncertainty about the true value may be quantified by replacing the fixed value with a probability distribution over possible values. In this way, retrieval algorithms may attempt to quantify the risk or uncertainty associated with their output rankings, or improve the stability or precision of their internal calculations. Current algorithms for pseudo-relevance feedback (PRF) tend to follow the same basic method whether we use vector space-based algorithms such as Rocchio\"s formula , or more recent language modeling approaches such as Relevance Models . First, a set of top-retrieved documents is obtained from an initial query and assumed to approximate a set of relevant documents. Next, a single feedback model vector is computed according to some sort of average, centroid, or expectation over the set of possibly-relevant document models. For example, the document vectors may be combined with equal weighting, as in Rocchio, or by query likelihood, as may be done using the Relevance Model1 . The use of an expectation is reasonable for practical and theoretical reasons, but by itself ignores potentially valuable information about the risk of the feedback model. Our main hypothesis in this paper is that estimating the uncertainty in feedback is useful and leads to better individual feedback models and more robust combined models. Therefore, we propose a method for estimating uncertainty associated with an individual feedback model in terms of a posterior distribution over language models. To do this, we systematically vary the inputs to the baseline feedback method and fit a Dirichlet distribution to the output. We use the posterior mean or mode as the improved feedback model estimate. This process is shown in Figure 1. As we show later, the mean and mode may vary significantly from the single feedback model proposed by the baseline method. We also perform model combination using several improved feedback language models obtained by a small number of new queries sampled from the original query. A model\"s weight combines two complementary factors: the model\"s probability of generating the query, and the variance of the model, with high-variance models getting lower weight. For example, an expected parameter vector conditioned on the query observation is formed from top-retrieved documents, which are treated as training strings (see , p. 62). Figure 1: Estimating the uncertainty of the feedback model for a single query.", "conclusion": "We have presented a new approach to pseudo-relevance feedback based on document and query sampling.. The use of sampling is a very flexible and powerful device and is motivated by our general desire to extend current models of retrieval by estimating the risk or variance associated with the parameters or output of retrieval processes.. Such variance estimates, for example, may be naturally used in a Bayesian framework for improved model estimation and combination.. Applications such as selective expansion may then be implemented in a principled way.. While our study uses the language modeling approach as a framework for experiments, we make few assumptions about the actual workings of the feedback algorithm.. We believe it is likely that any reasonably effective baseline feedback algorithm would benefit from our approach.. Our results on standard TREC collections show that our framework improves the robustness of a strong baseline feedback method across a variety of collections, without sacrificing average precision.. It also gives small but consistent gains in top10 precision.. In future work, we envision an investigation into how varying the set of sampling methods used and the number of samples controls the trade-off between robustness, accuracy, and efficiency."}
{"id": "I-76", "keywords": ["negoti", "extend abduct", "relax", "logic program"], "title": "Negotiation by Abduction and Relaxation", "abstract": "This paper studies a logical framework for automated negotiation between two agents. We suppose an agent who has a knowledge base represented by a logic program. Then, we introduce methods of constructing counter-proposals in response to proposals made by an agent. To this end, we combine the techniques of extended abduction in artificial intelligence and relaxation in cooperative query answering for databases. These techniques are respectively used for producing conditional proposals and neighborhood proposals in the process of negotiation. We provide a negotiation protocol based on the exchange of these proposals and develop procedures for computing new proposals.", "references": ["Arguments, dialogue, and negotiation", "A new framework for knowledge revision of abductive agents through their interaction", "Repeated negotiation of logic programs", "Cooperative query answering via type abstraction hierarchy", "Negotiating logic programs", "Relaxation as a platform for cooperative answering", "Classical negation in logic programs and disjunctive databases", "Abductive framework for nonmonotonic theory change", "The role of abduction in logic programming", "Adaptive agent negotiation via argumentation", "Logical foundation of negotiation: outcome, concession and adaptation", "A theory and methodology of inductive learning", "Agents that reason and negotiate by arguing", "An abductive logic programming architecture for negotiating agents", "An abductive framework for computing knowledge base updates"], "full_text": "1. INTRODUCTION Automated negotiation has been received increasing attention in multi-agent systems, and a number of frameworks have been proposed in different contexts (, for instance). Negotiation usually proceeds in a series of rounds and each agent makes a proposal at every round. An agent that received a proposal responds in two ways. One is a critique which is a remark as to whether or not (parts of) the proposal is accepted. The other is a counter-proposal which is an alternative proposal made in response to a previous proposal . To see these proposals in one-to-one negotiation, suppose the following negotiation dialogue between a buyer agent B and a seller agent S. (Bi (or Si) represents an utterance of B (or S) in the i-th round.) B1: I want to buy a personal computer of the brand b1, with the specification of CPU:1GHz, Memory:512MB, HDD: 80GB, and a DVD-RW driver. USD. S1: We can provide a PC with the requested specification if you pay for it by cash. In this case, however, service points are not added for this special discount. B2: I cannot pay it by cash. S2: In a normal price, USD. B3: I cannot accept the price. USD. S3: We can provide another computer with the requested specification, except that it is made by the brand b2. USD. B4: I do not want a PC of the brand b2. Instead, I can downgrade a driver from DVD-RW to CD-RW in my initial proposal. S4: Ok, I accept your offer. In this dialogue, in response to the opening proposal B1, the counter-proposal S1 is returned. In the rest of the dialogue, B2, B3, S4 are critiques, while S2, S3, B4 are counterproposals. Critiques are produced by evaluating a proposal in a knowledge base of an agent. In contrast, making counter-proposals involves generating an alternative proposal which is more favorable to the responding agent than the original one. It is known that there are two ways of producing counterproposals: extending the initial proposal or amending part of the initial proposal. According to , the first type appears in the dialogue: A: I propose that you provide me with service X. B: I propose that I provide you with service X if you provide me with service Z. The second type is in the dialogue: A: I propose that I provide you with service Y if you provide me with service X. B: I propose that I provide you with service X if you provide me with service Z. A negotiation proceeds by iterating such give-andtake dialogues until it reaches an agreement/disagreement. In those dialogues, agents generate (counter-)proposals by reasoning on their own goals or objectives. The objective of the agent A in the above dialogues is to obtain service X. The agent B proposes conditions to provide the service. In the process of negotiation, however, it may happen that agents are obliged to weaken or change their initial goals to reach a negotiated compromise. 978-81--7-5 (RPS) IFAAMAS a buyer agent and a seller agent presented above, a buyer agent changes its initial goal by downgrading a driver from DVD-RW to CD-RW. Such behavior is usually represented as specific meta-knowledge of an agent or specified as negotiation protocols in particular problems. Currently, there is no computational logic for automated negotiation which has general inference rules for producing (counter-)proposals. The purpose of this paper is to mechanize a process of building (counter-)proposals in one-to-one negotiation dialogues. We suppose an agent who has a knowledge base represented by a logic program. We then introduce methods for generating three different types of proposals. First, we use the technique of extended abduction in artificial intelligence to construct a conditional proposal as an extension of the original one. Second, we use the technique of relaxation in cooperative query answering for databases to construct a neighborhood proposal as an amendment of the original one. Third, combining extended abduction and relaxation, conditional neighborhood proposals are constructed as amended extensions of the original proposal. We develop a negotiation protocol between two agents based on the exchange of these counter-proposals and critiques. We also provide procedures for computing proposals in logic programming. This paper is organized as follows. Section 2 introduces a logical framework used in this paper. Section 3 presents methods for constructing proposals, and provides a negotiation protocol. Section 4 provides methods for computing proposals in logic programming. Section 5 discusses related works, and Section 6 concludes the paper. 2. PRELIMINARIES Logic programs considered in this paper are extended disjunctive programs (EDP) . An EDP (or simply a program) is a set of rules of the form: L1 ; \u00b7 \u00b7 \u00b7 ; Ll \u2190 Ll+1 , . . . , Lm, not Lm+1 , . . . , not Ln (n \u2265 m \u2265 l \u2265 0) where each Li is a positive/negative literal, i.e., A or \u00acA for an atom A, and not is negation as failure (NAF). not L is called an NAF-literal. The symbol ; represents disjunction. The left-hand side of the rule is the head, and the right-hand side is the body. For each rule r of the above form, head(r), body+ (r) and body\u2212 (r) denote the sets of literals {L1, . . . , Ll}, {Ll+1, . . . , Lm}, and {Lm+1, . . . , Ln}, respectively. Also, not body\u2212 (r) denotes the set of NAF-literals {not Lm+1, . . . , not Ln}. A disjunction of literals and a conjunction of (NAF-)literals in a rule are identified with its corresponding sets of literals. A rule r is often written as head(r) \u2190 body+ (r), not body\u2212 (r) or head(r) \u2190 body(r) where body(r) = body+ (r)\u222anot body\u2212 (r). A rule r is disjunctive if head(r) contains more than one literal. A rule r is an integrity constraint if head(r) = \u2205; and r is a fact if body(r) = \u2205. A program is NAF-free if no rule contains NAF-literals. Two rules/literals are identified with respect to variable renaming. A substitution is a mapping from variables to terms \u03b8 = {x1/t1, . . . , xn/tn}, where x1, . . . , xn are distinct variables and each ti is a term distinct from xi. Given a conjunction G of (NAF-)literals, G\u03b8 denotes the conjunction obtained by applying \u03b8 to G. A program, rule, or literal is ground if it contains no variable. A program P with variables is a shorthand of its ground instantiation Ground(P), the set of ground rules obtained from P by substituting variables in P by elements of its Herbrand universe in every possible way. The semantics of an EDP is defined by the answer set semantics . Let Lit be the set of all ground literals in the language of a program. Suppose a program P and a set of literals S(\u2286 Lit). Then, the reduct P S is the program which contains the ground rule head(r) \u2190 body+ (r) iff there is a rule r in Ground(P) such that body\u2212 (r)\u2229S = \u2205. Given an NAF-free EDP P, Cn(P) denotes the smallest set of ground literals which is (i) closed under P, i.e., for every ground rule r in Ground(P), body(r) \u2286 Cn(P) implies head(r) \u2229 Cn(P) = \u2205; and (ii) logically closed, i.e., it is either consistent or equal to Lit. Given an EDP P and a set S of literals, S is an answer set of P if S = Cn(P S ). A program has none, one, or multiple answer sets in general. An answer set is consistent if it is not Lit. A program P is consistent if it has a consistent answer set; otherwise, P is inconsistent. Abductive logic programming introduces a mechanism of hypothetical reasoning to logic programming. An abductive framework used in this paper is the extended abduction introduced by Inoue and Sakama . An abductive program is a pair P, H where P is an EDP and H is a set of literals called abducibles. When a literal L \u2208 H contains variables, any instance of L is also an abducible. An abductive program P, H is consistent if P is consistent. Throughout the paper, abductive programs are assumed to be consistent unless stated otherwise. Let G = L1, . . . , Lm, not Lm+1, . . . , not Ln be a conjunction, where all variables in G are existentially quantified at the front and range-restricted, i.e., every variable in Lm+1, . . . , Ln appears in L1, . . . , Lm. A set S of ground literals satisfies the conjunction G if { L1\u03b8, . . . , Lm\u03b8 } \u2286 S and { Lm+1\u03b8, . . . , Ln\u03b8 }\u2229 S = \u2205 for some ground instance G\u03b8 with a substitution \u03b8. Let P, H be an abductive program and G a conjunction as above. A pair (E, F) is an explanation of an observation G in P, H if1 1. (P \\ F) \u222a E has an answer set which satisfies G, 2. (P \\ F) \u222a E is consistent, 3. E and F are sets of ground literals such that E \u2286 H\\P and F \u2286 H \u2229 P. When (P \\ F) \u222a E has an answer set S satisfying the above three conditions, S is called a belief set of an abductive program P, H satisfying G (with respect to (E, F)). Note that if P has a consistent answer set S satisfying G, S is also a belief set of P, H satisfying G with respect to (E, F) = (\u2205, \u2205). Extended abduction introduces/removes hypotheses to/from a program to explain an observation. Note that normal abduction (as in ) considers only introducing hypotheses to explain an observation. An explanation (E, F) of an observation G is called minimal if for any explanation (E , F ) of G, E \u2286 E and F \u2286 F imply E = E and F = F. Example 2.1. Consider the abductive program P, H : P : flies(x) \u2190 bird(x), not ab(x) , ab(x) \u2190 broken-wing(x) , bird(tweety) \u2190 , bird(opus) \u2190 , broken-wing(tweety) \u2190 . H : broken-wing(x) . The observation G = flies(tweety) has the minimal explanation (E, F) = (\u2205, {broken-wing(tweety)}). This defines credulous explanations . Skeptical explanations are used in . The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 3. NEGOTIATION 3.1 Conditional Proposals by Abduction We suppose an agent who has a knowledge base represented by an abductive program P, H . A program P consists of two types of knowledge, belief B and desire D, where B represents objective knowledge of an agent, while D represents subjective knowledge in general. We define P = B \u222a D, but do not distinguish B and D if such distinction is not important in the context. In contrast, abducibles H are used for representing permissible conditions to make a compromise in the process of negotiation. Definition 3.1. A proposal G is a conjunction of literals and NAF-literals: L1, . . . , Lm, not Lm+1, . . . , not Ln where every variable in G is existentially quantified at the front and range-restricted. In particular, G is called a critique if G = accept or G = reject where accept and reject are the reserved propositions. A counter-proposal is a proposal made in response to a proposal. Definition 3.2. A proposal G is accepted in an abductive program P, H if P has an answer set satisfying G. When a proposal is not accepted, abduction is used for seeking conditions to make it acceptable. Definition 3.3. Let P, H be an abductive program and G a proposal. If (E, F) is a minimal explanation of G\u03b8 for some substitution \u03b8 in P, H , the conjunction G : G\u03b8, E, not F is called a conditional proposal (for G), where E, not F represents the conjunction: A1, . . . , Ak, not Ak+1, . . . , not Al for E = {A1, . . . , Ak} and F = { Ak+1, . . . , Al }. Proposition 3.1. Let P, H be an abductive program and G a proposal. If G is a conditional proposal, there is a belief set S of P, H satisfying G . Proof. When G = G\u03b8, E, not F, (P \\ F) \u222a E has a consistent answer set S satisfying G\u03b8 and E \u2229 F = \u2205. In this case, S satisfies G\u03b8, E, not F. A conditional proposal G provides a minimal requirement for accepting the proposal G. If G\u03b8 has multiple minimal explanations, several conditional proposals exist accordingly. When (E, F) = (\u2205, \u2205), a conditional proposal is used as a new proposal made in response to the proposal G. Example 3.1. An agent seeks a position of a research assistant at the computer department of a university with the condition that the salary is at least 50,000 USD per year. The agent makes his/her request as the proposal:2 G = assist(compt dept), salary(x), x \u2265 50, 000. The university has the abductive program P, H : P : salary(40, 000) \u2190 assist(compt dept), not has PhD, salary(60, 000) \u2190 assist(compt dept), has PhD, salary(50, 000) \u2190 assist(math dept), salary(55, 000) \u2190 system admin(compt dept), For notational convenience, we often include mathematical (in)equations in proposals/programs. They are written by literals, for instance, x \u2265 y by geq(x, y) with a suitable definition of the predicate geq. employee(x) \u2190 assist(x), employee(x) \u2190 system admin(x), assist(compt dept); assist(math dept) ; system admin(compt dept) \u2190, H : has PhD, where available positions are represented by disjunction. According to P, the base salary of a research assistant at the computer department is 40,000 USD, but if he/she has PhD, it is 60,000 USD. In this case, (E, F) = ({has PhD}, \u2205) becomes the minimal explanation of G\u03b8 = assist(compt dept), salary(60, 000) with \u03b8 = { x/60, 000 }. Then, the conditional proposal made by the university becomes assist(compt dept), salary(60, 000), has PhD . 3.2 Neighborhood Proposals by Relaxation When a proposal is unacceptable, an agent tries to construct a new counter-proposal by weakening constraints in the initial proposal. We use techniques of relaxation for this purpose. Relaxation is used as a technique of cooperative query answering in databases . When an original query fails in a database, relaxation expands the scope of the query by relaxing the constraints in the query. This allows the database to return neighborhood answers which are related to the original query. We use the technique for producing proposals in the process of negotiation. Definition 3.4. Let P, H be an abductive program and G a proposal. Then, G is relaxed to G in the following three ways: Anti-instantiation: Construct G such that G \u03b8 = G for some substitution \u03b8. Dropping conditions: Construct G such that G \u2282 G. Goal replacement: If G is a conjunction G1, G2, where G1 and G2 are conjunctions, and there is a rule L \u2190 G1 in P such that G1\u03b8 = G1 for some substitution \u03b8, then build G as L\u03b8, G2. Here, L\u03b8 is called a replaced literal. In each case, every variable in G is existentially quantified at the front and range-restricted. Anti-instantiation replaces constants (or terms) with fresh variables. Dropping conditions eliminates some conditions in a proposal. Goal replacement replaces the condition G1 in G with a literal L\u03b8 in the presence of a rule L \u2190 G1 in P under the condition G1\u03b8 = G1. All these operations generalize proposals in different ways. Each G obtained by these operations is called a relaxation of G. It is worth noting that these operations are also used in the context of inductive generalization . The relaxed proposal can produce new offers which are neighbor to the original proposal. Definition 3.5. Let P, H be an abductive program and G a proposal. 1. Let G be a proposal obtained by anti-instantiation. If P has an answer set S which satisfies G \u03b8 for some substitution \u03b8 and G \u03b8 = G, G \u03b8 is called a neighborhood proposal by anti-instantiation. 2. Let G be a proposal obtained by dropping conditions. If P has an answer set S which satisfies G \u03b8 for some substitution \u03b8, The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 3. Let G be a proposal obtained by goal replacement. For a replaced literal L \u2208 G and a rule H \u2190 B in P such that L = H\u03c3 and (G \\ {L}) \u222a B\u03c3 = G for some substitution \u03c3, put G = (G \\ {L}) \u222a B\u03c3. If P has an answer set S which satisfies G \u03b8 for some substitution \u03b8, G \u03b8 is called a neighborhood proposal by goal replacement. Example 3.2. (cont. Example 3.1) Given the proposal G = assist(compt dept), salary(x), x \u2265 50, 000, \u2022 G1 = assist(w), salary(x), x \u2265 50, 000 is produced by substituting compt dept with a variable w. As G1\u03b81 = assist(math dept), salary(50, 000) with \u03b81 = { w/math dept } is satisfied by an answer set of P, G1\u03b81 becomes a neighborhood proposal by anti-instantiation. \u2022 G2 = assist(compt dept), salary(x) is produced by dropping the salary condition x \u2265 50, 000. As G2\u03b82 = assist(compt dept), salary(40, 000) with \u03b82 = { x/40, 000 } is satisfied by an answer set of P, G2\u03b82 becomes a neighborhood proposal by dropping conditions. \u2022 G3 = employee(compt dept), salary(x), x \u2265 50, 000 is produced by replacing assist(compt dept) with employee(compt dept) using the rule employee(x) \u2190 assist(x) in P. By G3 and the rule employee(x) \u2190 system admin(x) in P, G3 = sys admin(compt dept), salary(x), x \u2265 50, 000 is produced. As G3 \u03b83 = sys admin(compt dept), salary(55, 000) with \u03b83 = { x/55, 000 } is satisfied by an answer set of P, G3 \u03b83 becomes a neighborhood proposal by goal replacement. Finally, extended abduction and relaxation are combined to produce conditional neighborhood proposals. Definition 3.6. Let P, H be an abductive program and G a proposal. 1. Let G be a proposal obtained by either anti-instantiation or dropping conditions. If (E, F) is a minimal explanation of G \u03b8(= G) for some substitution \u03b8, the conjunction G \u03b8, E, not F is called a conditional neighborhood proposal by anti-instantiation/dropping conditions. 2. Let G be a proposal obtained by goal replacement. Suppose G as in Definition 3.5(3). If (E, F) is a minimal explanation of G \u03b8 for some substitution \u03b8, the conjunction G \u03b8, E, not F is called a conditional neighborhood proposal by goal replacement. A conditional neighborhood proposal reduces to a neighborhood proposal when (E, F) = (\u2205, \u2205). 3.3 Negotiation Protocol A negotiation protocol defines how to exchange proposals in the process of negotiation. This section presents a negotiation protocol in our framework. We suppose one-to-one negotiation between two agents who have a common ontology and the same language for successful communication. Definition 3.7. A proposal L1, ..., Lm, not Lm+1, ..., not Ln violates an integrity constraint \u2190 body+ (r), not body\u2212 (r) if for any substitution \u03b8, there is a substitution \u03c3 such that body+ (r)\u03c3 \u2286 { L1\u03b8, . . . , Lm\u03b8 }, body\u2212 (r)\u03c3\u2229{ L1\u03b8, . . . , Lm\u03b8 } = \u2205, and body\u2212 (r)\u03c3 \u2286 { Lm+1\u03b8, . . . , Ln\u03b8 }. Integrity constraints are conditions which an agent should satisfy, so that they are used to explain why an agent does not accept a proposal. A negotiation proceeds in a series of rounds. Each i-th round (i \u2265 1) consists of a proposal Gi 1 made by one agent Ag1 and another proposal Gi 2 made by the other agent Ag2. Definition 3.8. Let P1, H1 be an abductive program of an agent Ag1 and Gi 2 a proposal made by Ag2 at the i-th round. A critique set of Ag1 (at the i-th round) is a set CSi 1(P1, Gj 2) = CSi\u22121 1 (P1, Gj\u22121 2 ) \u222a { r | r is an integrity constraint in P1 and Gj 2 violates r } where j = i \u2212 1 or i, and CS0 1 (P1, G0 2) = CS1 1 (P1, G0 2) = \u2205. A critique set of an agent Ag1 accumulates integrity constraints which are violated by proposals made by another agent Ag2. CSi 2(P2, Gj 1) is defined in the same manner. Definition 3.9. Let Pk, Hk be an abductive program of an agent Agk and Gj a proposal, which is not a critique, made by any agent at the j(\u2264 i)-th round. A negotiation set of Agk (at the i-th round) is a triple NSi k = (Si c, Si n, Si cn), where Si c is the set of conditional proposals, Si n is the set of neighborhood proposals, and Si cn is the set of conditional neighborhood proposals, produced by Gj and Pk, Hk . A negotiation set represents the space of possible proposals made by an agent. Si x (x \u2208 {c, n, cn}) accumulates proposals produced by Gj (1 \u2264 j \u2264 i) according to Definitions 3.3, 3.5, and 3.6. Note that an agent can construct counter-proposals by modifying its own previous proposals or another agent\"s proposals. An agent Agk accumulates proposals that are made by Agk but are rejected by another agent, in the failed proposal set FP i k (at the i-th round), where FP 0 k = \u2205. Suppose two agents Ag1 and Ag2 who have abductive programs P1, H1 and P2, H2 , respectively. Given a proposal G1 1 which is satisfied by an answer set of P1, a negotiation starts. In response to the proposal Gi 1 made by Ag1 at the i-th round, Ag2 behaves as follows. 1. If Gi 1 = accept, an agreement is reached and negotiation ends in success. 2. Else if Gi 1 = reject, put FP i 2 = FPi\u22121 2 \u222a{Gi\u22121 2 } where {G0 2} = \u2205. Proceed to the step 4(b). 3. Else if P2 has an answer set satisfying Gi 1, Ag2 returns Gi 2 = accept to Ag1. Negotiation ends in success. 4. Otherwise, Ag2 behaves as follows. Put FP i 2 = FPi\u22121 2 . (a) If Gi 1 violates an integrity constraint in P2, return the critique Gi 2 = reject to Ag1, together with the critique set CSi 2(P2, Gi 1). (b) Otherwise, construct NSi 2 as follows. (i) Produce Si c. Let \u03bc(Si c) = { p | p \u2208 Si c \\ FPi 2 and p satisfies the constraints in CSi 1(P1, Gi\u22121 2 )}. If \u03bc(Si c) = \u2205, select one from \u03bc(Si c) and propose it as Gi 2 to Ag1; otherwise, go to (ii). (ii) Produce Si n. If \u03bc(Si n) = \u2205, select one from \u03bc(Si n) and propose it as Gi 2 to Ag1; otherwise, go to (iii). (iii) Produce Si cn. If \u03bc(Si cn) = \u2205, select one from \u03bc(Si cn) and propose it as Gi 2 to Ag1; otherwise, negotiation ends in failure. This means that Ag2 can make no counter-proposal or every counterproposal made by Ag2 is rejected by Ag1. In the step 4(a), Ag2 rejects the proposal Gi 1 and returns the reason of rejection as a critique set. This helps for Ag1 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) in preparing a next counter-proposal. In the step 4(b), Ag2 constructs a new proposal. In its construction, Ag2 should take care of the critique set CSi 1(P1, Gi\u22121 2 ), which represents integrity constraints, if any, accumulated in previous rounds, that Ag1 must satisfy. Also, FP i 2 is used for removing proposals which have been rejected. Construction of Si x (x \u2208 {c, n, cn}) in NSi 2 is incrementally done by adding new counter-proposals produced by Gi 1 or Gi\u22121 2 to Si\u22121 x . For instance, Si n in NSi 2 is computed as Si n = Si\u22121 n \u222a{ p | p is a neighborhood proposal made by Gi 1 } \u222a { p | p is a neighborhood proposal made by Gi\u22121 2 }, where S0 n = \u2205. That is, Si n is constructed from Si\u22121 n by adding new proposals which are obtained by modifying the proposal Gi 1 made by Ag1 at the i-th round or modifying the proposal Gi\u22121 2 made by Ag2 at the (i \u2212 1)-th round. Si and Si cn are obtained as well. In the above protocol, an agent produces Si c at first, secondly Si n, and finally Si cn. This strategy seeks conditions which satisfy the given proposal, prior to neighborhood proposals which change the original one. Another strategy, which prefers neighborhood proposals to conditional ones, is also considered. Conditional neighborhood proposals are to be considered in the last place, since they differ from the original one to the maximal extent. The above protocol produces the candidate proposals in Si x for each x \u2208 {c, n, cn} at once. We can consider a variant of the protocol in which each proposal in Si x is constructed one by one (see Example 3.3). The above protocol is repeatedly applied to each one of the two negotiating agents until a negotiation ends in success/failure. Formally, the above negotiation protocol has the following properties. Theorem 3.2. Let Ag1 and Ag2 be two agents having abductive programs P1, H1 and P2, H2 , respectively. 1. If P1, H1 and P2, H2 are function-free (i.e., both Pi and Hi contain no function symbol), any negotiation will terminate. 2. If a negotiation terminates with agreement on a proposal G, both P1, H1 and P2, H2 have belief sets satisfying G. Proof. 1. When an abductive program is function-free, abducibles and negotiation sets are both finite. Moreover, if a proposal is once rejected, it is not proposed again by the function \u03bc. Thus, negotiation will terminate in finite steps. 2. When a proposal G is made by Ag1, P1, H1 has a belief set satisfying G. If the agent Ag2 accepts the proposal G, it is satisfied by an answer set of P2 which is also a belief set of P2, H2 . Example 3.3. Suppose a buying-selling situation in the introduction. A seller agent has the abductive program Ps, Hs in which Ps consists of belief Bs and desire Ds: Bs : pc(b1, 1G, 512M, 80G) ; pc(b2, 1G, 512M, 80G) \u2190,(1) dvd-rw ; cd-rw \u2190, (2) Ds : normal price \u2190 pc(b1, 1G, 512M, 80G), dvd-rw, (3) normal price \u2190 pc(b1, 1G, 512M, 80G), cd-rw, (4) normal price \u2190 pc(b2, 1G, 512M, 80G), dvd-rw, (5) price(x) \u2190 normal price(x), add point, (6) price(x \u2217 0.9) \u2190 normal price(x), pay cash, not add point,(7) add point \u2190, (8) Hs : add point, pay cash. Here, (1) and (2) represent selection of products. The atom pc(b1, 1G, 512M, 80G) represents that the seller agent has a PC of the brand b1 such that CPU is 1GHz, memory is 512MB, and HDD is 80GB. Prices of products are represented as desire of the seller. The rules (3) - (5) are normal prices of products. A normal price is a selling price on the condition that service points are added (6). On the other hand, a discount price is applied if the paying method is cash and no service point is added (7). The fact (8) represents the addition of service points. This service would be withdrawn in case of discount prices, so add point is specified as an abducible. A buyer agent has the abductive program Pb, Hb in which Pb consists of belief Bb and desire Db: Bb : drive \u2190 dvd-rw, (9) drive \u2190 cd-rw, (10) price(x) \u2190, (11) Db : pc(b1, 1G, 512M, 80G) \u2190, (12) dvd-rw \u2190, (13) cd-rw \u2190 not dvd-rw, (14) \u2190 pay cash, (15) \u2190 price(x), x > (16) Hb : dvd-rw. Rules (12) - (16) are the buyer\"s desire. Among them, (15) and (16) impose constraints for buying a PC. A DVD-RW is specified as an abducible which is subject to concession. (1st round) First, the following proposal is given by the buyer agent: G1 b : pc(b1, 1G, 512M, 80G), dvd-rw, price(x), x \u2264. As Ps has no answer set which satisfies G1 b , the seller agent cannot accept the proposal. The seller takes an action of making a counter-proposal and performs abduction. As a result, the seller finds the minimal explanation (E, F) = ({ pay cash }, { add point }) which explains G1 b \u03b81 with \u03b81 = { x/ }. The seller constructs the conditional proposal: G1 s : pc(b1, 1G, 512M, 80G), dvd-rw, price, pay cash, not add point and offers it to the buyer. (2nd round) The buyer does not accept G1 s because he/she cannot pay it by cash (15). The buyer then returns the critique G2 b = reject to the seller, together with the critique set CS2 b (Pb, G1 s) = {(15)}. In response to this, the seller tries to make another proposal which satisfies the constraint in this critique set. As G1 s is stored in FP 2 s and no other conditional proposal satisfying the buyer\"s requirement exists, the seller produces neighborhood proposals. He/she relaxes G1 b by dropping x \u2264 in the condition, and produces pc(b1, 1G, 512M, 80G), dvd-rw, price(x). As Ps has an answer set which satisfies G2 s : pc(b1, 1G, 512M, 80G), dvd-rw, price The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) the seller offers G2 s as a new counter-proposal. (3rd round) The buyer does not accept G2 s because he/USD (16). The buyer again returns the critique G3 b = reject to the seller, together with the critique set CS3 b (Pb, G2 s) = CS2 b (Pb, G1 s) \u222a {(16)}. The seller then considers another proposal by replacing b1 with a variable w, G1 b now becomes pc(w, 1G, 512M, 80G), dvd-rw, price(x), x \u2264. As Ps has an answer set which satisfies G3 s : pc(b2, 1G, 512M, 80G), dvd-rw, price, the seller offers G3 s as a new counter-proposal. (4th round) The buyer does not accept G3 s because a PC of the brand b2 is out of his/her interest and Pb has no answer set satisfying G3 s. Then, the buyer makes a concession by changing his/her original goal. The buyer relaxes G1 b by goal replacement using the rule (9) in Pb, and produces pc(b1, 1G, 512M, 80G), drive, price(x), x \u2264. Using (10), the following proposal is produced: pc(b1, 1G, 512M, 80G), cd-rw, price(x), x \u2264. As Pb \\ { dvd-rw } has a consistent answer set satisfying the above proposal, the buyer proposes the conditional neighborhood proposal G4 b : pc(b1, 1G, 512M, 80G), cd-rw, not dvd-rw, price(x), x \u2264 to the seller agent. Since Ps also has an answer set satisfying G4 b , the seller accepts it and sends the message G4 s = accept to the buyer. Thus, the negotiation ends in success. 4. COMPUTATION In this section, we provide methods of computing proposals in terms of answer sets of programs. We first introduce some definitions from . Definition 4.1. Given an abductive program P, H , the set UR of update rules is defined as: UR = { L \u2190 not L, L \u2190 not L | L \u2208 H } \u222a { +L \u2190 L | L \u2208 H \\ P } \u222a { \u2212L \u2190 not L | L \u2208 H \u2229 P } , where L, +L, and \u2212L are new atoms uniquely associated with every L \u2208 H. The atoms +L and \u2212L are called update atoms. By the definition, the atom L becomes true iff L is not true. The pair of rules L \u2190 not L and L \u2190 not L specify the situation that an abducible L is true or not. When p(x) \u2208 H and p(a) \u2208 P but p(t) \u2208 P for t = a, the rule +L \u2190 L precisely becomes +p(t) \u2190 p(t) for any t = a. In this case, the rule is shortly written as +p(x) \u2190 p(x), x = a. Generally, the rule becomes +p(x) \u2190 p(x), x = t1, . . . , x = tn for n such instances. The rule +L \u2190 L derives the atom +L if an abducible L which is not in P is to be true. In contrast, the rule \u2212L \u2190 not L derives the atom \u2212L if an abducible L which is in P is not to be true. Thus, update atoms represent the change of truth values of abducibles in a program. That is, +L means the introduction of L, while \u2212L means the deletion of L. When an abducible L contains variables, the associated update atom +L or \u2212L is supposed to have exactly the same variables. In this case, an update atom is semantically identified with its ground instances. The set of all update atoms associated with the abducibles in H is denoted by UH, and UH = UH+ \u222a UH\u2212 where UH+ (resp. UH\u2212 ) is the set of update atoms of the form +L (resp. \u2212L). Definition 4.2. Given an abductive program P, H , its update program UP is defined as the program UP = (P \\ H) \u222a UR . An answer set S of UP is called U-minimal if there is no answer set T of UP such that T \u2229 UH \u2282 S \u2229 UH. By the definition, U-minimal answer sets exist whenever UP has answer sets. Update programs are used for computing (minimal) explanations of an observation. Given an observation G as a conjunction of literals and NAF-literals possibly containing variables, we introduce a new ground literal O together with the rule O \u2190 G. In this case, O has an explanation (E, F) iff G has the same explanation. With this replacement, an observation is assumed to be a ground literal without loss of generality. In what follows, E+ = { +L | L \u2208 E } and F \u2212 = { \u2212L | L \u2208 F } for E \u2286 H and F \u2286 H. Proposition 4.1. Let P, H be an abductive program, UP its update program, and G a ground literal representing an observation. Then, a pair (E, F) is an explanation of G iff UP \u222a { \u2190 not G } has a consistent answer set S such that E+ = S \u2229 UH+ and F\u2212 = S \u2229 UH\u2212 . In particular, (E, F) is a minimal explanation iff S is a U-minimal answer set. Example 4.1. To explain the observation G = flies(t) in the program P of Example 2.1, first construct the update program UP of P:3 UP : flies(x) \u2190 bird(x), not ab(x), ab(x) \u2190 broken-wing(x) , bird(t) \u2190 , bird(o) \u2190 , broken-wing(x) \u2190 not broken-wing(x), broken-wing(x) \u2190 not broken-wing(x), +broken-wing(x) \u2190 broken-wing(x), x = t , \u2212broken-wing(t) \u2190 not broken-wing(t) . Next, consider the program UP \u222a { \u2190 not flies(t) }. It has the single U-minimal answer set: S = { bird(t), bird(o), flies(t), flies(o), broken-wing(t), broken-wing(o), \u2212broken-wing(t) }. The unique minimal explanation (E, F) = (\u2205, {broken-wing(t)}) of G is expressed by the update atom \u2212broken-wing(t) in S \u2229 UH\u2212 Proposition 4.2. Let P, H be an abductive program and G a ground literal representing an observation. If P \u222a { \u2190 not G } has a consistent answer set S, G has the minimal explanation (E, F) = (\u2205, \u2205) and S satisfies G. Now we provide methods for computing (counter-)proposals. First, conditional proposals are computed as follows. input : an abductive program P, H , a proposal G; output : a set Sc of proposals. If G is a ground literal, compute its minimal explanation (E, F) in P, H using the update program. Put G, E, not F in Sc. Else if G is a conjunction possibly containing variables, consider the abductive program t represents tweety and o represents opus. The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) P \u222a{ O \u2190 G }, H with a ground literal O. Compute a minimal explanation of O in P \u222a { O \u2190 G }, H using its update program. If O has a minimal explanation (E, F) with a substitution \u03b8 for variables in G, put G\u03b8, E, not F in Sc. Next, neighborhood proposals are computed as follows. input : an abductive program P, H , a proposal G; output : a set Sn of proposals. % neighborhood proposals by anti-instantiation; Construct G by anti-instantiation. For a ground literal O, if P \u222a { O \u2190 G } \u222a { \u2190 not O } has a consistent answer set satisfying G \u03b8 with a substitution \u03b8 and G \u03b8 = G, put G \u03b8 in Sn. % neighborhood proposals by dropping conditions; Construct G by dropping conditions. If G is a ground literal and the program P \u222a { \u2190 not G } has a consistent answer set, put G in Sn. Else if G is a conjunction possibly containing variables, do the following. For a ground literal O, if P \u222a{ O \u2190 G }\u222a{ \u2190 not O } has a consistent answer set satisfying G \u03b8 with a substitution \u03b8, put G \u03b8 in Sn. % neighborhood proposals by goal replacement; Construct G by goal replacement. If G is a ground literal and there is a rule H \u2190 B in P such that G = H\u03c3 and B\u03c3 = G for some substitution \u03c3, put G = B\u03c3. If P \u222a { \u2190 not G } has a consistent answer set satisfying G \u03b8 with a substitution \u03b8, put G \u03b8 in Sn. Else if G is a conjunction possibly containing variables, do the following. For a replaced literal L \u2208 G , if there is a rule H \u2190 B in P such that L = H\u03c3 and (G \\ {L}) \u222a B\u03c3 = G for some substitution \u03c3, put G = (G \\ {L}) \u222a B\u03c3. For a ground literal O, if P \u222a { O \u2190 G } \u222a { \u2190 not O } has a consistent answer set satisfying G \u03b8 with a substitution \u03b8, put G \u03b8 in Sn. Theorem 4.3. The set Sc (resp. Sn) computed above coincides with the set of conditional proposals (resp. neighborhood proposals). Proof. The result for Sc follows from Definition 3.3 and Proposition 4.1. The result for Sn follows from Definition 3.5 and Proposition 4.2. Conditional neighborhood proposals are computed by combining the above two procedures. Those proposals are computed at each round. Note that the procedure for computing Sn contains some nondeterministic choices. For instance, there are generally several candidates of literals to relax in a proposal. Also, there might be several rules in a program for the usage of goal replacement. In practice, an agent can prespecify literals in a proposal for possible relaxation or rules in a program for the usage of goal replacement. 5. RELATED WORK As there are a number of literature on automated negotiation, this section focuses on comparison with negotiation frameworks based on logic and argumentation. Sadri et al. use abductive logic programming as a representation language of negotiating agents. Agents negotiate using common dialogue primitives, called dialogue moves. Each agent has an abductive logic program in which a sequence of dialogues are specified by a program, a dialogue protocol is specified as constraints, and dialogue moves are specified as abducibles. The behavior of agents is regulated by an observe-think-act cycle. Once a dialogue move is uttered by an agent, another agent that observed the utterance thinks and acts using a proof procedure. Their approach and ours both employ abductive logic programming as a platform of agent reasoning, but the use of it is quite different. First, they use abducibles to specify dialogue primitives of the form tell(utterer, receiver, subject, identifier, time), while we use abducibles to specify arbitrary permissible hypotheses to construct conditional proposals. Second, a program pre-specifies a plan to carry out in order to achieve a goal, together with available/missing resources in the context of resource-exchanging problems. This is in contrast with our method in which possible counter-proposals are newly constructed in response to a proposal made by an agent. Third, they specify a negotiation policy inside a program (as integrity constraints), while we give a protocol independent of individual agents. They provide an operational model that completely specifies the behavior of agents in terms of agent cycle. We do not provide such a complete specification of the behavior of agents. Our primary interest is to mechanize construction of proposals. Bracciali and Torroni formulate abductive agents that have knowledge in abductive logic programs. To explain an observation, two agents communicate by exchanging integrity constraints. In the process of communication, an agent can revise its own integrity constraints according to the information provided by the other agent. A set IC of integrity constraints relaxes a set IC (or IC tightens IC ) if any observation that can be proved with respect to IC can also be proved with respect to IC . For instance, IC : \u2190 a, b, c relaxes IC : \u2190 a, b. Thus, they use relaxation for weakening the constraints in an abductive logic program. In contrast, we use relaxation for weakening proposals and three different relaxation methods, anti-instantiation, dropping conditions, and goal replacement, are considered. Their goal is to explain an observation by revising integrity constraints of an agent through communication, while we use integrity constraints for communication to explain critiques and help other agents in making counter-proposals. Meyer et al. introduce a logical framework for negotiating agents. They introduce two different modes of negotiation: concession and adaptation. They provide rational postulates to characterize negotiated outcomes between two agents, and describe methods for constructing outcomes. They provide logical conditions for negotiated outcomes to satisfy, but they do not describe a process of negotiation nor negotiation protocols. Moreover, they represent agents by classical propositional theories, which is different from our abductive logic programming framework. Foo et al. model one-to-one negotiation as a one-time encounter between two extended logic programs. An agent offers an answer set of its program, and their mutual deal is regarded as a trade on their answer sets. Starting from the initial agreement set S\u2229T for an answer set S of an agent and an answer set T of another agent, each agent extends this set to reflect its own demand while keeping consistency with demand of the other agent. Their algorithm returns new programs having answer sets which are consistent with each other and keep the agreement set. The work is extended to repeated encounters in . In their framework, two agents exchange answer sets to produce a common belief set, which is different from our framework of exchanging proposals. The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) on argumentation. An advantage of argumentation-based negotiation is that it constructs a proposal with arguments supporting the proposal . The existence of arguments is useful to convince other agents of reasons why an agent offers (counter-)proposals or returns critiques. Parsons et al. develop a logic of argumentation-based negotiation among BDI agents. In one-to-one negotiation, an agent A generates a proposal together with its arguments, and passes it to another agent B. The proposal is evaluated by B which attempts to build arguments against it. If it conflicts with B\"s interest, B informs A of its objection by sending back its attacking argument. In response to this, A tries to find an alternative way of achieving its original objective, or a way of persuading B to drop its objection. If either type of argument can be found, A will submit it to B. If B finds no reason to reject the new proposal, it will be accepted and the negotiation ends in success. Otherwise, the process is iterated. In this negotiation processes, the agent A never changes its original objective, so that negotiation ends in failure if A fails to find an alternative way of achieving the original objective. In our framework, when a proposal is rejected by another agent, an agent can weaken or change its objective by abduction and relaxation. Our framework does not have a mechanism of argumentation, but reasons for critiques can be informed by responding critique sets. Kakas and Moraitis propose a negotiation protocol which integrates abduction within an argumentation framework. A proposal contains an offer corresponding to the negotiation object, together with supporting information representing conditions under which this offer is made. Supporting information is computed by abduction and is used for constructing conditional arguments during the process of negotiation. In their negotiation protocol, when an agent cannot satisfy its own goal, the agent considers the other agent\"s goal and searches for conditions under which the goal is acceptable. Our present approach differs from theirs in the following points. First, they use abduction to seek conditions to support arguments, while we use abduction to seek conditions for proposals to accept. Second, in their negotiation protocol, counter-proposals are chosen among candidates based on preference knowledge of an agent at meta-level, which represents policy under which an agent uses its object-level decision rules according to situations. In our framework, counter-proposals are newly constructed using abduction and relaxation. The method of construction is independent of particular negotiation protocols. As , abduction or abductive logic programming used in negotiation is mostly based on normal abduction. In contrast, our approach is based on extended abduction which can not only introduce hypotheses but remove them from a program. This is another important difference. Relaxation and neighborhood query answering are devised to make databases cooperative with their users . In this sense, those techniques have the spirit similar to cooperative problem solving in multi-agent systems. As far as the authors know, however, there is no study which applies those technique to agent negotiation. 6. CONCLUSION In this paper we proposed a logical framework for negotiating agents. To construct proposals in the process of negotiation, we combined the techniques of extended abduction and relaxation. It was shown that these two operations are used for general inference rules in producing proposals. We developed a negotiation protocol between two agents based on exchange of proposals and critiques, and provided procedures for computing proposals in abductive logic programming. This enables us to realize automated negotiation on top of the existing answer set solvers. The present framework does not have a mechanism of selecting an optimal (counter-)proposal among different alternatives. To compare and evaluate proposals, an agent must have preference knowledge of candidate proposals. Further elaboration to maximize the utility of agents is left for future study.", "body1": "Automated negotiation has been received increasing attention in multi-agent systems, and a number of frameworks have been proposed in different contexts (, for instance). To see these proposals in one-to-one negotiation, suppose the following negotiation dialogue between a buyer agent B and a seller agent S. (Bi (or Si) represents an utterance of B (or S) in the i-th round.) B2: I cannot pay it by cash. S2: In a normal price, USD. B3: I cannot accept the price. S3: We can provide another computer with the requested specification, except that it is made by the brand b2. USD. B4: I do not want a PC of the brand b2. S4: Ok, I accept your offer. In this dialogue, in response to the opening proposal B1, the counter-proposal S1 is returned. Critiques are produced by evaluating a proposal in a knowledge base of an agent. It is known that there are two ways of producing counterproposals: extending the initial proposal or amending part of the initial proposal. In those dialogues, agents generate (counter-)proposals by reasoning on their own goals or objectives. The purpose of this paper is to mechanize a process of building (counter-)proposals in one-to-one negotiation dialogues. This paper is organized as follows. Logic programs considered in this paper are extended disjunctive programs (EDP) . A rule r is disjunctive if head(r) contains more than one literal. The semantics of an EDP is defined by the answer set semantics . Given an NAF-free EDP P, Cn(P) denotes the smallest set of ground literals which is (i) closed under P, i.e., for every ground rule r in Ground(P), body(r) \u2286 Cn(P) implies head(r) \u2229 Cn(P) = \u2205; and (ii) logically closed, i.e., it is either consistent or equal to Lit. Abductive logic programming introduces a mechanism of hypothetical reasoning to logic programming. An abductive program P, H is consistent if P is consistent. Let P, H be an abductive program and G a conjunction as above. When (P \\ F) \u222a E has an answer set S satisfying the above three conditions, S is called a belief set of an abductive program P, H satisfying G (with respect to (E, F)). Example 2.1. H : broken-wing(x) . The observation G = flies(tweety) has the minimal explanation (E, F) = (\u2205, {broken-wing(tweety)}). This defines credulous explanations . 3.1 Conditional Proposals by Abduction We suppose an agent who has a knowledge base represented by an abductive program P, H . Definition 3.1. When a proposal is not accepted, abduction is used for seeking conditions to make it acceptable. Definition 3.3. Proposition 3.1. Proof. When (E, F) = (\u2205, \u2205), a conditional proposal is used as a new proposal made in response to the proposal G. Example 3.1. The agent makes his/her request as the proposal:2 G = assist(compt dept), salary(x), x \u2265 50, 000. The university has the abductive program P, H : P : salary(40, 000) \u2190 assist(compt dept), not has PhD, salary(60, 000) \u2190 assist(compt dept), has PhD, salary(50, 000) \u2190 assist(math dept), salary(55, 000) \u2190 system admin(compt dept), For notational convenience, we often include mathematical (in)equations in proposals/programs. employee(x) \u2190 assist(x), employee(x) \u2190 system admin(x), assist(compt dept); assist(math dept) ; system admin(compt dept) \u2190, H : has PhD, where available positions are represented by disjunction. According to P, the base salary of a research assistant at the computer department is 40,000 USD, but if he/she has PhD, it is 60,000 USD. 3.2 Neighborhood Proposals by Relaxation When a proposal is unacceptable, an agent tries to construct a new counter-proposal by weakening constraints in the initial proposal. Definition 3.4. Goal replacement: If G is a conjunction G1, G2, where G1 and G2 are conjunctions, and there is a rule L \u2190 G1 in P such that G1\u03b8 = G1 for some substitution \u03b8, then build G as L\u03b8, G2. In each case, every variable in G is existentially quantified at the front and range-restricted. Anti-instantiation replaces constants (or terms) with fresh variables. 1. If P has an answer set S which satisfies G \u03b8 for some substitution \u03b8, The Sixth Intl. Example 3.2. \u2022 G3 = employee(compt dept), salary(x), x \u2265 50, 000 is produced by replacing assist(compt dept) with employee(compt dept) using the rule employee(x) \u2190 assist(x) in P. By G3 and the rule employee(x) \u2190 system admin(x) in P, G3 = sys admin(compt dept), salary(x), x \u2265 50, 000 is produced. Finally, extended abduction and relaxation are combined to produce conditional neighborhood proposals. Definition 3.6. Suppose G as in Definition 3.5(3). A conditional neighborhood proposal reduces to a neighborhood proposal when (E, F) = (\u2205, \u2205). 3.3 Negotiation Protocol A negotiation protocol defines how to exchange proposals in the process of negotiation. Definition 3.7. A negotiation proceeds in a series of rounds. Definition 3.8. Definition 3.9. A negotiation set represents the space of possible proposals made by an agent. Suppose two agents Ag1 and Ag2 who have abductive programs P1, H1 and P2, H2 , respectively. 2. 3. 4. (b) Otherwise, construct NSi 2 as follows. (i) Produce Si c. Let \u03bc(Si c) = { p | p \u2208 Si c \\ FPi 2 and p satisfies the constraints in CSi 1(P1, Gi\u22121 2 )}. If \u03bc(Si c) = \u2205, select one from \u03bc(Si c) and propose it as Gi 2 to Ag1; otherwise, go to (ii). (ii) Produce Si n. If \u03bc(Si n) = \u2205, select one from \u03bc(Si n) and propose it as Gi 2 to Ag1; otherwise, go to (iii). (iii) Produce Si cn. In the step 4(a), Ag2 rejects the proposal Gi 1 and returns the reason of rejection as a critique set. In the above protocol, an agent produces Si c at first, secondly Si n, and finally Si cn. Theorem 3.2. 1. 2. Example 3.3. Here, (1) and (2) represent selection of products. Rules (12) - (16) are the buyer\"s desire. As Ps has no answer set which satisfies G1 b , the seller agent cannot accept the proposal. (2nd round) The buyer does not accept G1 s because he/she cannot pay it by cash (15). As Ps has an answer set which satisfies G2 s : pc(b1, 1G, 512M, 80G), dvd-rw, price The Sixth Intl. (3rd round) The buyer does not accept G2 s because he/USD (16). (4th round) The buyer does not accept G3 s because a PC of the brand b2 is out of his/her interest and Pb has no answer set satisfying G3 s. Then, the buyer makes a concession by changing his/her original goal. Using (10), the following proposal is produced: pc(b1, 1G, 512M, 80G), cd-rw, price(x), x \u2264. As Pb \\ { dvd-rw } has a consistent answer set satisfying the above proposal, the buyer proposes the conditional neighborhood proposal G4 b : pc(b1, 1G, 512M, 80G), cd-rw, not dvd-rw, price(x), x \u2264 to the seller agent. In this section, we provide methods of computing proposals in terms of answer sets of programs. By the definition, the atom L becomes true iff L is not true. Generally, the rule becomes +p(x) \u2190 p(x), x = t1, . The set of all update atoms associated with the abducibles in H is denoted by UH, and UH = UH+ \u222a UH\u2212 where UH+ (resp. An answer set S of UP is called U-minimal if there is no answer set T of UP such that T \u2229 UH \u2282 S \u2229 UH. By the definition, U-minimal answer sets exist whenever UP has answer sets. Proposition 4.1. Next, consider the program UP \u222a { \u2190 not flies(t) }. The unique minimal explanation (E, F) = (\u2205, {broken-wing(t)}) of G is expressed by the update atom \u2212broken-wing(t) in S \u2229 UH\u2212 Proposition 4.2. First, conditional proposals are computed as follows. input : an abductive program P, H , a proposal G; output : a set Sc of proposals. If G is a ground literal, compute its minimal explanation (E, F) in P, H using the update program. The Sixth Intl. input : an abductive program P, H , a proposal G; output : a set Sn of proposals. % neighborhood proposals by anti-instantiation; Construct G by anti-instantiation. % neighborhood proposals by dropping conditions; Construct G by dropping conditions. % neighborhood proposals by goal replacement; Construct G by goal replacement. If P \u222a { \u2190 not G } has a consistent answer set satisfying G \u03b8 with a substitution \u03b8, put G \u03b8 in Sn. Theorem 4.3. Conditional neighborhood proposals are computed by combining the above two procedures. As there are a number of literature on automated negotiation, this section focuses on comparison with negotiation frameworks based on logic and argumentation. Sadri et al. Each agent has an abductive logic program in which a sequence of dialogues are specified by a program, a dialogue protocol is specified as constraints, and dialogue moves are specified as abducibles. Bracciali and Torroni formulate abductive agents that have knowledge in abductive logic programs. Meyer et al. Foo et al. The Sixth Intl. Supporting information is computed by abduction and is used for constructing conditional arguments during the process of negotiation. In our framework, counter-proposals are newly constructed using abduction and relaxation. Relaxation and neighborhood query answering are devised to make databases cooperative with their users .", "body2": "The other is a counter-proposal which is an alternative proposal made in response to a previous proposal . In this case, however, service points are not added for this special discount. B2: I cannot pay it by cash. S2: In a normal price, USD. USD. S3: We can provide another computer with the requested specification, except that it is made by the brand b2. USD. Instead, I can downgrade a driver from DVD-RW to CD-RW in my initial proposal. S4: Ok, I accept your offer. In the rest of the dialogue, B2, B3, S4 are critiques, while S2, S3, B4 are counterproposals. In contrast, making counter-proposals involves generating an alternative proposal which is more favorable to the responding agent than the original one. A negotiation proceeds by iterating such give-andtake dialogues until it reaches an agreement/disagreement. Currently, there is no computational logic for automated negotiation which has general inference rules for producing (counter-)proposals. We also provide procedures for computing proposals in logic programming. Section 5 discusses related works, and Section 6 concludes the paper. A rule r is often written as head(r) \u2190 body+ (r), not body\u2212 (r) or head(r) \u2190 body(r) where body(r) = body+ (r)\u222anot body\u2212 (r). A program P with variables is a shorthand of its ground instantiation Ground(P), the set of ground rules obtained from P by substituting variables in P by elements of its Herbrand universe in every possible way. Then, the reduct P S is the program which contains the ground rule head(r) \u2190 body+ (r) iff there is a rule r in Ground(P) such that body\u2212 (r)\u2229S = \u2205. A program P is consistent if it has a consistent answer set; otherwise, P is inconsistent. When a literal L \u2208 H contains variables, any instance of L is also an abducible. , Ln\u03b8 }\u2229 S = \u2205 for some ground instance G\u03b8 with a substitution \u03b8. E and F are sets of ground literals such that E \u2286 H\\P and F \u2286 H \u2229 P. An explanation (E, F) of an observation G is called minimal if for any explanation (E , F ) of G, E \u2286 E and F \u2286 F imply E = E and F = F. Consider the abductive program P, H : P : flies(x) \u2190 bird(x), not ab(x) , ab(x) \u2190 broken-wing(x) , bird(tweety) \u2190 , bird(opus) \u2190 , broken-wing(tweety) \u2190 . H : broken-wing(x) . The observation G = flies(tweety) has the minimal explanation (E, F) = (\u2205, {broken-wing(tweety)}). Skeptical explanations are used in . In contrast, abducibles H are used for representing permissible conditions to make a compromise in the process of negotiation. A proposal G is accepted in an abductive program P, H if P has an answer set satisfying G. When a proposal is not accepted, abduction is used for seeking conditions to make it acceptable. , Al }. If G is a conditional proposal, there is a belief set S of P, H satisfying G . In this case, S satisfies G\u03b8, E, not F. A conditional proposal G provides a minimal requirement for accepting the proposal G. If G\u03b8 has multiple minimal explanations, several conditional proposals exist accordingly. When (E, F) = (\u2205, \u2205), a conditional proposal is used as a new proposal made in response to the proposal G. An agent seeks a position of a research assistant at the computer department of a university with the condition that the salary is at least 50,000 USD per year. The agent makes his/her request as the proposal:2 G = assist(compt dept), salary(x), x \u2265 50, 000. They are written by literals, for instance, x \u2265 y by geq(x, y) with a suitable definition of the predicate geq. employee(x) \u2190 assist(x), employee(x) \u2190 system admin(x), assist(compt dept); assist(math dept) ; system admin(compt dept) \u2190, H : has PhD, where available positions are represented by disjunction. Then, the conditional proposal made by the university becomes assist(compt dept), salary(60, 000), has PhD . We use the technique for producing proposals in the process of negotiation. Dropping conditions: Construct G such that G \u2282 G. Here, L\u03b8 is called a replaced literal. In each case, every variable in G is existentially quantified at the front and range-restricted. Let P, H be an abductive program and G a proposal. Let G be a proposal obtained by dropping conditions. If P has an answer set S which satisfies G \u03b8 for some substitution \u03b8, G \u03b8 is called a neighborhood proposal by dropping conditions. If P has an answer set S which satisfies G \u03b8 for some substitution \u03b8, G \u03b8 is called a neighborhood proposal by goal replacement. As G2\u03b82 = assist(compt dept), salary(40, 000) with \u03b82 = { x/40, 000 } is satisfied by an answer set of P, G2\u03b82 becomes a neighborhood proposal by dropping conditions. As G3 \u03b83 = sys admin(compt dept), salary(55, 000) with \u03b83 = { x/55, 000 } is satisfied by an answer set of P, G3 \u03b83 becomes a neighborhood proposal by goal replacement. Finally, extended abduction and relaxation are combined to produce conditional neighborhood proposals. Let G be a proposal obtained by goal replacement. If (E, F) is a minimal explanation of G \u03b8 for some substitution \u03b8, the conjunction G \u03b8, E, not F is called a conditional neighborhood proposal by goal replacement. A conditional neighborhood proposal reduces to a neighborhood proposal when (E, F) = (\u2205, \u2205). We suppose one-to-one negotiation between two agents who have a common ontology and the same language for successful communication. Integrity constraints are conditions which an agent should satisfy, so that they are used to explain why an agent does not accept a proposal. Each i-th round (i \u2265 1) consists of a proposal Gi 1 made by one agent Ag1 and another proposal Gi 2 made by the other agent Ag2. CSi 2(P2, Gj 1) is defined in the same manner. A negotiation set of Agk (at the i-th round) is a triple NSi k = (Si c, Si n, Si cn), where Si c is the set of conditional proposals, Si n is the set of neighborhood proposals, and Si cn is the set of conditional neighborhood proposals, produced by Gj and Pk, Hk . An agent Agk accumulates proposals that are made by Agk but are rejected by another agent, in the failed proposal set FP i k (at the i-th round), where FP 0 k = \u2205. If Gi 1 = accept, an agreement is reached and negotiation ends in success. Proceed to the step 4(b). Negotiation ends in success. (a) If Gi 1 violates an integrity constraint in P2, return the critique Gi 2 = reject to Ag1, together with the critique set CSi 2(P2, Gi 1). (b) Otherwise, construct NSi 2 as follows. (i) Produce Si c. Let \u03bc(Si c) = { p | p \u2208 Si c \\ FPi 2 and p satisfies the constraints in CSi 1(P1, Gi\u22121 2 )}. If \u03bc(Si c) = \u2205, select one from \u03bc(Si c) and propose it as Gi 2 to Ag1; otherwise, go to (ii). (ii) Produce Si n. If \u03bc(Si n) = \u2205, select one from \u03bc(Si n) and propose it as Gi 2 to Ag1; otherwise, go to (iii). This means that Ag2 can make no counter-proposal or every counterproposal made by Ag2 is rejected by Ag1. Si and Si cn are obtained as well. Formally, the above negotiation protocol has the following properties. Let Ag1 and Ag2 be two agents having abductive programs P1, H1 and P2, H2 , respectively. If P1, H1 and P2, H2 are function-free (i.e., both Pi and Hi contain no function symbol), any negotiation will terminate. If a negotiation terminates with agreement on a proposal G, both P1, H1 and P2, H2 have belief sets satisfying G. When a proposal G is made by Ag1, P1, H1 has a belief set satisfying G. If the agent Ag2 accepts the proposal G, it is satisfied by an answer set of P2 which is also a belief set of P2, H2 . A seller agent has the abductive program Ps, Hs in which Ps consists of belief Bs and desire Ds: Bs : pc(b1, 1G, 512M, 80G) ; pc(b2, 1G, 512M, 80G) \u2190,(1) dvd-rw ; cd-rw \u2190, (2) Ds : normal price \u2190 pc(b1, 1G, 512M, 80G), dvd-rw, (3) normal price \u2190 pc(b1, 1G, 512M, 80G), cd-rw, (4) normal price \u2190 pc(b2, 1G, 512M, 80G), dvd-rw, (5) price(x) \u2190 normal price(x), add point, (6) price(x \u2217 0.9) \u2190 normal price(x), pay cash, not add point,(7) add point \u2190, (8) Hs : add point, pay cash. A buyer agent has the abductive program Pb, Hb in which Pb consists of belief Bb and desire Db: Bb : drive \u2190 dvd-rw, (9) drive \u2190 cd-rw, (10) price(x) \u2190, (11) Db : pc(b1, 1G, 512M, 80G) \u2190, (12) dvd-rw \u2190, (13) cd-rw \u2190 not dvd-rw, (14) \u2190 pay cash, (15) \u2190 price(x), x > (16) Hb : dvd-rw. (1st round) First, the following proposal is given by the buyer agent: G1 b : pc(b1, 1G, 512M, 80G), dvd-rw, price(x), x \u2264. The seller constructs the conditional proposal: G1 s : pc(b1, 1G, 512M, 80G), dvd-rw, price, pay cash, not add point and offers it to the buyer. He/she relaxes G1 b by dropping x \u2264 in the condition, and produces pc(b1, 1G, 512M, 80G), dvd-rw, price(x). on Autonomous Agents and Multi-Agent Systems (AAMAS 07) the seller offers G2 s as a new counter-proposal. As Ps has an answer set which satisfies G3 s : pc(b2, 1G, 512M, 80G), dvd-rw, price, the seller offers G3 s as a new counter-proposal. The buyer relaxes G1 b by goal replacement using the rule (9) in Pb, and produces pc(b1, 1G, 512M, 80G), drive, price(x), x \u2264. Using (10), the following proposal is produced: pc(b1, 1G, 512M, 80G), cd-rw, price(x), x \u2264. Thus, the negotiation ends in success. Given an abductive program P, H , the set UR of update rules is defined as: UR = { L \u2190 not L, L \u2190 not L | L \u2208 H } \u222a { +L \u2190 L | L \u2208 H \\ P } \u222a { \u2212L \u2190 not L | L \u2208 H \u2229 P } , where L, +L, and \u2212L are new atoms uniquely associated with every L \u2208 H. The atoms +L and \u2212L are called update atoms. In this case, the rule is shortly written as +p(x) \u2190 p(x), x = a. In this case, an update atom is semantically identified with its ground instances. Given an abductive program P, H , its update program UP is defined as the program UP = (P \\ H) \u222a UR . An answer set S of UP is called U-minimal if there is no answer set T of UP such that T \u2229 UH \u2282 S \u2229 UH. In what follows, E+ = { +L | L \u2208 E } and F \u2212 = { \u2212L | L \u2208 F } for E \u2286 H and F \u2286 H. To explain the observation G = flies(t) in the program P of Example 2.1, first construct the update program UP of P:3 UP : flies(x) \u2190 bird(x), not ab(x), ab(x) \u2190 broken-wing(x) , bird(t) \u2190 , bird(o) \u2190 , broken-wing(x) \u2190 not broken-wing(x), broken-wing(x) \u2190 not broken-wing(x), +broken-wing(x) \u2190 broken-wing(x), x = t , \u2212broken-wing(t) \u2190 not broken-wing(t) . It has the single U-minimal answer set: S = { bird(t), bird(o), flies(t), flies(o), broken-wing(t), broken-wing(o), \u2212broken-wing(t) }. If P \u222a { \u2190 not G } has a consistent answer set S, G has the minimal explanation (E, F) = (\u2205, \u2205) and S satisfies G. Now we provide methods for computing (counter-)proposals. First, conditional proposals are computed as follows. input : an abductive program P, H , a proposal G; output : a set Sc of proposals. Else if G is a conjunction possibly containing variables, consider the abductive program t represents tweety and o represents opus. Next, neighborhood proposals are computed as follows. input : an abductive program P, H , a proposal G; output : a set Sn of proposals. For a ground literal O, if P \u222a { O \u2190 G } \u222a { \u2190 not O } has a consistent answer set satisfying G \u03b8 with a substitution \u03b8 and G \u03b8 = G, put G \u03b8 in Sn. For a ground literal O, if P \u222a{ O \u2190 G }\u222a{ \u2190 not O } has a consistent answer set satisfying G \u03b8 with a substitution \u03b8, put G \u03b8 in Sn. If G is a ground literal and there is a rule H \u2190 B in P such that G = H\u03c3 and B\u03c3 = G for some substitution \u03c3, put G = B\u03c3. For a ground literal O, if P \u222a { O \u2190 G } \u222a { \u2190 not O } has a consistent answer set satisfying G \u03b8 with a substitution \u03b8, put G \u03b8 in Sn. neighborhood proposals). The result for Sn follows from Definition 3.5 and Proposition 4.2. In practice, an agent can prespecify literals in a proposal for possible relaxation or rules in a program for the usage of goal replacement. As there are a number of literature on automated negotiation, this section focuses on comparison with negotiation frameworks based on logic and argumentation. Agents negotiate using common dialogue primitives, called dialogue moves. Our primary interest is to mechanize construction of proposals. Their goal is to explain an observation by revising integrity constraints of an agent through communication, while we use integrity constraints for communication to explain critiques and help other agents in making counter-proposals. Moreover, they represent agents by classical propositional theories, which is different from our abductive logic programming framework. In their framework, two agents exchange answer sets to produce a common belief set, which is different from our framework of exchanging proposals. A proposal contains an offer corresponding to the negotiation object, together with supporting information representing conditions under which this offer is made. Second, in their negotiation protocol, counter-proposals are chosen among candidates based on preference knowledge of an agent at meta-level, which represents policy under which an agent uses its object-level decision rules according to situations. This is another important difference. As far as the authors know, however, there is no study which applies those technique to agent negotiation.", "introduction": "Automated negotiation has been received increasing attention in multi-agent systems, and a number of frameworks have been proposed in different contexts (, for instance). Negotiation usually proceeds in a series of rounds and each agent makes a proposal at every round. An agent that received a proposal responds in two ways. One is a critique which is a remark as to whether or not (parts of) the proposal is accepted. The other is a counter-proposal which is an alternative proposal made in response to a previous proposal . To see these proposals in one-to-one negotiation, suppose the following negotiation dialogue between a buyer agent B and a seller agent S. (Bi (or Si) represents an utterance of B (or S) in the i-th round.) B1: I want to buy a personal computer of the brand b1, with the specification of CPU:1GHz, Memory:512MB, HDD: 80GB, and a DVD-RW driver. USD. S1: We can provide a PC with the requested specification if you pay for it by cash. In this case, however, service points are not added for this special discount. B2: I cannot pay it by cash. S2: In a normal price, USD. B3: I cannot accept the price. USD. S3: We can provide another computer with the requested specification, except that it is made by the brand b2. USD. B4: I do not want a PC of the brand b2. Instead, I can downgrade a driver from DVD-RW to CD-RW in my initial proposal. S4: Ok, I accept your offer. In this dialogue, in response to the opening proposal B1, the counter-proposal S1 is returned. In the rest of the dialogue, B2, B3, S4 are critiques, while S2, S3, B4 are counterproposals. Critiques are produced by evaluating a proposal in a knowledge base of an agent. In contrast, making counter-proposals involves generating an alternative proposal which is more favorable to the responding agent than the original one. It is known that there are two ways of producing counterproposals: extending the initial proposal or amending part of the initial proposal. According to , the first type appears in the dialogue: A: I propose that you provide me with service X. B: I propose that I provide you with service X if you provide me with service Z. The second type is in the dialogue: A: I propose that I provide you with service Y if you provide me with service X. B: I propose that I provide you with service X if you provide me with service Z. A negotiation proceeds by iterating such give-andtake dialogues until it reaches an agreement/disagreement. In those dialogues, agents generate (counter-)proposals by reasoning on their own goals or objectives. The objective of the agent A in the above dialogues is to obtain service X. The agent B proposes conditions to provide the service. In the process of negotiation, however, it may happen that agents are obliged to weaken or change their initial goals to reach a negotiated compromise. 978-81--7-5 (RPS) IFAAMAS a buyer agent and a seller agent presented above, a buyer agent changes its initial goal by downgrading a driver from DVD-RW to CD-RW. Such behavior is usually represented as specific meta-knowledge of an agent or specified as negotiation protocols in particular problems. Currently, there is no computational logic for automated negotiation which has general inference rules for producing (counter-)proposals. The purpose of this paper is to mechanize a process of building (counter-)proposals in one-to-one negotiation dialogues. We suppose an agent who has a knowledge base represented by a logic program. We then introduce methods for generating three different types of proposals. First, we use the technique of extended abduction in artificial intelligence to construct a conditional proposal as an extension of the original one. Second, we use the technique of relaxation in cooperative query answering for databases to construct a neighborhood proposal as an amendment of the original one. Third, combining extended abduction and relaxation, conditional neighborhood proposals are constructed as amended extensions of the original proposal. We develop a negotiation protocol between two agents based on the exchange of these counter-proposals and critiques. We also provide procedures for computing proposals in logic programming. This paper is organized as follows. Section 2 introduces a logical framework used in this paper. Section 3 presents methods for constructing proposals, and provides a negotiation protocol. Section 4 provides methods for computing proposals in logic programming. Section 5 discusses related works, and Section 6 concludes the paper.", "conclusion": "In this paper we proposed a logical framework for negotiating agents.. To construct proposals in the process of negotiation, we combined the techniques of extended abduction and relaxation.. It was shown that these two operations are used for general inference rules in producing proposals.. We developed a negotiation protocol between two agents based on exchange of proposals and critiques, and provided procedures for computing proposals in abductive logic programming.. This enables us to realize automated negotiation on top of the existing answer set solvers.. The present framework does not have a mechanism of selecting an optimal (counter-)proposal among different alternatives.. To compare and evaluate proposals, an agent must have preference knowledge of candidate proposals.. Further elaboration to maximize the utility of agents is left for future study."}
{"id": "I-21", "keywords": ["social network", "cognit model", "artifici social system"], "title": "Interactions between Market Barriers and Communication Networks in Marketing Systems", "abstract": "We investigate a framework where agents search for satis- fying products by using referrals from other agents. Our model of a mechanism for transmitting word-of-mouth and the resulting behavioural effects is based on integrating a module governing the local behaviour of agents with a mod- ule governing the structure and function of the underlying network of agents. Local behaviour incorporates a satis- ficing model of choice, a set of rules governing the interac- tions between agents, including learning about the trustwor- thiness of other agents over time, and external constraints on behaviour that may be imposed by market barriers or switching costs. Local behaviour takes place on a network substrate across which agents exchange positive and neg- ative information about products. We use various degree distributions dictating the extent of connectivity, and incor- porate both small-world effects and the notion of preferential attachment in our network models. We compare the effec- tiveness of referral systems over various network structures for easy and hard choice tasks, and evaluate how this effec- tiveness changes with the imposition of market barriers.", "references": ["Customer satisfaction and word-of-mouth", "Emergence of scaling in random networks", "Social ties and word-of-mouth referral behaviour", "Spreading the word: investigating positive word-of-mouth intentions and behaviours in a retailing context", "Consumer switching costs: A typology, antecedents, and consequences", "Effect of referrals on convergence to satisficing distributions", "Influences on consumer use of word-of-mouth recommendation sources", "Word-of-mouth communication and social learning", "Belief, Attitude, Intention, and Behaviour: An Introduction to the Theory and Research", "An investigation into the social context of early adoption behaviour", "Customer switching behaviour in service industries: an exploratory study", "Markets with consumer switching costs", "Recommending collaboration with social networks: a comparative evaluation", "A cognitive model of the antecedents and consequences of satisfaction decisions", "Administrative Behaviour", "Improving user satisfaction in agent-based electronic marketplaces by reputation modelling and adjustable product quality", "A New Model for Predicting Behavioural Intentions: An Alternative to Fishbein", "Networks, dynamics, and the small world phenomenon"], "full_text": "1. INTRODUCTION Defection behaviour, that is, why people might stop using a particular product or service, largely depends on the psychological affinity or satisfaction that they feel toward the currently-used product and the availability of more attractive alternatives . However, in many cases the decision about whether to defect or not is also dependent on various external constraints that are placed on switching behaviour, either by the structure of the market, by the suppliers themselves (in the guise of formal or informal contracts), or other so-called \u2018switching costs\" or market barriers . The key feature of all these cases is that the extent to which psychological affinity plays a role in actual decision-making is constrained by market barriers, so that agents are prevented from pursuing those courses of action which would be most satisfying in an unconstrained market. While the level of satisfaction with a currently-used product will largely be a function of one\"s own experiences of the product over the period of use, knowledge of any potentially more satisfying alternatives is likely to be gained by augmenting the information gained from personal experiences with information about the experiences of others gathered from casual word-of-mouth communication. Moreover, there is an important relationship between market barriers and word-of-mouth communication. In the presence of market barriers, constrained economic agents trapped in dissatisfying product relationships will tend to disseminate this information to other agents. In the absence of such barriers, agents are free to defect from unsatisfying products and word-of-mouth communication would thus tend to be of the positive variety. Since the imposition of at least some forms of market barriers is often a strategic decision taken by product suppliers, these relationships may be key to the success of a particular supplier. In addition, the relationship between market barriers and word-of-mouth communication may be a reciprocal one. The structure and function of the network across which word-ofmouth communication is conducted, and particularly the way in which the network changes in response to the imposition of market barriers, also plays a role in determining which market barriers are most effective. These are complex questions, and our main interest in this paper is to address the simpler problems of investigating (a) the extent to which network structure influences the ways in which information is disseminated across a network of decision makers, (b) the extent to which market barriers affect this dissemination, and (c) the consequent implications for overall system performance, in terms of the proportion of agents who are satisfied, and the speed with which the system moves towards equilibrium, which we term stability. An agent-based model framework allows for an investigation at the level of the individual decision maker, at the 387 978-81--7-5 (RPS) IFAAMAS product-level, or at the level of the entire system; we are particularly interested in the implications of market barriers for the latter two. The model presented here allows for an investigation into the effects of market barriers to be carried out in a complex environment where at every time period each agent in a population must decide which one of a set of products to purchase. These decisions are based on multiattribute information gathered by personal product trials as well as from the referrals of agents. Agents use this gathered information to search for a product that exceeds their satisfaction thresholds on all attributes - so that the agents may be said to be satisficing rather than optimising (e.g. ). Market barriers may act to influence an agent to continue to use a product that is no longer offering satisfactory performance. We allow agents to hold different opinions about the performance of a product, so that as a result a referral from another agent may not lead to a satisfying experience. Agents therefore adjust their evaluations of the validity of other agents\" referrals according to the success of past referrals, and use these evaluations to judge whether or not to make use of any further referrals. The level of satisfaction provided to an agent by a product is itself inherently dynamic, being subject to random fluctuations in product performance as well as a tendency for an agent to discount the performance of a product they have used for a long time - a process akin to habituation. 2. BACKGROUND 2.1 Word-of-mouth communication Much of the work done on word-of-mouth communication in the context of social psychology and marketing research has focused on its forms and determinants, suggesting that word-of-mouth arises in three possible ways: it may be induced by a particular transaction or product experience , particularly when that transaction has been an especially good or bad one ; it may be solicited from others , usually when the task involved is difficult, ambiguous, or new ; and it may come about when talk of products and brands arise in the course of informal conversation, particularly when a \u2018passion for the subject\" is present . Wordof-mouth becomes more influential when the source of the communication is credible, with credibility decisions based largely on one or a combination of evaluations of professional qualification, informal training, social distance , and similarity of views and experiences . The role of word-of-mouth communication on the behaviour of complex systems has been studied in both analytical and simulation models. The analytical work in investigates the conditions under which word-of-mouth leads to conformity in behaviour and the adoption of socially efficient outcomes (e.g. choosing an alternative that is on average better than another), finding that conformity of behaviour arises when agents are exposed to word-of-mouth communication from only a small number of other agents, but that this conformity may result in socially inefficient outcomes where the tendency toward conformity is so strong that it overwhelms the influence of the superior payoffs provided by the socially efficient outcome. Simulation-based investigations of wordof-mouth have focused on developing strategies for ensuring that a system reaches an equilibrium level where all agents are satisfied, largely by learning about the effectiveness of others\" referrals or by varying the degree of inertia in individual behaviour. These studies have found that, given a sufficient number of service providers, honest referrals lead to faster convergence to satisfactory distributions than deceitful ones, and that both forms of word-of-mouth provide better performance than none at all. The simulation framework allows for a more complex modelling of the environment than the analytical models, in which referrals are at random and only two choices are available, and the work in in particular is a close antecedent of the work presented in this paper, our main contribution being to include network structure and the constraints imposed by market barriers as additional effects. 2.2 Market barriers The extent to which market barriers are influential in affecting systems behaviour draws attention mostly from economists interested in how barriers distort competition and marketers interested in how barriers distort consumer choices. While the formalisation of the idea that satisfaction drives purchase behaviour can be traced back to the work of Fishbein and Ajzen on reasoned choice, nearly all writers, including Fishbein and Ajzen, recognise that this relationship can be thwarted by circumstances (e.g. ). A useful typology of market barriers distinguishes \u2018transactional\" barriers associated with the monetary cost of changing (e.g. in financial services), \u2018learning\" barriers associated with deciding to replace well-known existing products, and \u2018contractual\" barriers imposing legal constraints for the term of the contract . A different typology introduces the additional aspect of \u2018relational\" barriers arising from personal relationships that may be interwoven with the use of a particular product. There is generally little empirical evidence on the relationship between the creation of barriers to switching and the retention of a customer base, and to the best of our knowledge no previous work using agent-based modelling to generate empirical findings. Burnham et al. find that perceived market barriers account for nearly twice the variance in intention to stay with a product than that explained by satisfaction with the product (30% and 16% respectively), and that so-called relational barriers are considerably more influential than either transactional or learning barriers. Further, they find that switching costs are perceived by consumers to exist even in markets which are fluid and where barriers would seem to be weak. Simply put, market barriers appear to play a greater role in what people do than satisfaction; and their presence may be more pervasive than is generally thought. 3. MODEL FRAMEWORK 3.1 Product performance evaluations We use a problem representation in which, at each time period, every agent must decide which one of a set of products to choose. Let A = {ak}k=1...p be the set of agents, B = {bi}i=1...n be the set of products, and C = {cj }j=1...m be the set of attributes on which the choice decision is to be based i.e. the decision to be made is a multiattribute choice one. Let fj : B \u2192 be an increasing function providing the intrinsic performance of a product on attribute j (so that 0 and 1 are the worst- and best-possible performances respectively), and Sij : A \u00d7 \u2192 be a subjective opinion function of agents. The intrinsic performance of 388 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) product i on attribute j is given by fj (bi). However, the subjective opinion of the level of performance (of product i on attribute j) given by agent k is given by sij(ak, fj (bi)). All subsequent modelling is based on these subjective performance ratings. For the purposes of this paper, each agent belongs to one of three equally-sized groups, with each group possessing its own subjective performance ratings. We assume that the subjective performance ratings are not known a priori by the agents, and it is their task to discover these ratings by a combination of personal exploration and referral gathering. In order to model this process we introduce the notion of perceived performance ratings at time t, denoted by pij(ak, fj (bi), t). Initially, all perceived performance ratings are set to zero, so that the initial selection of a product is done randomly. Subsequent variation in product performance over time is modelled using two quantities: a random perturbation jkt applied at each purchase occasion ensures that the experience of a particular product can vary over purchase occasions for the same agent, and a habituation discounting factor Hikt tends to decrease the perceived performance of a product over time as boredom creeps in with repeated usage. Our habituation mechanism supposes that habituation builds up with repeated use of a product, and is used to discount the performance of the product. In most cases i.e. unless the habituation factor is one or extremely close to one, this habituation-based discounting eventually leads to defection, after which the level of habituation dissipates as time passes without the product being used. More formally, once a product i\u2217 has been chosen by agent k, the subjective level of performance is perceived and pi\u2217j(ak, fj (b\u2217 i ), t) is set equal to si\u2217j(ak, fj (b\u2217 i ))Hi\u2217kt + jkt, where jkt is distributed as N(0, \u03c3) and Hi\u2217kt is an decreasing function of the number of time periods that agent k has been exposed to i\u2217 In evaluating the performance of a product, agents make use of a satisficing framework by comparing the perceived performance of the chosen product with their satisfaction thresholds \u0393k = {g1k, . . . , gmk}, with 0 \u2264 gik \u2264 1. Agent k will be satisfied with a product i\u2217 selected in time t if pi\u2217j(ak, fj (b\u2217 i ), t) \u2265 gjk, \u2200j. 3.2 Decision processes In designing the mechanism by which agents make their choice decisions, we allow for the possibility that satisfied agents defect from the products that are currently satisfying them. Satisfied agents stay with their current product with probability Pr(stay), with a strategy prohibiting satisfied agents from moving (e.g. ) obtained as a special case when Pr(stay) = 1. A defecting satisfied agent decides on which product to choose by considering all other products for which it has information, either by previous personal exploration or by referrals from other agents. The selection of a new product begins by the agent identifying those products from which he or she expects to gain a satisfactory performance on all attributes i.e. those products for which \u03b4ik < 0, where \u03b4ik = maxj [gjk \u2212 pij(ak, fj(bi), t)], and selecting a product from this set with selection probabilities proportional to \u2212\u03b4ik. If no satisfactory product exists (or at least the agent is unaware of any such product) the agent identifies those products that offer at least a minimum level of \u2018acceptable\" performance \u03b3\u2212 k . The minimum level of acceptability is defined as the maximum deviation from his or her aspirations across all attributes that the agent is willing to accept i.e. a product is minimally acceptable if and only if \u03b4ik < \u03b3\u2212 k . Agents then select a product at random from the set of minimally acceptable products. If the set of minimally acceptable products is empty, agents select a product from the full set of products B at random. The decision process followed by unsatisfied agents is largely similar to that of defecting satisfied agents, with the exception that at the outset of the decision process agents will chose to explore a new product, chosen at random from the set of remaining products, with probability \u03b1. With probability 1 \u2212 \u03b1, they will use a decision process like the one outlined above for satisfied agents. 3.3 Constraints on decision processes In some circumstances market barriers may exist that make switching between products more difficult, particularly where some switching costs are incurred as a result of changing one\"s choice of product. When barriers are present, agents do not switch when they become unsatisfied, but rather only when the performance evaluation drops below some critical level i.e. when \u03b4ik > \u03b2, where \u03b2 > 0 measures the strength of the market barriers. Although in this paper market barriers do not vary over products or time, it is straightforward to allow this to occur by allowing barriers take the general form \u03b2 = max(\u03b2\u2217 +\u0394tuse, \u03b2\u2217 ), where \u03b2\u2217 is a barrier to defection that is applied when the product is purchased for the first time (e.g. a contractual agreement), \u0394 is the increase in barriers that are incurred for every additional time period the product is used for, and \u03b2\u2217 is the maximum possible barrier, and all three quantities are allowed to vary over products i.e. be a function of i. 3.4 Referral processes Each agent is assumed to be connected to qk < p agents i.e. to give and receive information from qk other agents. The network over which word-of-mouth communication travels is governed by the small-world effect , by which networks simultaneously exhibit a high degree of clustering of agents into \u2018communities\" and a short average path length between any two agents in the network, and preferential attachment , by which agents with greater numbers of existing connections are more likely to receive new ones. This is easily achieved by building a one-dimensional lattice with connections between all agent pairs separated by \u03ba or fewer lattice spacings, and creating a small-world network by choosing at random a small fraction r of the connections in the network and moving one end of each to a new agent, with that new agent chosen with probability proportional to its number of existing connections. This results in a distribution of the number of connections possessed by each agent i.e. a distribution of qk, that is strongly skewed to the right. In fact, if the construction of the network is slightly modified so that new connections are added with preferential attachment (but no connections are removed), the distribution of qk follows a power-law distribution, but a distribution with a non-zero probability of an agent having less than the modal number of connections seems more realistic in the context of word-of-mouth communication in marketing systems. When an agent purchases a product, they inform each of the other agents in their circle with probability equal to Pr(spr)k\u2217 + |\u03b4ik\u2217 |, where Pr(spr)k\u2217 is the basic propensity of agent k\u2217 to spread word of mouth and \u03b4ik\u2217 captures the The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 389 extent to which the agent\"s most recent experience was satisfying or dissatisfying. Agents are thus more likely to spread word-of-mouth about products that they have just experienced as either very good or very bad. If an agent receives information on the same product from more than one agent, he or she selects the referral of only one of these agents, with selection probabilities proportional to Tt(k\u2217 , k), the degree to which previous referrals from k\u2217 to k were successful i.e. resulted in satisfying experiences for agent k. Thus agents have the capacity to learn about the quality of other agents\" referrals and use this information to accept or block future referrals. In this paper, we employ a learning condition in which Tt(k\u2217 , k) is multiplied by a factor of 0.1 following an unsatisfying referral and a factor of 3 following a satisfying referral. The asymmetry in the weighting is similar to that employed in , and is motivated by the fact that an unsatisfying referral is likely to be more reliable evidence that a referring agent k\u2217 does not possess the same subjective preferences as agent k than a positive referral is of indicating the converse. Other referral process are certainly possible, for example one integrating multiple sources of word-of-mouth rather than choosing only the most-trusted source: our main reason for employing the process described above is simplicity. Integrating different sources considerably complicates the process of learning about the trustworthiness of others, and raises further questions about the precise nature of the integration. After determining who contacts whom, the actual referral is modelled as a transmittance of information about the perceived level of performance of an experience of product i\u2217 from the referring agent k\u2217 to the accepting agent k i.e. pi\u2217j(ak, fj (bi), t) takes on the value pi\u2217j(ak\u2217 , fj(bi), t\u22121), \u2200j, provided that agent k is not currently using i\u2217 . Information about other products is not transmitted, and an agent will ignore any word-of-mouth about the product he or she is currently using. In effect, the referral creates an expected level of performance in the mind of an accepting agent for the product referred to, which that agent may then use when making choice decision in subsequent time periods using the decision processes outlined in the previous section. Once an agent has personally experienced a product, any expected performance levels suggested by previous referrals are replaced by the experienced (subjective) performance levels sij(ak, fj(bi)) + jkt and Tt(k\u2217 , k) is adjusted depending on whether the experience was a satisfying one or not. 4. EXPERIMENTAL RESULTS We examine the behaviour of a system of 200 agents consisting of three groups of 67, 67, and 66 agents respectively. Agents in each of the three groups have homogeneous subjective opinion functions Sij. Simulations were run for 500 time periods, and twenty repetitions of each condition were used in order to generate aggregate results. 4.1 Choice task difficulty We begin by examining the effect of task difficulty on the ability of various network configurations to converge to a state in which an acceptable proportion of the population are satisfied. In the \u2018easy\" choice condition, there are 50 products to choose from in the market, evaluated over 4 attributes with all satisfaction thresholds set to 0.5 for all groups. There are therefore on average approximately 3 products that can be expected to satisfy any particular agent. In the \u2018hard\" choice condition, there are 500 products to choose from in the market, still evaluated over 4 attributes but with all satisfaction thresholds now set to 0.7 for all groups, so there are on average approximately 4 products that can be expected to satisfy any particular agent. Locating a satisfactory product is therefore far more difficult under the \u2018hard\" condition. The effect of task difficulty is evaluated on three network structures corresponding to r = 1 (random network), r = 0.05 (small-world network), and r = 0 (tight \u2018communities\" of agents), with results shown in Figure 1 for the case of \u03ba = 3. 0 100 200 300 400 500 0.00.10.20.30.40.50.60.7 Time Proportionsatisfied Easy task r = 1 r = 0.05 r = 0 (a) Proportion of agents satisfied 0 100 200 300 400 500 0.00.10.20.30.40.50.6 Time Shareofmarket Easy task r = 1 r = 0.05 r = 0 (b) Market share for leading product Figure 1: Moderating effect of task difficulty on relationship between network structure (r) and system behaviour Given a relatively easy task, the system very quickly i.e. in little over 50 time periods, converges to a state in which just less than 60% of agents are satisfied at any one time. Furthermore, different network structures have very little influence on results, so that only a single (smoothed) series is given for comparison with the \u2018hard\" condition. Clearly, there are enough agents independently solving the task i.e. 390 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) finding a satisfying brand, to make the dissemination of information relatively independent of the ways in which connections are made. However, when it is more difficult to locate a satisfying product, the structure of the network becomes integral to the speed at which the system converges to a stable state. Importantly, the overall satisfaction level to which the system converges remains just below 60% regardless of which network structure is used, but convergence is considerably speeded by the random rewiring of even a small proportion of connections. Thus while the random network (r = 1) converges quickest, the small-world network (r = 0.05) also shows a substantial improvement over the tight communities represented by the one-dimensional ring lattice. This effect of the rewiring parameter r is much less pronounced for more highly-connected networks (e.g. \u03ba = 9), which suggests that the degree distribution of a network is a more important determinant of system behaviour than the way in which agents are connected to one another. Similar results are observed when looking at the market share achieved by the market leading product under each level of choice difficulty: market share is essentially independent of network structure for the easy task, with average share converging quickly to around 35%. Set a more difficult task, the convergence of market share to an approximate long-run equilibrium is in fact fastest for the smallworld network, with the random and tight community networks taking different paths but similar times to reach their equilibrium levels. Also interesting is the finding that equilibrium market shares for the market leader appear to be slightly (of the order of 5-10%) higher when network connections are non-random - the random network seems to suffer more from the effects of habituatation than the other networks as a result of the rapid early adoption of the market leading product. 4.2 Market barriers In the remainder of this paper we focus on the effect of various forms of market barriers on the ability of a system of agents to reach a state of acceptable satisfaction. For simplicity, we concentrate on the smaller set of 50 products i.e. the \u2018easy\" choice task discussed above, but vary the number of connections that each agent begins with in order to simultaneously investigate the effect of degree distribution on system behaviour. Tables 1 and 2 show the effect of different degree distributions on the equilibrium proportion of agents that are satisfied at any one time, and the equilibrium proportion of agents switching products (moving) in any one time period, under various levels of market barriers constraining their behaviour. In these two tables, equilibrium results have been calculated by averaging over time periods 450 to 500, when the system is in equilibrium or extremely close to equilibrium (Table 3 and 4 make use of all time periods). No WoM \u03ba = 1 \u03ba = 3 \u03ba = 9 \u03b2 = 0 0.27 0.44 0.56 0.58 \u03b2 = 0.05 0.26 0.39 0.50 0.52 \u03b2 = 0.2 0.14 0.27 0.32 0.34 \u03b2 = 0.4 0.07 0.17 0.22 0.25 Table 1: Effect of degree distribution and market barriers on proportion of market satisfied No WoM \u03ba = 1 \u03ba = 3 \u03ba = 9 \u03b2 = 0 0.74 0.51 0.45 0.45 \u03b2 = 0.05 0.66 0.43 0.38 0.37 \u03b2 = 0.2 0.41 0.21 0.21 0.21 \u03b2 = 0.4 0.17 0.09 0.09 0.09 Table 2: Effect of degree distribution and market barriers on proportion of market moving Three aspects are worth noting. Firstly, there is a strong diminishing marginal return of additional connections beyond a small number. The first few connections one makes increases the probability of finding a satisfying product 60% from 0.27 to 0.44 (for the first two contacts), followed by a further increase of roughly 25% to 0.56 for the next four. In contrast, adding a further 12 contacts improves relative satisfaction levels by less than 4%. Secondly, word-of-mouth communication continues to play an important role in improving the performance of the system even when market barriers are high. In fact, the role may even be more important in constrained conditions, since the relative gains obtained from word-of-mouth are greater the higher market barriers are - just having two contacts more than doubles the aggregate satisfaction level under the most extreme barriers (\u03b2 = 0.4). Finally, it is clear that the mechanism by which barriers reduce satisfaction is by restricting movement (reflected in the lower proportion of agents moving in any particular column of Tables 1 and 2), but that increases in degree distribution act to increase satisfaction by precisely the same mechanism of reducing movement - this time by reducing the average amount of time required to find a satisfying brand. Positive referrals Negative referrals \u03ba = 1 \u03ba = 3 \u03ba = 9 \u03ba = 1 \u03ba = 3 \u03ba = 9 \u03b2 = 0 0.21 0.93 3.27 0.00 0.00 0.00 \u03b2 = 0.05 0.19 0.85 2.96 0.06 0.19 0.60 \u03b2 = 0.2 0.13 0.57 2.04 0.30 0.92 2.83 \u03b2 = 0.4 0.08 0.40 1.49 0.60 1.81 5.44 Table 3: Median number of positive and negative referrals made per agent per time period Perhaps the most interesting effects exerted by market barriers are those exerted over the market shares of leading products. Figure 2 shows the cumulative market share captured by the top three products in the market over time, for all types of market barriers using different degree distributions. Again, two comments can be made. Firstly, in the absence of market barriers, a greater proportion of the market is captured by the market leading products when markets are highly-connected relative to when they are poorlyconnected. This difference can amount to as much as 15%, and is explained by positive feedback within the more highlyconnected networks that serves to increase the probability that, once a set of satisfying products have emerged, one is kept informed about these leading products because at least one of one\"s contacts is using it. Secondly, the relatively higher market share enjoyed by market leaders in highly-connected networks is eroded by market barriers. In moving from \u03b2 = 0 to \u03b2 = 0.2 to \u03b2 = 0.4, market leaders collectively lose an absolute share of 15% and 10% under the larger degree distributions \u03ba = 9 and \u03ba = 3 respectively. The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 391 0 100 200 300 400 500 0.10.20.30.40.50.60.70.8 Time Shareofmarket \u03ba = 9 \u03ba = 3 \u03ba = 1 No WoM (a) \u03b2 = 0 0 100 200 300 400 500 0.10.20.30.40.50.60.70.8 Time Shareofmarket \u03ba = 9 \u03ba = 3 \u03ba = 1 No WoM (b) \u03b2 = 0.05 0 100 200 300 400 500 0.10.20.30.40.50.60.70.8 Time Shareofmarket \u03ba = 9 \u03ba = 3 \u03ba = 1 No WoM (c) \u03b2 = 0.2 0 100 200 300 400 500 0.10.20.30.40.50.60.70.8 Time Shareofmarket \u03ba = 9 \u03ba = 3 \u03ba = 1 No WoM (d) \u03b2 = 0.4 Figure 2: Effect of market barriers on the share of the market captured by the leading 3 products In contrast, no change in collective market share is observed when \u03ba = 1, although convergence to equilibrium conditions is slower. It seems reasonable to suggest that increases in negative word-of-mouth, which occurs when an unsatisfied agent is prevented from switching to another product, are particularly damaging to leading products when agents are well-connected, and that under moderate to strong market barriers these effects more than offset any gains achieved by the spread of positive word-of-mouth through the network. Table 3 displays the number of attempted referrals, both positive and negative, as a function of degree distribution and extent of market barriers, and shows that stronger market barriers act to simultaneously depress positive word-ofmouth communication and increase negative communication from those trapped in unsatisfying product relationships, and that this effect is particularly pronounced for more highly-connected networks. The reduction in the number of positive referrals as market barriers impose increasingly severe constraints is also reflected in Table 4, which shows the median number of product trials each agent makes per time period based on a referral from another agent. Whereas under few or no barriers agents in a highly-connected network make substantially more reference-based product trials than agents in poorly-connected networks, when barriers are severe both types of network carry only very little positive referral information. This clearly has a relatively greater impact on the highly-connected network, which relies on the spread of positive referral information to achieve higher satisfaction levels. Moreover, this result might be even stronger in reality if agents in poorly-connected networks attempt to compensate for the relative sparcity of connections by making more forceful or persuasive referrals where they do occur. \u03ba = 1 \u03ba = 3 \u03ba = 9 \u03b2 = 0 0.13 0.27 0.35 \u03b2 = 0.05 0.11 0.22 0.28 \u03b2 = 0.2 0.05 0.10 0.14 \u03b2 = 0.4 0.02 0.05 0.06 Table 4: Median number of referrals leading to a product trial received per agent per time period 392 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 5. CONCLUSIONS AND RELATED WORK Purchasing behaviour in many markets takes place on a substrate of networks of word-of-mouth communication across which agents exchange information about products and their likes and dislikes. Understanding the ways in which flows of word-of-mouth communication influence aggregate market behaviour requires one to study both the underlying structural properties of the network and the local rules governing the behaviour of agents on the network when making purchase decisions and when interacting with other agents. These local rules are often constrained by the nature of a particular market, or else imposed by strategic suppliers or social customs. The proper modelling of a mechanism for word-of-mouth transmittal and resulting behavioural effects thus requires a consideration of a number of complex and interacting components: networks of communication, source credibility, learning processes, habituation and memory, external constraints on behaviour, theories of information transfer, and adaptive behaviour. In this paper we have attempted to address some of these issues in a manner which reflects how agents might act in the real world. Using the key notions of a limited communication network, a simple learning process, and a satisficing heuristic that may be subject to external constraints, we showed (1) the importance of word-of-mouth communication to both system effectiveness and stability, (2) that the degree distribution of a network is more influential than the way in which agents are connected, but that both are important in more complex environments, (3) that rewiring even a small number of connections to create a small-world network can have dramatic results for the speed of convergence to satisficing distributions and market share allocations, (4) that word-of-mouth continues to be effective when movements between products are constrained by market barriers, and (5) that increases in negative word-of-mouth incurred as a result of market barriers can reduce the market share collectively captured by leading brands, but that this is dependent on the existence of a suitably well-connected network structure. It is the final finding that is likely to be most surprising and practically relevant for the marketing research field, and suggests that it may not always be in the best interests of a market leader to impose barriers that prevent customers from leaving. In poorly-connected networks, the effect of barriers on market shares is slight. In contrast, in well-connected networks, negative word-of-mouth can prevent agents from trying a product that they might otherwise have found satisfying, and this can inflict significant harm on market share. Products with small market share (which, in the context of our simulations, is generally due to the product offering poor performance) are relatively unaffected by negative word-of-mouth, since most product trials are likely to be unsatisfying in any case. Agent-based modelling provides a natural way for beginning to investigate the types of dynamics that occur in marketing systems. Naturally the usefulness of results is for the most part dependent on the quality of the modelling of the two \u2018modules\" comprising network structure and local behaviour. On the network side, future work might investigate the relationship between degree distributions, the way connections are created and destroyed over time, whether preferential attachment is influential, and the extent to which social identity informs network strucutre, all in larger networks of more heterogenous agents. On the behavioural side, one might look at the adaptation of satisfaction thresholds during the course of communication, responses to systematic changes in product performances over time, the integration of various information sources, and different market barrier structures. All these areas provide fascinating opportunities to introduce psychological realities into models of marketing systems and to observe the resulting behaviour of the system under increasingly realistic scenario descriptions.", "body1": "Defection behaviour, that is, why people might stop using a particular product or service, largely depends on the psychological affinity or satisfaction that they feel toward the currently-used product and the availability of more attractive alternatives . While the level of satisfaction with a currently-used product will largely be a function of one\"s own experiences of the product over the period of use, knowledge of any potentially more satisfying alternatives is likely to be gained by augmenting the information gained from personal experiences with information about the experiences of others gathered from casual word-of-mouth communication. In addition, the relationship between market barriers and word-of-mouth communication may be a reciprocal one. An agent-based model framework allows for an investigation at the level of the individual decision maker, at the 387 978-81--7-5 (RPS) IFAAMAS product-level, or at the level of the entire system; we are particularly interested in the implications of market barriers for the latter two. Agents therefore adjust their evaluations of the validity of other agents\" referrals according to the success of past referrals, and use these evaluations to judge whether or not to make use of any further referrals. 2.1 Word-of-mouth communication Much of the work done on word-of-mouth communication in the context of social psychology and marketing research has focused on its forms and determinants, suggesting that word-of-mouth arises in three possible ways: it may be induced by a particular transaction or product experience , particularly when that transaction has been an especially good or bad one ; it may be solicited from others , usually when the task involved is difficult, ambiguous, or new ; and it may come about when talk of products and brands arise in the course of informal conversation, particularly when a \u2018passion for the subject\" is present . The role of word-of-mouth communication on the behaviour of complex systems has been studied in both analytical and simulation models. 2.2 Market barriers The extent to which market barriers are influential in affecting systems behaviour draws attention mostly from economists interested in how barriers distort competition and marketers interested in how barriers distort consumer choices. A useful typology of market barriers distinguishes \u2018transactional\" barriers associated with the monetary cost of changing (e.g. There is generally little empirical evidence on the relationship between the creation of barriers to switching and the retention of a customer base, and to the best of our knowledge no previous work using agent-based modelling to generate empirical findings. 3.1 Product performance evaluations We use a problem representation in which, at each time period, every agent must decide which one of a set of products to choose. We assume that the subjective performance ratings are not known a priori by the agents, and it is their task to discover these ratings by a combination of personal exploration and referral gathering. In most cases i.e. 3.2 Decision processes In designing the mechanism by which agents make their choice decisions, we allow for the possibility that satisfied agents defect from the products that are currently satisfying them. A defecting satisfied agent decides on which product to choose by considering all other products for which it has information, either by previous personal exploration or by referrals from other agents. Agents then select a product at random from the set of minimally acceptable products. The decision process followed by unsatisfied agents is largely similar to that of defecting satisfied agents, with the exception that at the outset of the decision process agents will chose to explore a new product, chosen at random from the set of remaining products, with probability \u03b1. 3.3 Constraints on decision processes In some circumstances market barriers may exist that make switching between products more difficult, particularly where some switching costs are incurred as a result of changing one\"s choice of product. The network over which word-of-mouth communication travels is governed by the small-world effect , by which networks simultaneously exhibit a high degree of clustering of agents into \u2018communities\" and a short average path length between any two agents in the network, and preferential attachment , by which agents with greater numbers of existing connections are more likely to receive new ones. This is easily achieved by building a one-dimensional lattice with connections between all agent pairs separated by \u03ba or fewer lattice spacings, and creating a small-world network by choosing at random a small fraction r of the connections in the network and moving one end of each to a new agent, with that new agent chosen with probability proportional to its number of existing connections. When an agent purchases a product, they inform each of the other agents in their circle with probability equal to Pr(spr)k\u2217 + |\u03b4ik\u2217 |, where Pr(spr)k\u2217 is the basic propensity of agent k\u2217 to spread word of mouth and \u03b4ik\u2217 captures the The Sixth Intl. resulted in satisfying experiences for agent k. Thus agents have the capacity to learn about the quality of other agents\" referrals and use this information to accept or block future referrals. After determining who contacts whom, the actual referral is modelled as a transmittance of information about the perceived level of performance of an experience of product i\u2217 from the referring agent k\u2217 to the accepting agent k i.e. pi\u2217j(ak, fj (bi), t) takes on the value pi\u2217j(ak\u2217 , fj(bi), t\u22121), \u2200j, provided that agent k is not currently using i\u2217 . We examine the behaviour of a system of 200 agents consisting of three groups of 67, 67, and 66 agents respectively. 4.1 Choice task difficulty We begin by examining the effect of task difficulty on the ability of various network configurations to converge to a state in which an acceptable proportion of the population are satisfied. Furthermore, different network structures have very little influence on results, so that only a single (smoothed) series is given for comparison with the \u2018hard\" condition. 390 The Sixth Intl. Similar results are observed when looking at the market share achieved by the market leading product under each level of choice difficulty: market share is essentially independent of network structure for the easy task, with average share converging quickly to around 35%. the \u2018easy\" choice task discussed above, but vary the number of connections that each agent begins with in order to simultaneously investigate the effect of degree distribution on system behaviour. No WoM \u03ba = 1 \u03ba = 3 \u03ba = 9 \u03b2 = 0 0.27 0.44 0.56 0.58 \u03b2 = 0.05 0.26 0.39 0.50 0.52 \u03b2 = 0.2 0.14 0.27 0.32 0.34 \u03b2 = 0.4 0.07 0.17 0.22 0.25 Table 1: Effect of degree distribution and market barriers on proportion of market satisfied No WoM \u03ba = 1 \u03ba = 3 \u03ba = 9 \u03b2 = 0 0.74 0.51 0.45 0.45 \u03b2 = 0.05 0.66 0.43 0.38 0.37 \u03b2 = 0.2 0.41 0.21 0.21 0.21 \u03b2 = 0.4 0.17 0.09 0.09 0.09 Table 2: Effect of degree distribution and market barriers on proportion of market moving Three aspects are worth noting. In contrast, adding a further 12 contacts improves relative satisfaction levels by less than 4%. Positive referrals Negative referrals \u03ba = 1 \u03ba = 3 \u03ba = 9 \u03ba = 1 \u03ba = 3 \u03ba = 9 \u03b2 = 0 0.21 0.93 3.27 0.00 0.00 0.00 \u03b2 = 0.05 0.19 0.85 2.96 0.06 0.19 0.60 \u03b2 = 0.2 0.13 0.57 2.04 0.30 0.92 2.83 \u03b2 = 0.4 0.08 0.40 1.49 0.60 1.81 5.44 Table 3: Median number of positive and negative referrals made per agent per time period Perhaps the most interesting effects exerted by market barriers are those exerted over the market shares of leading products. The Sixth Intl. Whereas under few or no barriers agents in a highly-connected network make substantially more reference-based product trials than agents in poorly-connected networks, when barriers are severe both types of network carry only very little positive referral information.", "body2": "The key feature of all these cases is that the extent to which psychological affinity plays a role in actual decision-making is constrained by market barriers, so that agents are prevented from pursuing those courses of action which would be most satisfying in an unconstrained market. Since the imposition of at least some forms of market barriers is often a strategic decision taken by product suppliers, these relationships may be key to the success of a particular supplier. These are complex questions, and our main interest in this paper is to address the simpler problems of investigating (a) the extent to which network structure influences the ways in which information is disseminated across a network of decision makers, (b) the extent to which market barriers affect this dissemination, and (c) the consequent implications for overall system performance, in terms of the proportion of agents who are satisfied, and the speed with which the system moves towards equilibrium, which we term stability. We allow agents to hold different opinions about the performance of a product, so that as a result a referral from another agent may not lead to a satisfying experience. The level of satisfaction provided to an agent by a product is itself inherently dynamic, being subject to random fluctuations in product performance as well as a tendency for an agent to discount the performance of a product they have used for a long time - a process akin to habituation. Wordof-mouth becomes more influential when the source of the communication is credible, with credibility decisions based largely on one or a combination of evaluations of professional qualification, informal training, social distance , and similarity of views and experiences . The simulation framework allows for a more complex modelling of the environment than the analytical models, in which referrals are at random and only two choices are available, and the work in in particular is a close antecedent of the work presented in this paper, our main contribution being to include network structure and the constraints imposed by market barriers as additional effects. ). A different typology introduces the additional aspect of \u2018relational\" barriers arising from personal relationships that may be interwoven with the use of a particular product. Simply put, market barriers appear to play a greater role in what people do than satisfaction; and their presence may be more pervasive than is generally thought. For the purposes of this paper, each agent belongs to one of three equally-sized groups, with each group possessing its own subjective performance ratings. Our habituation mechanism supposes that habituation builds up with repeated use of a product, and is used to discount the performance of the product. Agent k will be satisfied with a product i\u2217 selected in time t if pi\u2217j(ak, fj (b\u2217 i ), t) \u2265 gjk, \u2200j. ) obtained as a special case when Pr(stay) = 1. a product is minimally acceptable if and only if \u03b4ik < \u03b3\u2212 k . If the set of minimally acceptable products is empty, agents select a product from the full set of products B at random. With probability 1 \u2212 \u03b1, they will use a decision process like the one outlined above for satisfied agents. to give and receive information from qk other agents. The network over which word-of-mouth communication travels is governed by the small-world effect , by which networks simultaneously exhibit a high degree of clustering of agents into \u2018communities\" and a short average path length between any two agents in the network, and preferential attachment , by which agents with greater numbers of existing connections are more likely to receive new ones. In fact, if the construction of the network is slightly modified so that new connections are added with preferential attachment (but no connections are removed), the distribution of qk follows a power-law distribution, but a distribution with a non-zero probability of an agent having less than the modal number of connections seems more realistic in the context of word-of-mouth communication in marketing systems. If an agent receives information on the same product from more than one agent, he or she selects the referral of only one of these agents, with selection probabilities proportional to Tt(k\u2217 , k), the degree to which previous referrals from k\u2217 to k were successful i.e. Integrating different sources considerably complicates the process of learning about the trustworthiness of others, and raises further questions about the precise nature of the integration. After determining who contacts whom, the actual referral is modelled as a transmittance of information about the perceived level of performance of an experience of product i\u2217 from the referring agent k\u2217 to the accepting agent k i.e. Once an agent has personally experienced a product, any expected performance levels suggested by previous referrals are replaced by the experienced (subjective) performance levels sij(ak, fj(bi)) + jkt and Tt(k\u2217 , k) is adjusted depending on whether the experience was a satisfying one or not. Simulations were run for 500 time periods, and twenty repetitions of each condition were used in order to generate aggregate results. in little over 50 time periods, converges to a state in which just less than 60% of agents are satisfied at any one time. Clearly, there are enough agents independently solving the task i.e. \u03ba = 9), which suggests that the degree distribution of a network is a more important determinant of system behaviour than the way in which agents are connected to one another. For simplicity, we concentrate on the smaller set of 50 products i.e. In these two tables, equilibrium results have been calculated by averaging over time periods 450 to 500, when the system is in equilibrium or extremely close to equilibrium (Table 3 and 4 make use of all time periods). The first few connections one makes increases the probability of finding a satisfying product 60% from 0.27 to 0.44 (for the first two contacts), followed by a further increase of roughly 25% to 0.56 for the next four. Finally, it is clear that the mechanism by which barriers reduce satisfaction is by restricting movement (reflected in the lower proportion of agents moving in any particular column of Tables 1 and 2), but that increases in degree distribution act to increase satisfaction by precisely the same mechanism of reducing movement - this time by reducing the average amount of time required to find a satisfying brand. In moving from \u03b2 = 0 to \u03b2 = 0.2 to \u03b2 = 0.4, market leaders collectively lose an absolute share of 15% and 10% under the larger degree distributions \u03ba = 9 and \u03ba = 3 respectively. The reduction in the number of positive referrals as market barriers impose increasingly severe constraints is also reflected in Table 4, which shows the median number of product trials each agent makes per time period based on a referral from another agent. Moreover, this result might be even stronger in reality if agents in poorly-connected networks attempt to compensate for the relative sparcity of connections by making more forceful or persuasive referrals where they do occur.", "introduction": "Defection behaviour, that is, why people might stop using a particular product or service, largely depends on the psychological affinity or satisfaction that they feel toward the currently-used product and the availability of more attractive alternatives . However, in many cases the decision about whether to defect or not is also dependent on various external constraints that are placed on switching behaviour, either by the structure of the market, by the suppliers themselves (in the guise of formal or informal contracts), or other so-called \u2018switching costs\" or market barriers . The key feature of all these cases is that the extent to which psychological affinity plays a role in actual decision-making is constrained by market barriers, so that agents are prevented from pursuing those courses of action which would be most satisfying in an unconstrained market. While the level of satisfaction with a currently-used product will largely be a function of one\"s own experiences of the product over the period of use, knowledge of any potentially more satisfying alternatives is likely to be gained by augmenting the information gained from personal experiences with information about the experiences of others gathered from casual word-of-mouth communication. Moreover, there is an important relationship between market barriers and word-of-mouth communication. In the presence of market barriers, constrained economic agents trapped in dissatisfying product relationships will tend to disseminate this information to other agents. In the absence of such barriers, agents are free to defect from unsatisfying products and word-of-mouth communication would thus tend to be of the positive variety. Since the imposition of at least some forms of market barriers is often a strategic decision taken by product suppliers, these relationships may be key to the success of a particular supplier. In addition, the relationship between market barriers and word-of-mouth communication may be a reciprocal one. The structure and function of the network across which word-ofmouth communication is conducted, and particularly the way in which the network changes in response to the imposition of market barriers, also plays a role in determining which market barriers are most effective. These are complex questions, and our main interest in this paper is to address the simpler problems of investigating (a) the extent to which network structure influences the ways in which information is disseminated across a network of decision makers, (b) the extent to which market barriers affect this dissemination, and (c) the consequent implications for overall system performance, in terms of the proportion of agents who are satisfied, and the speed with which the system moves towards equilibrium, which we term stability. An agent-based model framework allows for an investigation at the level of the individual decision maker, at the 387 978-81--7-5 (RPS) IFAAMAS product-level, or at the level of the entire system; we are particularly interested in the implications of market barriers for the latter two. The model presented here allows for an investigation into the effects of market barriers to be carried out in a complex environment where at every time period each agent in a population must decide which one of a set of products to purchase. These decisions are based on multiattribute information gathered by personal product trials as well as from the referrals of agents. Agents use this gathered information to search for a product that exceeds their satisfaction thresholds on all attributes - so that the agents may be said to be satisficing rather than optimising (e.g. Market barriers may act to influence an agent to continue to use a product that is no longer offering satisfactory performance. We allow agents to hold different opinions about the performance of a product, so that as a result a referral from another agent may not lead to a satisfying experience. Agents therefore adjust their evaluations of the validity of other agents\" referrals according to the success of past referrals, and use these evaluations to judge whether or not to make use of any further referrals. The level of satisfaction provided to an agent by a product is itself inherently dynamic, being subject to random fluctuations in product performance as well as a tendency for an agent to discount the performance of a product they have used for a long time - a process akin to habituation.", "conclusion": "Purchasing behaviour in many markets takes place on a substrate of networks of word-of-mouth communication across which agents exchange information about products and their likes and dislikes.. Understanding the ways in which flows of word-of-mouth communication influence aggregate market behaviour requires one to study both the underlying structural properties of the network and the local rules governing the behaviour of agents on the network when making purchase decisions and when interacting with other agents.. These local rules are often constrained by the nature of a particular market, or else imposed by strategic suppliers or social customs.. The proper modelling of a mechanism for word-of-mouth transmittal and resulting behavioural effects thus requires a consideration of a number of complex and interacting components: networks of communication, source credibility, learning processes, habituation and memory, external constraints on behaviour, theories of information transfer, and adaptive behaviour.. In this paper we have attempted to address some of these issues in a manner which reflects how agents might act in the real world.. Using the key notions of a limited communication network, a simple learning process, and a satisficing heuristic that may be subject to external constraints, we showed (1) the importance of word-of-mouth communication to both system effectiveness and stability, (2) that the degree distribution of a network is more influential than the way in which agents are connected, but that both are important in more complex environments, (3) that rewiring even a small number of connections to create a small-world network can have dramatic results for the speed of convergence to satisficing distributions and market share allocations, (4) that word-of-mouth continues to be effective when movements between products are constrained by market barriers, and (5) that increases in negative word-of-mouth incurred as a result of market barriers can reduce the market share collectively captured by leading brands, but that this is dependent on the existence of a suitably well-connected network structure.. It is the final finding that is likely to be most surprising and practically relevant for the marketing research field, and suggests that it may not always be in the best interests of a market leader to impose barriers that prevent customers from leaving.. In poorly-connected networks, the effect of barriers on market shares is slight.. In contrast, in well-connected networks, negative word-of-mouth can prevent agents from trying a product that they might otherwise have found satisfying, and this can inflict significant harm on market share.. Products with small market share (which, in the context of our simulations, is generally due to the product offering poor performance) are relatively unaffected by negative word-of-mouth, since most product trials are likely to be unsatisfying in any case.. Agent-based modelling provides a natural way for beginning to investigate the types of dynamics that occur in marketing systems.. Naturally the usefulness of results is for the most part dependent on the quality of the modelling of the two \u2018modules\" comprising network structure and local behaviour.. On the network side, future work might investigate the relationship between degree distributions, the way connections are created and destroyed over time, whether preferential attachment is influential, and the extent to which social identity informs network strucutre, all in larger networks of more heterogenous agents.. On the behavioural side, one might look at the adaptation of satisfaction thresholds during the course of communication, responses to systematic changes in product performances over time, the integration of various information sources, and different market barrier structures.. All these areas provide fascinating opportunities to introduce psychological realities into models of marketing systems and to observe the resulting behaviour of the system under increasingly realistic scenario descriptions."}
{"id": "C-30", "keywords": ["bandwidth", "overlai", "peer-to-peer"], "title": "Bullet: High Bandwidth Data Dissemination Using an Overlay Mesh", "abstract": "In recent years, overlay networks have become an effective alternative to IP multicast for efficient point to multipoint communication across the Internet. Typically, nodes self-organize with the goal of forming an efficient overlay tree, one that meets performance targets without placing undue burden on the underlying network. In this paper, we target high-bandwidth data distribution from a single source to a large number of receivers. Applications include large-file transfers and real-time multimedia streaming. For these applications, we argue that an overlay mesh, rather than a tree, can deliver fundamentally higher bandwidth and reliability relative to typical tree structures. This paper presents Bullet, a scalable and distributed algorithm that enables nodes spread across the Internet to self-organize into a high bandwidth overlay mesh. We construct Bullet around the insight that data should be distributed in a disjoint manner to strategic points in the network. Individual Bullet receivers are then responsible for locating and retrieving the data from multiple points in parallel.Key contributions of this work include: i) an algorithm that sends data to different points in the overlay such that any data object is equally likely to appear at any node, ii) a scalable and decentralized algorithm that allows nodes to locate and recover missing data items, and iii) a complete implementation and evaluation of Bullet running across the Internet and in a large-scale emulation environment reveals up to a factor two bandwidth improvements under a variety of circumstances. In addition, we find that, relative to tree-based solutions, Bullet reduces the need to perform expensive bandwidth probing. In a tree, it is critical that a node's parent delivers a high rate of application data to each child. In Bullet however, nodes simultaneously receive data from multiple sources in parallel, making it less important to locate any single source capable of sustaining a high transmission rate.", "references": ["Scalable Application Layer Multicast", "Bimodal Multicast", "Bittorrent", "Space/Time Trade-offs in Hash Coding with Allowable Errors", "On the Resemblance and Containment of Documents", "Informed Content Delivery Across Adaptive Overlay Networks", "A Digital Fountain Approach to Reliable Distribution of Bulk Data", "Modeling Internet Topology", "Splitstream: High-bandwidth Content Distribution in Cooperative Environments", "Towards Capturing Representative AS-Level Internet Topologies", "FastReplica: Efficient Large File Distribution within Content Delivery Networks", "A Unicast-based Approach for Streaming Multicast", "Lightweight Probabilistic Broadcast", "Lightweight Probabilistic Broadcast", "Equation-based congestion control for unicast applications", "A Reliable Multicast Framework for Light-weight Sessions and Application Level Framing", "Multiple Description Coding: Compression Meets the Network", "A Case For End System Multicast", "Enabling Conferencing Applications on the Internet using an Overlay Multicast Architecture", "End-to-end Available Bandwidth: Measurement Methodology, Dynamics, and Relation with TCP Throughput", "Overcast: Reliable Multicasting with an Overlay Network", " Kazaa media desktop", "Optimal Distribution Tree for Internet Streaming Media", "Using Random Subsets to Build Scalable Network Services", "LT Codes", "Practical Loss-Resilient Codes", "Modeling TCP Throughput: A Simple Model and its Empirical Validation", "Server-based Inference of Internet Link Lossiness", "Resilient Peer-to-Peer Streaming", "Distributing Streaming Media Content Using Cooperative Networking", "A Blueprint for Introducing Disruptive Technology into the Internet", "Shortest Connection Networks and Some Generalizations", "MACEDON: Methodology for Automatically Creating, Evaluating, and Designing Overlay Networks", "SCRIBE: The Design of a Large-scale Event Notification Infrastructure", "Sting: A TCP-based Network Measurement Tool", "Mesh-Based Content Routing Using XML", "Scalability and Accuracy in a Large-Scale Network Emulator"], "full_text": "1. INTRODUCTION In this paper, we consider the following general problem. Given a sender and a large set of interested receivers spread across the Internet, how can we maximize the amount of bandwidth delivered to receivers? Our problem domain includes software or video distribution and real-time multimedia streaming. Traditionally, native IP multicast has been the preferred method for delivering content to a set of receivers in a scalable fashion. However, a number of considerations, including scale, reliability, and congestion control, have limited the wide-scale deployment of IP multicast. Even if all these problems were to be addressed, IP multicast does not consider bandwidth when constructing its distribution tree. More recently, overlays have emerged as a promising alternative to multicast for network-efficient point to multipoint data delivery. Typical overlay structures attempt to mimic the structure of multicast routing trees. In network-layer multicast however, interior nodes consist of high speed routers with limited processing power and extensibility. Overlays, on the other hand, use programmable (and hence extensible) end hosts as interior nodes in the overlay tree, with these hosts acting as repeaters to multiple children down the tree. Overlays have shown tremendous promise for multicast-style applications. However, we argue that a tree structure has fundamental limitations both for high bandwidth multicast and for high reliability. One difficulty with trees is that bandwidth is guaranteed to be monotonically decreasing moving down the tree. Any loss high up the tree will reduce the bandwidth available to receivers lower down the tree. A number of techniques have been proposed to recover from losses and hence improve the available bandwidth in an overlay tree . However, fundamentally, the bandwidth available to any host is limited by the bandwidth available from that node\"s single parent in the tree. Thus, our work operates on the premise that the model for high-bandwidth multicast data dissemination should be re-examined. Rather than sending identical copies of the same data stream to all nodes in a tree and designing a scalable mechanism for recovering from loss, we propose that participants in a multicast overlay cooperate to strategically 282 transmit disjoint data sets to various points in the network. Here, the sender splits data into sequential blocks. Blocks are further subdivided into individual objects which are in turn transmitted to different points in the network. Nodes still receive a set of objects from their parents, but they are then responsible for locating peers that hold missing data objects. We use a distributed algorithm that aims to make the availability of data items uniformly spread across all overlay participants. In this way, we avoid the problem of locating the last object, which may only be available at a few nodes. One hypothesis of this work is that, relative to a tree, this model will result in higher bandwidth-leveraging the bandwidth from simultaneous parallel downloads from multiple sources rather than a single parent-and higher reliability-retrieving data from multiple peers reduces the potential damage from a single node failure. To illustrate Bullet\"s behavior, consider a simple three node overlay with a root R and two children A and B. R has 1 Mbps of available (TCP-friendly) bandwidth to each of A and B. However, there is also 1 Mbps of available bandwidth between A and B. In this example, Bullet would transmit a disjoint set of data at 1 Mbps to each of A and B. A and B would then each independently discover the availability of disjoint data at the remote peer and begin streaming data to one another, effectively achieving a retrieval rate of 2 Mbps. On the other hand, any overlay tree is restricted to delivering at most 1 Mbps even with a scalable technique for recovering lost data. Any solution for achieving the above model must maintain a number of properties. First, it must be TCP friendly . No flow should consume more than its fair share of the bottleneck bandwidth and each flow must respond to congestion signals (losses) by reducing its transmission rate. Second, it must impose low control overhead. There are many possible sources of such overhead, including probing for available bandwidth between nodes, locating appropriate nodes to peer with for data retrieval and redundantly receiving the same data objects from multiple sources. Third, the algorithm should be decentralized and scalable to thousands of participants. No node should be required to learn or maintain global knowledge, for instance global group membership or the set of data objects currently available at all nodes. Finally, the approach must be robust to individual failures. For example, the failure of a single node should result only in a temporary reduction in the bandwidth delivered to a small subset of participants; no single failure should result in the complete loss of data for any significant fraction of nodes, as might be the case for a single node failure high up in a multicast overlay tree. In this context, this paper presents the design and evaluation of Bullet, an algorithm for constructing an overlay mesh that attempts to maintain the above properties. Bullet nodes begin by self-organizing into an overlay tree, which can be constructed by any of a number of existing techniques . Each Bullet node, starting with the root of the underlying tree, then transmits a disjoint set of data to each of its children, with the goal of maintaining uniform representativeness of each data item across all participants. The level of disjointness is determined by the bandwidth available to each of its children. Bullet then employs a scalable and efficient algorithm to enable nodes to quickly locate multiple peers capable of transmitting missing data items to the node. Thus, Bullet layers a high-bandwidth mesh on top of an arbitrary overlay tree. Depending on the type of data being transmitted, Bullet can optionally employ a variety of encoding schemes, for instance Erasure codes or Multiple Description Coding (MDC) , to efficiently disseminate data, adapt to variable bandwidth, and recover from losses. Finally, we use TFRC to transfer data both down the overlay tree and among peers. This ensures that the entire overlay behaves in a congestion-friendly manner, adjusting its transmission rate on a per-connection basis based on prevailing network conditions. One important benefit of our approach is that the bandwidth delivered by the Bullet mesh is somewhat independent of the bandwidth available through the underlying overlay tree. One significant limitation to building high bandwidth overlay trees is the overhead associated with the tree construction protocol. In these trees, it is critical that each participant locates a parent via probing with a high level of available bandwidth because it receives data from only a single source (its parent). Thus, even once the tree is constructed, nodes must continue their probing to adapt to dynamically changing network conditions. While bandwidth probing is an active area of research , accurate results generally require the transfer of a large amount of data to gain confidence in the results. Our approach with Bullet allows receivers to obtain high bandwidth in aggregate using individual transfers from peers spread across the system. Thus, in Bullet, the bandwidth available from any individual peer is much less important than in any bandwidthoptimized tree. Further, all the bandwidth that would normally be consumed probing for bandwidth can be reallocated to streaming data across the Bullet mesh. We have completed a prototype of Bullet running on top of a number of overlay trees.-node overlay running across a wide variety of emulated 20,000 node network topologies shows that Bullet can deliver up to twice the bandwidth of a bandwidth-optimized tree (using an o\ufb04ine algorithm and global network topology information), all while remaining TCP friendly. We also deployed our prototype across the PlanetLab wide-area testbed. For these live Internet runs, we find that Bullet can deliver comparable bandwidth performance improvements. In both cases, the overhead of maintaining the Bullet mesh and locating the appropriate disjoint data is limited to 30 Kbps per node, acceptable for our target high-bandwidth, large-scale scenarios. The remainder of this paper is organized as follows. Section 2 presents Bullet\"s system components including RanSub, informed content delivery, and TFRC. Section 3 then details Bullet, an efficient data distribution system for bandwidth intensive applications. Section 4 evaluates Bullet\"s performance for a variety of network topologies, and compares it to existing multicast techniques. Section 5 places our work in the context of related efforts and Section 6 presents our conclusions. 2. SYSTEM COMPONENTS Our approach to high bandwidth data dissemination centers around the techniques depicted in Figure 1. First, we split the target data stream into blocks which are further subdivided into individual (typically packet-sized) objects. Depending on the requirements of the target applications, objects may be encoded to make data recovery more efficient. Next, we purposefully disseminate disjoint objects 283 A C Original data stream: 1 2 3 4 5 6 1 2 3 5 1 3 4 6 2 4 5 6 TFRC to determine available BW D E 1 2 5 1 3 4 Figure 1: High-level view of Bullet\"s operation. to different clients at a rate determined by the available bandwidth to each client. We use the equation-based TFRC protocol to communicate among all nodes in the overlay in a congestion responsive and TCP friendly manner. Given the above techniques, data is spread across the overlay tree at a rate commensurate with the available bandwidth in the overlay tree. Our overall goal however is to deliver more bandwidth than would otherwise be available through any tree. Thus, at this point, nodes require a scalable technique for locating and retrieving disjoint data from their peers. In essence, these perpendicular links across the overlay form a mesh to augment the bandwidth available through the tree. In Figure 1, node D only has sufficient bandwidth to receive 3 objects per time unit from its parent. However, it is able to locate two peers, C and E, who are able to transmit missing data objects, in this example increasing delivered bandwidth from 3 objects per time unit to 6 data objects per time unit. Locating appropriate remote peers cannot require global state or global communication. Thus, we propose the periodic dissemination of changing, uniformly random subsets of global state to each overlay node once per configurable time period. This random subset contains summary tickets of the objects available at a subset of the nodes in the system. Each node uses this information to request data objects from remote nodes that have significant divergence in object membership. It then attempts to establish a number of these peering relationships with the goals of minimizing overlap in the objects received from each peer and maximizing the total useful bandwidth delivered to it. In the remainder of this section, we provide brief background on each of the techniques that we employ as fundamental building blocks for our work. Section 3 then presents the details of the entire Bullet architecture. 2.1 Data Encoding Depending on the type of data being distributed through the system, a number of data encoding schemes can improve system efficiency. For instance, if multimedia data is being distributed to a set of heterogeneous receivers with variable bandwidth, MDC allows receivers obtaining different subsets of the data to still maintain a usable multimedia stream. For dissemination of a large file among a set of receivers, Erasure codes enable receivers not to focus on retrieving every transmitted data packet. Rather, after obtaining a threshold minimum number of packets, receivers are able to decode the original data stream. Of course, Bullet is amenable to a variety of other encoding schemes or even the null encoding scheme, where the original data stream is transmitted best-effort through the system. In this paper, we focus on the benefits of a special class of erasure-correcting codes used to implement the digital fountain approach. Redundant Tornado codes are created by performing XOR operations on a selected number of original data packets, and then transmitted along with the original data packets. Tornado codes require any (1+ )k correctly received packets to reconstruct the original k data packets, with the typically low reception overhead ( ) of 0.03 \u2212 0.05. In return, they provide significantly faster encoding and decoding times. Additionally, the decoding algorithm can run in real-time, and the reconstruction process can start as soon as sufficiently many packets have arrived. Tornado codes require a predetermined stretch factor (n/k, where n is the total number of encoded packets), and their encoding time is proportional to n. LT codes remove these two limitations, while maintaining a low reception overhead of 0.05. 2.2 RanSub To address the challenge of locating disjoint content within the system, we use RanSub , a scalable approach to distributing changing, uniform random subsets of global state to all nodes of an overlay tree. RanSub assumes the presence of some scalable mechanism for efficiently building and maintaining the underlying tree. A number of such techniques are described in . RanSub distributes random subsets of participating nodes throughout the tree using collect and distribute messages. Collect messages start at the leaves and propagate up the tree, leaving state at each node along the path to the root. Distribute messages start at the root and travel down the tree, using the information left at the nodes during the previous collect round to distribute uniformly random subsets to all participants. Using the collect and distribute messages, RanSub distributes a random subset of participants to each node once per epoch. The lower bound on the length of an epoch is determined by the time it takes to propagate data up then back down the tree, or roughly twice the height of the tree. For appropriately constructed trees, the minimum epoch length will grow with the logarithm of the number of participants, though this is not required for correctness. As part of the distribute message, each participant sends a uniformly random subset of remote nodes, called a distribute set, down to its children. The contents of the distribute set are constructed using the collect set gathered during the previous collect phase. During this phase, each participant sends a collect set consisting of a random subset of its descendant nodes up the tree to the root along with an estimate of its total number of descendants. After the root receives all collect sets and the collect phase completes, the distribute phase begins again in a new epoch. One of the key features of RanSub is the Compact operation. This is the process used to ensure that membership in a collect set propagated by a node to its parent is both random and uniformly representative of all members of the sub-tree rooted at that node. Compact takes multiple fixedsize subsets and the total population represented by each subset as input, and generates a new fixed-size subset. The 284 CSC={Cs}, CSD={Ds} CSF={Fs}, CSG={Gs} CSB={Bs,Cs,Ds}, CSE={Es,Fs,Gs} D GF D GF DSE={As,Bs,Cs, Ds} DSB={As,Es,Fs,Gs} DSG={As,Bs,Cs, Ds,Es,Fs} DSD={As,Bs, Cs,Es,Fs,Gs} DSF={As,Bs,Cs, Ds,Es,Gs} DSC={As,Bs, Ds,Es,Fs,Gs} Figure 2: This example shows the two phases of the RanSub protocol that occur in one epoch. The collect phase is shown on the left, where the collect sets are traveling up the overlay to the root. The distribute phase on the right shows the distribute sets traveling down the overlay to the leaf nodes. members of the resulting set are uniformly random representatives of the input subset members. RanSub offers several ways of constructing distribute sets. For our system, we choose the RanSub-nondescendants option. In this case, each node receives a random subset consisting of all nodes excluding its descendants. This is appropriate for our download structure where descendants are expected to have less content than an ancestor node in most cases. A parent creates RanSub-nondescendants distribute sets for each child by compacting collect sets from that child\"s siblings and its own distribute set. The result is a distribute set that contains a random subset representing all nodes in the tree except for those rooted at that particular child. We depict an example of RanSub\"s collect-distribute process in Figure 2. In the figure, AS stands for node A\"s state. 2.3 Informed Content Delivery Techniques Assuming we can enable a node to locate a peer with disjoint content using RanSub, we need a method for reconciling the differences in the data. Additionally, we require a bandwidth-efficient method with low computational overhead. We chose to implement the approximate reconciliation techniques proposed in for these tasks in Bullet. To describe the content, nodes maintain working sets. The working set contains sequence numbers of packets that have been successfully received by each node over some period of time. We need the ability to quickly discern the resemblance between working sets from two nodes and decide whether a fine-grained reconciliation is beneficial. Summary tickets, or min-wise sketches , serve this purpose. The main idea is to create a summary ticket that is an unbiased random sample of the working set. A summary ticket is a small fixed-size array. Each entry in this array is maintained by a specific permutation function. The goal is to have each entry populated by the element with the smallest permuted value. To insert a new element into the summary ticket, we apply the permutation functions in order and update array values as appropriate. The permutation function can be thought of as a specialized hash function. The choice of permutation functions is important as the quality of the summary ticket depends directly on the randomness properties of the permutation functions. Since we require them to have a low computational overhead, we use simple permutation functions, such as Pj(x) = (ax+b)mod|U|, where U is the universe size (dependant on the data encoding scheme). To compute the resemblance between two working sets, we compute the number of summary ticket entries that have the same value, and divide it by the total number of entries in the summary tickets. Figure 3 shows the way the permutation functions are used to populate the summary ticket. 12 10 2 27 18 19 40 Workingset 14 42 17 33 38 15 12 P1 33 29 28 44 57 15 P2 22 28 45 61 14 51 Pn\u2026 Summary ticket minminmin 10 Figure 3: Example showing a sample summary ticket being constructed from the working set. To perform approximate fine-grain reconciliation, a peer A sends its digest to peer B and expects to receive packets not described in the digest. For this purpose, we use a Bloom filter , a bit array of size m with k independent associated hash functions. An element s from the set of received keys S = {so, s2, . . . , sn\u22121} is inserted into the filter by computing the hash values h0, h1, . . . , hk\u22121 of s and setting the bits in the array that correspond to the hashed 285 values. To check whether an element x is in the Bloom filter, we hash it using the hash functions and check whether all positions in the bit array are set. If at least one is not set, we know that the Bloom filter does not contain x. When using Bloom filters, the insertion of different elements might cause all the positions in the bit array corresponding to an element that is not in the set to be nonzero. In this case, we have a false positive. Therefore, it is possible that peer B will not send a packet to peer A even though A is missing it. On the other hand, a node will never send a packet that is described in the Bloom filter, i.e. there are no false negatives. The probability of getting a false positive pf on the membership query can be expressed as a function of the ratio m and the number of hash functions k: pf = (1 \u2212 e\u2212kn/m )k . We can therefore choose the size of the Bloom filter and the number of hash functions that will yield a desired false positive ratio. 2.4 TCP Friendly Rate Control Although most traffic in the Internet today is best served by TCP, applications that require a smooth sending rate and that have a higher tolerance for loss often find TCP\"s reaction to a single dropped packet to be unnecessarily severe. TCP Friendly Rate Control, or TFRC, targets unicast streaming multimedia applications with a need for less drastic responses to single packet losses . TCP halves the sending rate as soon as one packet loss is detected. Alternatively, TFRC is an equation-based congestion control protocol that is based on loss events, which consist of multiple packets being dropped within one round-trip time. Unlike TCP, the goal of TFRC is not to find and use all available bandwidth, but instead to maintain a relatively steady sending rate while still being responsive to congestion. To guarantee fairness with TCP, TFRC uses the response function that describes the steady-state sending rate of TCP to determine the transmission rate in TFRC. The formula of the TCP response function used in TFRC to describe the sending rate is: T = s \u00d52p +tRT O(3 \u00d53p )p(1+32p2) This is the expression for the sending rate T in bytes/second, as a function of the round-trip time R in seconds, loss event rate p, packet size s in bytes, and TCP retransmit value tRT O in seconds. TFRC senders and receivers must cooperate to achieve a smooth transmission rate. The sender is responsible for computing the weighted round-trip time estimate R between sender and receiver, as well as determining a reasonable retransmit timeout value tRT O. In most cases, using the simple formula tRT O = 4R provides the necessary fairness with TCP. The sender is also responsible for adjusting the sending rate T in response to new values of the loss event rate p reported by the receiver. The sender obtains a new measure for the loss event rate each time a feedback packet is received from the receiver. Until the first loss is reported, the sender doubles its transmission rate each time it receives feedback just as TCP does during slow-start. The main role of the receiver is to send feedback to the sender once per round-trip time and to calculate the loss event rate included in the feedback packets. To obtain the loss event rate, the receiver maintains a loss interval array that contains values for the last eight loss intervals. A loss interval is defined as the number of packets received correctly between two loss events. The array is continually updated as losses are detected. A weighted average is computed based on the sum of the loss interval values, and the inverse of the sum is the reported loss event rate, p. When implementing Bullet, we used an unreliable version of TFRC. We wanted a transport protocol that was congestion aware and TCP friendly. Lost packets were more easily recovered from other sources rather than waiting for a retransmission from the initial sender. Hence, we eliminate retransmissions from TFRC. Further, TFRC does not aggressively seek newly available bandwidth like TCP, a desirable trait in an overlay tree where there might be multiple competing flows sharing the same links. For example, if a leaf node in the tree tried to aggressively seek out new bandwidth, it could create congestion all the way up to the root of the tree. By using TFRC we were able to avoid these scenarios. 3. BULLET Bullet is an efficient data distribution system for bandwidth intensive applications. While many current overlay network distribution algorithms use a distribution tree to deliver data from the tree\"s root to all other nodes, Bullet layers a mesh on top of an original overlay tree to increase overall bandwidth to all nodes in the tree. Hence, each node receives a parent stream from its parent in the tree and some number of perpendicular streams from chosen peers in the overlay. This has significant bandwidth impact when a single node in the overlay is unable to deliver adequate bandwidth to a receiving node. Bullet requires an underlying overlay tree for RanSub to deliver random subsets of participants\"s state to nodes in the overlay, informing them of a set of nodes that may be good candidates for retrieving data not available from any of the node\"s current peers and parent. While we also use the underlying tree for baseline streaming, this is not critical to Bullet\"s ability to efficiently deliver data to nodes in the overlay. As a result, Bullet is capable of functioning on top of essentially any overlay tree. In our experiments, we have run Bullet over random and bandwidth-optimized trees created o\ufb04ine (with global topological knowledge). Bullet registers itself with the underlying overlay tree so that it is informed when the overlay changes as nodes come and go or make performance transformations in the overlay. As with streaming overlays trees, Bullet can use standard transports such as TCP and UDP as well as our implementation of TFRC. For the remainder of this paper, we assume the use of TFRC since we primarily target streaming highbandwidth content and we do not require reliable or in-order delivery. For simplicity, we assume that packets originate at the root of the tree and are tagged with increasing sequence numbers. Each node receiving a packet will optionally forward it to each of its children, depending on a number of factors relating to the child\"s bandwidth and its relative position in the tree. 3.1 Finding Overlay Peers RanSub periodically delivers subsets of uniformly random selected nodes to each participant in the overlay. Bullet receivers use these lists to locate remote peers able to transmit missing data items with good bandwidth. RanSub messages contain a set of summary tickets that include a small (120 286 bytes) summary of the data that each node contains. RanSub delivers subsets of these summary tickets to nodes every configurable epoch (5 seconds by default). Each node in the tree maintains a working set of the packets it has received thus far, indexed by sequence numbers. Nodes associate each working set with a Bloom filter that maintains a summary of the packets received thus far. Since the Bloom filter does not exceed a specific size (m) and we would like to limit the rate of false positives, Bullet periodically cleans up the Bloom filter by removing lower sequence numbers from it. This allows us to keep the Bloom filter population n from growing at an unbounded rate. The net effect is that a node will attempt to recover packets for a finite amount of time depending on the packet arrival rate. Similarly, Bullet removes older items that are not needed for data reconstruction from its working set and summary ticket. We use the collect and distribute phases of RanSub to carry Bullet summary tickets up and down the tree. In our current implementation, we use a set size of 10 summary tickets, allowing each collect and distribute to fit well within the size of a non-fragmented IP packet. Though Bullet supports larger set sizes, we expect this parameter to be tunable to specific applications\" needs. In practice, our default size of 10 yields favorable results for a variety of overlays and network topologies. In essence, during an epoch a node receives a summarized partial view of the system\"s state at that time. Upon receiving a random subset each epoch, a Bullet node may choose to peer with the node having the lowest similarity ratio when compared to its own summary ticket. This is done only when the node has sufficient space in its sender list to accept another sender (senders with lackluster performance are removed from the current sender list as described in section 3.4). Once a node has chosen the best node it sends it a peering request containing the requesting node\"s Bloom filter. Such a request is accepted by the potential sender if it has sufficient space in its receiver list for the incoming receiver. Otherwise, the send request is rejected (space is periodically created in the receiver lists as further described in section 3.4). 3.2 Recovering Data From Peers Assuming it has space for the new peer, a recipient of the peering request installs the received Bloom filter and will periodically transmit keys not present in the Bloom filter to the requesting node. The requesting node will refresh its installed Bloom filters at each of its sending peers periodically. Along with the fresh filter, a receiving node will also assign a portion of the sequence space to each of its senders. In this way, a node is able the reduce the likelihood that two peers simultaneously transmit the same key to it, wasting network resources. A node divides the sequence space in its current working set among each of its senders uniformly. As illustrated in Figure 4, a Bullet receiver views the data space as a matrix of packet sequences containing s rows, where s is its current number of sending peers. A receiver periodically (every 5 seconds by default) updates each sender with its current Bloom filter and the range of sequences covered in its Bloom filter. This identifies the range of packets that the receiver is currently interested in recovering. Over time, this range shifts as depicted in Figure 4-b). In addition, the receiving node assigns to each sender a row from the matrix, labeled mod. A sender will forward packets to b) Mod = a) Senders = 7Mod = Figure 4: A Bullet receiver views data as a matrix of sequenced packets with rows equal to the number of peer senders it currently has. It requests data within the range (Low, High) of sequence numbers based on what it has received. a) The receiver requests a specific row in the sequence matrix from each sender. b) As it receives more data, the range of sequences advances and the receiver requests different rows from senders. the receiver that have a sequence number x such that x modulo s equals the mod number. In this fashion, receivers register to receive disjoint data from their sending peers. By specifying ranges and matrix rows, a receiver is unlikely to receive duplicate data items, which would result in wasted bandwidth. A duplicate packet, however, may be received when a parent recovers a packet from one of its peers and relays the packet to its children (and descendants). In this case, a descendant would receive the packet out of order and may have already recovered it from one of its peers. In practice, this wasteful reception of duplicate packets is tolerable; less than 10% of all received packets are duplicates in our experiments. 3.3 Making Data Disjoint We now provide details of Bullet\"s mechanisms to increase the ease by which nodes can find disjoint data not provided by parents. We operate on the premise that the main challenge in recovering lost data packets transmitted over an overlay distribution tree lies in finding the peer node housing the data to recover. Many systems take a hierarchical approach to this problem, propagating repair requests up the distribution tree until the request can be satisfied. This ultimately leads to scalability issues at higher levels in the hierarchy particularly when overlay links are bandwidthconstrained. On the other hand, Bullet attempts to recover lost data from any non-descendant node, not just ancestors, thereby increasing overall system scalability. In traditional overlay distribution trees, packets are lost by the transmission transport and/or the network. Nodes attempt to stream data as fast as possible to each child and have essentially no control over which portions of the data stream are dropped by the transport or network. As a result, the streaming subsystem has no control over how many nodes in the system will ultimately receive a particular portion of the data. If few nodes receive a particular range of packets, recovering these pieces of data becomes more difficult, requiring increased communication costs, and leading to scalability problems. In contrast, Bullet nodes are aware of the bandwidth achievable to each of its children using the underlying transport. If 287 a child is unable to receive the streaming rate that the parent receives, the parent consciously decides which portion of the data stream to forward to the constrained child. In addition, because nodes recover data from participants chosen uniformly at random from the set of non-descendants, it is advantageous to make each transmitted packet recoverable from approximately the same number of participant nodes. That is, given a randomly chosen subset of peer nodes, it is with the same probability that each node has a particular data packet. While not explicitly proven here, we believe that this approach maximizes the probability that a lost data packet can be recovered, regardless of which packet is lost. To this end, Bullet distributes incoming packets among one or more children in hopes that the expected number of nodes receiving each packet is approximately the same. A node p maintains for each child, i, a limiting and sending factor, lfi and sfi. These factors determine the proportion of p\"s received data rate that it will forward to each child. The sending factor sfi is the portion of the parent stream (rate) that each child should own based on the number of descendants the child has. The more descendants a child has, the larger the portion of received data it should own. The limiting factor lfi represents the proportion of the parent rate beyond the sending factor that each child can handle. For example, a child with one descendant, but high bandwidth would have a low sending factor, but a very high limiting factor. Though the child is responsible for owning a small portion of the received data, it actually can receive a large portion of it. Because RanSub collects descendant counts di for each child i, Bullet simply makes a call into RanSub when sending data to determine the current sending factors of its children. For each child i out of k total, we set the sending factor to be: sfi = di\u00c8k j=1 dj In addition, a node tracks the data successfully transmitted via the transport. That is, Bullet data transport sockets are non-blocking; successful transmissions are send attempts that are accepted by the non-blocking transport. If the transport would block on a send (i.e., transmission of the packet would exceed the TCP-friendly fair share of network resources), the send fails and is counted as an unsuccessful send attempt. When a data packet is received by a parent, it calculates the proportion of the total data stream that has been sent to each child, thus far, in this epoch. It then assigns ownership of the current packet to the child with sending proportion farthest away from its sfi as illustrated in Figure 5. Having chosen the target of a particular packet, the parent attempts to forward the packet to the child. If the send is not successful, the node must find an alternate child to own the packet. This occurs when a child\"s bandwidth is not adequate to fulfill its responsibilities based on its descendants (sfi). To compensate, the node attempts to deterministically find a child that can own the packet (as evidenced by its transport accepting the packet). The net result is that children with more than adequate bandwidth will own more of their share of packets than those with inadequate bandwidth. In the event that no child can accept a packet, it must be dropped, corresponding to the case where the sum of all children bandwidths is inadequate to serve the received foreach child in children { if ( (child->sent / total_sent) < child->sending_factor) target_child = child; if (!senddata( target_child->addr, msg, size, key)) { // send succeeded target_child->sent++; target_child->child_filter.insert(got_key); sent_packet = 1; foreach child in children { should_send = 0; if (!sent_packet) // transfer ownership should_send = 1; else // test for available bandwidth if ( key % (1.0/child->limiting_factor) == 0 ) should_send = 1; if (should_send) { if (!senddata( child->addr, msg, size, key)) { if (!sent_packet) // i received ownership child->sent++; else increase(child->limiting_factor); child->child_filter.insert(got_key); sent_packet = 1; else // send failed if (sent_packet) // was for extra bw decrease(child->limiting_factor); Figure 5: Pseudo code for Bullet\"s disjoint data send routine stream. While making data more difficult to recover, Bullet still allows for recovery of such data to its children. The sending node will cache the data packet and serve it to its requesting peers. This process allows its children to potentially recover the packet from one of their own peers, to whom additional bandwidth may be available. Once a packet has been successfully sent to the owning child, the node attempts to send the packet to all other children depending on the limiting factors lfi. For each child i, a node attempts to forward the packet deterministically if the packet\"s sequence modulo 1/lfi is zero. Essentially, this identifies which lfi fraction of packets of the received data stream should be forwarded to each child to make use of the available bandwidth to each. If the packet transmission is successful, lfi is increased such that one more packet is to be sent per epoch. If the transmission fails, lfi is decreased by the same amount. This allows children limiting factors to be continuously adjusted in response to changing network conditions. It is important to realize that by maintaining limiting factors, we are essentially using feedback from children (by observing transport behavior) to determine the best data to stop sending during times when a child cannot handle the entire parent stream. In one extreme, if the sum of children bandwidths is not enough to receive the entire parent stream, each child will receive a completely disjoint data stream of packets it owns. In the other extreme, if each 288 child has ample bandwidth, it will receive the entire parent stream as each lfi would settle on 1.0. In the general case, our owning strategy attempts to make data disjoint among children subtrees with the guiding premise that, as much as possible, the expected number of nodes receiving a packet is the same across all packets. 3.4 Improving the Bullet Mesh Bullet allows a maximum number of peering relationships. That is, a node can have up to a certain number of receivers and a certain number of senders (each defaults to 10 in our implementation). A number of considerations can make the current peering relationships sub-optimal at any given time: i) the probabilistic nature of RanSub means that a node may not have been exposed to a sufficiently appropriate peer, ii) receivers greedily choose peers, and iii) network conditions are constantly changing. For example, a sender node may wind up being unable to provide a node with very much useful (non-duplicate) data. In such a case, it would be advantageous to remove that sender as a peer and find some other peer that offers better utility. Each node periodically (every few RanSub epochs) evaluates the bandwidth performance it is receiving from its sending peers. A node will drop a peer if it is sending too many duplicate packets when compared to the total number of packets received. This threshold is set to 50% by default. If no such wasteful sender is found, a node will drop the sender that is delivering the least amount of useful data to it. It will replace this sender with some other sending peer candidate, essentially reserving a trial slot in its sender list. In this way, we are assured of keeping the best senders seen so far and will eliminate senders whose performance deteriorates with changing network conditions. Likewise, a Bullet sender will periodically evaluate its receivers. Each receiver updates senders of the total received bandwidth. The sender, knowing the amount of data it has sent to each receiver, can determine which receiver is benefiting the least by peering with this sender. This corresponds to the one receiver acquiring the least portion of its bandwidth through this sender. The sender drops this receiver, creating an empty slot for some other trial receiver. This is similar to the concept of weans presented in . 4. EVALUATION We have evaluated Bullet\"s performance in real Internet environments as well as the ModelNet IP emulation framework. While the bulk of our experiments use ModelNet, we also report on our experience with Bullet on the PlanetLab Internet testbed . In addition, we have implemented a number of underlying overlay network trees upon which Bullet can execute. Because Bullet performs well over a randomly created overlay tree, we present results with Bullet running over such a tree compared against an o\ufb04ine greedy bottleneck bandwidth tree algorithm using global topological information described in Section 4.1. All of our implementations leverage a common development infrastructure called MACEDON that allows for the specification of overlay algorithms in a simple domainspecific language. It enables the reuse of the majority of common functionality in these distributed systems, including probing infrastructures, thread management, message passing, and debugging environment. As a result, we believe that our comparisons qualitatively show algorithmic differences rather than implementation intricacies. lines of code in this infrastructure. Our ModelNet experiments make use of 50 2Ghz Pentium4\"s running Linux 2.4.20 and interconnected with 100 Mbps and 1 Gbps Ethernet switches. For the majority of these experiments, we multiplex one thousand instances (overlay participants) of our overlay applications across the 50 Linux nodes (20 per machine). In ModelNet, packet transmissions are routed through emulators responsible for accurately emulating the hop-by-hop delay, bandwidth, and congestion of a network topology. In our evaluations, we used four 1.4Ghz Pentium III\"s running FreeBSD-4.7 as emulators. This platform supports approximately 2-3 Gbps of aggregate simultaneous communication among end hosts. For most of our ModelNet experiments, we use 20,000-node INET-generated topologies . We randomly assign our participant nodes to act as clients connected to one-degree stub nodes in the topology. We randomly select one of these participants to act as the source of the data stream. Propagation delays in the network topology are calculated based on the relative placement of the network nodes in the plane by INET. Based on the classification in , we classify network links as being Client-Stub, Stub-Stub, TransitStub, and Transit-Transit depending on their location in the network. We restrict topological bandwidth by setting the bandwidth for each link depending on its type. Each type of link has an associated bandwidth range from which the bandwidth is chosen uniformly at random. By changing these ranges, we vary bandwidth constraints in our topologies. For our experiments, we created three different ranges corresponding to low, medium, and high bandwidths relative to our typical streaming rates of 600- Kbps as specified in Table 1. While the presented ModelNet results are restricted to two topologies with varying bandwidth constraints, the results of experiments with additional topologies all show qualitatively similar behavior. We do not implement any particular coding scheme for our experiments. Rather, we assume that either each sequence number directly specifies a particular data block and the block offset for each packet, or we are distributing data within the same block for LT Codes, e.g., when distributing a file. 4.1 Offline Bottleneck Bandwidth Tree One of our goals is to determine Bullet\"s performance relative to the best possible bandwidth-optimized tree for a given network topology. This allows us to quantify the possible improvements of an overlay mesh constructed using Bullet relative to the best possible tree. While we have not yet proven this, we believe that this problem is NP-hard. Thus, in this section we present a simple greedy o\ufb04ine algorithm to determine the connectivity of a tree likely to deliver a high level of bandwidth. In practice, we are not aware of any scalable online algorithms that are able to deliver the bandwidth of an o\ufb04ine algorithm. At the same time, trees constructed by our algorithm tend to be long and skinny making them less resilient to failures and inappropriate for delay sensitive applications (such as multimedia streaming). In addition to any performance comparisons, a Bullet mesh has much lower depth than the bottleneck tree and is more resilient to failure, as discussed in Section 4.6. 289 Topology classification Client-Stub Stub-Stub Transit-Stub Transit-Transit Low bandwidth 300-600 500--- Medium bandwidth 800-------- Table 1: Bandwidth ranges for link types used in our topologies expressed in Kbps. Specifically, we consider the following problem: given complete knowledge of the topology (individual link latencies, bandwidth, and packet loss rates), what is the overlay tree that will deliver the highest bandwidth to a set of predetermined overlay nodes? We assume that the throughput of the slowest overlay link (the bottleneck link) determines the throughput of the entire tree. We are, therefore, trying to find the directed overlay tree with the maximum bottleneck link. Accordingly, we refer to this problem as the overlay maximum bottleneck tree (OMBT). In a simplified case, assuming that congestion only exists on access links and there are no lossy links, there exists an optimal algorithm . In the more general case of contention on any physical link, and when the system is allowed to choose the routing path between the two endpoints, this problem is known to be NP-hard , even in the absence of link losses. For the purposes of this paper, our goal is to determine a good overlay streaming tree that provides each overlay participant with substantial bandwidth, while avoiding overlay links with high end-to-end loss rates. We make the following assumptions: 1. The routing path between any two overlay participants is fixed. This closely models the existing overlay network model with IP for unicast routing. 2. The overlay tree will use TCP-friendly unicast connections to transfer data point-to-point. 3. In the absence of other flows, we can estimate the throughput of a TCP-friendly flow using a steady-state formula . 4. When several (n) flows share the same bottleneck link, each flow can achieve throughput of at most c , where c is the physical capacity of the link. Given these assumptions, we concentrate on estimating the throughput available between two participants in the overlay. We start by calculating the throughput using the steady-state formula. We then route the flow in the network, and consider the physical links one at a time. On each physical link, we compute the fair-share for each of the competing flows. The throughput of an overlay link is then approximated by the minimum of the fair-shares along the routing path, and the formula rate. If some flow does not require the same share of the bottleneck link as other competing flows (i.e., its throughput might be limited by losses elsewhere in the network), then the other flows might end up with a greater share than the one we compute. We do not account for this, as the major goal of this estimate is simply to avoid lossy and highly congested physical links. More formally, we define the problem as follows: Overlay Maximum Bottleneck Tree (OMBT). Given a physical network represented as a graph G = (V, E), set of overlay participants P \u2282 V , source node (s \u2208 P), bandwidth B : E \u2192 R+ , loss rate L : E \u2192 , propagation delay D : E \u2192 R+ of each link, set of possible overlay links O = {(v, w) | v, w \u2208 P, v = w}, routing table RT : O \u00d7 E \u2192 {0, 1}, find the overlay tree T = {o | o \u2208 O} (|T| = |P| \u2212 1, \u2200v \u2208 P there exists a path ov = s \u2740 v) that maximizes min o|o\u2208T (min(f(o), min e|e\u2208o b(e) |{p | p \u2208 T, e \u2208 p}| )) where f(o) is the TCP steady-state sending rate, computed from round-trip time d(o) = \u00c8e\u2208o d(e) + \u00c8e\u2208o d(e) (given overlay link o = (v, w), o = (w, v)), and loss rate l(o) = 1 \u2212 \u00c9e\u2208o (1 \u2212 l(e)). We write e \u2208 o to express that link e is included in the o\"s routing path (RT(o, e) = 1). Assuming that we can estimate the throughput of a flow, we proceed to formulate a greedy OMBT algorithm. This algorithm is non-optimal, but a similar approach was found to perform well . Our algorithm is similar to the Widest Path Heuristic (WPH) , and more generally to Prim\"s MST algorithm . During its execution, we maintain the set of nodes already in the tree, and the set of remaining nodes. To grow the tree, we consider all the overlay links leading from the nodes in the tree to the remaining nodes. We greedily pick the node with the highest throughput overlay link. Using this overlay link might cause us to route traffic over physical links traversed by some other tree flows. Since we do not re-examine the throughput of nodes that are already in the tree, they might end up being connected to the tree with slower overlay links than initially estimated. However, by attaching the node with the highest residual bandwidth at every step, we hope to lessen the effects of after-the-fact physical link sharing. With the synthetic topologies we use for our emulation environment, we have not found this inaccuracy to severely impact the quality of the tree. 4.2 Bullet vs. Streaming We have implemented a simple streaming application that is capable of streaming data over any specified tree. In our implementation, we are able to stream data through overlay trees using UDP, TFRC, or TCP. nodes receives via this streaming as time progresses on the x-axis. In this example, we use TFRC to stream 600 Kbps over our o\ufb04ine bottleneck bandwidth tree and a random tree (other random trees exhibit qualitatively similar behavior). In these experiments, streaming begins 100 seconds into each run. While the random tree delivers an achieved bandwidth of under 100 Kbps, our o\ufb04ine algorithm overlay delivers approximately 400 Kbps of data. For this experiment, bandwidths were set to the medium range from Table 1. We believe that any degree-constrained online bandwidth overlay tree algorithm would exhibit similar (or lower) 0 50 100 150 200 250 300 350 400 Bandwidth(Kbps) Time (s) Bottleneck bandwidth tree Random tree Figure 6: Achieved bandwidth over time for TFRC streaming over the bottleneck bandwidth tree and a random tree. optimized overlay. Hence, Bullet\"s goal is to overcome this bandwidth limit by allowing for the perpendicular reception of data and by utilizing disjoint data flows in an attempt to match or exceed the performance of our o\ufb04ine algorithm. To evaluate Bullet\"s ability to exceed the bandwidth achievable via tree distribution overlays, we compare Bullet running over a random overlay tree to the streaming behavior shown in Figure 6. Figure 7 shows the average bandwidth received by each node (labeled Useful total) with standard deviation. The graph also plots the total amount of data received and the amount of data a node receives from its parent. For this topology and bandwidth setting, Bullet was able to achieve an average bandwidth of 500 Kbps, fives times that achieved by the random tree and more than 25% higher than the o\ufb04ine bottleneck bandwidth algorithm. Further, the total bandwidth (including redundant data) received by each node is only slightly higher than the useful content, meaning that Bullet is able to achieve high bandwidth while wasting little network resources. Bullet\"s use of TFRC in this example ensures that the overlay is TCP friendly throughout. The average per-node control overhead is approximately 30 Kbps. By tracing certain packets as they move through the system, we are able to acquire link stress estimates of our system. Though the link stress can be different for each packet since each can take a different path through the overlay mesh, we average link stress due to each traced packet. For this experiment, Bullet has an average link stress of approximately 1.5 with an absolute maximum link stress of 22. The standard deviation in most of our runs is fairly high because of the limited bandwidth randomly assigned to some Client-Stub and Stub-Stub links. We feel that this is consistent with real Internet behavior where clients have widely varying network connectivity. A time slice is shown in Figure 8 that plots the CDF of instantaneous bandwidths that each node receives. The graph shows that few client nodes receive inadequate bandwidth even though they are bandwidth constrained. The distribution rises sharply starting at approximately 500 Kbps. The vast majority of nodes receive a stream of 500-600 Kbps. 0 50 100 150 200 250 300 350 400 450 500 Bandwidth(Kbps) Time (s) Raw total Useful total From parent Figure 7: Achieved bandwidth over time for Bullet over a random tree. 0.2 0.4 0.6 0.8 0 100 200 300 400 500 600 700 800 Percentageofnodes Bandwidth(Kbps) Figure 8: CDF of instantaneous achieved bandwidth at time 430 seconds. available bandwidth of the underlying topology. Table 1 describes representative bandwidth settings for our streaming rate of 600 Kbps. The intent of these settings is to show a scenario where more than enough bandwidth is available to achieve a target rate even with traditional tree streaming, an example of where it is slightly not sufficient, and one in which the available bandwidth is quite restricted. Figure 9 shows achieved bandwidths for Bullet and the bottleneck bandwidth tree over time generated from topologies with bandwidths in each range. In all of our experiments, Bullet outperforms the bottleneck bandwidth tree by a factor of up to 100%, depending on how much bandwidth is constrained in the underlying topology. In one extreme, having more than ample bandwidth, Bullet and the bottleneck bandwidth tree are both able to stream at the requested rate (600 Kbps in our example). In the other extreme, heavily constrained topologies allow Bullet to achieve twice the bandwidth achievable via the bottleneck bandwidth tree. For all other topologies, Bullet\"s benefits are somewhere in between. In our example, Bullet running over our medium-constrained bandwidth topology is able to outperform the bottleneck bandwidth tree by a factor of 25%. Further, 0 50 100 150 200 250 300 350 400 Bandwidth(Kbps) Time (s) Bullet - High Bandwidth Bottleneck tree - High Bandwidth Bullet - Medium Bandwidth Bottleneck tree - Medium Bandwidth Bullet - Low Bandwidth Bottleneck tree - Low Bandwidth Figure 9: Achieved bandwidth for Bullet and bottleneck tree over time for high, medium, and low bandwidth topologies. be extremely difficult for any online tree-based algorithm to exceed the bandwidth achievable by our o\ufb04ine bottleneck algorithm that makes use of global topological information. For instance, we built a simple bandwidth optimizing overlay tree construction based on Overcast . The resulting dynamically constructed trees never achieved more than 75% of the bandwidth of our own o\ufb04ine algorithm. 4.3 Creating Disjoint Data Bullet\"s ability to deliver high bandwidth levels to nodes depends on its disjoint transmission strategy. That is, when bandwidth to a child is limited, Bullet attempts to send the correct portions of data so that recovery of the lost data is facilitated. A Bullet parent sends different data to its children in hopes that each data item will be readily available to nodes spread throughout its subtree. It does so by assigning ownership of data objects to children in a manner that makes the expected number of nodes holding a particular data object equal for all data objects it transmits. Figure 10 shows the resulting bandwidth over time for the non-disjoint strategy in which a node (and more importantly, the root of the tree) attempts to send all data to each of its children (subject to independent losses at individual child links). Because the children transports throttle the sending rate at each parent, some data is inherently sent disjointly (by chance). By not explicitly choosing which data to send its child, this approach deprives Bullet of 25% of its bandwidth capability, when compared to the case when our disjoint strategy is enabled in Figure 7. 4.4 Epidemic Approaches In this section, we explore how Bullet compares to data dissemination approaches that use some form of epidemic routing. We implemented a form of gossiping, where a node forwards non-duplicate packets to a randomly chosen number of nodes in its local view. This technique does not use a tree for dissemination, and is similar to lpbcast (recently improved to incorporate retrieval of data objects ). We do not disseminate packets every T seconds; instead we forward them as soon as they arrive. 0 50 100 150 200 250 300 350 400 450 500 Bandwidth(Kbps) Time (s) Raw total Useful total From parent Figure 10: Achieved bandwidth over time using nondisjoint data transmission. We also implemented a pbcast-like approach for retrieving data missing from a data distribution tree. The idea here is that nodes are expected to obtain most of their data from their parent. Nodes then attempt to retrieve any missing data items through gossiping with random peers. Instead of using gossiping with a fixed number of rounds for each packet, we use anti-entropy with a FIFO Bloom filter to attempt to locate peers that hold any locally missing data items. To make our evaluation conservative, we assume that nodes employing gossip and anti-entropy recovery are able to maintain full group membership. While this might be difficult in practice, we assume that RanSub could also be applied to these ideas, specifically in the case of anti-entropy recovery that employs an underlying tree. Further, we also allow both techniques to reuse other aspects of our implementation: Bloom filters, TFRC transport, etc. To reduce the number of duplicate packets, we use less peers in each round (5) than Bullet (10). For our configuration, we experimentally found that 5 peers results in the best performance with the lowest overhead. In our experiments, increasing the number of peers did not improve the average bandwidth achieved throughout the system. To allow TFRC enough time to ramp up to the appropriate TCP-friendly sending rate, we set the epoch length for anti-entropy recovery to 20 seconds. For these experiments,-node INET topology with no explicit physical link losses. We set link bandwidths according to the medium range from Table 1, and randomly assign 100 overlay participants. The randomly chosen root either streams at 900 Kbps (over a random tree for Bullet and greedy bottleneck tree for anti-entropy recovery), or sends packets at that rate to randomly chosen nodes for gossiping. Figure 11 shows the resulting bandwidth over time achieved by Bullet and the two epidemic approaches. As expected, Bullet comes close to providing the target bandwidth to all participants, achieving approximately 60 percent more then gossiping and streaming with anti-entropy. The two epidemic techniques send an excessive number of duplicates, effectively reducing the useful bandwidth provided to each node. More importantly, both approaches assign equal significance to other peers, 0 50 100 150 200 250 300 Bandwidth(Kbps) Time (s) Push gossiping raw Streaming w/AE raw Bullet raw Bullet useful Push gossiping useful Streaming w/AE useful Figure 11: Achieved bandwidth over time for Bullet and epidemic approaches. width and the similarity ratio. Bullet, on the other hand, establishes long-term connections with peers that provide good bandwidth and disjoint content, and avoids most of the duplicates by requesting disjoint data from each node\"s peers. 4.5 Bullet on a Lossy Network To evaluate Bullet\"s performance under more lossy network conditions, we have modified our 20,000-node topologies used in our previous experiments to include random packet losses. ModelNet allows the specification of a packet loss rate in the description of a network link. Our goal by modifying these loss rates is to simulate queuing behavior when the network is under load due to background network traffic. To effect this behavior, we first modify all non-transit links in each topology to have a packet loss rate chosen uniformly random from [0, 0.003] resulting in a maximum loss rate of 0.3%. Transit links are likewise modified, but with a maximum loss rate of 0.1%. Similar to the approach in , we randomly designated 5% of the links in the topologies as overloaded and set their loss rates uniformly random from [0.05, 0.1] resulting in a maximum packet loss rate of 10%. Figure 12 shows achieved bandwidths for streaming over Bullet and using our greedy o\ufb04ine bottleneck bandwidth tree. Because losses adversely affect the bandwidth achievable over TCP-friendly transport and since bandwidths are strictly monotonically decreasing over a streaming tree, treebased algorithms perform considerably worse than Bullet when used on a lossy network. In all cases, Bullet delivers at least twice as much bandwidth than the bottleneck bandwidth tree. Additionally, losses in the low bandwidth topology essentially keep the bottleneck bandwidth tree from delivering any data, an artifact that is avoided by Bullet. 4.6 Performance Under Failure In this section, we discuss Bullet\"s behavior in the face of node failure. In contrast to streaming distribution trees that must quickly detect and make tree transformations to overcome failure, Bullet\"s failure resilience rests on its ability to maintain a higher level of achieved bandwidth by virtue of perpendicular (peer) streaming. 0 50 100 150 200 250 300 350 400 Bandwidth(Kbps) Time (s) Bullet - High Bandwidth Bullet - Medium Bandwidth Bottleneck tree - High Bandwidth Bottleneck tree - Medium Bandwidth Bullet - Low Bandwidth Bottleneck tree - Low Bandwidth Figure 12: Achieved bandwidths for Bullet and bottleneck bandwidth tree over a lossy network topology. disruption in service, Bullet nodes are able compensate for this by receiving data from peers throughout the outage. Because Bullet, and, more importantly, RanSub makes use of an underlying tree overlay, part of Bullet\"s failure recovery properties will depend on the failure recovery behavior of the underlying tree. For the purposes of this discussion, we simply assume the worst-case scenario where an underlying tree has no failure recovery. In our failure experiments, we fail one of root\"s children ( nodes as descendants) 250 seconds after data streaming is started. By failing one of root\"s children, we are able to show Bullet\"s worst-case performance under a single node failure. In our first scenario, we disable failure detection in RanSub so that after a failure occurs, Bullet nodes request data only from their current peers. That is, at this point, RanSub stops functioning and no new peer relationships are created for the remainder of the run. Figure 13 shows Bullet\"s achieved bandwidth over time for this case. While the average achieved rate drops from 500 Kbps to 350 Kbps, most nodes (including the descendants of the failed root child) are able to recover a large portion of the data rate. Next, we enable RanSub failure detection that recognizes a node\"s failure when a RanSub epoch has lasted longer than the predetermined maximum (5 seconds for this test). In this case, the root simply initiates the next distribute phase upon RanSub timeout. The net result is that nodes that are not descendants of the failed node will continue to receive updated random subsets allowing them to peer with appropriate nodes reflecting the new network conditions. As shown in Figure 14, the failure causes a negligible disruption in performance. With RanSub failure detection enabled, nodes quickly learn of other nodes from which to receive data. Once such recovery completes, the descendants of the failed node use their already established peer relationships to compensate for their ancestor\"s failure. Hence, because Bullet is an overlay mesh, its reliability characteristics far exceed that of typical overlay distribution trees. 4.7 PlanetLab This section contains results from the deployment of Bullet over the PlanetLab wide-area network testbed. 0 50 100 150 200 250 300 350 400 Bandwidth(Kbps) Time (s) Bandwidth received Useful total From parent Figure 13: Bandwidth over time with a worst-case node failure and no RanSub recovery. 0 50 100 150 200 250 300 350 400 Bandwidth(Kbps) Time (s) Bandwidth received Useful total From parent Figure 14: Bandwidth over time with a worst-case node failure and RanSub recovery enabled. our first experiment, we chose 47 nodes for our deployment, with no two machines being deployed at the same site. Since there is currently ample bandwidth available throughout the PlanetLab overlay (a characteristic not necessarily representative of the Internet at large), we designed this experiment to show that Bullet can achieve higher bandwidth than an overlay tree when the source is constrained, for instance in cases of congestion on its outbound access link, or of overload by a flash-crowd. We did this by choosing a root in Europe connected to PlanetLab with fairly low bandwidth. The node we selected was in Italy (cs.unibo.it) and we had 10 other overlay nodes in Europe. Without global knowledge of the topology in PlanetLab (and the Internet), we are, of course, unable to produce our greedy bottleneck bandwidth tree for comparison. We ran Bullet over a random overlay tree for 300 seconds while attempting to stream at a rate of 1.5 Mbps. We waited 50 seconds before starting to stream data to allow nodes to successfully join the tree. We compare the performance of Bullet to data streaming over multiple handcrafted trees. Figure 15 shows our results for two such trees. The good tree has all nodes in Europe located high in the tree, close to the root. 0 50 100 150 200 250 Bandwidth(Kbps) Time (s) Bullet Good Tree Worst Tree Figure 15: Achieved bandwidth over time for Bullet and TFRC streaming over different trees on PlanetLab with a root in Europe. available bandwidth between the root and all other nodes. Nodes with high bandwidth measurements were placed close to the root. In this case, we are able to achieve a bandwidth of approximately 300 Kbps. The worst tree was created by setting the root\"s children to be the three nodes with the worst bandwidth characteristics from the root as measured by pathload. All subsequent levels in the tree were set in this fashion. For comparison, we replaced all nodes in Europe from our topology with nodes in the US, creating a topology that only included US nodes with high bandwidth characteristics. As expected, Bullet was able to achieve the full 1.5 Mbps rate in this case. A well constructed tree over this highbandwidth topology yielded slightly lower than 1.5 Mbps, verifying that our approach does not sacrifice performance under high bandwidth conditions and improves performance under constrained bandwidth scenarios. 5. RELATED WORK Snoeren et al. use an overlay mesh to achieve reliable and timely delivery of mission-critical data. In this system, every node chooses n parents from which to receive duplicate packet streams. Since its foremost emphasis is reliability, the system does not attempt to improve the bandwidth delivered to the overlay participants by sending disjoint data at each level. Further, during recovery from parent failure, it limits an overlay router\"s choice of parents to nodes with a level number that is less than its own level number. The power of perpendicular downloads is perhaps best illustrated by Kazaa , the popular peer-to-peer file swapping network. Kazaa nodes are organized into a scalable, hierarchical structure. Individual users search for desired content in the structure and proceed to simultaneously download potentially disjoint pieces from nodes that already have it. Since Kazaa does not address the multicast communication model, a large fraction of users downloading the same file would consume more bandwidth than nodes organized into the Bullet overlay structure. Kazaa does not use erasure coding; therefore it may take considerable time to locate the last few bytes. 294 BitTorrent is another example of a file distribution system currently deployed on the Internet. It utilizes trackers that direct downloaders to random subsets of machines that already have portions of the file. The tracker poses a scalability limit, as it continuously updates the systemwide distribution of the file. Lowering the tracker communication rate could hurt the overall system performance, as information might be out of date. Further, BitTorrent does not employ any strategy to disseminate data to different regions of the network, potentially making it more difficult to recover data depending on client access patterns. Similar to Bullet, BitTorrent incorporates the notion of choking at each node with the goal of identifying receivers that benefit the most by downloading from that particular source. FastReplica addresses the problem of reliable and efficient file distribution in content distribution networks (CDNs). In the basic algorithm, nodes are organized into groups of fixed size (n), with full group membership information at each node. To distribute the file, a node splits it into n equal-sized portions, sends the portions to other group members, and instructs them to download the missing pieces in parallel from other group members. Since only a fixed portion of the file is transmitted along each of the overlay links, the impact of congestion is smaller than in the case of tree distribution. However, since it treats all paths equally, FastReplica does not take full advantage of highbandwidth overlay links in the system. Since it requires file store-and-forward logic at each level of the hierarchy necessary for scaling the system, it may not be applicable to high-bandwidth streaming. There are numerous protocols that aim to add reliability to IP multicast. In Scalable Reliable Multicast (SRM) , nodes multicast retransmission requests for missed packets. Two techniques attempt to improve the scalability of this approach: probabilistic choice of retransmission timeouts, and organization of receivers into hierarchical local recovery groups. However, it is difficult to find appropriate timer values and local scoping settings (via the TTL field) for a wide range of topologies, number of receivers, etc. even when adaptive techniques are used. One recent study shows that SRM may have significant overhead due to retransmission requests. Bullet is closely related to efforts that use epidemic data propagation techniques to recover from losses in the nonreliable IP-multicast tree. In pbcast , a node has global group membership, and periodically chooses a random subset of peers to send a digest of its received packets. A node that receives the digest responds to the sender with the missing packets in a last-in, first-out fashion. Lbpcast addresses pbcast\"s scalability issues (associated with global knowledge) by constructing, in a decentralized fashion, a partial group membership view at each node. The average size of the views is engineered to allow a message to reach all participants with high probability. Since lbpcast does not require an underlying tree for data distribution and relies on the push-gossiping model, its network overhead can be quite high. Compared to the reliable multicast efforts, Bullet behaves favorably in terms of the network overhead because nodes do not blindly request retransmissions from their peers. Instead, Bullet uses the summary views it obtains through RanSub to guide its actions toward nodes with disjoint content. Further, a Bullet node splits the retransmission load between all of its peers. We note that pbcast nodes contain a mechanism to rate-limit retransmitted packets and to send different packets in response to the same digest. However, this does not guarantee that packets received in parallel from multiple peers will not be duplicates. More importantly, the multicast recovery methods are limited by the bandwidth through the tree, while Bullet strives to provide more bandwidth to all receivers by making data deliberately disjoint throughout the tree. Narada builds a delay-optimized mesh interconnecting all participating nodes and actively measures the available bandwidth on overlay links. It then runs a standard routing protocol on top of the overlay mesh to construct forwarding trees using each node as a possible source. Narada nodes maintain global knowledge about all group participants, limiting system scalability to several tens of nodes. Further, the bandwidth available through a Narada tree is still limited to the bandwidth available from each parent. On the other hand, the fundamental goal of Bullet is to increase bandwidth through download of disjoint data from multiple peers. Overcast is an example of a bandwidth-efficient overlay tree construction algorithm. In this system, all nodes join at the root and migrate down to the point in the tree where they are still able to maintain some minimum level of bandwidth. Bullet is expected to be more resilient to node departures than any tree, including Overcast. Instead of a node waiting to get the data it missed from a new parent, a node can start getting data from its perpendicular peers. This transition is seamless, as the node that is disconnected from its parent will start demanding more missing packets from its peers during the standard round of refreshing its filters. Overcast convergence time is limited by probes to immediate siblings and ancestors. Bullet is able to provide approximately a target bandwidth without having a fully converged tree. In parallel to our own work, SplitStream also has the goal of achieving high bandwidth data dissemination. It operates by splitting the multicast stream into k stripes, transmitting each stripe along a separate multicast tree built using Scribe . The key design goal of the tree construction mechanism is to have each node be an intermediate node in at most one tree (while observing both inbound and outbound node bandwidth constraints), thereby reducing the impact of a single node\"s sudden departure on the rest of the system. The join procedure can potentially sacrifice the interior-node-disjointness achieved by Scribe. Perhaps more importantly, SplitStream assumes that there is enough available bandwidth to carry each stripe on every link of the tree, including the links between the data source and the roots of individual stripe trees independently chosen by Scribe. To some extent, Bullet and SplitStream are complementary. For instance, Bullet could run on each of the stripes to maximize the bandwidth delivered to each node along each stripe. CoopNet considers live content streaming in a peerto-peer environment, subject to high node churn. Consequently, the system favors resilience over network efficiency. It uses a centralized approach for constructing either random or deterministic node-disjoint (similar to SplitStream) trees, and it includes an MDC adaptation framework based on scalable receiver feedback that attempts to maximize the signal-to-noise ratio perceived by receivers. In the case of on-demand streaming, CoopNet addresses 295 the flash-crowd problem at the central server by redirecting incoming clients to a fixed number of nodes that have previously retrieved portions of the same content. Compared to CoopNet, Bullet provides nodes with a uniformly random subset of the system-wide distribution of the file. 6. CONCLUSIONS Typically, high bandwidth overlay data streaming takes place over a distribution tree. In this paper, we argue that, in fact, an overlay mesh is able to deliver fundamentally higher bandwidth. Of course, a number of difficult challenges must be overcome to ensure that nodes in the mesh do not repeatedly receive the same data from peers. This paper presents the design and implementation of Bullet, a scalable and efficient overlay construction algorithm that overcomes this challenge to deliver significant bandwidth improvements relative to traditional tree structures. Specifically, this paper makes the following contributions: \u2022 We present the design and analysis of Bullet, an overlay construction algorithm that creates a mesh over any distribution tree and allows overlay participants to achieve a higher bandwidth throughput than traditional data streaming. As a related benefit, we eliminate the overhead required to probe for available bandwidth in traditional distributed tree construction techniques. \u2022 We provide a technique for recovering missing data from peers in a scalable and efficient manner. RanSub periodically disseminates summaries of data sets received by a changing, uniformly random subset of global participants. \u2022 We propose a mechanism for making data disjoint and then distributing it in a uniform way that makes the probability of finding a peer containing missing data equal for all nodes. \u2022 A large- overlay participants running in an emulated 20,000 node network topology, as well as experimentation on top of the PlanetLab Internet testbed, shows that Bullet running over a random tree can achieve twice the throughput of streaming over a traditional bandwidth tree. Acknowledgments We would like to thank David Becker for his invaluable help with our ModelNet experiments and Ken Yocum for his help with ModelNet emulation optimizations. In addition, we thank our shepherd Barbara Liskov and our anonymous reviewers who provided excellent feedback.", "body1": "In this paper, we consider the following general problem. Given a sender and a large set of interested receivers spread across the Internet, how can we maximize the amount of bandwidth delivered to receivers? Typical overlay structures attempt to mimic the structure of multicast routing trees. However, we argue that a tree structure has fundamental limitations both for high bandwidth multicast and for high reliability. Here, the sender splits data into sequential blocks. To illustrate Bullet\"s behavior, consider a simple three node overlay with a root R and two children A and B. R has 1 Mbps of available (TCP-friendly) bandwidth to each of A and B. No flow should consume more than its fair share of the bottleneck bandwidth and each flow must respond to congestion signals (losses) by reducing its transmission rate. Bullet nodes begin by self-organizing into an overlay tree, which can be constructed by any of a number of existing techniques . One important benefit of our approach is that the bandwidth delivered by the Bullet mesh is somewhat independent of the bandwidth available through the underlying overlay tree. We have completed a prototype of Bullet running on top of a number of overlay trees. The remainder of this paper is organized as follows. Section 2 presents Bullet\"s system components including RanSub, informed content delivery, and TFRC. Our approach to high bandwidth data dissemination centers around the techniques depicted in Figure 1. to different clients at a rate determined by the available bandwidth to each client. Given the above techniques, data is spread across the overlay tree at a rate commensurate with the available bandwidth in the overlay tree. 2.1 Data Encoding Depending on the type of data being distributed through the system, a number of data encoding schemes can improve system efficiency. In this paper, we focus on the benefits of a special class of erasure-correcting codes used to implement the digital fountain approach. 2.2 RanSub To address the challenge of locating disjoint content within the system, we use RanSub , a scalable approach to distributing changing, uniform random subsets of global state to all nodes of an overlay tree. Collect messages start at the leaves and propagate up the tree, leaving state at each node along the path to the root. Distribute messages start at the root and travel down the tree, using the information left at the nodes during the previous collect round to distribute uniformly random subsets to all participants. As part of the distribute message, each participant sends a uniformly random subset of remote nodes, called a distribute set, down to its children. One of the key features of RanSub is the Compact operation. RanSub offers several ways of constructing distribute sets. For our system, we choose the RanSub-nondescendants option. A parent creates RanSub-nondescendants distribute sets for each child by compacting collect sets from that child\"s siblings and its own distribute set. 2.3 Informed Content Delivery Techniques Assuming we can enable a node to locate a peer with disjoint content using RanSub, we need a method for reconciling the differences in the data. To describe the content, nodes maintain working sets. The permutation function can be thought of as a specialized hash function. To perform approximate fine-grain reconciliation, a peer A sends its digest to peer B and expects to receive packets not described in the digest. In this case, we have a false positive. 2.4 TCP Friendly Rate Control Although most traffic in the Internet today is best served by TCP, applications that require a smooth sending rate and that have a higher tolerance for loss often find TCP\"s reaction to a single dropped packet to be unnecessarily severe. To guarantee fairness with TCP, TFRC uses the response function that describes the steady-state sending rate of TCP to determine the transmission rate in TFRC. TFRC senders and receivers must cooperate to achieve a smooth transmission rate. The main role of the receiver is to send feedback to the sender once per round-trip time and to calculate the loss event rate included in the feedback packets. When implementing Bullet, we used an unreliable version of TFRC. Bullet is an efficient data distribution system for bandwidth intensive applications. Bullet requires an underlying overlay tree for RanSub to deliver random subsets of participants\"s state to nodes in the overlay, informing them of a set of nodes that may be good candidates for retrieving data not available from any of the node\"s current peers and parent. As with streaming overlays trees, Bullet can use standard transports such as TCP and UDP as well as our implementation of TFRC. 3.1 Finding Overlay Peers RanSub periodically delivers subsets of uniformly random selected nodes to each participant in the overlay. RanSub delivers subsets of these summary tickets to nodes every configurable epoch (5 seconds by default). This allows us to keep the Bloom filter population n from growing at an unbounded rate. We use the collect and distribute phases of RanSub to carry Bullet summary tickets up and down the tree. 3.2 Recovering Data From Peers Assuming it has space for the new peer, a recipient of the peering request installs the received Bloom filter and will periodically transmit keys not present in the Bloom filter to the requesting node. As illustrated in Figure 4, a Bullet receiver views the data space as a matrix of packet sequences containing s rows, where s is its current number of sending peers. By specifying ranges and matrix rows, a receiver is unlikely to receive duplicate data items, which would result in wasted bandwidth. 3.3 Making Data Disjoint We now provide details of Bullet\"s mechanisms to increase the ease by which nodes can find disjoint data not provided by parents. On the other hand, Bullet attempts to recover lost data from any non-descendant node, not just ancestors, thereby increasing overall system scalability. In contrast, Bullet nodes are aware of the bandwidth achievable to each of its children using the underlying transport. That is, given a randomly chosen subset of peer nodes, it is with the same probability that each node has a particular data packet. The sending factor sfi is the portion of the parent stream (rate) that each child should own based on the number of descendants the child has. The limiting factor lfi represents the proportion of the parent rate beyond the sending factor that each child can handle. For each child i out of k total, we set the sending factor to be: sfi = di\u00c8k j=1 dj In addition, a node tracks the data successfully transmitted via the transport. Having chosen the target of a particular packet, the parent attempts to forward the packet to the child. Once a packet has been successfully sent to the owning child, the node attempts to send the packet to all other children depending on the limiting factors lfi. It is important to realize that by maintaining limiting factors, we are essentially using feedback from children (by observing transport behavior) to determine the best data to stop sending during times when a child cannot handle the entire parent stream. That is, a node can have up to a certain number of receivers and a certain number of senders (each defaults to 10 in our implementation). Each node periodically (every few RanSub epochs) evaluates the bandwidth performance it is receiving from its sending peers. In this way, we are assured of keeping the best senders seen so far and will eliminate senders whose performance deteriorates with changing network conditions. Likewise, a Bullet sender will periodically evaluate its receivers. We have evaluated Bullet\"s performance in real Internet environments as well as the ModelNet IP emulation framework. All of our implementations leverage a common development infrastructure called MACEDON that allows for the specification of overlay algorithms in a simple domainspecific language. Our ModelNet experiments make use of 50 2Ghz Pentium4\"s running Linux 2.4.20 and interconnected with 100 Mbps and 1 Gbps Ethernet switches. Propagation delays in the network topology are calculated based on the relative placement of the network nodes in the plane by INET. 4.1 Offline Bottleneck Bandwidth Tree One of our goals is to determine Bullet\"s performance relative to the best possible bandwidth-optimized tree for a given network topology. Thus, in this section we present a simple greedy o\ufb04ine algorithm to determine the connectivity of a tree likely to deliver a high level of bandwidth. 289 Topology classification Client-Stub Stub-Stub Transit-Stub Transit-Transit Low bandwidth 300-600 500--- Medium bandwidth 800-------- Table 1: Bandwidth ranges for link types used in our topologies expressed in Kbps. Specifically, we consider the following problem: given complete knowledge of the topology (individual link latencies, bandwidth, and packet loss rates), what is the overlay tree that will deliver the highest bandwidth to a set of predetermined overlay nodes? We make the following assumptions: 1. 3. 4. Given these assumptions, we concentrate on estimating the throughput available between two participants in the overlay. Given a physical network represented as a graph G = (V, E), set of overlay participants P \u2282 V , source node (s \u2208 P), bandwidth B : E \u2192 R+ , loss rate L : E \u2192 , propagation delay D : E \u2192 R+ of each link, set of possible overlay links O = {(v, w) | v, w \u2208 P, v = w}, routing table RT : O \u00d7 E \u2192 {0, 1}, find the overlay tree T = {o | o \u2208 O} (|T| = |P| \u2212 1, \u2200v \u2208 P there exists a path ov = s \u2740 v) that maximizes min o|o\u2208T (min(f(o), min e|e\u2208o b(e) |{p | p \u2208 T, e \u2208 p}| )) where f(o) is the TCP steady-state sending rate, computed from round-trip time d(o) = \u00c8e\u2208o d(e) + \u00c8e\u2208o d(e) (given overlay link o = (v, w), o = (w, v)), and loss rate l(o) = 1 \u2212 \u00c9e\u2208o (1 \u2212 l(e)). Assuming that we can estimate the throughput of a flow, we proceed to formulate a greedy OMBT algorithm. Our algorithm is similar to the Widest Path Heuristic (WPH) , and more generally to Prim\"s MST algorithm . During its execution, we maintain the set of nodes already in the tree, and the set of remaining nodes. 4.2 Bullet vs. Streaming We have implemented a simple streaming application that is capable of streaming data over any specified tree. To evaluate Bullet\"s ability to exceed the bandwidth achievable via tree distribution overlays, we compare Bullet running over a random overlay tree to the streaming behavior shown in Figure 6. Further, the total bandwidth (including redundant data) received by each node is only slightly higher than the useful content, meaning that Bullet is able to achieve high bandwidth while wasting little network resources. The standard deviation in most of our runs is fairly high because of the limited bandwidth randomly assigned to some Client-Stub and Stub-Stub links. 0.2 0.4 0.6 0.8 0 100 200 300 400 500 600 700 800 Percentageofnodes Bandwidth(Kbps) Figure 8: CDF of instantaneous achieved bandwidth at time 430 seconds. available bandwidth of the underlying topology. In all of our experiments, Bullet outperforms the bottleneck bandwidth tree by a factor of up to 100%, depending on how much bandwidth is constrained in the underlying topology. For instance, we built a simple bandwidth optimizing overlay tree construction based on Overcast . 4.3 Creating Disjoint Data Bullet\"s ability to deliver high bandwidth levels to nodes depends on its disjoint transmission strategy. 4.4 Epidemic Approaches In this section, we explore how Bullet compares to data dissemination approaches that use some form of epidemic routing. 0 50 100 150 200 250 300 350 400 450 500 Bandwidth(Kbps) Time (s) Raw total Useful total From parent Figure 10: Achieved bandwidth over time using nondisjoint data transmission. We also implemented a pbcast-like approach for retrieving data missing from a data distribution tree. To make our evaluation conservative, we assume that nodes employing gossip and anti-entropy recovery are able to maintain full group membership. For these experiments,-node INET topology with no explicit physical link losses. 4.5 Bullet on a Lossy Network To evaluate Bullet\"s performance under more lossy network conditions, we have modified our 20,000-node topologies used in our previous experiments to include random packet losses. To effect this behavior, we first modify all non-transit links in each topology to have a packet loss rate chosen uniformly random from [0, 0.003] resulting in a maximum loss rate of 0.3%. Figure 12 shows achieved bandwidths for streaming over Bullet and using our greedy o\ufb04ine bottleneck bandwidth tree. 4.6 Performance Under Failure In this section, we discuss Bullet\"s behavior in the face of node failure. Because Bullet, and, more importantly, RanSub makes use of an underlying tree overlay, part of Bullet\"s failure recovery properties will depend on the failure recovery behavior of the underlying tree. In our first scenario, we disable failure detection in RanSub so that after a failure occurs, Bullet nodes request data only from their current peers. In this case, the root simply initiates the next distribute phase upon RanSub timeout. 0 50 100 150 200 250 300 350 400 Bandwidth(Kbps) Time (s) Bandwidth received Useful total From parent Figure 14: Bandwidth over time with a worst-case node failure and RanSub recovery enabled. our first experiment, we chose 47 nodes for our deployment, with no two machines being deployed at the same site. We did this by choosing a root in Europe connected to PlanetLab with fairly low bandwidth. We waited 50 seconds before starting to stream data to allow nodes to successfully join the tree. available bandwidth between the root and all other nodes. Nodes with high bandwidth measurements were placed close to the root. As expected, Bullet was able to achieve the full 1.5 Mbps rate in this case. Snoeren et al. The power of perpendicular downloads is perhaps best illustrated by Kazaa , the popular peer-to-peer file swapping network. 294 BitTorrent is another example of a file distribution system currently deployed on the Internet. FastReplica addresses the problem of reliable and efficient file distribution in content distribution networks (CDNs). Two techniques attempt to improve the scalability of this approach: probabilistic choice of retransmission timeouts, and organization of receivers into hierarchical local recovery groups. Bullet is closely related to efforts that use epidemic data propagation techniques to recover from losses in the nonreliable IP-multicast tree. Instead, Bullet uses the summary views it obtains through RanSub to guide its actions toward nodes with disjoint content. Narada builds a delay-optimized mesh interconnecting all participating nodes and actively measures the available bandwidth on overlay links. On the other hand, the fundamental goal of Bullet is to increase bandwidth through download of disjoint data from multiple peers. Overcast is an example of a bandwidth-efficient overlay tree construction algorithm. This transition is seamless, as the node that is disconnected from its parent will start demanding more missing packets from its peers during the standard round of refreshing its filters. In parallel to our own work, SplitStream also has the goal of achieving high bandwidth data dissemination. For instance, Bullet could run on each of the stripes to maximize the bandwidth delivered to each node along each stripe. CoopNet considers live content streaming in a peerto-peer environment, subject to high node churn. Consequently, the system favors resilience over network efficiency. It uses a centralized approach for constructing either random or deterministic node-disjoint (similar to SplitStream) trees, and it includes an MDC adaptation framework based on scalable receiver feedback that attempts to maximize the signal-to-noise ratio perceived by receivers.", "body2": "In this paper, we consider the following general problem. More recently, overlays have emerged as a promising alternative to multicast for network-efficient point to multipoint data delivery. Overlays have shown tremendous promise for multicast-style applications. Rather than sending identical copies of the same data stream to all nodes in a tree and designing a scalable mechanism for recovering from loss, we propose that participants in a multicast overlay cooperate to strategically 282 transmit disjoint data sets to various points in the network. One hypothesis of this work is that, relative to a tree, this model will result in higher bandwidth-leveraging the bandwidth from simultaneous parallel downloads from multiple sources rather than a single parent-and higher reliability-retrieving data from multiple peers reduces the potential damage from a single node failure. First, it must be TCP friendly . In this context, this paper presents the design and evaluation of Bullet, an algorithm for constructing an overlay mesh that attempts to maintain the above properties. This ensures that the entire overlay behaves in a congestion-friendly manner, adjusting its transmission rate on a per-connection basis based on prevailing network conditions. Further, all the bandwidth that would normally be consumed probing for bandwidth can be reallocated to streaming data across the Bullet mesh. In both cases, the overhead of maintaining the Bullet mesh and locating the appropriate disjoint data is limited to 30 Kbps per node, acceptable for our target high-bandwidth, large-scale scenarios. The remainder of this paper is organized as follows. Section 5 places our work in the context of related efforts and Section 6 presents our conclusions. Next, we purposefully disseminate disjoint objects 283 A C Original data stream: 1 2 3 4 5 6 1 2 3 5 1 3 4 6 2 4 5 6 TFRC to determine available BW D E 1 2 5 1 3 4 Figure 1: High-level view of Bullet\"s operation. We use the equation-based TFRC protocol to communicate among all nodes in the overlay in a congestion responsive and TCP friendly manner. Section 3 then presents the details of the entire Bullet architecture. Of course, Bullet is amenable to a variety of other encoding schemes or even the null encoding scheme, where the original data stream is transmitted best-effort through the system. Tornado codes require a predetermined stretch factor (n/k, where n is the total number of encoded packets), and their encoding time is proportional to n. LT codes remove these two limitations, while maintaining a low reception overhead of 0.05. RanSub distributes random subsets of participating nodes throughout the tree using collect and distribute messages. Collect messages start at the leaves and propagate up the tree, leaving state at each node along the path to the root. For appropriately constructed trees, the minimum epoch length will grow with the logarithm of the number of participants, though this is not required for correctness. After the root receives all collect sets and the collect phase completes, the distribute phase begins again in a new epoch. members of the resulting set are uniformly random representatives of the input subset members. RanSub offers several ways of constructing distribute sets. This is appropriate for our download structure where descendants are expected to have less content than an ancestor node in most cases. In the figure, AS stands for node A\"s state. We chose to implement the approximate reconciliation techniques proposed in for these tasks in Bullet. To insert a new element into the summary ticket, we apply the permutation functions in order and update array values as appropriate. 12 10 2 27 18 19 40 Workingset 14 42 17 33 38 15 12 P1 33 29 28 44 57 15 P2 22 28 45 61 14 51 Pn\u2026 Summary ticket minminmin 10 Figure 3: Example showing a sample summary ticket being constructed from the working set. When using Bloom filters, the insertion of different elements might cause all the positions in the bit array corresponding to an element that is not in the set to be nonzero. We can therefore choose the size of the Bloom filter and the number of hash functions that will yield a desired false positive ratio. Unlike TCP, the goal of TFRC is not to find and use all available bandwidth, but instead to maintain a relatively steady sending rate while still being responsive to congestion. The formula of the TCP response function used in TFRC to describe the sending rate is: T = s \u00d52p +tRT O(3 \u00d53p )p(1+32p2) This is the expression for the sending rate T in bytes/second, as a function of the round-trip time R in seconds, loss event rate p, packet size s in bytes, and TCP retransmit value tRT O in seconds. Until the first loss is reported, the sender doubles its transmission rate each time it receives feedback just as TCP does during slow-start. A weighted average is computed based on the sum of the loss interval values, and the inverse of the sum is the reported loss event rate, p. By using TFRC we were able to avoid these scenarios. This has significant bandwidth impact when a single node in the overlay is unable to deliver adequate bandwidth to a receiving node. Bullet registers itself with the underlying overlay tree so that it is informed when the overlay changes as nodes come and go or make performance transformations in the overlay. Each node receiving a packet will optionally forward it to each of its children, depending on a number of factors relating to the child\"s bandwidth and its relative position in the tree. RanSub messages contain a set of summary tickets that include a small (120 286 bytes) summary of the data that each node contains. Since the Bloom filter does not exceed a specific size (m) and we would like to limit the rate of false positives, Bullet periodically cleans up the Bloom filter by removing lower sequence numbers from it. Similarly, Bullet removes older items that are not needed for data reconstruction from its working set and summary ticket. Otherwise, the send request is rejected (space is periodically created in the receiver lists as further described in section 3.4). A node divides the sequence space in its current working set among each of its senders uniformly. In this fashion, receivers register to receive disjoint data from their sending peers. In practice, this wasteful reception of duplicate packets is tolerable; less than 10% of all received packets are duplicates in our experiments. This ultimately leads to scalability issues at higher levels in the hierarchy particularly when overlay links are bandwidthconstrained. If few nodes receive a particular range of packets, recovering these pieces of data becomes more difficult, requiring increased communication costs, and leading to scalability problems. In addition, because nodes recover data from participants chosen uniformly at random from the set of non-descendants, it is advantageous to make each transmitted packet recoverable from approximately the same number of participant nodes. These factors determine the proportion of p\"s received data rate that it will forward to each child. The more descendants a child has, the larger the portion of received data it should own. Because RanSub collects descendant counts di for each child i, Bullet simply makes a call into RanSub when sending data to determine the current sending factors of its children. It then assigns ownership of the current packet to the child with sending proportion farthest away from its sfi as illustrated in Figure 5. This process allows its children to potentially recover the packet from one of their own peers, to whom additional bandwidth may be available. This allows children limiting factors to be continuously adjusted in response to changing network conditions. 3.4 Improving the Bullet Mesh Bullet allows a maximum number of peering relationships. In such a case, it would be advantageous to remove that sender as a peer and find some other peer that offers better utility. It will replace this sender with some other sending peer candidate, essentially reserving a trial slot in its sender list. In this way, we are assured of keeping the best senders seen so far and will eliminate senders whose performance deteriorates with changing network conditions. This is similar to the concept of weans presented in . Because Bullet performs well over a randomly created overlay tree, we present results with Bullet running over such a tree compared against an o\ufb04ine greedy bottleneck bandwidth tree algorithm using global topological information described in Section 4.1. lines of code in this infrastructure. We randomly select one of these participants to act as the source of the data stream. Rather, we assume that either each sequence number directly specifies a particular data block and the block offset for each packet, or we are distributing data within the same block for LT Codes, e.g., when distributing a file. While we have not yet proven this, we believe that this problem is NP-hard. In addition to any performance comparisons, a Bullet mesh has much lower depth than the bottleneck tree and is more resilient to failure, as discussed in Section 4.6. 289 Topology classification Client-Stub Stub-Stub Transit-Stub Transit-Transit Low bandwidth 300-600 500--- Medium bandwidth 800-------- Table 1: Bandwidth ranges for link types used in our topologies expressed in Kbps. For the purposes of this paper, our goal is to determine a good overlay streaming tree that provides each overlay participant with substantial bandwidth, while avoiding overlay links with high end-to-end loss rates. The overlay tree will use TCP-friendly unicast connections to transfer data point-to-point. In the absence of other flows, we can estimate the throughput of a TCP-friendly flow using a steady-state formula . When several (n) flows share the same bottleneck link, each flow can achieve throughput of at most c , where c is the physical capacity of the link. More formally, we define the problem as follows: Overlay Maximum Bottleneck Tree (OMBT). We write e \u2208 o to express that link e is included in the o\"s routing path (RT(o, e) = 1). This algorithm is non-optimal, but a similar approach was found to perform well . Our algorithm is similar to the Widest Path Heuristic (WPH) , and more generally to Prim\"s MST algorithm . With the synthetic topologies we use for our emulation environment, we have not found this inaccuracy to severely impact the quality of the tree. Hence, Bullet\"s goal is to overcome this bandwidth limit by allowing for the perpendicular reception of data and by utilizing disjoint data flows in an attempt to match or exceed the performance of our o\ufb04ine algorithm. For this topology and bandwidth setting, Bullet was able to achieve an average bandwidth of 500 Kbps, fives times that achieved by the random tree and more than 25% higher than the o\ufb04ine bottleneck bandwidth algorithm. For this experiment, Bullet has an average link stress of approximately 1.5 with an absolute maximum link stress of 22. 0 50 100 150 200 250 300 350 400 450 500 Bandwidth(Kbps) Time (s) Raw total Useful total From parent Figure 7: Achieved bandwidth over time for Bullet over a random tree. 0.2 0.4 0.6 0.8 0 100 200 300 400 500 600 700 800 Percentageofnodes Bandwidth(Kbps) Figure 8: CDF of instantaneous achieved bandwidth at time 430 seconds. Figure 9 shows achieved bandwidths for Bullet and the bottleneck bandwidth tree over time generated from topologies with bandwidths in each range. be extremely difficult for any online tree-based algorithm to exceed the bandwidth achievable by our o\ufb04ine bottleneck algorithm that makes use of global topological information. The resulting dynamically constructed trees never achieved more than 75% of the bandwidth of our own o\ufb04ine algorithm. By not explicitly choosing which data to send its child, this approach deprives Bullet of 25% of its bandwidth capability, when compared to the case when our disjoint strategy is enabled in Figure 7. We do not disseminate packets every T seconds; instead we forward them as soon as they arrive. 0 50 100 150 200 250 300 350 400 450 500 Bandwidth(Kbps) Time (s) Raw total Useful total From parent Figure 10: Achieved bandwidth over time using nondisjoint data transmission. Instead of using gossiping with a fixed number of rounds for each packet, we use anti-entropy with a FIFO Bloom filter to attempt to locate peers that hold any locally missing data items. To allow TFRC enough time to ramp up to the appropriate TCP-friendly sending rate, we set the epoch length for anti-entropy recovery to 20 seconds. Bullet, on the other hand, establishes long-term connections with peers that provide good bandwidth and disjoint content, and avoids most of the duplicates by requesting disjoint data from each node\"s peers. Our goal by modifying these loss rates is to simulate queuing behavior when the network is under load due to background network traffic. Similar to the approach in , we randomly designated 5% of the links in the topologies as overloaded and set their loss rates uniformly random from [0.05, 0.1] resulting in a maximum packet loss rate of 10%. Additionally, losses in the low bandwidth topology essentially keep the bottleneck bandwidth tree from delivering any data, an artifact that is avoided by Bullet. disruption in service, Bullet nodes are able compensate for this by receiving data from peers throughout the outage. By failing one of root\"s children, we are able to show Bullet\"s worst-case performance under a single node failure. Next, we enable RanSub failure detection that recognizes a node\"s failure when a RanSub epoch has lasted longer than the predetermined maximum (5 seconds for this test). 0 50 100 150 200 250 300 350 400 Bandwidth(Kbps) Time (s) Bandwidth received Useful total From parent Figure 13: Bandwidth over time with a worst-case node failure and no RanSub recovery. 0 50 100 150 200 250 300 350 400 Bandwidth(Kbps) Time (s) Bandwidth received Useful total From parent Figure 14: Bandwidth over time with a worst-case node failure and RanSub recovery enabled. Since there is currently ample bandwidth available throughout the PlanetLab overlay (a characteristic not necessarily representative of the Internet at large), we designed this experiment to show that Bullet can achieve higher bandwidth than an overlay tree when the source is constrained, for instance in cases of congestion on its outbound access link, or of overload by a flash-crowd. We ran Bullet over a random overlay tree for 300 seconds while attempting to stream at a rate of 1.5 Mbps. 0 50 100 150 200 250 Bandwidth(Kbps) Time (s) Bullet Good Tree Worst Tree Figure 15: Achieved bandwidth over time for Bullet and TFRC streaming over different trees on PlanetLab with a root in Europe. available bandwidth between the root and all other nodes. For comparison, we replaced all nodes in Europe from our topology with nodes in the US, creating a topology that only included US nodes with high bandwidth characteristics. A well constructed tree over this highbandwidth topology yielded slightly lower than 1.5 Mbps, verifying that our approach does not sacrifice performance under high bandwidth conditions and improves performance under constrained bandwidth scenarios. Further, during recovery from parent failure, it limits an overlay router\"s choice of parents to nodes with a level number that is less than its own level number. Kazaa does not use erasure coding; therefore it may take considerable time to locate the last few bytes. Similar to Bullet, BitTorrent incorporates the notion of choking at each node with the goal of identifying receivers that benefit the most by downloading from that particular source. In Scalable Reliable Multicast (SRM) , nodes multicast retransmission requests for missed packets. One recent study shows that SRM may have significant overhead due to retransmission requests. Compared to the reliable multicast efforts, Bullet behaves favorably in terms of the network overhead because nodes do not blindly request retransmissions from their peers. More importantly, the multicast recovery methods are limited by the bandwidth through the tree, while Bullet strives to provide more bandwidth to all receivers by making data deliberately disjoint throughout the tree. Further, the bandwidth available through a Narada tree is still limited to the bandwidth available from each parent. On the other hand, the fundamental goal of Bullet is to increase bandwidth through download of disjoint data from multiple peers. Instead of a node waiting to get the data it missed from a new parent, a node can start getting data from its perpendicular peers. Bullet is able to provide approximately a target bandwidth without having a fully converged tree. To some extent, Bullet and SplitStream are complementary. For instance, Bullet could run on each of the stripes to maximize the bandwidth delivered to each node along each stripe. CoopNet considers live content streaming in a peerto-peer environment, subject to high node churn. Consequently, the system favors resilience over network efficiency. Compared to CoopNet, Bullet provides nodes with a uniformly random subset of the system-wide distribution of the file.", "introduction": "In this paper, we consider the following general problem. Given a sender and a large set of interested receivers spread across the Internet, how can we maximize the amount of bandwidth delivered to receivers? Our problem domain includes software or video distribution and real-time multimedia streaming. Traditionally, native IP multicast has been the preferred method for delivering content to a set of receivers in a scalable fashion. However, a number of considerations, including scale, reliability, and congestion control, have limited the wide-scale deployment of IP multicast. Even if all these problems were to be addressed, IP multicast does not consider bandwidth when constructing its distribution tree. More recently, overlays have emerged as a promising alternative to multicast for network-efficient point to multipoint data delivery. Typical overlay structures attempt to mimic the structure of multicast routing trees. In network-layer multicast however, interior nodes consist of high speed routers with limited processing power and extensibility. Overlays, on the other hand, use programmable (and hence extensible) end hosts as interior nodes in the overlay tree, with these hosts acting as repeaters to multiple children down the tree. Overlays have shown tremendous promise for multicast-style applications. However, we argue that a tree structure has fundamental limitations both for high bandwidth multicast and for high reliability. One difficulty with trees is that bandwidth is guaranteed to be monotonically decreasing moving down the tree. Any loss high up the tree will reduce the bandwidth available to receivers lower down the tree. A number of techniques have been proposed to recover from losses and hence improve the available bandwidth in an overlay tree . However, fundamentally, the bandwidth available to any host is limited by the bandwidth available from that node\"s single parent in the tree. Thus, our work operates on the premise that the model for high-bandwidth multicast data dissemination should be re-examined. Rather than sending identical copies of the same data stream to all nodes in a tree and designing a scalable mechanism for recovering from loss, we propose that participants in a multicast overlay cooperate to strategically 282 transmit disjoint data sets to various points in the network. Here, the sender splits data into sequential blocks. Blocks are further subdivided into individual objects which are in turn transmitted to different points in the network. Nodes still receive a set of objects from their parents, but they are then responsible for locating peers that hold missing data objects. We use a distributed algorithm that aims to make the availability of data items uniformly spread across all overlay participants. In this way, we avoid the problem of locating the last object, which may only be available at a few nodes. One hypothesis of this work is that, relative to a tree, this model will result in higher bandwidth-leveraging the bandwidth from simultaneous parallel downloads from multiple sources rather than a single parent-and higher reliability-retrieving data from multiple peers reduces the potential damage from a single node failure. To illustrate Bullet\"s behavior, consider a simple three node overlay with a root R and two children A and B. R has 1 Mbps of available (TCP-friendly) bandwidth to each of A and B. However, there is also 1 Mbps of available bandwidth between A and B. In this example, Bullet would transmit a disjoint set of data at 1 Mbps to each of A and B. A and B would then each independently discover the availability of disjoint data at the remote peer and begin streaming data to one another, effectively achieving a retrieval rate of 2 Mbps. On the other hand, any overlay tree is restricted to delivering at most 1 Mbps even with a scalable technique for recovering lost data. Any solution for achieving the above model must maintain a number of properties. First, it must be TCP friendly . No flow should consume more than its fair share of the bottleneck bandwidth and each flow must respond to congestion signals (losses) by reducing its transmission rate. Second, it must impose low control overhead. There are many possible sources of such overhead, including probing for available bandwidth between nodes, locating appropriate nodes to peer with for data retrieval and redundantly receiving the same data objects from multiple sources. Third, the algorithm should be decentralized and scalable to thousands of participants. No node should be required to learn or maintain global knowledge, for instance global group membership or the set of data objects currently available at all nodes. Finally, the approach must be robust to individual failures. For example, the failure of a single node should result only in a temporary reduction in the bandwidth delivered to a small subset of participants; no single failure should result in the complete loss of data for any significant fraction of nodes, as might be the case for a single node failure high up in a multicast overlay tree. In this context, this paper presents the design and evaluation of Bullet, an algorithm for constructing an overlay mesh that attempts to maintain the above properties. Bullet nodes begin by self-organizing into an overlay tree, which can be constructed by any of a number of existing techniques . Each Bullet node, starting with the root of the underlying tree, then transmits a disjoint set of data to each of its children, with the goal of maintaining uniform representativeness of each data item across all participants. The level of disjointness is determined by the bandwidth available to each of its children. Bullet then employs a scalable and efficient algorithm to enable nodes to quickly locate multiple peers capable of transmitting missing data items to the node. Thus, Bullet layers a high-bandwidth mesh on top of an arbitrary overlay tree. Depending on the type of data being transmitted, Bullet can optionally employ a variety of encoding schemes, for instance Erasure codes or Multiple Description Coding (MDC) , to efficiently disseminate data, adapt to variable bandwidth, and recover from losses. Finally, we use TFRC to transfer data both down the overlay tree and among peers. This ensures that the entire overlay behaves in a congestion-friendly manner, adjusting its transmission rate on a per-connection basis based on prevailing network conditions. One important benefit of our approach is that the bandwidth delivered by the Bullet mesh is somewhat independent of the bandwidth available through the underlying overlay tree. One significant limitation to building high bandwidth overlay trees is the overhead associated with the tree construction protocol. In these trees, it is critical that each participant locates a parent via probing with a high level of available bandwidth because it receives data from only a single source (its parent). Thus, even once the tree is constructed, nodes must continue their probing to adapt to dynamically changing network conditions. While bandwidth probing is an active area of research , accurate results generally require the transfer of a large amount of data to gain confidence in the results. Our approach with Bullet allows receivers to obtain high bandwidth in aggregate using individual transfers from peers spread across the system. Thus, in Bullet, the bandwidth available from any individual peer is much less important than in any bandwidthoptimized tree. Further, all the bandwidth that would normally be consumed probing for bandwidth can be reallocated to streaming data across the Bullet mesh. We have completed a prototype of Bullet running on top of a number of overlay trees.-node overlay running across a wide variety of emulated 20,000 node network topologies shows that Bullet can deliver up to twice the bandwidth of a bandwidth-optimized tree (using an o\ufb04ine algorithm and global network topology information), all while remaining TCP friendly. We also deployed our prototype across the PlanetLab wide-area testbed. For these live Internet runs, we find that Bullet can deliver comparable bandwidth performance improvements. In both cases, the overhead of maintaining the Bullet mesh and locating the appropriate disjoint data is limited to 30 Kbps per node, acceptable for our target high-bandwidth, large-scale scenarios. The remainder of this paper is organized as follows. Section 2 presents Bullet\"s system components including RanSub, informed content delivery, and TFRC. Section 3 then details Bullet, an efficient data distribution system for bandwidth intensive applications. Section 4 evaluates Bullet\"s performance for a variety of network topologies, and compares it to existing multicast techniques. Section 5 places our work in the context of related efforts and Section 6 presents our conclusions.", "conclusion": "Typically, high bandwidth overlay data streaming takes place over a distribution tree.. In this paper, we argue that, in fact, an overlay mesh is able to deliver fundamentally higher bandwidth.. Of course, a number of difficult challenges must be overcome to ensure that nodes in the mesh do not repeatedly receive the same data from peers.. This paper presents the design and implementation of Bullet, a scalable and efficient overlay construction algorithm that overcomes this challenge to deliver significant bandwidth improvements relative to traditional tree structures.. Specifically, this paper makes the following contributions: \u2022 We present the design and analysis of Bullet, an overlay construction algorithm that creates a mesh over any distribution tree and allows overlay participants to achieve a higher bandwidth throughput than traditional data streaming.. As a related benefit, we eliminate the overhead required to probe for available bandwidth in traditional distributed tree construction techniques.. \u2022 We provide a technique for recovering missing data from peers in a scalable and efficient manner.. RanSub periodically disseminates summaries of data sets received by a changing, uniformly random subset of global participants.. \u2022 We propose a mechanism for making data disjoint and then distributing it in a uniform way that makes the probability of finding a peer containing missing data equal for all nodes.. \u2022 A large- overlay participants running in an emulated 20,000 node network topology, as well as experimentation on top of the PlanetLab Internet testbed, shows that Bullet running over a random tree can achieve twice the throughput of streaming over a traditional bandwidth tree.. Acknowledgments We would like to thank David Becker for his invaluable help with our ModelNet experiments and Ken Yocum for his help with ModelNet emulation optimizations.. In addition, we thank our shepherd Barbara Liskov and our anonymous reviewers who provided excellent feedback."}
{"id": "I-56", "keywords": ["distribut constraint satisfact problem", "belief-desireintent model", "agent negoti"], "title": "Unifying Distributed Constraint Algorithms in a BDI Negotiation Framework", "abstract": "This paper presents a novel, unified distributed constraint satisfaction framework based on automated negotiation. The Distributed Constraint Satisfaction Problem (DCSP) is one that entails several agents to search for an agreement, which is a consistent combination of actions that satisfies their mutual constraints in a shared environment. By anchoring the DCSP search on automated negotiation, we show that several well-known DCSP algorithms are actually mechanisms that can reach agreements through a common Belief-Desire-Intention (BDI) protocol, but using different strategies. A major motivation for this BDI framework is that it not only provides a conceptually clearer understanding of existing DCSP algorithms from an agent model perspective, but also opens up the opportunities to extend and develop new strategies for DCSP. To this end, a new strategy called Unsolicited Mutual Advice (UMA) is proposed. Performance evaluation shows that the UMA strategy can outperform some existing mechanisms in terms of computational cycles.", "references": ["Dynamic distributed resource allocation: A distributed constraint satisfaction approach", "Simulating large railway networks using distributed constraint satisfaction", "Distributed Constraint Satisfaction : Foundations of Cooperation in Multiagent Systems", "The distributed constraint satisfaction problem : Formalization and algorithms", "Using cooperative mediation to solve distributed constraint satisfaction problems", "Foundation of Constraint Satisfaction", "Soft Real-Time, Cooperative Negotiation for Distributed Resource Allocation", "Secure distributed constraint satisfaction: Reaching agreement without revealing private information", "Rules of Encounter", "Distributed constraint satisfaction algorithm for complex local problems", "Intentions, Plans and Practical Reason", "Multiagent System : A Modern Approach to Distributed Artificial Intelligence", "Minimizing conflicts: A heuristic repair method for constraint satisfaction and scheduling problems"], "full_text": "1. INTRODUCTION At the core of many emerging distributed applications is the distributed constraint satisfaction problem (DCSP) - one which involves finding a consistent combination of actions (abstracted as domain values) to satisfy the constraints among multiple agents in a shared environment. Important application examples include distributed resource allocation and distributed scheduling . Many important algorithms, such as distributed breakout (DBO) , asynchronous backtracking (ABT) , asynchronous partial overlay (APO) and asynchronous weak-commitment (AWC) , have been developed to address the DCSP and provide the agent solution basis for its applications. Broadly speaking, these algorithms are based on two different approaches, either extending from classical backtracking algorithms or introducing mediation among the agents. While there has been no lack of efforts in this promising research field, especially in dealing with outstanding issues such as resource restrictions (e.g., limits on time and communication) and privacy requirements , there is unfortunately no conceptually clear treatment to prise open the model-theoretic workings of the various agent algorithms that have been developed. As a result, for instance, a deeper intellectual understanding on why one algorithm is better than the other, beyond computational issues, is not possible. In this paper, we present a novel, unified distributed constraint satisfaction framework based on automated negotiation . Negotiation is viewed as a process of several agents searching for a solution called an agreement. The search can be realized via a negotiation mechanism (or algorithm) by which the agents follow a high level protocol prescribing the rules of interactions, using a set of strategies devised to select their own preferences at each negotiation step. Anchoring the DCSP search on automated negotiation, we show in this paper that several well-known DCSP algorithms are actually mechanisms that share the same Belief-DesireIntention (BDI) interaction protocol to reach agreements, but use different action or value selection strategies. The proposed framework provides not only a clearer understanding of existing DCSP algorithms from a unified BDI agent perspective, but also opens up the opportunities to extend and develop new strategies for DCSP. To this end, a new strategy called Unsolicited Mutual Advice (UMA) is proposed. Our performance evaluation shows that UMA can outperform ABT and AWC in terms of the average number of computational cycles for both the sparse and critical coloring problems . The rest of this paper is organized as follows. In Section 2, we provide a formal overview of DCSP. Section 3 presents a BDI negotiation model by which a DCSP agent reasons. Section 4 presents the existing algorithms ABT, AWC and DBO as different strategies formalized on a common protocol. A new strategy called Unsolicited Mutual Advice is proposed in Section 5; our empirical results and discussion attempt to highlight the merits of the new strategy over existing ones. Section 6 concludes the paper and points to some future work. 2. DCSP: PROBLEM FORMALIZATION The DCSP considers the following environment. \u2022 There are n agents with k variables x0, x1, \u00b7 \u00b7 \u00b7 , xk\u22121, n \u2264 k, which have values in domains D1, D2, \u00b7 \u00b7 \u00b7 , Dk, respectively. We define a partial function B over the productrange {0, 1, . . . , (n\u22121)}\u00d7{0, 1, . . . , (k \u22121)} such that, that variable xj belongs to agent i is denoted by B(i, j)!. The exclamation mark \u2018!\" means \u2018is defined\". \u2022 There are m constraints c0, c1, \u00b7 \u00b7 \u00b7 cm\u22121 to be conjunctively satisfied. In a similar fashion as defined for B(i, j), we use E(l, j)!, (0 \u2264 l < m, 0 \u2264 j < k), to denote that xj is relevant to the constraint cl. The DCSP may be formally stated as follows. Problem Statement: \u2200i, j (0 \u2264 i < n)(0 \u2264 j < k) where B(i, j)!, find the assignment xj = dj \u2208 Dj such that \u2200l (0 \u2264 l < m) where E(l, j)!, cl is satisfied. A constraint may consist of different variables belonging to different agents. An agent cannot change or modify the assignment values of other agents\" variables. Therefore, in cooperatively searching for a DCSP solution, the agents would need to communicate with one another, and adjust and re-adjust their own variable assignments in the process. 2.1 DCSP Agent Model In general, all DCSP agents must cooperatively interact, and essentially perform the assignment and reassignment of domain values to variables to resolve all constraint violations. If the agents succeed in their resolution, a solution is found. In order to engage in cooperative behavior, a DCSP agent needs five fundamental parameters, namely, (i) a variable or a variable set , (ii) domains, (iii) priority, (iv) a neighbor list and (v) a constraint list. Each variable assumes a range of values called a domain. A domain value, which usually abstracts an action, is a possible option that an agent may take. Each agent has an assigned priority. These priority values help decide the order in which they revise or modify their variable assignments. An agent\"s priority may be fixed (static) or changing (dynamic) when searching for a solution. If an agent has more than one variable, each variable can be assigned a different priority, to help determine which variable assignment the agent should modify first. An agent which shares the same constraint with another agent is called the latter\"s neighbor. Each agent needs to refer to its list of neighbors during the search process. This list may also be kept unchanged or updated accordingly in runtime. Similarly, each agent maintains a constraint list. The agent needs to ensure that there is no violation of the constraints in this list. Constraints can be added or removed from an agent\"s constraint list in runtime. As with an agent, a constraint can also be associated with a priority value. Constraints with a high priority are said to be more important than constraints with a lower priority. To distinguish it from the priority of an agent, the priority of a constraint is called its weight. 3. THE BDI NEGOTIATION MODEL The BDI model originates with the work of M. Bratman . According to [12, Ch.1], the BDI architecture is based on a philosophical model of human practical reasoning, and draws out the process of reasoning by which an agent decides which actions to perform at consecutive moments when pursuing certain goals. Grounding the scope to the DCSP framework, the common goal of all agents is finding a combination of domain values to satisfy a set of predefined constraints. In automated negotiation , such a solution is called an agreement among the agents. Within this scope, we found that we were able to unearth the generic behavior of a DCSP agent and formulate it in a negotiation protocol, prescribed using the powerful concepts of BDI. Thus, our proposed negotiation model can be said to combine the BDI concepts with automated negotiation in a multiagent framework, allowing us to conceptually separate DCSP mechanisms into a common BDI interaction protocol and the adopted strategies. 3.1 The generic protocol Figure 1 shows the basic reasoning steps in an arbitrary round of negotiation that constitute the new protocol. The solid line indicates the common component or transition which always exists regardless of the strategy used. The dotted line indicates the Percept Belief Desire Intention Mediation Execution Info Message Info Message Negotiation Message Negotiation Message Negotiation Message Negotiation Message Negotiation Message Negotiation Message Negotiation Message Figure 1: The BDI interaction protocol component or transition which may or may not appear depending on the adopted strategy. Two types of messages are exchanged through this protocol, namely, the info message and the negotiation message. An info message perceived is a message sent by another agent. The message will contain the current selected values and priorities of the variables of that sending agent. The main purpose of this message is to update the agent about the current environment. Info message is sent out at the end of one negotiation round (also called a negotiation cycle), and received at the beginning of next round. A negotiation message is a message which may be sent within a round. This message is for mediation purposes. The agent may put different contents into this type of message as long as it is agreed among the group. The format of the negotiation message and when it is to be sent out are subject to the strategy. A negotiation message can be sent out at the end of one reasoning step and received at the beginning of the next step. Mediation is a step of the protocol that depends on whether the agent\"s interaction with others is synchronous or asynchronous. In synchronous mechanism, mediation is required in every negotiation round. In an asynchronous one, mediation is needed only in a negotiation round when the agent receives a negotiation message. A more in-depth view of this mediation step is provided later in this section. The BDI protocol prescribes the skeletal structure for DCSP negotiation. We will show in Section 4 that several well-known DCSP mechanisms all inherit this generic model. The details of the six main reasoning steps for the protocol (see Figure 1) are described as follows for a DCSP agent. For a conceptually clearer description, we assume that there is only one variable per agent. \u2022 Percept. In this step, the agent receives info messages from its neighbors in the environment, and using its Percept function, returns an image P. This image contains the current values assigned to the variables of all agents in its neighbor list. The image P will drive the agent\"s actions in subsequent steps. The agent also updates its constraint list C using some criteria of the adopted strategy. \u2022 Belief. Using the image P and constraint list C, the agent will check if there is any violated constraint. If there is no violation, the agent will believe it is choosing a correct option and therefore will take no action. The agent will do nothing if it is in a local stable state - a snapshot of the variables assignments of the agent and all its neighbors by which they satisfy their shared constraints. When all agents are in their local stable states, the whole environment is said to be in a global stable state and an agreeThe Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 525 ment is found. In case the agent finds its value in conflict with some of its neighbors\", i.e., the combination of values assigned to the variables leads to a constraint violation, the agent will first try to reassign its own variable using a specific strategy. If it finds a suitable option which meets some criteria of the adopted strategy, the agent will believe it should change to the new option. However it does not always happen that an agent can successfully find such an option. If no option can be found, the agent will believe it has no option, and therefore will request its neighbors to reconsider their variable assignments. To summarize, there are three types of beliefs that a DCSP agent can form: (i) it can change its variable assignment to improve the current situation, (ii) it cannot change its variable assignment and some constraints violations cannot be resolved and (iii) it need not change its variable assignment as all the constraints are satisfied. Once the beliefs are formed, the agent will determine its desires, which are the options that attempt to resolve the current constraint violations. \u2022 Desire. If the agent takes Belief (i), it will generate a list of its own suitable domain values as its desire set. If the agent takes Belief (ii), it cannot ascertain its desire set, but will generate a sublist of agents from its neighbor list, whom it will ask to reconsider their variable assignments. How this sublist is created depends on the strategy devised for the agent. In this situation, the agent will use a virtual desire set that it determines based on its adopted strategy. If the agent takes Belief (iii), it will have no desire to revise its domain value, and hence no intention. \u2022 Intention. The agent will select a value from its desire set as its intention. An intention is the best desired option that the agent assigns to its variable. The criteria for selecting a desire as the agent\"s intention depend on the strategy used. Once the intention is formed, the agent may either proceed to the execution step, or undergo mediation. Again, the decision to do so is determined by some criteria of the adopted strategy. \u2022 Mediation. This is an important function of the agent. Since, if the agent executes its intention without performing intention mediation with its neighbors, the constraint violation between the agents may not be resolved. Take for example, suppose two agents have variables, x1 and x2, associated with the same domain {1, 2}, and their shared constraint is (x1 + x2 = 3). Then if both the variables are initialized with value 1, they will both concurrently switch between the values 2 and 1 in the absence of mediation between them. There are two types of mediation: local mediation and group mediation. In the former, the agents exchange their intentions. When an agent receives another\"s intention which conflicts with its own, the agent must mediate between the intentions, by either changing its own intention or informing the other agent to change its intention. In the latter, there is an agent which acts as a group mediator. This mediator will collect the intentions from the group - a union of the agent and its neighbors - and determine which intention is to be executed. The result of this mediation is passed back to the agents in the group. Following mediation, the agent may proceed to the next reasoning step to execute its intention or begin a new negotiation round. \u2022 Execution. This is the last step of a negotiation round. The agent will execute by updating its variable assignment if the intention obtained at this step is its own. Following execution, the agent will inform its neighbors about its new variable assignment and updated priority. To do so, the agent will send out an info message. 3.2 The strategy A strategy plays an important role in the negotiation process. Within the protocol, it will often determine the efficiency of the Percept Belief Desire Intention Mediation Execution Info Message Info Message Negotiation Message Negotiation Message Negotiation Message Figure 2: BDI protocol with Asynchronous Backtracking strategy search process in terms of computational cycles and message communication costs. The design space when devising a strategy is influenced by the following dimensions: (i) asynchronous or synchronous, (ii) dynamic or static priority, (iii) dynamic or static constraint weight, (iv) number of negotiation messages to be communicated, (v) the negotiation message format and (vi) the completeness property. In other words, these dimensions provide technical considerations for a strategy design. 4. DCSP ALGORITHMS: BDI PROTOCOL + STRATEGIES In this section, we apply the proposed BDI negotiation model presented in Section 3 to expose the BDI protocol and the different strategies used for three well-known algorithms, ABT, AWC and DBO. All these algorithms assume that there is only one variable per agent. Under our framework, we call the strategies applied the ABT, AWC and DBO strategies, respectively. To describe each strategy formally, the following mathematical notations are used: \u2022 n is the number of agents, m is the number of constraints; \u2022 xi denotes the variable held by agent i, (0 \u2264 i < n); \u2022 Di denotes the domain of variable xi; Fi denotes the neighbor list of agent i; Ci denotes its constraint list; \u2022 pi denotes the priority of agent i; and Pi = {(xj = vj, pj = k) | agent j \u2208 Fi, vj \u2208 Dj is the current value assigned to xj and the priority value k is a positive integer } is the perception of agent i; \u2022 wl denotes the weight of constraint l, (0 \u2264 l < m); \u2022 Si(v) is the total weight of the violated constraints in Ci when its variable has the value v \u2208 Di. 4.1 Asynchronous Backtracking Figure 2 presents the BDI negotiation model incorporating the Asynchronous Backtracking (ABT) strategy. As mentioned in Section 3, for an asynchronous mechanism that ABT is, the mediation step is needed only in a negotiation round when an agent receives a negotiation message. For agent i, beginning initially with (wl = 1, (0 \u2264 l < m); pi = i, (0 \u2264 i < n)) and Fi contains all the agents who share the constraints with agent i, its BDI-driven ABT strategy is described as follows. Step 1 - Percept: Update Pi upon receiving the info messages from the neighbors (in Fi). Update Ci to be the list of 526 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) constraints which only consists of agents in Fi that have equal or higher priority than this agent. Step 2 - Belief: The belief function GB (Pi,Ci) will return a value bi \u2208 {0, 1, 2}, decided as follows: \u2022 bi = 0 when agent i can find an optimal option, i.e., if (Si(vi) = 0 or vi is in bad values list) and (\u2203a \u2208 Di)(Si(a) = 0) and a is not in a list of domain values called bad values list. Initially this list is empty and it will be cleared when a neighbor of higher priority changes its variable assignment. \u2022 bi = 1 when it cannot find an optimal option, i.e., if (\u2200a \u2208 Di)(Si(a) = 0) or a is in bad values list. \u2022 bi = 2 when its current variable assignment is an optimal option, i.e., if Si(vi) = 0 and vi is not in bad value list. Step 3 - Desire: The desire function GD (bi) will return a desire set denoted by DS, decided as follows: \u2022 If bi = 0, then DS = {a | (a = vi), (Si(a) = 0) and a is not in the bad value list }. \u2022 If bi = 1, then DS = \u2205, the agent also finds agent k which is determined by {k | pk = min(pj) with agent j \u2208 Fi and pk > pi }. \u2022 If bi = 2, then DS = \u2205. Step 4 - Intention: The intention function GI (DS) will return an intention, decided as follows: \u2022 If DS = \u2205, then select an arbitrary value (say, vi) from DS as the intention. \u2022 If DS = \u2205, then assign nil as the intention (to denote its lack thereof). Step 5 - Execution: \u2022 If agent i has a domain value as its intention, the agent will update its variable assignment with this value. \u2022 If bi = 1, agent i will send a negotiation message to agent k, then remove k from Fi and begin its next negotiation round. The negotiation message will contain the list of variable assignments of those agents in its neighbor list Fi that have a higher priority than agent i in the current image Pi. Mediation: When agent i receives a negotiation message, several sub-steps are carried out, as follows: \u2022 If the list of agents associated with the negotiation message contains agents which are not in Fi, it will add these agents to Fi, and request these agents to add itself to their neighbor lists. The request is considered as a type of negotiation message. \u2022 Agent i will first check if the sender agent is updated with its current value vi. The agent will add vi to its bad values list if it is so, or otherwise send its current value to the sender agent. Following this step, agent i proceeds to the next negotiation round. 4.2 Asynchronous Weak Commitment Search Figure 3 presents the BDI negotiation model incorporating the Asynchronous Weak Commitment (AWC) strategy. The model is similar to that of incorporating the ABT strategy (see Figure 2). This is not surprising; AWC and ABT are found to be strategically similar, differing only in the details of some reasoning steps. The distinguishing point of AWC is that when the agent cannot find a suitable variable assignment, it will change its priority to the highest among its group members ({i} \u222a Fi). For agent i, beginning initially with (wl = 1, (0 \u2264 l < m); pi = i, (0 \u2264 i < n)) and Fi contains all the agents who share the constraints with agent i, its BDI-driven AWC strategy is described as follows. Step 1 - Percept: This step is identical to the Percept step of ABT. Step 2 - Belief: The belief function GB (Pi,Ci) will return a value bi \u2208 {0, 1, 2}, decided as follows: Percept Belief Desire Intention Mediation Execution Info Message Info Message Negotiation Message Negotiation Message Negotiation Message Figure 3: BDI protocol with Asynchronous WeakCommitment strategy \u2022 bi = 0 when the agent can find an optimal option i.e., if (Si(vi) = 0 or the assignment xi = vi and the current variables assignments of the neighbors in Fi who have higher priority form a nogood ) stored in a list called nogood list and \u2203a \u2208 Di, Si(a) = 0 (initially the list is empty). \u2022 bi = 1 when the agent cannot find any optimal option i.e., if \u2200a \u2208 Di, Si(a) = 0. \u2022 bi = 2 when the current assignment is an optimal option i.e., if Si(vi) = 0 and the current state is not a nogood in nogood list. Step 3 - Desire: The desire function GD (bi) will return a desire set DS, decided as follows: \u2022 If bi = 0, then DS = {a | (a = vi), (Si(a) = 0) and the number of constraint violations with lower priority agents is minimized }. \u2022 If bi = 1, then DS = {a | a \u2208 Di and the number of violations of all relevant constraints is minimized }. \u2022 If bi = 2, then DS = \u2205. Following, if bi = 1, agent i will find a list Ki of higher priority neighbors, defined by Ki = {k | agent k \u2208 Fi and pk > pi}. Step 4 - Intention: This step is similar to the Intention step of ABT. However, for this strategy, the negotiation message will contain the variable assignments (of the current image Pi) for all the agents in Ki. This list of assignment is considered as a nogood. If the same negotiation message had been sent out before, agent i will have nil intention. Otherwise, the agent will send the message and save the nogood in the nogood list. Step 5 - Execution: \u2022 If agent i has a domain value as its intention, the agent will update its variable assignment with this value. \u2022 If bi = 1, it will send the negotiation message to its neighbors in Ki, and set pi = max{pj} + 1, with agent j \u2208 Fi. Mediation: This step is identical to the Mediation step of ABT, except that agent i will now add the nogood contained in the negotiation message received to its own nogood list. 4.3 Distributed Breakout Figure 4 presents the BDI negotiation model incorporating the Distributed Breakout (DBO) strategy. Essentially, by this synchronous strategy, each agent will search iteratively for improvement by reducing the total weight of the violated constraints. The iteration will continue until no agent can improve further, at which time if some constraints remain violated, the weights of The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 527 Percept Belief Desire Intention Mediation Execution Info Message Info Message Negotiation Message Negotiation Message Figure 4: BDI protocol with Distributed Breakout strategy these constraints will be increased by 1 to help \u2018breakout\" from a local minimum. For agent i, beginning initially with (wl = 1, (0 \u2264 l < m), pi = i, (0 \u2264 i < n)) and Fi contains all the agents who share the constraints with agent i, its BDI-driven DBO strategy is described as follows. Step 1 - Percept: Update Pi upon receiving the info messages from the neighbors (in Fi). Update Ci to be the list of its relevant constraints. Step 2 - Belief: The belief function GB (Pi,Ci) will return a value bi \u2208 {0, 1, 2}, decided as follows: \u2022 bi = 0 when agent i can find an option to reduce the number violations of the constraints in Ci, i.e., if \u2203a \u2208 Di, Si(a) < Si(vi). \u2022 bi = 1 when it cannot find any option to improve situation, i.e., if \u2200a \u2208 Di, a = vi, Si(a) \u2265 Si(vi). \u2022 bi = 2 when its current assignment is an optimal option, i.e., if Si(vi) = 0. Step 3 - Desire: The desire function GD (bi) will return a desire set DS, decided as follows: \u2022 If bi = 0, then DS = {a | a = vi, Si(a) < Si(vi) and (Si(vi)\u2212Si(a)) is maximized }. (max{(Si(vi)\u2212Si(a))} will be referenced by hmax i in subsequent steps, and it defines the maximal reduction in constraint violations). \u2022 Otherwise, DS = \u2205. Step 4 - Intention: The intention function GI (DS) will return an intention, decided as follows: \u2022 If DS = \u2205, then select an arbitrary value (say, vi) from DS as the intention. \u2022 If DS = \u2205, then assign nil as the intention. Following, agent i will send its intention to all its neighbors. In return, it will receive intentions from these agents before proceeding to Mediation step. Mediation: Agent i receives all the intentions from its neighbors. If it finds that the intention received from a neighbor agent j is associated with hmax j > hmax i , the agent will automatically cancel its current intention. Step 5 - Execution: \u2022 If agent i did not cancel its intention, it will update its variable assignment with the intended value. Percept Belief Desire Intention Mediation Execution Info Message Info Message Negotiation Message Negotiation Message Negotiation Message Negotiation Message Figure 5: BDI protocol with Unsolicited Mutual Advice strategy \u2022 If all intentions received and its own one are nil intention, the agent will increase the weight of each currently violated constraint by 1. 5. THE UMA STRATEGY Figure 5 presents the BDI negotiation model incorporating the Unsolicited Mutual Advice(UMA) strategy. Unlike when using the strategies of the previous section, a DCSP agent using UMA will not only send out a negotiation message when concluding its Intention step, but also when concluding its Desire step. The negotiation message that it sends out to conclude the Desire step constitutes an unsolicited advice for all its neighbors. In turn, the agent will wait to receive unsolicited advices from all its neighbors, before proceeding on to determine its intention. For agent i, beginning initially with (wl = 1, (0 \u2264 l < m), pi = i, (0 \u2264 i < n)) and Fi contains all the agents who share the constraints with agent i, its BDI-driven UMA strategy is described as follows. Step 1 - Percept: Update Pi upon receiving the info messages from the neighbors (in Fi). Update Ci to be the list of constraints relevant to agent i. Step 2 - Belief: The belief function GB (Pi,Ci) will return a value bi \u2208 {0, 1, 2}, decided as follows: \u2022 bi = 0 when agent i can find an option to reduce the number violations of the constraints in Ci, i.e., if \u2203a \u2208 Di, Si(a) < Si(vi) and the assignment xi = a and the current variable assignments of its neighbors do not form a local state stored in a list called bad states list (initially this list is empty). \u2022 bi = 1 when it cannot find a value a such as a \u2208 Di, Si(a) < Si(vi), and the assignment xi = a and the current variable assignments of its neighbors do not form a local state stored in the bad states list. \u2022 bi = 2 when its current assignment is an optimal option, i.e., if Si(vi) = 0. Step 3 - Desire: The desire function GD (bi) will return a desire set DS, decided as follows: \u2022 If bi = 0, then DS = {a | a = vi, Si(a) < Si(vi) and (Si(vi) \u2212 Si(a)) is maximized } and the assignment xi = a and the current variable assignments of agent i\"s neighbors do not form a state in the bad states list. In this case, DS is called a set of voluntary desires. max{(Si(vi)\u2212Si(a))} will be referenced by hmax i in subsequent steps, and it defines 528 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) the maximal reduction in constraint violations. It is also referred to as an improvement). \u2022 If bi = 1, then DS = {a | a = vi, Si(a) is minimized } and the assignment xi = a and the current variable assignments of agent i\"s neighbors do not form a state in the bad states list. In this case, DS is called a set of reluctant desires \u2022 If bi = 2, then DS = \u2205. Following, if bi = 0, agent i will send a negotiation message containing hmax i to all its neighbors. This message is called a voluntary advice. If bi = 1, agent i will send a negotiation message called change advice to the neighbors in Fi who share the violated constraints with agent i. Agent i receives advices from all its neighbors and stores them in a list called A, before proceeding to the next step. Step 4 - Intention: The intention function GI (DS, A) will return an intention, decided as follows: \u2022 If there is a voluntary advice from an agent j which is associated with hmax j > hmax i , assign nil as the intention. \u2022 If DS = \u2205, DS is a set of voluntary desires and hmax i is the biggest improvement among those associated with the voluntary advices received, select an arbitrary value (say, vi) from DS as the intention. This intention is called a voluntary intention. \u2022 If DS = \u2205, DS is a set of reluctant desires and agent i receives some change advices, select an arbitrary value (say, vi) from DS as the intention. This intention is called reluctant intention. \u2022 If DS = \u2205, then assign nil as the intention. Following, if the improvement hmax i is the biggest improvement and equal to some improvements associated with the received voluntary advices, agent i will send its computed intention to all its neighbors. If agent i has a reluctant intention, it will also send this intention to all its neighbors. In both cases, agent i will attach the number of received change advices in the current negotiation round with its intention. In return, agent i will receive the intentions from its neighbors before proceeding to Mediation step. Mediation: If agent i does not send out its intention before this step, i.e., the agent has either a nil intention or a voluntary intention with biggest improvement, it will proceed to next step. Otherwise, agent i will select the best intention among all the intentions received, including its own (if any). The criteria to select the best intention are listed, applied in descending order of importance as follows. \u2022 A voluntary intention is preferred over a reluctant intention. \u2022 A voluntary intention (if any) with biggest improvement is selected. \u2022 If there is no voluntary intention, the reluctant intention with the lowest number of constraint violations is selected. \u2022 The intention from an agent who has received a higher number of change advices in the current negotiation round is selected. \u2022 Intention from an agent with highest priority is selected. If the selected intention is not agent i\"s intention, it will cancel its intention. Step 5 - Execution: If agent i does not cancel its intention, it will update its variable assignment with the intended value. Termination Condition: Since each agent does not have full information about the global state, it may not know when it has reached a solution, i.e., when all the agents are in a global stable state. Hence an observer is needed that will keep track of the negotiation messages communicated in the environment. Following a certain period of time when there is no more message communication (and this happens when all the agents have no more intention to update their variable assignments), the observer will inform the agents in the environment that a solution has been found. 6 7 10 Figure 6: Example problem 5.1 An Example To illustrate how UMA works, consider a 2-color graph problem as shown in Figure 6. In this example, each agent has a color variable representing a node. There are 10 color variables sharing the same domain {Black, White}. The following records the outcome of each step in every negotiation round executed. Round 1: Step 1 - Percept: Each agent obtains the current color assignments of those nodes (agents) adjacent to it, i.e., its neighbors\". Step 2 - Belief: Agents which have positive improvements are agent 1 (this agent believes it should change its color to White), agent 2 (this believes should change its color to White), agent 7 (this agent believes it should change its color to Black) and agent 10 (this agent believes it should change its value to Black). In this negotiation round, the improvements achieved by these agents are 1. Agents which do not have any improvements are agents 4, 5 and 8. Agents 3, 6 and 9 need not change as all their relevant constraints are satisfied. Step 3 - Desire: Agents 1, 2, 7 and 10 have the voluntary desire (White color for agents 1, 2 and Black color for agents 7, 10). These agents will send the voluntary advices to all their neighbors. Meanwhile, agents 4, 5 and 8 have the reluctant desires (White color for agent 4 and Black color for agents 5, 8). Agent 4 will send a change advice to agent 2 as agent 2 is sharing the violated constraint with it. Similarly, agents 5 and 8 will send change advices to agents 7 and 10 respectively. Agents 3, 6 and 9 do not have any desire to update their color assignments. Step 4 - Intention: Agents 2, 7 and 10 receive the change advices from agents 4, 5 and 8, respectively. They form their voluntary intentions. Agents 4, 5 and 8 receive the voluntary advices from agents 2, 7 and 10, hence they will not have any intention. Agents 3, 6 and 9 do not have any intention. Following, the intention from the agents will be sent to all their neighbors. Mediation: Agent 1 finds that the intention from agent 2 is better than its intention. This is because, although both agents have voluntary intentions with improvement of 1, agent 2 has received one change advice from agent 4 while agent 1 has not received any. Hence agent 1 cancels its intention. Agent 2 will keep its intention. Agents 7 and 10 keep their intentions since none of their neighbors has an intention. The rest of the agents do nothing in this step as they do not have any intention. Step 5 - Execution: Agent 2 changes its color to White. Agents 7 and 10 change their colors to Black. The new state after round 1 is shown in Figure 7. Round 2: Step 1 - Percept: The agents obtain the current color assignments of their neighbors. Step 2 - Belief: Agent 3 is the only agent who has a positive improvement which is 1. It believes it should change its The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 529 6 7 10 Figure 7: The graph after round 1 color to Black. Agent 2 does not have any positive improvement. The rest of the agents need not make any change as all their relevant constraints are satisfied. They will have no desire, and hence no intention. Step 3 - Desire: Agent 3 desires to change its color to Black voluntarily, hence it sends out a voluntary advice to its neighbor, i.e., agent 2. Agent 2 does not have any value for its reluctant desire set as the only option, Black color, will bring agent 2 and its neighbors to the previous state which is known to be a bad state. Since agent 2 is sharing the constraint violation with agent 3, it sends a change advice to agent 3. Step 4 - Intention: Agent 3 will have a voluntary intention while agent 2 will not have any intention as it receives the voluntary advice from agent 3. Mediation: Agent 3 will keep its intention as its only neighbor, agent 2, does not have any intention. Step 5 - Execution: Agent 3 changes its color to Black. The new state after round 2 is shown in Figure 8. Round 3: In this round, every agent finds that it has no desire and hence no intention to revise its variable assignment. Following, with no more negotiation message communication in the environment, the observer will inform all the agents that a solution has been found. 6 7 91 10 Figure 8: The solution obtained 5.2 Performance Evaluation To facilitate credible comparisons with existing strategies, we measured the execution time in terms of computational cycles as defined in , and built a simulator that could reproduce the published results for ABT and AWC. The definition of a computational cycle is as follows. \u2022 In one cycle, each agent receives all the incoming messages, performs local computation and sends out a reply. \u2022 A message which is sent at time t will be received at time t + 1. The network delay is neglected. \u2022 Each agent has it own clock. The initial clock\"s value is 0. Agents attach their clock value as a time-stamp in the outgoing message and use the time-stamp in the incoming message to update their own clock\"s value. Four benchmark problems were considered, namely, n-queens and node coloring for sparse, dense and critical graphs. For each problem, a finite number of test cases were generated for various problem sizes n. 10 50 100 Number of queens Cycles Asynchronous Backtracking Asynchronous Weak Commitment Unsolicited Mutual Advice Figure 9: cycles for other problems. The simulator program was terminated after this period and the algorithm was considered to fail a test case if it did not find a solution by then. In such a case, cycles. 5.2.1 Evaluation with n-queens problem The n-queens problem is a traditional problem of constraint satisfaction. 10 test cases were generated for each problem size n \u2208 {10, 50 and 100}. Figure 9 shows the execution time for different problem sizes when ABT, AWC and UMA were run. 5.2.2 Evaluation with graph coloring problem The graph coloring problem can be characterized by three parameters: (i) the number of colors k, the number of nodes/agents n and the number of links m. Based on the ratio m/n, the problem can be classified into three types : (i) sparse (with m/n = 2), (ii) critical (with m/n = 2.7 or 4.7) and (iii) dense (with m/n = (n \u2212 1)/4). For this problem, we did not include ABT in our empirical results as its failure rate was found to be very high. This poor performance of ABT was expected since the graph coloring problem is more difficult than the n-queens problem, on which ABT already did not perform well (see Figure 9). The sparse and dense (coloring) problem types are relatively easy while the critical type is difficult to solve. In the experiments, we fix k = 3. 10 test cases were created using the method described in for each value of n \u2208 {60, 90, 120}, for each problem type. The simulation results for each type of problem are shown in Figures 10 - 12. 40 80 120 160 200 60 90 120 150 Number of Nodes Cycles Asynchronous Weak Commitment Unsolicited Mutual Advice Figure 10: Comparison between AWC and UMA (sparse graph coloring) 5.3 Discussion 5.3.1 Comparison with ABT and AWC 530 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 60 90 120 Number of Nodes Cycles Asynchronous Weak Commitment Unsolicited Mutual Advice Figure 11: Comparison between AWC and UMA (critical graph coloring) 10 20 30 40 50 60 90 120 Number of Nodes Cycles Asynchronous Weak Commitment Unsolicited Mutual Advice Figure 12: Comparison between AWC and UMA (dense graph coloring) Figure 10 shows that the average performance of UMA is slightly better than AWC for the sparse problem. UMA outperforms AWC in solving the critical problem as shown in Figure 11. It was observed that the latter strategy failed in some test cases. However, as seen in Figure 12, both the strategies are very efficient when solving the dense problem, with AWC showing slightly better performance. The performance of UMA, in the worst (time complexity) case, is similar to that of all evaluated strategies. The worst case occurs when all the possible global states of the search are reached. Since only a few agents have the right to change their variable assignments in a negotiation round, the number of redundant computational cycles and info messages is reduced. As we observe from the backtracking in ABT and AWC, the difference in the ordering of incoming messages can result in a different number of computational cycles to be executed by the agents. 5.3.2 Comparison with DBO The computational performance of UMA is arguably better than DBO for the following reasons: \u2022 UMA can guarantee that there will be a variable reassignment following every negotiation round whereas DBO cannot. \u2022 UMA introduces one more communication round trip (that of sending a message and awaiting a reply) than DBO, which occurs due to the need to communicate unsolicited advices. Although this increases the communication cost per negotiation round, we observed from our simulations that the overall communication cost incurred by UMA is lower due to the significantly lower number of negotiation rounds. \u2022 Using UMA, in the worst case, an agent will only take 2 or 3 communication round trips per negotiation round, following which the agent or its neighbor will do a variable assignment update. Using DBO, this number of round trips is uncertain as each agent might have to increase the weights of the violated constraints until an agent has a positive improvement; this could result in a infinite loop . 6. CONCLUSION Applying automated negotiation to DCSP, this paper has proposed a protocol that prescribes the generic reasoning of a DCSP agent in a BDI architecture. Our work shows that several wellknown DCSP algorithms, namely ABT, AWC and DBO, can be described as mechanisms sharing the same proposed protocol, and only differ in the strategies employed for the reasoning steps per negotiation round as governed by the protocol. Importantly, this means that it might furnish a unified framework for DCSP that not only provides a clearer BDI agent-theoretic view of existing DCSP approaches, but also opens up the opportunities to enhance or develop new strategies. Towards the latter, we have proposed and formulated a new strategy - the UMA strategy. Empirical results and our discussion suggest that UMA is superior to ABT, AWC and DBO in some specific aspects. It was observed from our simulations that UMA possesses the completeness property. Future work will attempt to formally establish this property, as well as formalize other existing DSCP algorithms as BDI negotiation mechanisms, including the recent endeavor that employs a group mediator . The idea of DCSP agents using different strategies in the same environment will also be investigated.", "body1": "At the core of many emerging distributed applications is the distributed constraint satisfaction problem (DCSP) - one which involves finding a consistent combination of actions (abstracted as domain values) to satisfy the constraints among multiple agents in a shared environment. While there has been no lack of efforts in this promising research field, especially in dealing with outstanding issues such as resource restrictions (e.g., limits on time and communication) and privacy requirements , there is unfortunately no conceptually clear treatment to prise open the model-theoretic workings of the various agent algorithms that have been developed. In this paper, we present a novel, unified distributed constraint satisfaction framework based on automated negotiation . Negotiation is viewed as a process of several agents searching for a solution called an agreement. Anchoring the DCSP search on automated negotiation, we show in this paper that several well-known DCSP algorithms are actually mechanisms that share the same Belief-DesireIntention (BDI) interaction protocol to reach agreements, but use different action or value selection strategies. The rest of this paper is organized as follows. The DCSP considers the following environment. \u2022 There are n agents with k variables x0, x1, \u00b7 \u00b7 \u00b7 , xk\u22121, n \u2264 k, which have values in domains D1, D2, \u00b7 \u00b7 \u00b7 , Dk, respectively. The DCSP may be formally stated as follows. Problem Statement: \u2200i, j (0 \u2264 i < n)(0 \u2264 j < k) where B(i, j)!, find the assignment xj = dj \u2208 Dj such that \u2200l (0 \u2264 l < m) where E(l, j)!, cl is satisfied. A constraint may consist of different variables belonging to different agents. In order to engage in cooperative behavior, a DCSP agent needs five fundamental parameters, namely, (i) a variable or a variable set , (ii) domains, (iii) priority, (iv) a neighbor list and (v) a constraint list. Each variable assumes a range of values called a domain. These priority values help decide the order in which they revise or modify their variable assignments. An agent which shares the same constraint with another agent is called the latter\"s neighbor. As with an agent, a constraint can also be associated with a priority value. The BDI model originates with the work of M. Bratman . Grounding the scope to the DCSP framework, the common goal of all agents is finding a combination of domain values to satisfy a set of predefined constraints. 3.1 The generic protocol Figure 1 shows the basic reasoning steps in an arbitrary round of negotiation that constitute the new protocol. An info message perceived is a message sent by another agent. The message will contain the current selected values and priorities of the variables of that sending agent. Info message is sent out at the end of one negotiation round (also called a negotiation cycle), and received at the beginning of next round. A negotiation message is a message which may be sent within a round. In synchronous mechanism, mediation is required in every negotiation round. The details of the six main reasoning steps for the protocol (see Figure 1) are described as follows for a DCSP agent. \u2022 Percept. \u2022 Belief. Once the beliefs are formed, the agent will determine its desires, which are the options that attempt to resolve the current constraint violations. \u2022 Desire. \u2022 Intention. \u2022 Mediation. Since, if the agent executes its intention without performing intention mediation with its neighbors, the constraint violation between the agents may not be resolved. There are two types of mediation: local mediation and group mediation. This mediator will collect the intentions from the group - a union of the agent and its neighbors - and determine which intention is to be executed. The agent will execute by updating its variable assignment if the intention obtained at this step is its own. Within the protocol, it will often determine the efficiency of the Percept Belief Desire Intention Mediation Execution Info Message Info Message Negotiation Message Negotiation Message Negotiation Message Figure 2: BDI protocol with Asynchronous Backtracking strategy search process in terms of computational cycles and message communication costs. The design space when devising a strategy is influenced by the following dimensions: (i) asynchronous or synchronous, (ii) dynamic or static priority, (iii) dynamic or static constraint weight, (iv) number of negotiation messages to be communicated, (v) the negotiation message format and (vi) the completeness property. In other words, these dimensions provide technical considerations for a strategy design. + STRATEGIES In this section, we apply the proposed BDI negotiation model presented in Section 3 to expose the BDI protocol and the different strategies used for three well-known algorithms, ABT, AWC and DBO. 4.1 Asynchronous Backtracking Figure 2 presents the BDI negotiation model incorporating the Asynchronous Backtracking (ABT) strategy. For agent i, beginning initially with (wl = 1, (0 \u2264 l < m); pi = i, (0 \u2264 i < n)) and Fi contains all the agents who share the constraints with agent i, its BDI-driven ABT strategy is described as follows. Step 1 - Percept: Update Pi upon receiving the info messages from the neighbors (in Fi). \u2022 bi = 1 when it cannot find an optimal option, i.e., if (\u2200a \u2208 Di)(Si(a) = 0) or a is in bad values list. \u2022 bi = 2 when its current variable assignment is an optimal option, i.e., if Si(vi) = 0 and vi is not in bad value list. Step 3 - Desire: The desire function GD (bi) will return a desire set denoted by DS, decided as follows: \u2022 If bi = 0, then DS = {a | (a = vi), (Si(a) = 0) and a is not in the bad value list }. \u2022 If bi = 1, then DS = \u2205, the agent also finds agent k which is determined by {k | pk = min(pj) with agent j \u2208 Fi and pk > pi }. \u2022 If bi = 2, then DS = \u2205. Step 4 - Intention: The intention function GI (DS) will return an intention, decided as follows: \u2022 If DS = \u2205, then select an arbitrary value (say, vi) from DS as the intention. \u2022 If DS = \u2205, then assign nil as the intention (to denote its lack thereof). Step 5 - Execution: \u2022 If agent i has a domain value as its intention, the agent will update its variable assignment with this value. \u2022 If bi = 1, agent i will send a negotiation message to agent k, then remove k from Fi and begin its next negotiation round. Mediation: When agent i receives a negotiation message, several sub-steps are carried out, as follows: \u2022 If the list of agents associated with the negotiation message contains agents which are not in Fi, it will add these agents to Fi, and request these agents to add itself to their neighbor lists. \u2022 Agent i will first check if the sender agent is updated with its current value vi. Following this step, agent i proceeds to the next negotiation round. 4.2 Asynchronous Weak Commitment Search Figure 3 presents the BDI negotiation model incorporating the Asynchronous Weak Commitment (AWC) strategy. This is not surprising; AWC and ABT are found to be strategically similar, differing only in the details of some reasoning steps. The distinguishing point of AWC is that when the agent cannot find a suitable variable assignment, it will change its priority to the highest among its group members ({i} \u222a Fi). For agent i, beginning initially with (wl = 1, (0 \u2264 l < m); pi = i, (0 \u2264 i < n)) and Fi contains all the agents who share the constraints with agent i, its BDI-driven AWC strategy is described as follows. Step 1 - Percept: This step is identical to the Percept step of ABT. Step 2 - Belief: The belief function GB (Pi,Ci) will return a value bi \u2208 {0, 1, 2}, decided as follows: Percept Belief Desire Intention Mediation Execution Info Message Info Message Negotiation Message Negotiation Message Negotiation Message Figure 3: BDI protocol with Asynchronous WeakCommitment strategy \u2022 bi = 0 when the agent can find an optimal option i.e., if (Si(vi) = 0 or the assignment xi = vi and the current variables assignments of the neighbors in Fi who have higher priority form a nogood ) stored in a list called nogood list and \u2203a \u2208 Di, Si(a) = 0 (initially the list is empty). \u2022 bi = 1 when the agent cannot find any optimal option i.e., if \u2200a \u2208 Di, Si(a) = 0. \u2022 bi = 2 when the current assignment is an optimal option i.e., if Si(vi) = 0 and the current state is not a nogood in nogood list. Step 3 - Desire: The desire function GD (bi) will return a desire set DS, decided as follows: \u2022 If bi = 0, then DS = {a | (a = vi), (Si(a) = 0) and the number of constraint violations with lower priority agents is minimized }. \u2022 If bi = 1, then DS = {a | a \u2208 Di and the number of violations of all relevant constraints is minimized }. Following, if bi = 1, agent i will find a list Ki of higher priority neighbors, defined by Ki = {k | agent k \u2208 Fi and pk > pi}. Step 4 - Intention: This step is similar to the Intention step of ABT. \u2022 If bi = 1, it will send the negotiation message to its neighbors in Ki, and set pi = max{pj} + 1, with agent j \u2208 Fi. Mediation: This step is identical to the Mediation step of ABT, except that agent i will now add the nogood contained in the negotiation message received to its own nogood list. 4.3 Distributed Breakout Figure 4 presents the BDI negotiation model incorporating the Distributed Breakout (DBO) strategy. The iteration will continue until no agent can improve further, at which time if some constraints remain violated, the weights of The Sixth Intl. Step 1 - Percept: Update Pi upon receiving the info messages from the neighbors (in Fi). Step 2 - Belief: The belief function GB (Pi,Ci) will return a value bi \u2208 {0, 1, 2}, decided as follows: \u2022 bi = 0 when agent i can find an option to reduce the number violations of the constraints in Ci, i.e., if \u2203a \u2208 Di, Si(a) < Si(vi). \u2022 bi = 1 when it cannot find any option to improve situation, i.e., if \u2200a \u2208 Di, a = vi, Si(a) \u2265 Si(vi). \u2022 bi = 2 when its current assignment is an optimal option, i.e., if Si(vi) = 0. Step 3 - Desire: The desire function GD (bi) will return a desire set DS, decided as follows: \u2022 If bi = 0, then DS = {a | a = vi, Si(a) < Si(vi) and (Si(vi)\u2212Si(a)) is maximized }. \u2022 Otherwise, DS = \u2205. \u2022 If DS = \u2205, then assign nil as the intention. Following, agent i will send its intention to all its neighbors. In return, it will receive intentions from these agents before proceeding to Mediation step. Mediation: Agent i receives all the intentions from its neighbors. Step 5 - Execution: \u2022 If agent i did not cancel its intention, it will update its variable assignment with the intended value. Percept Belief Desire Intention Mediation Execution Info Message Info Message Negotiation Message Negotiation Message Negotiation Message Negotiation Message Figure 5: BDI protocol with Unsolicited Mutual Advice strategy \u2022 If all intentions received and its own one are nil intention, the agent will increase the weight of each currently violated constraint by 1. Figure 5 presents the BDI negotiation model incorporating the Unsolicited Mutual Advice(UMA) strategy. Unlike when using the strategies of the previous section, a DCSP agent using UMA will not only send out a negotiation message when concluding its Intention step, but also when concluding its Desire step. Step 2 - Belief: The belief function GB (Pi,Ci) will return a value bi \u2208 {0, 1, 2}, decided as follows: \u2022 bi = 0 when agent i can find an option to reduce the number violations of the constraints in Ci, i.e., if \u2203a \u2208 Di, Si(a) < Si(vi) and the assignment xi = a and the current variable assignments of its neighbors do not form a local state stored in a list called bad states list (initially this list is empty). \u2022 bi = 1 when it cannot find a value a such as a \u2208 Di, Si(a) < Si(vi), and the assignment xi = a and the current variable assignments of its neighbors do not form a local state stored in the bad states list. Step 3 - Desire: The desire function GD (bi) will return a desire set DS, decided as follows: \u2022 If bi = 0, then DS = {a | a = vi, Si(a) < Si(vi) and (Si(vi) \u2212 Si(a)) is maximized } and the assignment xi = a and the current variable assignments of agent i\"s neighbors do not form a state in the bad states list. Following, if bi = 0, agent i will send a negotiation message containing hmax i to all its neighbors. Step 4 - Intention: The intention function GI (DS, A) will return an intention, decided as follows: \u2022 If there is a voluntary advice from an agent j which is associated with hmax j > hmax i , assign nil as the intention. \u2022 If DS = \u2205, DS is a set of voluntary desires and hmax i is the biggest improvement among those associated with the voluntary advices received, select an arbitrary value (say, vi) from DS as the intention. \u2022 If DS = \u2205, DS is a set of reluctant desires and agent i receives some change advices, select an arbitrary value (say, vi) from DS as the intention. Following, if the improvement hmax i is the biggest improvement and equal to some improvements associated with the received voluntary advices, agent i will send its computed intention to all its neighbors. Otherwise, agent i will select the best intention among all the intentions received, including its own (if any). \u2022 A voluntary intention is preferred over a reluctant intention. \u2022 A voluntary intention (if any) with biggest improvement is selected. \u2022 If there is no voluntary intention, the reluctant intention with the lowest number of constraint violations is selected. \u2022 The intention from an agent who has received a higher number of change advices in the current negotiation round is selected. \u2022 Intention from an agent with highest priority is selected. If the selected intention is not agent i\"s intention, it will cancel its intention. Step 5 - Execution: If agent i does not cancel its intention, it will update its variable assignment with the intended value. Termination Condition: Since each agent does not have full information about the global state, it may not know when it has reached a solution, i.e., when all the agents are in a global stable state. Following a certain period of time when there is no more message communication (and this happens when all the agents have no more intention to update their variable assignments), the observer will inform the agents in the environment that a solution has been found. 6 7 10 Figure 6: Example problem 5.1 An Example To illustrate how UMA works, consider a 2-color graph problem as shown in Figure 6. Round 1: Step 1 - Percept: Each agent obtains the current color assignments of those nodes (agents) adjacent to it, i.e., its neighbors\". Step 2 - Belief: Agents which have positive improvements are agent 1 (this agent believes it should change its color to White), agent 2 (this believes should change its color to White), agent 7 (this agent believes it should change its color to Black) and agent 10 (this agent believes it should change its value to Black). Step 3 - Desire: Agents 1, 2, 7 and 10 have the voluntary desire (White color for agents 1, 2 and Black color for agents 7, 10). Step 4 - Intention: Agents 2, 7 and 10 receive the change advices from agents 4, 5 and 8, respectively. Mediation: Agent 1 finds that the intention from agent 2 is better than its intention. The rest of the agents do nothing in this step as they do not have any intention. Step 5 - Execution: Agent 2 changes its color to White. The new state after round 1 is shown in Figure 7. Round 2: Step 1 - Percept: The agents obtain the current color assignments of their neighbors. Step 2 - Belief: Agent 3 is the only agent who has a positive improvement which is 1. Step 3 - Desire: Agent 3 desires to change its color to Black voluntarily, hence it sends out a voluntary advice to its neighbor, i.e., agent 2. Mediation: Agent 3 will keep its intention as its only neighbor, agent 2, does not have any intention. Step 5 - Execution: Agent 3 changes its color to Black. The new state after round 2 is shown in Figure 8. Round 3: In this round, every agent finds that it has no desire and hence no intention to revise its variable assignment. Following, with no more negotiation message communication in the environment, the observer will inform all the agents that a solution has been found. 6 7 91 10 Figure 8: The solution obtained 5.2 Performance Evaluation To facilitate credible comparisons with existing strategies, we measured the execution time in terms of computational cycles as defined in , and built a simulator that could reproduce the published results for ABT and AWC. \u2022 In one cycle, each agent receives all the incoming messages, performs local computation and sends out a reply. \u2022 A message which is sent at time t will be received at time t + 1. \u2022 Each agent has it own clock. Four benchmark problems were considered, namely, n-queens and node coloring for sparse, dense and critical graphs. Figure 9 shows the execution time for different problem sizes when ABT, AWC and UMA were run. 5.2.2 Evaluation with graph coloring problem The graph coloring problem can be characterized by three parameters: (i) the number of colors k, the number of nodes/agents n and the number of links m. Based on the ratio m/n, the problem can be classified into three types : (i) sparse (with m/n = 2), (ii) critical (with m/n = 2.7 or 4.7) and (iii) dense (with m/n = (n \u2212 1)/4). The sparse and dense (coloring) problem types are relatively easy while the critical type is difficult to solve. 40 80 120 160 200 60 90 120 150 Number of Nodes Cycles Asynchronous Weak Commitment Unsolicited Mutual Advice Figure 10: Comparison between AWC and UMA (sparse graph coloring) 5.3 Discussion 5.3.1 Comparison with ABT and AWC 530 The Sixth Intl. The performance of UMA, in the worst (time complexity) case, is similar to that of all evaluated strategies. Since only a few agents have the right to change their variable assignments in a negotiation round, the number of redundant computational cycles and info messages is reduced. 5.3.2 Comparison with DBO The computational performance of UMA is arguably better than DBO for the following reasons: \u2022 UMA can guarantee that there will be a variable reassignment following every negotiation round whereas DBO cannot. \u2022 UMA introduces one more communication round trip (that of sending a message and awaiting a reply) than DBO, which occurs due to the need to communicate unsolicited advices. \u2022 Using UMA, in the worst case, an agent will only take 2 or 3 communication round trips per negotiation round, following which the agent or its neighbor will do a variable assignment update.", "body2": "Broadly speaking, these algorithms are based on two different approaches, either extending from classical backtracking algorithms or introducing mediation among the agents. As a result, for instance, a deeper intellectual understanding on why one algorithm is better than the other, beyond computational issues, is not possible. In this paper, we present a novel, unified distributed constraint satisfaction framework based on automated negotiation . The search can be realized via a negotiation mechanism (or algorithm) by which the agents follow a high level protocol prescribing the rules of interactions, using a set of strategies devised to select their own preferences at each negotiation step. Our performance evaluation shows that UMA can outperform ABT and AWC in terms of the average number of computational cycles for both the sparse and critical coloring problems . Section 6 concludes the paper and points to some future work. The DCSP considers the following environment. In a similar fashion as defined for B(i, j), we use E(l, j)!, (0 \u2264 l < m, 0 \u2264 j < k), to denote that xj is relevant to the constraint cl. The DCSP may be formally stated as follows. Problem Statement: \u2200i, j (0 \u2264 i < n)(0 \u2264 j < k) where B(i, j)!, find the assignment xj = dj \u2208 Dj such that \u2200l (0 \u2264 l < m) where E(l, j)!, cl is satisfied. If the agents succeed in their resolution, a solution is found. In order to engage in cooperative behavior, a DCSP agent needs five fundamental parameters, namely, (i) a variable or a variable set , (ii) domains, (iii) priority, (iv) a neighbor list and (v) a constraint list. Each agent has an assigned priority. If an agent has more than one variable, each variable can be assigned a different priority, to help determine which variable assignment the agent should modify first. Constraints can be added or removed from an agent\"s constraint list in runtime. To distinguish it from the priority of an agent, the priority of a constraint is called its weight. According to [12, Ch.1], the BDI architecture is based on a philosophical model of human practical reasoning, and draws out the process of reasoning by which an agent decides which actions to perform at consecutive moments when pursuing certain goals. Thus, our proposed negotiation model can be said to combine the BDI concepts with automated negotiation in a multiagent framework, allowing us to conceptually separate DCSP mechanisms into a common BDI interaction protocol and the adopted strategies. Two types of messages are exchanged through this protocol, namely, the info message and the negotiation message. An info message perceived is a message sent by another agent. The main purpose of this message is to update the agent about the current environment. Info message is sent out at the end of one negotiation round (also called a negotiation cycle), and received at the beginning of next round. Mediation is a step of the protocol that depends on whether the agent\"s interaction with others is synchronous or asynchronous. We will show in Section 4 that several well-known DCSP mechanisms all inherit this generic model. For a conceptually clearer description, we assume that there is only one variable per agent. The agent also updates its constraint list C using some criteria of the adopted strategy. To summarize, there are three types of beliefs that a DCSP agent can form: (i) it can change its variable assignment to improve the current situation, (ii) it cannot change its variable assignment and some constraints violations cannot be resolved and (iii) it need not change its variable assignment as all the constraints are satisfied. Once the beliefs are formed, the agent will determine its desires, which are the options that attempt to resolve the current constraint violations. If the agent takes Belief (iii), it will have no desire to revise its domain value, and hence no intention. Again, the decision to do so is determined by some criteria of the adopted strategy. This is an important function of the agent. Then if both the variables are initialized with value 1, they will both concurrently switch between the values 2 and 1 in the absence of mediation between them. In the latter, there is an agent which acts as a group mediator. This is the last step of a negotiation round. 3.2 The strategy A strategy plays an important role in the negotiation process. Within the protocol, it will often determine the efficiency of the Percept Belief Desire Intention Mediation Execution Info Message Info Message Negotiation Message Negotiation Message Negotiation Message Figure 2: BDI protocol with Asynchronous Backtracking strategy search process in terms of computational cycles and message communication costs. The design space when devising a strategy is influenced by the following dimensions: (i) asynchronous or synchronous, (ii) dynamic or static priority, (iii) dynamic or static constraint weight, (iv) number of negotiation messages to be communicated, (v) the negotiation message format and (vi) the completeness property. In other words, these dimensions provide technical considerations for a strategy design. To describe each strategy formally, the following mathematical notations are used: \u2022 n is the number of agents, m is the number of constraints; \u2022 xi denotes the variable held by agent i, (0 \u2264 i < n); \u2022 Di denotes the domain of variable xi; Fi denotes the neighbor list of agent i; Ci denotes its constraint list; \u2022 pi denotes the priority of agent i; and Pi = {(xj = vj, pj = k) | agent j \u2208 Fi, vj \u2208 Dj is the current value assigned to xj and the priority value k is a positive integer } is the perception of agent i; \u2022 wl denotes the weight of constraint l, (0 \u2264 l < m); \u2022 Si(v) is the total weight of the violated constraints in Ci when its variable has the value v \u2208 Di. As mentioned in Section 3, for an asynchronous mechanism that ABT is, the mediation step is needed only in a negotiation round when an agent receives a negotiation message. For agent i, beginning initially with (wl = 1, (0 \u2264 l < m); pi = i, (0 \u2264 i < n)) and Fi contains all the agents who share the constraints with agent i, its BDI-driven ABT strategy is described as follows. Initially this list is empty and it will be cleared when a neighbor of higher priority changes its variable assignment. \u2022 bi = 1 when it cannot find an optimal option, i.e., if (\u2200a \u2208 Di)(Si(a) = 0) or a is in bad values list. \u2022 bi = 2 when its current variable assignment is an optimal option, i.e., if Si(vi) = 0 and vi is not in bad value list. Step 3 - Desire: The desire function GD (bi) will return a desire set denoted by DS, decided as follows: \u2022 If bi = 0, then DS = {a | (a = vi), (Si(a) = 0) and a is not in the bad value list }. \u2022 If bi = 1, then DS = \u2205, the agent also finds agent k which is determined by {k | pk = min(pj) with agent j \u2208 Fi and pk > pi }. \u2022 If bi = 2, then DS = \u2205. Step 4 - Intention: The intention function GI (DS) will return an intention, decided as follows: \u2022 If DS = \u2205, then select an arbitrary value (say, vi) from DS as the intention. \u2022 If DS = \u2205, then assign nil as the intention (to denote its lack thereof). Step 5 - Execution: \u2022 If agent i has a domain value as its intention, the agent will update its variable assignment with this value. The negotiation message will contain the list of variable assignments of those agents in its neighbor list Fi that have a higher priority than agent i in the current image Pi. The request is considered as a type of negotiation message. The agent will add vi to its bad values list if it is so, or otherwise send its current value to the sender agent. Following this step, agent i proceeds to the next negotiation round. The model is similar to that of incorporating the ABT strategy (see Figure 2). This is not surprising; AWC and ABT are found to be strategically similar, differing only in the details of some reasoning steps. The distinguishing point of AWC is that when the agent cannot find a suitable variable assignment, it will change its priority to the highest among its group members ({i} \u222a Fi). For agent i, beginning initially with (wl = 1, (0 \u2264 l < m); pi = i, (0 \u2264 i < n)) and Fi contains all the agents who share the constraints with agent i, its BDI-driven AWC strategy is described as follows. Step 1 - Percept: This step is identical to the Percept step of ABT. Step 2 - Belief: The belief function GB (Pi,Ci) will return a value bi \u2208 {0, 1, 2}, decided as follows: Percept Belief Desire Intention Mediation Execution Info Message Info Message Negotiation Message Negotiation Message Negotiation Message Figure 3: BDI protocol with Asynchronous WeakCommitment strategy \u2022 bi = 0 when the agent can find an optimal option i.e., if (Si(vi) = 0 or the assignment xi = vi and the current variables assignments of the neighbors in Fi who have higher priority form a nogood ) stored in a list called nogood list and \u2203a \u2208 Di, Si(a) = 0 (initially the list is empty). \u2022 bi = 1 when the agent cannot find any optimal option i.e., if \u2200a \u2208 Di, Si(a) = 0. \u2022 bi = 2 when the current assignment is an optimal option i.e., if Si(vi) = 0 and the current state is not a nogood in nogood list. Step 3 - Desire: The desire function GD (bi) will return a desire set DS, decided as follows: \u2022 If bi = 0, then DS = {a | (a = vi), (Si(a) = 0) and the number of constraint violations with lower priority agents is minimized }. \u2022 If bi = 1, then DS = {a | a \u2208 Di and the number of violations of all relevant constraints is minimized }. Following, if bi = 1, agent i will find a list Ki of higher priority neighbors, defined by Ki = {k | agent k \u2208 Fi and pk > pi}. Step 5 - Execution: \u2022 If agent i has a domain value as its intention, the agent will update its variable assignment with this value. \u2022 If bi = 1, it will send the negotiation message to its neighbors in Ki, and set pi = max{pj} + 1, with agent j \u2208 Fi. Mediation: This step is identical to the Mediation step of ABT, except that agent i will now add the nogood contained in the negotiation message received to its own nogood list. Essentially, by this synchronous strategy, each agent will search iteratively for improvement by reducing the total weight of the violated constraints. For agent i, beginning initially with (wl = 1, (0 \u2264 l < m), pi = i, (0 \u2264 i < n)) and Fi contains all the agents who share the constraints with agent i, its BDI-driven DBO strategy is described as follows. Update Ci to be the list of its relevant constraints. Step 2 - Belief: The belief function GB (Pi,Ci) will return a value bi \u2208 {0, 1, 2}, decided as follows: \u2022 bi = 0 when agent i can find an option to reduce the number violations of the constraints in Ci, i.e., if \u2203a \u2208 Di, Si(a) < Si(vi). \u2022 bi = 1 when it cannot find any option to improve situation, i.e., if \u2200a \u2208 Di, a = vi, Si(a) \u2265 Si(vi). \u2022 bi = 2 when its current assignment is an optimal option, i.e., if Si(vi) = 0. (max{(Si(vi)\u2212Si(a))} will be referenced by hmax i in subsequent steps, and it defines the maximal reduction in constraint violations). \u2022 Otherwise, DS = \u2205. \u2022 If DS = \u2205, then assign nil as the intention. Following, agent i will send its intention to all its neighbors. In return, it will receive intentions from these agents before proceeding to Mediation step. If it finds that the intention received from a neighbor agent j is associated with hmax j > hmax i , the agent will automatically cancel its current intention. Step 5 - Execution: \u2022 If agent i did not cancel its intention, it will update its variable assignment with the intended value. Percept Belief Desire Intention Mediation Execution Info Message Info Message Negotiation Message Negotiation Message Negotiation Message Negotiation Message Figure 5: BDI protocol with Unsolicited Mutual Advice strategy \u2022 If all intentions received and its own one are nil intention, the agent will increase the weight of each currently violated constraint by 1. Figure 5 presents the BDI negotiation model incorporating the Unsolicited Mutual Advice(UMA) strategy. For agent i, beginning initially with (wl = 1, (0 \u2264 l < m), pi = i, (0 \u2264 i < n)) and Fi contains all the agents who share the constraints with agent i, its BDI-driven UMA strategy is described as follows. Update Ci to be the list of constraints relevant to agent i. Step 2 - Belief: The belief function GB (Pi,Ci) will return a value bi \u2208 {0, 1, 2}, decided as follows: \u2022 bi = 0 when agent i can find an option to reduce the number violations of the constraints in Ci, i.e., if \u2203a \u2208 Di, Si(a) < Si(vi) and the assignment xi = a and the current variable assignments of its neighbors do not form a local state stored in a list called bad states list (initially this list is empty). \u2022 bi = 1 when it cannot find a value a such as a \u2208 Di, Si(a) < Si(vi), and the assignment xi = a and the current variable assignments of its neighbors do not form a local state stored in the bad states list. In this case, DS is called a set of reluctant desires \u2022 If bi = 2, then DS = \u2205. Agent i receives advices from all its neighbors and stores them in a list called A, before proceeding to the next step. Step 4 - Intention: The intention function GI (DS, A) will return an intention, decided as follows: \u2022 If there is a voluntary advice from an agent j which is associated with hmax j > hmax i , assign nil as the intention. This intention is called a voluntary intention. This intention is called reluctant intention. Mediation: If agent i does not send out its intention before this step, i.e., the agent has either a nil intention or a voluntary intention with biggest improvement, it will proceed to next step. The criteria to select the best intention are listed, applied in descending order of importance as follows. \u2022 A voluntary intention is preferred over a reluctant intention. \u2022 A voluntary intention (if any) with biggest improvement is selected. \u2022 If there is no voluntary intention, the reluctant intention with the lowest number of constraint violations is selected. \u2022 The intention from an agent who has received a higher number of change advices in the current negotiation round is selected. \u2022 Intention from an agent with highest priority is selected. If the selected intention is not agent i\"s intention, it will cancel its intention. Step 5 - Execution: If agent i does not cancel its intention, it will update its variable assignment with the intended value. Hence an observer is needed that will keep track of the negotiation messages communicated in the environment. Following a certain period of time when there is no more message communication (and this happens when all the agents have no more intention to update their variable assignments), the observer will inform the agents in the environment that a solution has been found. The following records the outcome of each step in every negotiation round executed. Round 1: Step 1 - Percept: Each agent obtains the current color assignments of those nodes (agents) adjacent to it, i.e., its neighbors\". Agents 3, 6 and 9 need not change as all their relevant constraints are satisfied. Agents 3, 6 and 9 do not have any desire to update their color assignments. Following, the intention from the agents will be sent to all their neighbors. Agents 7 and 10 keep their intentions since none of their neighbors has an intention. The rest of the agents do nothing in this step as they do not have any intention. Agents 7 and 10 change their colors to Black. The new state after round 1 is shown in Figure 7. Round 2: Step 1 - Percept: The agents obtain the current color assignments of their neighbors. They will have no desire, and hence no intention. Step 4 - Intention: Agent 3 will have a voluntary intention while agent 2 will not have any intention as it receives the voluntary advice from agent 3. Mediation: Agent 3 will keep its intention as its only neighbor, agent 2, does not have any intention. Step 5 - Execution: Agent 3 changes its color to Black. The new state after round 2 is shown in Figure 8. Round 3: In this round, every agent finds that it has no desire and hence no intention to revise its variable assignment. Following, with no more negotiation message communication in the environment, the observer will inform all the agents that a solution has been found. The definition of a computational cycle is as follows. \u2022 In one cycle, each agent receives all the incoming messages, performs local computation and sends out a reply. The network delay is neglected. Agents attach their clock value as a time-stamp in the outgoing message and use the time-stamp in the incoming message to update their own clock\"s value. 10 test cases were generated for each problem size n \u2208 {10, 50 and 100}. Figure 9 shows the execution time for different problem sizes when ABT, AWC and UMA were run. This poor performance of ABT was expected since the graph coloring problem is more difficult than the n-queens problem, on which ABT already did not perform well (see Figure 9). The simulation results for each type of problem are shown in Figures 10 - 12. However, as seen in Figure 12, both the strategies are very efficient when solving the dense problem, with AWC showing slightly better performance. The worst case occurs when all the possible global states of the search are reached. As we observe from the backtracking in ABT and AWC, the difference in the ordering of incoming messages can result in a different number of computational cycles to be executed by the agents. 5.3.2 Comparison with DBO The computational performance of UMA is arguably better than DBO for the following reasons: \u2022 UMA can guarantee that there will be a variable reassignment following every negotiation round whereas DBO cannot. Although this increases the communication cost per negotiation round, we observed from our simulations that the overall communication cost incurred by UMA is lower due to the significantly lower number of negotiation rounds. Using DBO, this number of round trips is uncertain as each agent might have to increase the weights of the violated constraints until an agent has a positive improvement; this could result in a infinite loop .", "introduction": "At the core of many emerging distributed applications is the distributed constraint satisfaction problem (DCSP) - one which involves finding a consistent combination of actions (abstracted as domain values) to satisfy the constraints among multiple agents in a shared environment. Important application examples include distributed resource allocation and distributed scheduling . Many important algorithms, such as distributed breakout (DBO) , asynchronous backtracking (ABT) , asynchronous partial overlay (APO) and asynchronous weak-commitment (AWC) , have been developed to address the DCSP and provide the agent solution basis for its applications. Broadly speaking, these algorithms are based on two different approaches, either extending from classical backtracking algorithms or introducing mediation among the agents. While there has been no lack of efforts in this promising research field, especially in dealing with outstanding issues such as resource restrictions (e.g., limits on time and communication) and privacy requirements , there is unfortunately no conceptually clear treatment to prise open the model-theoretic workings of the various agent algorithms that have been developed. As a result, for instance, a deeper intellectual understanding on why one algorithm is better than the other, beyond computational issues, is not possible. In this paper, we present a novel, unified distributed constraint satisfaction framework based on automated negotiation . Negotiation is viewed as a process of several agents searching for a solution called an agreement. The search can be realized via a negotiation mechanism (or algorithm) by which the agents follow a high level protocol prescribing the rules of interactions, using a set of strategies devised to select their own preferences at each negotiation step. Anchoring the DCSP search on automated negotiation, we show in this paper that several well-known DCSP algorithms are actually mechanisms that share the same Belief-DesireIntention (BDI) interaction protocol to reach agreements, but use different action or value selection strategies. The proposed framework provides not only a clearer understanding of existing DCSP algorithms from a unified BDI agent perspective, but also opens up the opportunities to extend and develop new strategies for DCSP. To this end, a new strategy called Unsolicited Mutual Advice (UMA) is proposed. Our performance evaluation shows that UMA can outperform ABT and AWC in terms of the average number of computational cycles for both the sparse and critical coloring problems . The rest of this paper is organized as follows. In Section 2, we provide a formal overview of DCSP. Section 3 presents a BDI negotiation model by which a DCSP agent reasons. Section 4 presents the existing algorithms ABT, AWC and DBO as different strategies formalized on a common protocol. A new strategy called Unsolicited Mutual Advice is proposed in Section 5; our empirical results and discussion attempt to highlight the merits of the new strategy over existing ones. Section 6 concludes the paper and points to some future work.", "conclusion": "Applying automated negotiation to DCSP, this paper has proposed a protocol that prescribes the generic reasoning of a DCSP agent in a BDI architecture.. Our work shows that several wellknown DCSP algorithms, namely ABT, AWC and DBO, can be described as mechanisms sharing the same proposed protocol, and only differ in the strategies employed for the reasoning steps per negotiation round as governed by the protocol.. Importantly, this means that it might furnish a unified framework for DCSP that not only provides a clearer BDI agent-theoretic view of existing DCSP approaches, but also opens up the opportunities to enhance or develop new strategies.. Towards the latter, we have proposed and formulated a new strategy - the UMA strategy.. Empirical results and our discussion suggest that UMA is superior to ABT, AWC and DBO in some specific aspects.. It was observed from our simulations that UMA possesses the completeness property.. Future work will attempt to formally establish this property, as well as formalize other existing DSCP algorithms as BDI negotiation mechanisms, including the recent endeavor that employs a group mediator .. The idea of DCSP agents using different strategies in the same environment will also be investigated."}
{"id": "C-6", "keywords": ["distribut content manag", "continu media storag"], "title": "Design and Implementation of a Distributed Content Management System", "abstract": "The convergence of advances in storage, encoding, and networking technologies has brought us to an environment where huge amounts of continuous media content is routinely stored and exchanged between network enabled devices. Keeping track of (or managing) such content remains challenging due to the sheer volume of data. Storing \"live\" continuous media (such as TV or radio content) adds to the complexity in that this content has no well defined start or end and is therefore cumbersome to deal with. Networked storage allows content that is logically viewed as part of the same collection to in fact be distributed across a network, making the task of content management all but impossible to deal with without a content management system. In this paper we present the design and implementation of the Spectrum content management system, which deals with rich media content effectively in this environment.Spectrum has a modular architecture that allows its application to both stand- alone and various networked scenarios. A unique aspect of Spectrum is that it requires one (or more) retention policies to apply to every piece of content that is stored in the system. This means that there are no eviction policies. Content that no longer has a retention policy applied to it is simply removed from the system. Different retention policies can easily be applied to the same content thus naturally facilitating sharing without duplication. This approach also allows Spectrum to easily apply time based policies which are basic building blocks required to deal with the storage of live continuous media, to content. We not only describe the details of the Spectrum architecture but also give typical use cases.", "references": ["Multicache-based Content Management for Web Caching", "PRISM Architecture: Supporting Enhanced Streaming Services in a Content Distribution Network", "NED: a Network-Enabled Digital Video Recorder", "A Demand Adaptive and Locality Aware (DALA) Streaming Media Server Cluster Architecture", "A multi-agent TV recommender"], "full_text": "1. INTRODUCTION Manipulating and managing content is and has always been one of the primary functions of a computer. Initial computing applications include text formatters and program compilers. Content was initially managed by explicit user interaction through the use of files and filesystems. As technology has advanced, both the types of content and the way people wish to use it have greatly changed. New content types such as continuous multimedia streams have become commonplace due to the convergence of advances in storage, encoding, and networking technologies. For example, by combining improvements in storage and encoding, it is now possible to store many hours of TV-quality encoded video on a single disk drive. This has led to the introduction of stand alone digital video recording or personal video recording (PVR) systems such as TiVO and ReplayTV . Another example is the combination of encoding and broadband networking technology. This combination has allowed users to access and share multimedia content in both local and remote area networks with the network itself acting as a huge data repository. The proliferation of high quality content enabled by these advances in storage, encoding, and networking technology creates the need for new ways to manipulate and manage the data. The focus of our work is on the storage of media rich content and in particular the storage of continuous media content in either pre-packaged or live forms. The need for content management in this area is apparent when one consider the following: \u2022 Increases in the capacity and decreases in the cost of storage means that even modest desktop systems today have the ability to store massive amounts of content. Managing such content manually (or more correctly manual non-management of such content) lead to great inefficiencies where unwanted and forgotten content waste storage and where wanted content cannot be found. \u2022 While true for all types of content the storage of continuous media content is especially problematic. First continuous media content is still very demanding in terms of storage resources which means that a policy-less approach to storing it will not work for all but the smallest systems. Second, the storing of live content such as TV or radio is inherently problematic as these signals are continuous streams with no endpoints. This means that before one can even think about managing such content there is a need to abstract it into something that could be manipulated and managed. \u2022 When dealing with stored continuous media there is a need to manage such content at both a fine-grained as well as an aggregate level. For example, an individual PVR user wanting to keep only the highlights of a particular sporting event should not be required to have to store the content pertaining to the complete event. At the same time the user might want to think of content in the aggregate, e.g. remove all of the content that I have not watched for the last month except that content which was explicitly marked for archival. \u2022 As indicated above, trying to keep track of content on a standalone system without a content management system is very difficult. However, when the actual storage devices are distributed across a network the task of keeping track of content is almost impossible. This scenario is increasingly common in network based content distribution systems and is likely to also become important in home-networking scenarios. It would seem clear then that a content management system that can efficiently handle media rich content while also exploiting the networked capability of storage devices is needed. This system should allow efficient storage of and access to content across heterogeneous network storage devices according to user preferences. The content management system should translate user preferences into appropriate low-level storage policies and should allow those preferences to be expressed at a fine level of granularity (while not requiring it in general). The content management system should allow the user to manipulate and reason about (i.e. change the storage policy associated with) the storage of (parts of) continuous media content. Addressing this distributed content management problem is difficult due to the number of requirements placed on the system. For example: \u2022 The content management system must operate on a large number of heterogeneous systems. In some cases the system may be managing content stored on a local filesystem, while in others the content may be stored on a separate network storage appliance. The content manager may be responsible for implementing the policies it uses to reference content or that role may be delegated to a separate computer. A application program interface (API) and associated network protocols are needed in order for the content management system to provide a uniform interface. \u2022 The content management system should be flexible and be able to handle differing requirements for content management policies. These policies reflect what content should be obtained, when it should be fetched, how long it should be retained, and under what circumstances it should be discarded. This means that the content management system should allow multiple applications to reference content with a rich set of policies and that it should all work together seamlessly. \u2022 The content management system needs to be able to monitor references for content and use that information to place content in the right location in the network for efficient application access. \u2022 The content management system must handle the interaction between implicit and explicit population of content at the network edge. \u2022 The content system must be able to efficiently manage large sets of content, including continuous streams. It needs to be able to package this content in such a way that it is convenient for users to access. To address these issues we have designed and implemented the Spectrum content management system architecture. Our layered architecture is flexible - its API allows the layers to reside either on a single computer or on multiple networked heterogeneous computers. It allows multiple applications to reference content using differing policies. Note that the Spectrum architecture assumes the existence of a content distribution network (CDN) that can facilitate the efficient distribution of content (for example, the PRISM CDN architecture ). The rest of this paper is organized as follows. Section 2 describes the architecture of our content management system. In Section 3 we describe both our implementation of the Spectrum architecture and examples of its use. Related work is described in Section 4, and Section 5 contains our conclusion and suggestions for future work. 2. THE SPECTRUM DISTRIBUTED CONTENT MANAGEMENT SYSTEM ARCHITECTURE The Spectrum architecture consists of three distinct management layers that may or may not be distributed across multiple machines, as shown in Figure 1. The three layers are: content manager: contains application specific information that is used to manage all of an application\"s content according to user preferences. For example, in a personal video recorder (PVR) application the content manager receives requests for content from a user interface and interacts with the lower layers of the Spectrum architecture to store and manage content on the device. policy manager: implements and enforces various storage polices that the content manager uses to refer to content. The policy manager exports an interface to the content manager that allows the content manager to request that a piece content be treated according to a specific policy. Spectrum allows for arbitrary policies to be realized by providing a fixed set of base-policy templates that can easily be parameterized. It is our belief that for most implementations this will be adequate (if not, Spectrum can easily be extended to dynamically load new base-policy template code at run time). A key aspect of the policy manager is that it allows different policies to be simultaneously applied to the same content (or parts of the same content). Furthermore content can only exist in the system so long as it is referenced by at least one existing policy. Policy conflicts are eliminated by having the policy manager deal exclusively with retention policies rather than with a mix of retention and eviction policies. This means that content with no policy associated with it is immediately and automatically removed from the system. This approach allows us to naturally support sharing of content across different policies which is critical to the efficient storage of large objects. Note that a key difference between the content manager and the policy manager is that the content manager manages references to multiple pieces of content, i.e. it has an applicationview of content. On the other hand, the policy manager is only concerned with the policy used to manage standalone pieces of content. For example, in a PVR application, the content manager layer would know about the different groups of managed content such as keep-indefinitely, keep for one day, and keep if available diskspace. However, at the policy manager level, each piece of content has Content Manager Policy Manager Storage Manager Content Manager Content Manager Content Manager Policy Manager Policy Manager Policy Manager Storage Manager Storage Manager Storage Manager Remote Invocation Figure 1: The components of the Spectrum architecture and the four ways they can be configured its own policy (or policies) applied to it and is independent from other content. storage manager: stores content in an efficient manner while facilitating the objectives of the higher layers. Specifically the storage manager stores content in sub-object chunks. This approach has advantages for the efficient retrieval of content but more importantly allows policies to be applied at a subobject level which is critically important when dealing with very large objects such as parts of continuous media, e.g. selected pieces of TV content being stored on a PVR. Note that the storage manager has no knowledge of the policies being used by the content and policy managers. Another unique part of our approach is that the interfaces between the layers can either be local or distributed. Figure 1 shows the four possible cases. The case on the far left of the Figure shows the simplest (non-distributed) case where all the layers are implemented on a single box. This configuration would be used in selfcontained applications such as PVRs. The next case over corresponds to the case where there is a centralized content manager that controls distributed storage devices each of which is responsible for implementing policy based storage. In this case although the remote devices are controlled by the central manager they operate much more independently. For example, once they receive instructions from the central manager they typically operate in autonomous fashion. An example of this type of configuration is a content distribution network (CDN) that distributes and stores content based on a schedule determined by some centralized controller. For example, the CDN could pre-populate edge devices with content that is expected to be very popular or distribute large files to branch offices during off-peak hours in a bandwidth constrained enterprise environment. Allowing a single policy manager to control several storage managers leads to the next combination of functions and the most distributed case. The need for this sort of separation might occur for scalability reasons or when different specialized storage devices or appliances are required to be controlled by a single policy manager. The final case shows a content manager combined with a policy manager controlling a remote storage manager. This separation would be possible if the storage manager is somewhat autonomous and does not require continuous fine grained control by the policy manager. We now examine the function of the three layers in detail. 2.1 Content Manager The content manager layer is the primary interface through which specific applications use the Spectrum architecture. As such the content manager layer provides an API for the application to manipulate all aspects of the Spectrum architecture at different levels of granularity. The content manager API has functions that handle: Physical devices: This set of functions allows physical storage devices to be added to Spectrum thereby putting them under control of the content manager and making the storage available to the system. Physical devices can be local or remote - this is the only place in the architecture where the application is required to be aware of this distinction. Once a device is mapped into the application through this interface, the system tracks its type and location. Users simply refer to the content through an application-provided label. Stores: Stores are subsets of physical storage devices. Through these functions an application can create a store on a physical device and assign resources (e.g. disk space) to it. Stores can only be created in physical devices that are mapped into the system. Policy Groups: Policy groups are the means whereby an application specifies, instantiates, and modifies the policies that are applied to Spectrum content. Typical usage of this set of functions is to select one of a small set of base policies and to parameterize this specific instance of the policy. Policy groups are created within existing stores in the system. The Spectrum architecture has policies that are normally associated with storage that aim to optimize disk usage. In addition a set of policies that take a sophisticated time specification enable storage that is cognizant of time. For example, a simple time-based policy could evict content from the system at a certain absolute or relative time. A slightly more involved time-based policy enabled by the Spectrum architecture could allow content to be stored in rolling window of a number of hours (for example, the most recent N-number of hours is kept in the system). Time-based polices are of particular use when dealing with continuous content like a live broadcast. Content: At the finest level of granularity content can be added to or removed from the system. Content is specified to the system by means of a uniform resource locator (URL) which concisely indicates the location of the content as well as the protocol to be used to retrieve it. Optionally a time specification can be associated with content. This allows content to be fetched into the system at some future time, or at future time intervals. Again, this is particularly useful for dealing with the storage and management of live content. 2.2 Policy Manager The policy manager layer of the Spectrum architecture has two main types of API functions. First, there are functions that operate on managed storage areas and policy-based references (prefs) to content stored there. Second, there are sets of functions used to implement each management policy. The first class of functions is used by the content manager layer to access storage. Operations include: create, open, and close: These operations are used by the content manager to control its access to storage. The policy manager\"s create operation is used to establish contact with a store for the first time. Once this is done, the store can be open and closed using the appropriate routines. Note that the parameters used to create a store contain information on how to reach it. For example, local stores have a path associated with them, while remote stores have a remote host and remote path associated with them. The information only needs to be passed to the policy manager once at create time. For open operations, the policy manager will use cached information to contact the store. lookup: The lookup operation provides a way for the content manager to query the policy manager about what content is currently present for a given URL. For continuous media time ranges of present media will be returned. resource: The resource routines are used to query the policy manager about its current resource usage. There are two resource routines: one that applies to the store as a whole and another that applies to a particular policy reference. The resource API is extensible, we currently support queries on disk usage and I/O load. pref establish/update: The pref establish operation is used by the content manager to reference content on the store. If the content is not present, this call will result in the content being fetched (or being scheduled to be fetched if the content is not currently available). Parameters of this function include the URL to store it under, the URL to fetch data from if it is not present, the policy to store the content under, and the arguments used to parameterize the policy. The result of a successful pref establish operation is a policy reference ID string. This ID can be used with the update operation to either change the storage policy parameters or delete the reference entirely. The second group of policy manager functions are used to implement all the polices supported by Spectrum. We envision a small set of base-level policy functions that can be parameterized to produce a wide range of storage polices. For example, a policy that implements recording a repeating time window can be parameterized to function daily, weekly, or monthly. Note that the policy manager is only concerned with executing a specific policy. The higher-level reasons for choosing a given policy are handled by the content and application manager. A base policy is implemented using six functions: establish: called when a pref is established with the required URLs and base policy\"s parameters. The establish routine references any content already present in the store and then determines the next time it needs to take action (e.g. start a download) and schedules a callback for that time. It can also register to receive callbacks if new content is received for a given URL. update: called to change the parameters of a pref, or to discard the policy reference. newclip: called when a chunk of new content is received for a URL of interest. The base policy typically arranges for newclip to be called for a given URL when the pref is established. When newclip is called, the base policy checks its parameters to determine if it wishes to add a reference to the clip just received. callback: called when the pref schedules a timer-based callback. This is a useful wakeup mechanism for prefs that need to be idle for a long period of time (e.g. between programs). boot/shutdown: called when the content management system is booting or shutting down. The boot operation is typically used to schedule initial callbacks or start I/O operations. The shutdown operation is used to gracefully shutdown I/O streams and save state. 2.3 Storage Manager The role of Spectrum\"s storage manager is to control all I/O operations associated with a given store. Spectrum\"s storage manager supports storing content both on a local filesystem and on a remote fileserver (e.g. a storage appliance). For continuous media, at the storage manager level content is stored as a collection of time-based chunks. Depending on the underlying filesystem, a chunk could correspond to a single file or a data node in a storage database. The two main storage manager operations are input and output. The input routine is used to store content in a store under a given name. The output routine is used to send data from the store to a client. For streaming media both the input and output routines take time ranges that schedule when the I/O operation should happen, and both routines return an I/O handle that can be used to modify or cancel the I/O request in the future. Much like the policy manager, the storage manager also provides API functions to create, open, and close stores. It also supports operations to query the resource usages and options supported by the store. Finally, the storage manager also has a discard routine that may be used by the policy manager to inform the store to remove content from the store. 3. IMPLEMENTATION AND USE CASES In this section we describe our implementation of Spectrum and describe how it can be used. 3.1 Implementation We have implemented Spectrum\"s three layers in C as part of a library that can be linked with Spectrum-based applications. Each layer keeps track of its state through a set of local data files that persist across reboots, thus allowing Spectrum to smoothly handle power cycles. For layers that reside on remote systems (e.g. a remote store) only the meta-information needed to contact the remote Content Manager Policy Manager Storage Manager Storage Fetcher Program Listings Graphical User Interface Network Enabled DVR Program Information Content DVR Application Figure 2: Spectrum in a Network Enabled DVR node is stored locally. Our test application uses a local policy and storage manager to fetch content and store it in a normal Unixbased filesystem. To efficiently handle communications with layers running on remote systems, all Spectrum\"s API calls support both synchronous and asynchronous modes through a uniform interface defined by the reqinfo structure. Each API call takes a pointer to a reqinfo structure as one of its arguments. This structure is used to hold the call state and return status. For async calls, the reqinfo also contains a pointer to a callback function. To use a Spectrum API function, the caller first chooses either the sync or async mode and allocates a reqinfo structure. For sync calls, the reqinfo can be allocated on the stack, otherwise it is allocated with malloc. For async calls, a callback function must be provided when the reqinfo is allocated. Next the caller invokes the desired Spectrum API function passing the reqinfo structure as an argument. For sync calls, the result of the calls is returned immediately in the reqinfo structure. For successful async calls, a call in progress value is returned. Later, when the async call completes or a timeout occurs, the async callback function is called with the appropriate information needed to complete processing. The modular/layered design of the Spectrum architecture simplifies the objective of distribution of functionality. Furthermore, communication between functions is typically of a master-slave(s) nature. This means that several approaches to distributed operation are possible that would satisfy the architectural requirements. In our implementation we have opted to realize this functionality with a simple modular design. We provide a set of asynchronous remote access stub routines that allow users to select the transport protocol to use and to select the encoding method that should be used with the data to be transferred. Transport protocols can range simple protocols such as UDP up to more complex protocols such as HTTP. We currently are using plain TCP for most of our transport. Function calls across the different Spectrum APIs can be encoded using a variety of formats include plain text, XDR, and XML. We are currently using the eXpat XML library to encode our calls. While we are current transferring our XML encoded messages using a simple TCP connection, in a real world setting this can easily be replaced with an implementation based on secure sockets layer (SSL) to improve security by adding SSL as a transport protocol. An important aspect of Spectrum is that it can manage content based on a given policy across heterogenous platforms. As we explained previously in Section 2.2, envision a small set of base-level policy functions that can be parameterized to produce a wide range of storage polices. In order for this to work properly, all Spectrumbased applications must understand the base-level policies and how they can be parameterized. To address this issue, we treat each base-level policy as if it was a separate program. Each base-level policy should have a well known name and command line options for parameterization. In fact, in our implementation we pass parameters to base-level policies as a string that can be parsed using a getopt-like function. This format is easily understood and provides portability since byte order is not an issue in a string. Since this part of Spectrum is not on the critical data path, this type of formatting is not a performance issue. 3.2 Using the Spectrum Content Management System In this section we show two examples of the use of the Spectrum Content Management System in our environment. The focus of our previous work has been content distribution for streaming media content and network enabled digital video recording . The Spectrum system is applicable to both scenarios as follows. Figure 2 shows the Network Enabled DVR (NED) architecture. In this case all layers of the Spectrum architecture reside on the same physical device in a local configuration. The DVR application obtains program listings from some network source, deals with user presentation through a graphical user interface (GUI), and interface with the Spectrum system through the content management layer APIs. This combination of higher level functions allows the user to select both content to be stored and what storage policies to Content Manager Centralized Content Management station Content InformationUser Interface Policy Manager Storage Manager Storage Fetcher Edge Portal Server Policy Manager Storage Manager Storage Fetcher Edge Portal Server Distributed Content To Media Endpoints To Media Endpoints Figure 3: Spectrum in a Content Distribution Architecture apply to such content. Obtaining the content (through the network or locally) and the subsequent storage on the local system is then handled by the policy and storage managers. The use of Spectrum in a streaming content distribution architecture (e.g. PRISM ) is depicted in Figure 3. In this environment streaming media content (both live, canned-live and on-demand) is being distributed to edge portals from where streaming endpoints are being served. In our environment content distribution and storage is done from a centralized content management station which controls several of the edge portals. The centralized station allows administrators to manage the distribution and storage of content without requiring continuous communication between the content manager and the edge devices, i.e. once instructions have been given to edge devices they can operate independently until changes are to be made. 3.3 Spectrum Operational Example To illustrate how Spectrum handles references to content, consider a Spectrum-based PVR application programmed to store one days worth of streaming content in a rolling window. To set up the rolling window, the application would use the content manager API to create a policy group and policy reference to the desired content. The establishment of the one-day rolling window policy reference would cause the policy manger to ask the storage manager to start receiving the stream. As each chunk of streaming data arrives, the policy manager executes the policy reference\"s newclip function. The newclip function adds a reference to each arriving chunk, and schedules a callback a day later. At that time, the policy will drop its now day-old reference to the content and the content will be discarded unless it is referenced by some other policy. Now, consider the case where the user decides to save part of the content (e.g. a specific program) in the rolling window for an extra week. To do this, the application requests that the content manager add an additional new policy reference to the part of the content to preserved. Thus, the preserved content has two references to it: one from the rolling window and one from the request to preserve the content for an additional week. After one day the reference from the rolling window will be discarded, but the content will be ref2, etc. base data url1 url2 (media files...) (media files...) meta store (general info...) url1 chunks prefs ranges media chunks, etc.url2 poly host ref1 ref1.files ref1.state Figure 4: Data layout of Spectrum policy store preserved by the second reference. After the additional week has past, the callback function for the second reference will be called. This function will discard the remaining reference to the content and as there are no remaining references the content will be freed. In order to function in scenarios like the ones described above, Spectrum\"s policy manager must manage and maintain all the references to various chunks of media. These references are persistent and thus must be able to survive even if the machine maintaining them is rebooted. Our Spectrum policy manager implementation accomplishes this using the file and directory structure shown in Figure 4. There are three classes of data stored, and each class has its own top level directory. The directories are: data: this directory is used by the storage manager to store each active URL\"s chunks of media. The media files can be encoded in any format, for example MPEG, Windows Media, or QuickTime. Note that this directory is used only if the storage manager is local. If the policy manager is using an external storage manager (e.g. a storage appliance), then the media files are stored remotely and are only remotely referenced by the policy manager. meta: this directory contains general meta information about the storage manager being used and the data it is storing. General information is stored in the store subdirectory and includes the location of the store (local or remote) and information about the types of chunks of data the store can handle. The meta directory also contains a subdirectory per-URL that contains information about the chunks of data stored. The chunks file contains a list of chunks currently stored and their reference counts. The prefs file contains a list of active policy references that point to this URL. The ranges file contains a list of time ranges of data currently stored. Finally, the media file describes the format of the media being stored under the current URL. poly: this directory contains a set of host subdirectories. Each host subdirectory contains the set of policy references created by that host. Information on each policy reference is broken up into three files. For example, a policy reference named ref1 would be stored in ref1, ref1.files, and ref1.state. The ref1 file contains information about the policy reference that does not change frequently. This information includes the base-policy and the parameters used to create the reference. The ref1.files file contains the list of references to chunks that pref ref1 owns. Finally, the ref1.state file contains optional policy-specific state information that can change over time. Together, these files and directories are used to track references in our implementation of Spectrum. Note that other implementations are possible. For example, a carrier-grade Spectrum manager might store all its policy and reference information in a high-performance database system. 10 4. RELATED WORK Several authors have addressed the problem of the management of content in distributed networks. Much of the work focuses on the policy management aspect. For example in , the problem of serving multimedia content via distributed servers is considered. Content is distributed among server resources in proportion to user demand using a Demand Dissemination Protocol. The performance of the scheme is benchmarked via simulation. In content is distributed among sub-caches. The authors construct a system employing various components, such as a Central Router, Cache Knowledge base, Subcaches, and a Subcache eviction judge. The Cache Knowledge base allows sophisticated policies to be employed. Simulation is used to compare the proposed scheme with well-known replacement algorithms. Our work differs in that we are considering more than the policy management aspects of the problem. After carefully considering the required functionality to implement content management in the networked environment, we have partitioned the system into three simple functions, namely Content manager, Policy manager and Storage manager. This has allowed us to easily implement and experiment with a prototype system. Other related work involves so called TV recommendation systems which are used in PVRs to automatically select content for users, e.g. . In the case where Spectrum is used in a PVR configuration this type of system would perform a higher level function and could clearly benefit from the functionalities of the Spectrum architecture. Finally, in the commercial CDN environment vendors (e.g. Cisco and Netapp) have developed and implemented content management products and tools. Unlike the Spectrum architecture which allows edge devices to operate in a largely autonomous fashion, the vendor solutions typically are more tightly coupled to a centralized controller and do not have the sophisticated time-based operations offered by Spectrum. 5. CONCLUSION AND FUTURE WORK In this paper we presented the design and implementation of the Spectrum content management architecture. Spectrum allows storage policies to be applied to large volumes of content to facilitate efficient storage. Specifically, the system allows different policies to be applied to the same content without replication. Spectrum can also apply policies that are time-aware which effectively deals with the storage of continuous media content. Finally, the modular design of the Spectrum architecture allows both stand-alone and distributed realizations so that the system can be deployed in a variety of applications. There are a number of open issues that will require future work. Some of these issues include: \u2022 We envision Spectrum being able to manage content on systems ranging from large CDNs down to smaller appliances such as TiVO . In order for these smaller systems to support Spectrum they will require networking and an external API. When that API becomes available, we will have to work out how it can be fit into the Spectrum architecture. \u2022 Spectrum names content by URL, but we have intentionally not defined the format of Spectrum URLs, how they map back to the content\"s actual name, or how the names and URLs should be presented to the user. While we previously touched on these issues elsewhere , we believe there is more work to be done and that consensus-based standards on naming need to be written. \u2022 In this paper we\"ve focused on content management for continuous media objects. We also believe the Spectrum architecture can be applied to any type of document including plain files, but we have yet to work out the details necessary to support this in our prototype environment. \u2022 Any project that helps allow multimedia content to be easily shared over the Internet will have legal hurdles to overcome before it can achieve widespread acceptance. Adapting Spectrum to meet legal requirements will likely require more technical work.", "body1": "Manipulating and managing content is and has always been one of the primary functions of a computer. New content types such as continuous multimedia streams have become commonplace due to the convergence of advances in storage, encoding, and networking technologies. The proliferation of high quality content enabled by these advances in storage, encoding, and networking technology creates the need for new ways to manipulate and manage the data. Second, the storing of live content such as TV or radio is inherently problematic as these signals are continuous streams with no endpoints. \u2022 When dealing with stored continuous media there is a need to manage such content at both a fine-grained as well as an aggregate level. \u2022 As indicated above, trying to keep track of content on a standalone system without a content management system is very difficult. The content management system should translate user preferences into appropriate low-level storage policies and should allow those preferences to be expressed at a fine level of granularity (while not requiring it in general). Addressing this distributed content management problem is difficult due to the number of requirements placed on the system. This means that the content management system should allow multiple applications to reference content with a rich set of policies and that it should all work together seamlessly. \u2022 The content management system needs to be able to monitor references for content and use that information to place content in the right location in the network for efficient application access. \u2022 The content management system must handle the interaction between implicit and explicit population of content at the network edge. \u2022 The content system must be able to efficiently manage large sets of content, including continuous streams. To address these issues we have designed and implemented the Spectrum content management system architecture. The rest of this paper is organized as follows. ARCHITECTURE The Spectrum architecture consists of three distinct management layers that may or may not be distributed across multiple machines, as shown in Figure 1. policy manager: implements and enforces various storage polices that the content manager uses to refer to content. Note that a key difference between the content manager and the policy manager is that the content manager manages references to multiple pieces of content, i.e. storage manager: stores content in an efficient manner while facilitating the objectives of the higher layers. Another unique part of our approach is that the interfaces between the layers can either be local or distributed. The next case over corresponds to the case where there is a centralized content manager that controls distributed storage devices each of which is responsible for implementing policy based storage. The final case shows a content manager combined with a policy manager controlling a remote storage manager. We now examine the function of the three layers in detail. 2.1 Content Manager The content manager layer is the primary interface through which specific applications use the Spectrum architecture. Stores: Stores are subsets of physical storage devices. Policy Groups: Policy groups are the means whereby an application specifies, instantiates, and modifies the policies that are applied to Spectrum content. Content: At the finest level of granularity content can be added to or removed from the system. 2.2 Policy Manager The policy manager layer of the Spectrum architecture has two main types of API functions. resource: The resource routines are used to query the policy manager about its current resource usage. pref establish/update: The pref establish operation is used by the content manager to reference content on the store. The second group of policy manager functions are used to implement all the polices supported by Spectrum. A base policy is implemented using six functions: establish: called when a pref is established with the required URLs and base policy\"s parameters. newclip: called when a chunk of new content is received for a URL of interest. When newclip is called, the base policy checks its parameters to determine if it wishes to add a reference to the clip just received. callback: called when the pref schedules a timer-based callback. This is a useful wakeup mechanism for prefs that need to be idle for a long period of time (e.g. boot/shutdown: called when the content management system is booting or shutting down. 2.3 Storage Manager The role of Spectrum\"s storage manager is to control all I/O operations associated with a given store. The input routine is used to store content in a store under a given name. Much like the policy manager, the storage manager also provides API functions to create, open, and close stores. In this section we describe our implementation of Spectrum and describe how it can be used. 3.1 Implementation We have implemented Spectrum\"s three layers in C as part of a library that can be linked with Spectrum-based applications. To efficiently handle communications with layers running on remote systems, all Spectrum\"s API calls support both synchronous and asynchronous modes through a uniform interface defined by the reqinfo structure. The modular/layered design of the Spectrum architecture simplifies the objective of distribution of functionality. We are currently using the eXpat XML library to encode our calls. An important aspect of Spectrum is that it can manage content based on a given policy across heterogenous platforms. 3.2 Using the Spectrum Content Management System In this section we show two examples of the use of the Spectrum Content Management System in our environment. In this case all layers of the Spectrum architecture reside on the same physical device in a local configuration. The use of Spectrum in a streaming content distribution architecture (e.g. The establishment of the one-day rolling window policy reference would cause the policy manger to ask the storage manager to start receiving the stream. The newclip function adds a reference to each arriving chunk, and schedules a callback a day later. Now, consider the case where the user decides to save part of the content (e.g. This function will discard the remaining reference to the content and as there are no remaining references the content will be freed. In order to function in scenarios like the ones described above, Spectrum\"s policy manager must manage and maintain all the references to various chunks of media. General information is stored in the store subdirectory and includes the location of the store (local or remote) and information about the types of chunks of data the store can handle. The meta directory also contains a subdirectory per-URL that contains information about the chunks of data stored. The chunks file contains a list of chunks currently stored and their reference counts. poly: this directory contains a set of host subdirectories. Together, these files and directories are used to track references in our implementation of Spectrum. Several authors have addressed the problem of the management of content in distributed networks. The Cache Knowledge base allows sophisticated policies to be employed. Other related work involves so called TV recommendation systems which are used in PVRs to automatically select content for users, e.g. Finally, in the commercial CDN environment vendors (e.g.", "body2": "As technology has advanced, both the types of content and the way people wish to use it have greatly changed. This combination has allowed users to access and share multimedia content in both local and remote area networks with the network itself acting as a huge data repository. First continuous media content is still very demanding in terms of storage resources which means that a policy-less approach to storing it will not work for all but the smallest systems. This means that before one can even think about managing such content there is a need to abstract it into something that could be manipulated and managed. remove all of the content that I have not watched for the last month except that content which was explicitly marked for archival. This system should allow efficient storage of and access to content across heterogeneous network storage devices according to user preferences. change the storage policy associated with) the storage of (parts of) continuous media content. These policies reflect what content should be obtained, when it should be fetched, how long it should be retained, and under what circumstances it should be discarded. This means that the content management system should allow multiple applications to reference content with a rich set of policies and that it should all work together seamlessly. \u2022 The content management system needs to be able to monitor references for content and use that information to place content in the right location in the network for efficient application access. \u2022 The content management system must handle the interaction between implicit and explicit population of content at the network edge. It needs to be able to package this content in such a way that it is convenient for users to access. Note that the Spectrum architecture assumes the existence of a content distribution network (CDN) that can facilitate the efficient distribution of content (for example, the PRISM CDN architecture ). Related work is described in Section 4, and Section 5 contains our conclusion and suggestions for future work. For example, in a personal video recorder (PVR) application the content manager receives requests for content from a user interface and interacts with the lower layers of the Spectrum architecture to store and manage content on the device. This approach allows us to naturally support sharing of content across different policies which is critical to the efficient storage of large objects. However, at the policy manager level, each piece of content has Content Manager Policy Manager Storage Manager Content Manager Content Manager Content Manager Policy Manager Policy Manager Policy Manager Storage Manager Storage Manager Storage Manager Remote Invocation Figure 1: The components of the Spectrum architecture and the four ways they can be configured its own policy (or policies) applied to it and is independent from other content. Note that the storage manager has no knowledge of the policies being used by the content and policy managers. This configuration would be used in selfcontained applications such as PVRs. The need for this sort of separation might occur for scalability reasons or when different specialized storage devices or appliances are required to be controlled by a single policy manager. This separation would be possible if the storage manager is somewhat autonomous and does not require continuous fine grained control by the policy manager. We now examine the function of the three layers in detail. Users simply refer to the content through an application-provided label. Stores can only be created in physical devices that are mapped into the system. Time-based polices are of particular use when dealing with continuous content like a live broadcast. Again, this is particularly useful for dealing with the storage and management of live content. For continuous media time ranges of present media will be returned. The resource API is extensible, we currently support queries on disk usage and I/O load. This ID can be used with the update operation to either change the storage policy parameters or delete the reference entirely. The higher-level reasons for choosing a given policy are handled by the content and application manager. update: called to change the parameters of a pref, or to discard the policy reference. The base policy typically arranges for newclip to be called for a given URL when the pref is established. When newclip is called, the base policy checks its parameters to determine if it wishes to add a reference to the clip just received. callback: called when the pref schedules a timer-based callback. between programs). The shutdown operation is used to gracefully shutdown I/O streams and save state. The two main storage manager operations are input and output. For streaming media both the input and output routines take time ranges that schedule when the I/O operation should happen, and both routines return an I/O handle that can be used to modify or cancel the I/O request in the future. Finally, the storage manager also has a discard routine that may be used by the policy manager to inform the store to remove content from the store. In this section we describe our implementation of Spectrum and describe how it can be used. Our test application uses a local policy and storage manager to fetch content and store it in a normal Unixbased filesystem. Later, when the async call completes or a timeout occurs, the async callback function is called with the appropriate information needed to complete processing. Function calls across the different Spectrum APIs can be encoded using a variety of formats include plain text, XDR, and XML. While we are current transferring our XML encoded messages using a simple TCP connection, in a real world setting this can easily be replaced with an implementation based on secure sockets layer (SSL) to improve security by adding SSL as a transport protocol. Since this part of Spectrum is not on the critical data path, this type of formatting is not a performance issue. Figure 2 shows the Network Enabled DVR (NED) architecture. Obtaining the content (through the network or locally) and the subsequent storage on the local system is then handled by the policy and storage managers. To set up the rolling window, the application would use the content manager API to create a policy group and policy reference to the desired content. As each chunk of streaming data arrives, the policy manager executes the policy reference\"s newclip function. At that time, the policy will drop its now day-old reference to the content and the content will be discarded unless it is referenced by some other policy. After the additional week has past, the callback function for the second reference will be called. This function will discard the remaining reference to the content and as there are no remaining references the content will be freed. meta: this directory contains general meta information about the storage manager being used and the data it is storing. General information is stored in the store subdirectory and includes the location of the store (local or remote) and information about the types of chunks of data the store can handle. The meta directory also contains a subdirectory per-URL that contains information about the chunks of data stored. Finally, the media file describes the format of the media being stored under the current URL. Finally, the ref1.state file contains optional policy-specific state information that can change over time. For example, a carrier-grade Spectrum manager might store all its policy and reference information in a high-performance database system. The authors construct a system employing various components, such as a Central Router, Cache Knowledge base, Subcaches, and a Subcache eviction judge. This has allowed us to easily implement and experiment with a prototype system. In the case where Spectrum is used in a PVR configuration this type of system would perform a higher level function and could clearly benefit from the functionalities of the Spectrum architecture. Unlike the Spectrum architecture which allows edge devices to operate in a largely autonomous fashion, the vendor solutions typically are more tightly coupled to a centralized controller and do not have the sophisticated time-based operations offered by Spectrum.", "introduction": "Manipulating and managing content is and has always been one of the primary functions of a computer. Initial computing applications include text formatters and program compilers. Content was initially managed by explicit user interaction through the use of files and filesystems. As technology has advanced, both the types of content and the way people wish to use it have greatly changed. New content types such as continuous multimedia streams have become commonplace due to the convergence of advances in storage, encoding, and networking technologies. For example, by combining improvements in storage and encoding, it is now possible to store many hours of TV-quality encoded video on a single disk drive. This has led to the introduction of stand alone digital video recording or personal video recording (PVR) systems such as TiVO and ReplayTV . Another example is the combination of encoding and broadband networking technology. This combination has allowed users to access and share multimedia content in both local and remote area networks with the network itself acting as a huge data repository. The proliferation of high quality content enabled by these advances in storage, encoding, and networking technology creates the need for new ways to manipulate and manage the data. The focus of our work is on the storage of media rich content and in particular the storage of continuous media content in either pre-packaged or live forms. The need for content management in this area is apparent when one consider the following: \u2022 Increases in the capacity and decreases in the cost of storage means that even modest desktop systems today have the ability to store massive amounts of content. Managing such content manually (or more correctly manual non-management of such content) lead to great inefficiencies where unwanted and forgotten content waste storage and where wanted content cannot be found. \u2022 While true for all types of content the storage of continuous media content is especially problematic. First continuous media content is still very demanding in terms of storage resources which means that a policy-less approach to storing it will not work for all but the smallest systems. Second, the storing of live content such as TV or radio is inherently problematic as these signals are continuous streams with no endpoints. This means that before one can even think about managing such content there is a need to abstract it into something that could be manipulated and managed. \u2022 When dealing with stored continuous media there is a need to manage such content at both a fine-grained as well as an aggregate level. For example, an individual PVR user wanting to keep only the highlights of a particular sporting event should not be required to have to store the content pertaining to the complete event. At the same time the user might want to think of content in the aggregate, e.g. remove all of the content that I have not watched for the last month except that content which was explicitly marked for archival. \u2022 As indicated above, trying to keep track of content on a standalone system without a content management system is very difficult. However, when the actual storage devices are distributed across a network the task of keeping track of content is almost impossible. This scenario is increasingly common in network based content distribution systems and is likely to also become important in home-networking scenarios. It would seem clear then that a content management system that can efficiently handle media rich content while also exploiting the networked capability of storage devices is needed. This system should allow efficient storage of and access to content across heterogeneous network storage devices according to user preferences. The content management system should translate user preferences into appropriate low-level storage policies and should allow those preferences to be expressed at a fine level of granularity (while not requiring it in general). The content management system should allow the user to manipulate and reason about (i.e. change the storage policy associated with) the storage of (parts of) continuous media content. Addressing this distributed content management problem is difficult due to the number of requirements placed on the system. For example: \u2022 The content management system must operate on a large number of heterogeneous systems. In some cases the system may be managing content stored on a local filesystem, while in others the content may be stored on a separate network storage appliance. The content manager may be responsible for implementing the policies it uses to reference content or that role may be delegated to a separate computer. A application program interface (API) and associated network protocols are needed in order for the content management system to provide a uniform interface. \u2022 The content management system should be flexible and be able to handle differing requirements for content management policies. These policies reflect what content should be obtained, when it should be fetched, how long it should be retained, and under what circumstances it should be discarded. This means that the content management system should allow multiple applications to reference content with a rich set of policies and that it should all work together seamlessly. \u2022 The content management system needs to be able to monitor references for content and use that information to place content in the right location in the network for efficient application access. \u2022 The content management system must handle the interaction between implicit and explicit population of content at the network edge. \u2022 The content system must be able to efficiently manage large sets of content, including continuous streams. It needs to be able to package this content in such a way that it is convenient for users to access. To address these issues we have designed and implemented the Spectrum content management system architecture. Our layered architecture is flexible - its API allows the layers to reside either on a single computer or on multiple networked heterogeneous computers. It allows multiple applications to reference content using differing policies. Note that the Spectrum architecture assumes the existence of a content distribution network (CDN) that can facilitate the efficient distribution of content (for example, the PRISM CDN architecture ). The rest of this paper is organized as follows. Section 2 describes the architecture of our content management system. In Section 3 we describe both our implementation of the Spectrum architecture and examples of its use. Related work is described in Section 4, and Section 5 contains our conclusion and suggestions for future work.", "conclusion": "In this paper we presented the design and implementation of the Spectrum content management architecture.. Spectrum allows storage policies to be applied to large volumes of content to facilitate efficient storage.. Specifically, the system allows different policies to be applied to the same content without replication.. Spectrum can also apply policies that are time-aware which effectively deals with the storage of continuous media content.. Finally, the modular design of the Spectrum architecture allows both stand-alone and distributed realizations so that the system can be deployed in a variety of applications.. There are a number of open issues that will require future work.. Some of these issues include: \u2022 We envision Spectrum being able to manage content on systems ranging from large CDNs down to smaller appliances such as TiVO .. In order for these smaller systems to support Spectrum they will require networking and an external API.. When that API becomes available, we will have to work out how it can be fit into the Spectrum architecture.. \u2022 Spectrum names content by URL, but we have intentionally not defined the format of Spectrum URLs, how they map back to the content\"s actual name, or how the names and URLs should be presented to the user.. While we previously touched on these issues elsewhere , we believe there is more work to be done and that consensus-based standards on naming need to be written.. \u2022 In this paper we\"ve focused on content management for continuous media objects.. We also believe the Spectrum architecture can be applied to any type of document including plain files, but we have yet to work out the details necessary to support this in our prototype environment.. \u2022 Any project that helps allow multimedia content to be easily shared over the Internet will have legal hurdles to overcome before it can achieve widespread acceptance.. Adapting Spectrum to meet legal requirements will likely require more technical work."}
{"id": "J-56", "keywords": ["combinatori auction", "bid withdraw", "robust", "constraint program", "weight super solut"], "title": "Robust Solutions for Combinatorial Auctions", "abstract": "Bids submitted in auctions are usually treated as enforceable commitments in most bidding and auction theory literature. In reality bidders often withdraw winning bids before the transaction when it is in their best interests to do so. Given a bid withdrawal in a combinatorial auction, finding an alternative repair solution of adequate revenue without causing undue disturbance to the remaining winning bids in the original solution may be difficult or even impossible. We have called this the \"Bid-taker's Exposure Problem\". When faced with such unreliable bidders, it is preferable for the bid-taker to preempt such uncertainty by having a solution that is robust to bid withdrawal and provides a guarantee that possible withdrawals may be repaired easily with a bounded loss in revenue.In this paper, we propose an approach to addressing the Bid-taker's Exposure Problem. Firstly, we use the Weighted Super Solutions framework [13], from the field of constraint programming, to solve the problem of finding a robust solution. A weighted super solution guarantees that any subset of bids likely to be withdrawn can be repaired to form a new solution of at least a given revenue by making limited changes. Secondly, we introduce an auction model that uses a form of leveled commitment contract [26, 27], which we have called mutual bid bonds, to improve solution reparability by facilitating backtracking on winning bids by the bid-taker. We then examine the trade-off between robustness and revenue in different economically motivated auction scenarios for different constraints on the revenue of repair solutions. We also demonstrate experimentally that fewer winning bids partake in robust solutions, thereby reducing any associated overhead in dealing with extra bidders. Robust solutions can also provide a means of selectively discriminating against distrusted bidders in a measured manner.", "references": ["Leveled commitment contracts with myopic and strategic agents", "Fahiem Bacchus and George Katsirelos", "Constraint Processing", "Combinatorial auctions: A survey", "Reverse auctions: Bad idea", "Supermodels and Robustness", "Withdrawable bids as winner's curse insurance", "Robust solutions for constraint satisfaction and optimization", "Super solutions in constraint programming", "Combinatorial and quantity-discount procurement auctions benefit Mars Incorporated and its suppliers", "Super solutions for combinatorial auctions", "Weighted super solutions for constraint programs", "Business insurance", "On the sensitivity of incremental algorithms for combinatorial auctions", "Towards a universal test suite for combinatorial auction algorithms", "Associated general contractors of America white paper on reverse auctions for procurement of construction", "A basic guide to surety bonds", "Combination bidding in multi-unit auctions", "Mechanism design with execution uncertainty", "Global constraints and filtering algorithms", "Combinatorial auction design", "Computationally manageable combinatorial auctions", "Contradicting conventional wisdom in constraint satisfaction", "Algorithm for optimal winner determination in combinatorial auctions", "Leveled Commitment Contracts and Strategic Breach", "Leveled commitment contracting: A backtracking instrument for multiagent systems", "Algorithms for optimizing leveled commitment contracts", "Surplus equivalence of leveled commitment contracts", "Combinatorial auctions for supply chain formation", "On reformulation of constraint satisfaction problems", "Access spectrum bid withdrawal"], "full_text": "1. INTRODUCTION A combinatorial auction (CA) provides an efficient means of allocating multiple distinguishable items amongst bidders whose perceived valuations for combinations of items differ. Such auctions are gaining in popularity and there is a proliferation in their usage across various industries such as telecoms, B2B procurement and transportation . Revenue is the most obvious optimization criterion for such auctions, but another desirable attribute is solution robustness. In terms of combinatorial auctions, a robust solution is one that can withstand bid withdrawal (a break) by making changes easily to form a repair solution of adequate revenue. A brittle solution to a CA is one in which an unacceptable loss in revenue is unavoidable if a winning bid is withdrawn. In such situations the bid-taker may be left with a set of items deemed to be of low value by all other bidders. These bidders may associate a higher value for these items if they were combined with items already awarded to others, hence the bid-taker is left in an undesirable local optimum in which a form of backtracking is required to reallocate the items in a manner that results in sufficient revenue. We have called this the Bid-taker\"s Exposure Problem that bears similarities to the Exposure Problem faced by bidders seeking multiple items in separate single-unit auctions but holding little or no value for a subset of those items. However, reallocating items may be regarded as disruptive to a solution in many real-life scenarios. Consider a scenario where procurement for a business is conducted using a CA. It would be highly undesirable to retract contracts from a group of suppliers because of the failure of a third party. A robust solution that is tolerant of such breaks is preferable. Robustness may be regarded as a preventative measure protecting against future uncertainty by sacrificing revenue in place of solution stability and reparability. We assume a probabilistic approach whereby the bid-taker has knowledge of the reliability of bidders from which the likelihood of an incomplete transaction may be inferred. Repair solutions are required for bids that are seen as brittle (i.e. likely to break). Repairs may also be required for sets of bids deemed brittle. We propose the use of the Weighted Super 183 Solutions (WSS) framework for constraint programming, that is ideal for establishing such robust solutions. As we shall see, this framework can enforce constraints on solutions so that possible breakages are reparable. This paper is organized as follows. Section 2 presents the Winner Determination Problem (WDP) for combinatorial auctions, outlines some possible reasons for bid withdrawal and shows how simply maximizing expected revenue can lead to intolerable revenue losses for risk-averse bid-takers. This motivates the use of robust solutions and Section 3 introduces a constraint programming (CP) framework, Weighted Super Solutions , that finds such solutions. We then propose an auction model in Section 4 that enhances reparability by introducing mandatory mutual bid bonds, that may be seen as a form of leveled commitment contract . Section 5 presents an extensive empirical evaluation of the approach presented in this paper, in the context of a number of well-known combinatorial auction distributions, with very encouraging results. Section 6 discusses possible extensions and questions raised by our research that deserve future work. Finally, in Section 7 a number of concluding remarks are made. 2. COMBINATORIAL AUCTIONS Before presenting the technical details of our solution to the Bid-taker\"s Exposure Problem, we shall present a brief survey of combinatorial auctions and existing techniques for handling bid withdrawal. Combinatorial auctions involve a single bid-taker allocating multiple distinguishable items amongst a group of bidders. The bidtaker has a set of m items for sale, M = {1, 2, . . . , m}, and bidders submit a set of bids, B = {B1, B2, . . . , Bn}. A bid is a tuple Bj = Sj, pj where Sj \u2286 M is a subset of the items for sale and pj \u2265 0 is a price. The WDP for a CA is to label all bids as either winning or losing so as to maximize the revenue from winning bids without allocating any item to more than one bid. The following is the integer programming formulation for the WDP: max j=1 pjxj s.t. j|i\u2208Sj xj \u2264 1, \u2200i \u2208 {1 . . . m}, xj \u2208 {0, 1}. This problem is NP-complete and inapproximable , and is otherwise known as the Set Packing Problem. The above problem formulation assumes the notion of free disposal. This means that the optimal solution need not necessarily sell all of the items. If the auction rules stipulate that all items must be sold, the problem becomes a Set Partition Problem . The WDP has been extensively studied in recent years. The fastest search algorithms that find optimal solutions (e.g. CABOB ) can, in practice, solve very large problems involving thousands of bids very quickly. 2.1 The Problem of Bid Withdrawal We assume an auction protocol with a three stage process involving the submission of bids, winner determination, and finally a transaction phase. We are interested in bid withdrawals that occur between the announcement of winning bids and the end of the transaction phase. All bids are valid until the transaction is complete, so we anticipate an expedient transaction process1 In some instances the transaction period may be so lengthy that consideration of non-winning bids as still being valid may not be fair. Breaks that occur during a lengthy transaction phase are more difficult to remedy and may require a subsequent auction. For example, if the item is a service contract for a given period of time and the break occurs after partial fulfilment of this contract, the other An example of a winning bid withdrawal occurred in an FCC spectrum auction . Withdrawals, or breaks, may occur for various reasons. Bid withdrawal may be instigated by the bid-taker when Quality of Service agreements are broken or payment deadlines are not met. We refer to bid withdrawal by the bid-taker as item withdrawal in this paper to distinguish between the actions of a bidder and the bid-taker. Harstad and Rothkopf outlined several possibilities for breaks in single item auctions that include: 1. an erroneous initial valuation/bid; 2. unexpected events outside the winning bidder\"s control; 3. a desire to have the second-best bid honored; 4. information obtained or events that occurred after the auction but before the transaction that reduces the value of an item; 5. the revelation of competing bidders\" valuations infers reduced profitability, a problem known as the Winner\"s Curse. Kastner et al. examined how to handle perturbations given a solution whilst minimizing necessary changes to that solution. These perturbations may include bid withdrawals, change of valuation/items of a bid or the submission of a new bid. They looked at the problem of finding incremental solutions to restructure a supply chain whose formation is determined using combinatorial auctions . Following a perturbation in the optimal solution they proceed to impose involuntary item withdrawals from winning bidders. They formulated an incremental integer linear program (ILP) that sought to maximize the valuation of the repair solution whilst preserving the previous solution as much as possible. 2.2 Being Proactive against Bid Withdrawal When a bid is withdrawn there may be constraints on how the solution can be repaired. If the bid-taker was freely able to revoke the awarding of items to other bidders then the solution could be repaired easily by reassigning all the items to the optimal solution without the withdrawn bid. Alternatively, the bidder who reneged upon a bid may have all his other bids disqualified and the items could be reassigned based on the optimum solution without that bidder present. However, the bid-taker is often unable to freely reassign the items already awarded to other bidders. When items cannot be withdrawn from winning bidders, following the failure of another bidder to honor his bid, repair solutions are restricted to the set of bids whose items only include those in the bid(s) that were reneged upon. We are free to award items to any of the previously unsuccessful bids when finding a repair solution. When faced with uncertainty over the reliability of bidders a possible approach is to maximize expected revenue. This approach does not make allowances for risk-averse bid-takers who may view a small possibility of very low revenue as unacceptable. Consider the example in Table 1, and the optimal expected revenue in the situation where a single bid may be withdrawn. There are three submitted bids for items A and B, the third being a combination bid for the pair of items at a value of 190. The optimal solution has a value of 200, with the first and second bids as winners. When we consider the probabilities of failure, in the fourth column, the problem of which solution to choose becomes more difficult. Computing the expected revenue for the solution with the first and second bids winning the items, denoted 1, 1, 0 , gives: (200\u00d70.9\u00d70.9)+(2\u00d7100\u00d70.9\u00d70.1)+(190\u00d70.1\u00d70.1) = 181.90. bidders\" valuations for the item may have decreased in a non-linear fashion. 184 Table 1: Example Combinatorial Auction. Items Bids A B AB Withdrawal prob x1 100 0 0 0.1 x2 0 100 0 0.1 x3 0 0 190 0.1 If a single bid is withdrawn there is probability of 0.18 of a revenue of 100, given the fact that we cannot withdraw an item from the other winning bidder. The expected revenue for 0, 0, 1 is: (190 \u00d7 0.9) + (200 \u00d7 0.1) = 191.00. We can therefore surmise that the second solution is preferable to the first based on expected revenue. Determining the maximum expected revenue in the presence of such uncertainty becomes computationally infeasible however, as the number of brittle bids grows. A WDP needs to be solved for all possible combinations of bids that may fail. The possible loss in revenue for breaks is also not tightly bounded using this approach, therefore a large loss may be possible for a small number of breaks. Consider the previous example where the bid amount for x3 becomes 175. The expected revenue of 1, 1, 0 (181.75) becomes greater than that of 0, 0, 1 (177.50). There are some bid-takers who may prefer the latter solution because the revenue is never less than 175, but the former solution returns revenue of only 100 with probability 0.18. A risk-averse bid-taker may not tolerate such a possibility, preferring to sacrifice revenue for reduced risk. If we modify our repair search so that a solution of at least a given revenue is guaranteed, the search for a repair solution becomes a satisfiability test rather than an optimization problem. The approaches described above are in contrast to that which we propose in the next section. Our approach can be seen as preventative in that we find an initial allocation of items to bidders which is robust to bid withdrawal. Possible losses in revenue are bounded by a fixed percentage of the true optimal allocation. Perturbations to the original solution are also limited so as to minimize disruption. We regard this as the ideal approach for real-world combinatorial auctions. DEFINITION 1 (ROBUST SOLUTION FOR A CA). A robust solution for a combinatorial auction is one where any subset of successful bids whose probability of withdrawal is greater than or equal to \u03b1 can be repaired by reassigning items at a cost of at most \u03b2 to other previously losing bids to form a repair solution. Constraints on acceptable revenue, e.g. being a minimum percentage of the optimum, are defined in the problem model and are thus satisfied by all solutions. The maximum cost of repair, \u03b2, may be a fixed value that may be thought of as a fund for compensating winning bidders whose items are withdrawn from them when creating a repair solution. Alternatively, \u03b2 may be a function of the bids that were withdrawn. Section 4 will give an example of such a mechanism. In the following section we describe an ideal constraint-based framework for the establishment of such robust solutions. 3. FINDING ROBUST SOLUTIONS In constraint programming (CP), a constraint satisfaction problem (CSP) is modeled as a set of n variables X = {x1, . . . , xn}, a set of domains D = {D(x1), . . . , D(xn)}, where D(xi) is the set of finite possible values for variable xi and a set C = {C1, . . . , Cm} of constraints, each restricting the assignments of some subset of the variables in X. Constraint satisfaction involves finding values for each of the problem variables such that all constraints are satisfied. Its main advantages are its declarative nature and flexibility in tackling problems with arbitrary side constraints. Constraint optimization seeks to find a solution to a CSP that optimizes some objective function. A common technique for solving constraint optimization problems is to use branch-and-bound techniques that avoid exploring sub-trees that are known not to contain a better solution than the best found so far. An initial bound can be determined by finding a solution that satisfies all constraints in C or by using some heuristic methods. A classical super solution (SS) is a solution to a CSP in which, if a small number of variables lose their values, repair solutions are guaranteed with only a few changes, thus providing solution robustness . It is a generalization of both fault tolerance in CP and supermodels in propositional satisfiability (SAT) . An (a,b)-super solution is one in which if at most a variables lose their values, a repair solution can be found by changing at most b other variables . Super solutions for combinatorial auctions minimize the number of bids whose status needs to be changed when forming a repair solution . Only a particular set of variables in the solution may be subject to change and these are said to be members of the breakset. For each combination of brittle assignments in the break-set, a repair-set is required that comprises the set of variables whose values must change to provide another solution. The cardinality of the repair set is used to measure the cost of repair. In reality, changing some variable assignments in a repair solution incurs a lower cost than others thereby motivating the use of a different metric for determining the legality of repair sets. The Weighted Super Solution (WSS) framework considers the cost of repair required, rather than simply the number of assignments modified, to form an alternative solution. For CAs this may be a measure of the compensation penalties paid to winning bidders to break existing agreements. Robust solutions are particularly desirable for applications where unreliability is a problem and potential breakages may incur severe penalties. Weighted super solutions offer a means of expressing which variables are easily re-assigned and those that incur a heavy cost . Hebrard et al. describe how some variables may fail (such as machines in a job-shop problem) and others may not. A WSS generalizes this approach so that there is a probability of failure associated with each assignment and sets of variables whose assignments have probabilities of failure greater than or equal to a threshold value, \u03b1, require repair solutions. A WSS measures the cost of repairing, or reassigning, other variables using inertia as a metric. Inertia is a measure of a variable\"s aversion to change and depends on its current assignment, future assignment and the breakage variable(s). It may be desirable to reassign items to different bidders in order to find a repair solution of satisfactory revenue. Compensation may have to be paid to bidders who lose items during the formation of a repair solution. The inertia of a bid reflects the cost of changing its state. For winning bids this may reflect the necessary compensation penalty for the bid-taker to break the agreement (if such breaches are permitted), whereas for previously losing bids this is a free operation. The total amount of compensation payable to bidders may depend upon other factors, such as the cause of the break. There is a limit to how much these overall repair costs should be, and this is given by the value \u03b2. This value may not be known in advance and 185 Algorithm 1: WSS(int level, double \u03b1, double \u03b2):Boolean begin if level > number of variables then return true choose unassigned variable x foreach value v in the domain of x do assign x : v if problem is consistent then foreach combination of brittle assignments, A do if \u00acreparable(A, \u03b2) then return false; if WSS(level+1) then return true unassign x return false end may depend upon the break. Therefore, \u03b2 may be viewed as the fund used to compensate winning bidders for the unilateral withdrawal of their bids by the bid-taker. In summary, an (\u03b1,\u03b2)-WSS allows any set of variables whose probability of breaking is greater than or equal to \u03b1 be repaired with changes to the original robust solution with a cost of at most \u03b2. The depth-first search for a WSS (see pseudo-code description in Algorithm 1) maintains arc-consistency at each node of the tree. As search progresses, the reparability of each previous assignment is verified at each node by extending a partial repair solution to the same depth as the current partial solution. This may be thought of as maintaining concurrent search trees for repairs. A repair solution is provided for every possible set of break variables, A. The WSS algorithm attempts to extend the current partial assignment by choosing a variable and assigning it a value. Backtracking may then occur for one of two reasons: we cannot extend the assignment to satisfy the given constraints, or the current partial assignment cannot be associated with a repair solution whose cost of repair is less than \u03b2 should a break occur. The procedure reparable searches for partial repair solutions using backtracking and attempts to extend the last repair found, just as in (1,b)super solutions ; the differences being that a repair is provided for a set of breakage variables rather than a single variable and the cost of repair is considered. A summation operator is used to determine the overall cost of repair. If a fixed bound upon the size of any potential break-set can be formed, the WSS algorithm is NPcomplete. For a more detailed description of the WSS search algorithm, the reader is referred to , since a complete description of the algorithm is beyond the scope of this paper. EXAMPLE 1. We shall step through the example given in Table 1 when searching for a WSS. Each bid is represented by a single variable with domain values of 0 and 1, the former representing bid-failure and the latter bid-success. The probability of failure of the variables are 0.1 when they are assigned to 1 and 0.0 otherwise. The problem is initially solved using an ILP solver such as lp_solve or CPLEX, and the optimal revenue is found to be 200. A fixed percentage of this revenue can be used as a threshold value for a robust solution and its repairs. The bid-taker wishes to have a robust solution so that if a single winning bid is withdrawn, a repair solution can be formed without withdrawing items from any other winning bidder. This example may be seen as searching for a (0.1,0)-weighted super solution, \u03b2 is 0 because no funds are available to compensate the withdrawal of items from winning bidders. The bid-taker is willing to compromise on revenue, but only by 5%, say, of the optimal value. Bids 1 and 3 cannot both succeed, since they both require item A, so a constraint is added precluding the assignment in which both variables take the value 1. Similarly, bids 2 and 3 cannot both win so another constraint is added between these two variables. Therefore, in this example the set of CSP variables is V = {x1, x2, x3}, whose domains are all {0, 1}. The constraints are x1 + x3 \u2264 1, x2 + x3 \u2264 1 and xi\u2208V aixi \u2265 190, where ai reflects the relevant bid-amounts for the respective bid variables. In order to find a robust solution of optimal revenue we seek to maximize the sum of these amounts, max xi\u2208V aixi. When all variables are set to 0 (see Figure 1(a) branch 3), this is not a solution because the minimum revenue of 190 has not been met, so we try assigning bid3 to 1 (branch 4). This is a valid solution but this variable is brittle because there is a 10% chance that this bid may be withdrawn (see Table 1). Therefore we need to determine if a repair can be formed should it break. The search for a repair begins at the first node, see Figure 1(b). Notice that value 1 has been removed from bid3 because this search tree is simulating the withdrawal of this bid. When bid1 is set to 0 (branch 4.1), the maximum revenue solution in the remaining subtree has revenue of only 100, therefore search is discontinued at that node of the tree. Bid1 and bid2 are both assigned to 1 (branches 4.2 and 4.4) and the total cost of both these changes is still 0 because no compensation needs to be paid for bids that change from losing to winning. With bid3 now losing (branch 4.5), this gives a repair solution of 200. Hence 0, 0, 1 is reparable and therefore a WSS. We continue our search in Figure 1(a) however, because we are seeking a robust solution of optimal revenue. When bid1 is assigned to 1 (branch 6) we seek a partial repair for this variable breaking (branch 5 is not considered since it offers insufficient revenue). The repair search sets bid1 to 0 in a separate search tree, (not shown), and control is returned to the search for a WSS. Bid2 is set to 0 (branch 7), but this solution would not produce sufficient revenue so bid2 is then set to 1 (branch 8). We then attempt to extend the repair for bid1 (not shown). This fails because the repair for bid1 cannot assign bid2 to 0 because the cost of repairing such an assignment would be \u221e, given that the auction rules do not permit the withdrawal of items from winning bids. A repair for bid1 breaking is therefore not possible because items have already been awarded to bid2. A repair solution with bid2 assigned to 1 does not produce sufficient revenue when bid1 is assigned to 0. The inability to withdraw items from winning bids implies that 1, 1, 0 is an irreparable solution when the minimum tolerable revenue is greater than 100. The italicized comments and dashed line in Figure 1(a) illustrate the search path for a WSS if both of these bids were deemed reparable. Section 4 introduces an alternative auction model that will allow the bid-taker to receive compensation for breakages and in turn use this payment to compensate other bidders for withdrawal of items from winning bids. This will enable the reallocation of items and permit the establishment of 1, 1, 0 as a second WSS for this example. 4. MUTUAL BID BONDS: A BACKTRACKING MECHANISM Some auction solutions are inherently brittle and it may be impossible to find a robust solution. If we can alter the rules of an auction so that the bid-taker can retract items from winning bidders, then the reparability of solutions to such auctions may be improved. In this section we propose an auction model that permits bid and item withdrawal by the bidders and bid-taker, respectively. We propose a model that incorporates mutual bid bonds to enable solution reparability for the bid-taker, a form of insurance against 186 0 0 0 1 1 1 1 1 1 Insufficient revenue Find repair solution for bid 3 breakage Find partial repair for bid 1 breakage Insufficient revenue (a) Extend partial repair for bid 1 breakage (b) Find partial repair for bid 2 breakage Bid 1 Bid 2 Bid 3 Find repair solutions for bid 1 & 2 breakages 3 4 7 8 Insufficient revenue (a) Search for WSS. 0 0 0 1 1 1 1 1 1 Insufficient revenue Insufficient revenue Bid 1 Bid 2 Bid 3 inertia=0 inertia=0 inertia=0 4.1 4.2 4.3 4.4 4.5 (b) Search for a repair for bid 3 breakage. Figure 1: Search Tree for a WSS without item withdrawal. the winner\"s curse for the bidder whilst also compensating bidders in the case of item withdrawal from winning bids. We propose that such Winner\"s Curse & Bid-taker\"s Exposure insurance comprise a fixed percentage, \u03ba, of the bid amount for all bids. Such mutual bid bonds are mandatory for each bid in our model2 . The conditions attached to the bid bonds are that the bid-taker be allowed to annul winning bids (item withdrawal) when repairing breaks elsewhere in the solution. In the interests of fairness, compensation is paid to bidders from whom items are withdrawn and is equivalent to the penalty that would have been imposed on the bidder should he have withdrawn the bid. Combinatorial auctions impose a heavy computational burden on the bidder so it is important that the hedging of risk should be a simple and transparent operation for the bidder so as not to further increase this burden unnecessarily. We also contend that it is imperative that the bidder knows the potential penalty for withdrawal in advance of bid submission. This information is essential for bidders when determining how aggressive they should be in their bidding strategy. Bid bonds are commonplace in procurement for construction projects. Usually they are mandatory for all bids, are a fixed percentage, \u03ba, of the bid amount and are unidirectional in that item withdrawal by the bid-taker is not permitted. Mutual bid bonds may be seen as a form of leveled commitment contract in which both parties may break the contract for the same fixed penalty. Such contracts permit unilateral decommitment for prespecified penalties. Sandholm et al. showed that this can increase the expected payoffs of all parties and enables deals that would be impossible under full commitment . In practice a bid bond typically ranges between 5 and 20% of the Making the insurance optional may be beneficial in some instances. If a bidder does not agree to the insurance, it may be inferred that he may have accurately determined the valuation for the items and therefore less likely to fall victim to the winner\"s curse. The probability of such a bid being withdrawn may be less, so a repair solution may be deemed unnecessary for this bid. On the other hand it decreases the reparability of solutions. bid amount . If the decommitment penalties are the same for both parties in all bids, \u03ba does not influence the reparability of a given set of bids. It merely influences the levels of penalties and compensation transacted by agents. Low values of \u03ba incur low bid withdrawal penalties and simulate a dictatorial bid-taker who does not adequately compensate bidders for item withdrawal. Andersson and Sandholm found that myopic agents reach a higher social welfare quicker if they act selfishly rather than cooperatively when penalties in leveled commitment contracts are low. Increased levels of bid withdrawal are likely when the penalties are low also. High values of \u03ba tend towards full-commitment and reduce the advantages of such Winner\"s Curse & Bid-taker\"s Exposure insurance. The penalties paid are used to fund a reassignment of items to form a repair solution of sufficient revenue by compensating previously successful bidders for withdrawal of the items from them. EXAMPLE 2. Consider the example given in Table 1 once more, where the bids also comprise a mutual bid bond of 5% of the bid amount. If a bid is withdrawn, the bidder forfeits this amount and the bid-taker can then compensate winning bidders whose items are withdrawn when trying to form a repair solution later. The search for repair solutions for breaks to bid1 and bid2 appear in Figures 2(a) and 2(b), respectively3 When bid1 breaks, there is a compensation penalty paid to the bid-taker equal to 5 that can be used to fund a reassignment of the items. We therefore set \u03b2 to 5 and this becomes the maximum expenditure allowed to withdraw items from winning bidders. \u03b2 may also be viewed as the size of the fund available to facilitate backtracking by the bid-taker. When we extend the partial repair for bid1 so that bid2 loses an item (branch 8.1), the overall cost of repair increases to 5, due to this item withdrawal by the bid-taker, The actual implementation of WSS search checks previous solutions to see if they can repair breaks before searching for a new repair solution. 0, 0, 1 is a solution that has already been found so the search for a repair in this example is not strictly necessary but is described for pedagogical reasons. 187 Bid 1 Bid 2 Bid 3 Insufficient revenue inertia=5 =5 inertia=0 =5 inertia=5 =5 6.1 8.1 9.1 9.2 (a) Search for a repair for bid 1 breakage. Bid 1 Bid 2 Bid 3 Insufficient revenue inertia=10 =10 inertia=10 =10 inertia=10 =10 8.2 8.3 9.3 9.4 (b) Search for a repair for bid 2 breakage. Figure 2: Repair Search Tree for breaks 1 and 2, \u03ba = 0.05. and is just within the limit given by \u03b2. In Figure 1(a) the search path follows the dashed line and sets bid3 to be 0 (branch 9). The repair solutions for bids 1 and 2 can be extended further by assigning bid3 to 1 (branches 9.2 and 9.4). Therefore, 1, 1, 0 may be considered a robust solution. Recall, that previously this was not the case. Using mutual bid bonds thus increases reparability and allows a robust solution of revenue 200 as opposed to 190, as was previously the case. 5. EXPERIMENTS We have used the Combinatorial Auction Test Suite (CATS) to generate sample auction data. We generated 100 instances of problems in which there are 20 items for sale and 100- bids that may be dominated in some instances4 . Such dominated bids can participate in repair solutions although they do not feature in optimal solutions. CATS uses economically motivated bidding patterns to generate auction data in various scenarios. To motivate the research presented in this paper we use sensitivity analysis to examine the brittleness of optimal solutions and hence determine the types of auctions most likely to benefit from a robust solution. We then establish robust solutions for CAs using the WSS framework. 5.1 Sensitivity Analysis for the WDP We have performed sensitivity analysis of the following four distributions: airport take-off/landing slots (matching), electronic components (arbitrary), property/spectrum-rights (regions) and transportation (paths). These distributions were chosen because they describe a broad array of bidding patterns in different application domains. The method used is as follows. We first of all determined the optimal solution using lp_solve, a mixed integer linear program solver . We then simulated a single bid withdrawal and re-solved the problem with the other winning bids remaining fixed, i.e. there were no involuntary dropouts. The optimal repair solution was then determined. This process is repeated for all winning bids in the overall optimal solution, thus assuming that all bids are brittle. Figure 3 shows the average revenue of such repair solutions as a percentage of the optimum. Also shown is the average worst-case scenario over 100 auctions. We also implemented an auction rule that disallows bids from the reneging bidder participate in a repair5 Figure 3(a) illustrates how the paths distribution is inherently the most robust distribution since when any winning bid is withdrawn the solution can be repaired to achieve over 98.5%. We assumed that all bids in a given XOR bid with the same dummy item were from the same bidder. optimal revenue on average for auctions with more than 250 bids. There are some cases however when such withdrawals result in solutions whose revenue is significantly lower than optimum. bids there are occasions when a single bid withdrawal can result in a drop in revenue of over 5%, although the average worst-case drop in revenue is only 1%. Figure 3(b) shows how the matching distribution is more brittle on average than paths and also has an inferior worst-case revenue on average. This trend continues as the regions-npv (Figure 3(c)) and arbitrary-npv (Figure 3(d)) distributions are more brittle still. These distributions are clearly sensitive to bid withdrawal when no other winning bids in the solution may be involuntarily withdrawn by the bid-taker. 5.2 Robust Solutions using WSS In this section we focus upon both the arbitrary-npv and regions-npv distributions because the sensitivity analysis indicated that these types of auctions produce optimal solutions that tend to be most brittle, and therefore stand to benefit most from solution robustness. bids because the sensitivity analysis has indicated that these auctions are inherently robust with a very low average drop in revenue following a bid withdrawal. They would also be very computationally expensive, given the extra complexity of finding robust solutions. A pure CP approach needs to be augmented with global constraints that incorporate operations research techniques to increase pruning sufficiently so that thousands of bids may be examined. Global constraints exploit special-purpose filtering algorithms to improve performance . There are a number of ways to speed up the search for a weighted super solution in a CA, although this is not the main focus of our current work. Polynomial matching algorithms may be used in auctions whose bid length is short, such as those for airport landing/take-off slots for example. The integer programming formulation of the WDP stipulates that a bid either loses or wins. If we relax this constraint so that bids can partially win, this corresponds to the linear relaxation of the problem and is solvable in polynomial time. At each node of the search tree we can quickly solve the linear relaxation of the remaining problem in the subtree below the current node to establish an upper bound on remaining revenue. If this upper bound plus revenue in the parent tree is less than the current lower bound on revenue, search at that node can cease. The (continuous) LP relaxation thus provides a vital speed-up in the search for weighted super solutions, which we have exploited in our implementation. The LP formulation is as follows: max xi\u2208 Revenue(%ofoptimum) Bids Average Repair Solution Revenue Worst-case Repair Solution Revenue (a) Revenue(%ofoptimum) Bids Average Repair Solution Revenue Worst-case Repair Solution Revenue (b) Revenue(%ofoptimum) Bids Average Repair Solution Revenue Worst-case Repair Solution Revenue (c) regions- Revenue(%ofoptimum) Bids Average Repair Solution Revenue Worst-case Repair Solution Revenue (d) arbitrary-npv Figure 3: Sensitivity of bid distributions to single bid withdrawal. s.t. j|i\u2208Sj xj \u2264 1, \u2200i \u2208 {1 . . . m}, xj \u2265 0, xj \u2208 R. Additional techniques, that are outlined in , can aid the scalability of a CP approach but our main aim in these experiments is to examine the robustness of various auction distributions and consider the tradeoff between robustness and revenue. The WSS solver we have developed is an extension of the super solution solver presented in . This solver is, in turn, based upon the EFC constraint solver . Combinatorial auctions are easily modeled as a constraint optimization problems. We have chosen the branch-on-bids formulation because in tests it worked faster than a branch-on-items formulation for the arbitrary-npv and regions-npv distributions. All variables are binary and our search mechanism uses a reverse lexicographic value ordering heuristic. This complements our dynamic variable ordering heuristic that selects the most promising unassigned variable as the next one in the search tree. We use the product of the solution of the LP relaxation and the degree of a variable to determine the likelihood of its participation in a robust solution. High values in the LP solution are a strong indication of variables most likely to form a high revenue solution whilst the a variable\"s degree reflects the number of other bids that overlap in terms of desired items. Bids for large numbers of items tend to be more robust, which is why we weight our robust solution search in this manner. We found this heuristic to be slightly more effective than the LP solution alone. As the number of bids in the auction increases however, there is an increase in the inherent robustness of solutions so the degree of a variable loses significance as the auction size increases. 5.3 Results Our experiments simulate three different constraints on repair solutions. The first is that no winning bids are withdrawn by the bid-taker and a repair solution must return a revenue of at least 90% of the optimal overall solution. Secondly, we relaxed the revenue constraint to 85% of optimum. Thirdly, we allowed backtracking by the bid-taker on winning bids using mutual bid bonds but maintaining the revenue constraint at 90% of optimum. Prior to finding a robust solution we solved the WDP optimally using lp_solve . We then set the minimum tolerable revenue for a solution to be 90% (then 85%) of the revenue of this optimal solution. We assumed that all bids were brittle, thus a repair solution is required for every bid in the solution. Initially we assume that no backtracking was permitted on assignments of items to other winning bids given a bid withdrawal elsewhere in the solution. Table 2 shows the percentage of optimal solutions that are robust for minimum revenue constraints for repair solutions of 90% and 85% of optimal revenue. Relaxing the revenue constraint on repair solutions to 85% of the optimum revenue greatly increases the number of optimal solutions that are robust. We also conducted experiments on the same auctions in which backtracking by the bid-taker is permitted using mutual bid bonds. This significantly improves the reparability of optimal solutions whilst still maintaining repair solutions of 90% of optimum. An interesting feature of the arbitrary-npv distribution is that optimal solutions can become more brittle as the number of bids increases. The reason for this is that optimal solutions for larger auctions have more winning bids. Some of the optimal solutions for the smallest auctions with 100 bids have only one winning bidder. If this bid is withdrawn it is usually easy to find a new repair solution within 90% of the previous optimal revenue. Also, repair solutions for bids that contain a small number of items may be made difficult by the fact that a reduced number of bids cover only a subset of those items. A mitigating factor is that such bids form a smaller percentage of the revenue of the optimal solution on average. We also implemented a rule stipulating that any losing bids from 189 Table 2: Optimal Solutions that are Inherently Robust (%). # arbitrary-npv repair \u2265 90% 21 5 3 37 93 repair \u2265 85% 26 15 40 87 100 MBB & repair \u2265 90% 41 35 60 94 \u2265 93 regions-npv repair \u2265 90% 30 33 61 91 98 repair \u2265 85% 50 71 95 100 100 MBB & repair \u2265 90% 60 78 96 99 \u2265 98 Table 3: Occurrence of Robust Solutions (%). # arbitrary-npv repair \u2265 90% 58 39 51 98 repair \u2265 85% 86 88 94 99 MBB & repair \u2265 90% 78 86 98 100 regions-npv repair \u2265 90% 61 70 97 100 repair \u2265 85% 89 99 99 100 MBB & repair \u2265 90% 83 96 100 100 a withdrawing bidder cannot participate in a repair solution. This acts as a disincentive for strategic withdrawal and was also used previously in the sensitivity analysis. In some auctions, a robust solution may not exist. Table 3 shows the percentage of auctions that support robust solutions for the arbitrary-npv and regions -npv distributions. It is clear that finding robust solutions for the former distribution is particularly difficult for auctions with 250 and 500 bids when revenue constraints are 90% of optimum. This difficulty was previously alluded to by the low percentage of optimal solutions that were robust for these auctions. Relaxing the revenue constraint helps increase the percentage of auctions in which robust solutions are achievable to 88% and 94%, respectively. This improves the reparability of all solutions thereby increasing the average revenue of the optimal robust solution. It is somewhat counterintuitive to expect a reduction in reparability of auction solutions as the number of bids increases because there tends to be an increased number of solutions above a revenue threshold in larger auctions. The MBB auction model performs very well however, and ensures that robust solutions are achievable for such inherently brittle auctions without sacrificing over 10% of optimal revenue to achieve repair solutions. Figure 4 shows the average revenue of the optimal robust solution as a percentage of the overall optimum. Repair solutions found for a WSS provide a lower bound on possible revenue following a bid withdrawal. Note that in some instances it is possible for a repair solution to have higher revenue than the original solution. When backtracking on winning bids by the bid-taker is disallowed, this can only happen when the repair solution includes two or more bids that were not in the original. Otherwise the repair bids would participate in the optimal robust solution in place of the bid that was withdrawn. A WSS guarantees minimum levels of revenue for repair solutions but this is not to say that repair solutions cannot be improved upon. Revenue(%ofoptimum) Bids Repair Revenue: Min 90% Optimal Repair Revenue: Min 85% Optimal MBB: Repair Revenue: Min 90% Optimal (a) regions- Revenue(%ofoptimum) Bids Repair Revenue: Min 90% Optimal Repair Revenue: Min 85% Optimal MBB: Repair Revenue: Min 90% Optimal (b) arbitrary-npv Figure 4: Revenue of optimal robust solutions. determine an optimal repair solution following a break, whilst safe in the knowledge that in advance of any possible bid withdrawal we can establish a lower bound on the revenue of a repair. Kastner et al. have provided such an incremental ILP formulation . Mutual bid bonds facilitate backtracking by the bid-taker on already assigned items. This improves the reparability of all possible solutions thus increasing the revenue of the optimal robust solution on average. Figure 4 shows the increase in revenue of robust solutions in such instances. The revenues of repair solutions are bounded by at least 90% of the optimum in our experiments thereby allowing a direct comparison with robust solutions already found using the same revenue constraint but not providing for backtracking. It is immediately obvious that such a mechanism can significantly increase revenue whilst still maintaining solution robustness. Table 4 shows the number of winning bids participating in optimal and optimal robust solutions given the three different constraints on repairing solutions listed at the beginning of this section. As the number of bids increases, more of the optimal overall solutions are robust. This leads to a convergence in the number of winning bids. bids are robust. We can therefore infer that the average number of winning bids in revenuemaximizing robust solutions converges towards that of the optimal overall solutions. A notable side-effect of robust solutions is that fewer bids participate in the solutions. It can be clearly seen from Table 4 that when revenue constraints on repair solutions are tight, there are fewer winning bids in the optimal robust solution on average. This is particularly pronounced for smaller auctions in both distributions. This can win benefits for the bid-taker such as reduced overheads in dealing with fewer suppliers. Although MBBs aid solution repara190 Table 4: Number of winning bids. # arbitrary-npv Optimal 3.31 5.60 7.17 9.31 10.63 Repair \u2265 90% 1.40 2.18 6.10 9.03 (\u2248 10.63) Repair \u2265 85% 1.65 3.81 6.78 9.31 (10.63) MBB (\u2265 90%) 2.33 5.49 7.33 9.34 (\u2248 10.63) regions-npv Optimal 4.34 7.05 9.10 10.67 12.76 Repair \u2265 90% 3.03 5.76 8.67 10.63 (\u2248 12.76) Repair \u2265 85% 3.45 6.75 9.07 (10.67) (12.76) MBB (\u2265 90%) 3.90 6.86 9.10 10.68 (\u2248 12.76) bility, the number of bids in the solutions increases on average. This is to be expected because a greater fraction of these solutions are in fact optimal, as we saw in Table 2. 6. DISCUSSION AND FUTURE WORK Bidding strategies can become complex in non-incentive-compatible mechanisms where winner determination is no longer necessarily optimal. The perceived reparability of a bid may influence the bid amount, with reparable bids reaching a lower equilibrium point and perceived irreparable bids being more aggressive. Penalty payments for bid withdrawal also create an incentive for more aggressive bidding by providing a form of insurance against the winner\"s curse . If a winning bidder\"s revised valuation for a set of items drops by more than the penalty for withdrawal of the bid, then it is in his best interests to forfeit the item(s) and pay the penalty. Should the auction rules state that the bid-taker will refuse to sell the items to any of the remaining bidders in the event of a withdrawal, then insurance against potential losses will stimulate more aggressive bidding. However, in our case we are seeking to repair the solution with the given bids. A side-effect of such a policy is to offset the increased aggressiveness by incentivizing reduced valuations in expectation that another bidder\"s successful bid is withdrawn. Harstad and Rothkopf examined the conditions required to ensure an equilibrium position in which bidding was at least as aggressive as if no bid withdrawal was permitted, given this countervailing incentive to under-estimate a valuation. Three major results arose from their study of bid withdrawal in a single item auction: 1. Equilibrium bidding is more aggressive with withdrawal for sufficiently small probabilities of an award to the second highest bidder in the event of a bid withdrawal; 2. Equilibrium bidding is more aggressive with withdrawal if the number of bidders is large enough; 3. For many distributions of costs and estimates, equilibrium bidding is more aggressive with withdrawal if the variability of the estimating distribution is sufficiently large. It is important that mutual bid bonds do not result in depressed bidding in equilibrium. An analysis of the resultant behavior of bidders must incorporate the possibility of a bidder winning an item and having it withdrawn in order for the bid-taker to formulate a repair solution after a break elsewhere. Harstad and Rothkopf have analyzed bidder aggressiveness using a strictly game-theoretic model in which the only reason for bid withdrawal is the winner\"s curse. They assumed all bidders were risk-neutral, but surmised that it is entirely possible for the bid-taker to collect a risk premium from risk-averse bidders with the offer of such insurance. Combinatorial auctions with mutual bid bonds add an extra incentive to bid aggressively because of the possibility of being compensated for having a winning bid withdrawn by a bid-taker. This is militated against by the increased probability of not having items withdrawn in a repair solution. We leave an in-depth analysis of the sufficient conditions for more aggressive bidding for future work. Whilst the WSS framework provides ample flexibility and expressiveness, scalability becomes a problem for larger auctions. Although solutions to larger auctions tend to be naturally more robust, some bid-takers in such auctions may require robustness. A possible extension of our work in this paper may be to examine the feasibility of reformulating integer linear programs so that the solutions are robust. Hebrard et al. examined reformulation of CSPs for finding super solutions. Alternatively, it may be possible to use a top-down approach by looking at the k-best solutions sequentially, in terms of revenue, and performing sensitivity analysis upon each solution until a robust one is found. In procurement settings the principle of free disposal is often discounted and all items must be sold. This reduces the number of potential solutions and thereby reduces the reparability of each solution. The impact of such a constraint on revenue of robust solutions is also left for future work. There is another interesting direction this work may take, namely robust mechanism design. Porter et al. introduced the notion of fault tolerant mechanism design in which agents have private information regarding costs for task completion, but also their probabilities of failure . When the bid-taker has combinatorial valuations for task completions it may be desirable to assign the same task to multiple agents to ensure solution robustness. It is desirable to minimize such potentially redundant task assignments but not to the detriment of completed task valuations. This problem could be modeled using the WSS framework in a similar manner to that of combinatorial auctions. In the case where no robust solutions are found, it is possible to optimize robustness, instead of revenue, by finding a solution of at least a given revenue that minimizes the probability of an irreparable break. In this manner the least brittle solution of adequate revenue may be chosen. 7. CONCLUSION Fairness is often cited as a reason for choosing the optimal solution in terms of revenue only . Robust solutions militate against bids deemed brittle, therefore bidders must earn a reputation for being reliable to relax the reparability constraint attached to their bids. This may be seen as being fair to long-standing business partners whose reliability is unquestioned. Internet-based auctions are often seen as unwelcome price-gouging exercises by suppliers in many sectors . Traditional business partnerships are being severed by increased competition amongst suppliers. Quality of Service can suffer because of the increased focus on short-term profitability to the detriment of the bid-taker in the long-term. Robust solutions can provide a means of selectively discriminating against distrusted bidders in a measured manner. As combinatorial auction deployment moves from large value auctions with a small pool of trusted bidders (e.g. spectrum-rights sales) towards lower value auctions with potentially unknown bidders (e.g. Supply Chain Management ), solution robustness becomes more relevant. As well as being used to ensure that the bid-taker is not left vulnerable to bid withdrawal, it may also be used to cement relationships with preferred, possibly incumbent, suppliers. 191 We have shown that it is possible to attain robust solutions for CAs with only a small loss in revenue. We have also illustrated how such solutions tend to have fewer winning bids than overall optimal solutions, thereby reducing any overheads associated with dealing with more bidders. We have also demonstrated that introducing mutual bid bonds, a form of leveled commitment contract, can significantly increase the revenue of optimal robust solutions by improving reparability. We contend that robust solutions using such a mechanism can allow a bid-taker to offer the possibility of bid withdrawal to bidders whilst remaining confident about postrepair revenue and also facilitating increased bidder aggressiveness.", "body1": "A combinatorial auction (CA) provides an efficient means of allocating multiple distinguishable items amongst bidders whose perceived valuations for combinations of items differ. Revenue is the most obvious optimization criterion for such auctions, but another desirable attribute is solution robustness. However, reallocating items may be regarded as disruptive to a solution in many real-life scenarios. Repair solutions are required for bids that are seen as brittle (i.e. This paper is organized as follows. Section 6 discusses possible extensions and questions raised by our research that deserve future work. Before presenting the technical details of our solution to the Bid-taker\"s Exposure Problem, we shall present a brief survey of combinatorial auctions and existing techniques for handling bid withdrawal. Combinatorial auctions involve a single bid-taker allocating multiple distinguishable items amongst a group of bidders. j|i\u2208Sj xj \u2264 1, \u2200i \u2208 {1 . This problem is NP-complete and inapproximable , and is otherwise known as the Set Packing Problem. 2.1 The Problem of Bid Withdrawal We assume an auction protocol with a three stage process involving the submission of bids, winner determination, and finally a transaction phase. These perturbations may include bid withdrawals, change of valuation/items of a bid or the submission of a new bid. 2.2 Being Proactive against Bid Withdrawal When a bid is withdrawn there may be constraints on how the solution can be repaired. Consider the example in Table 1, and the optimal expected revenue in the situation where a single bid may be withdrawn. bidders\" valuations for the item may have decreased in a non-linear fashion. 184 Table 1: Example Combinatorial Auction. Items Bids A B AB Withdrawal prob x1 100 0 0 0.1 x2 0 100 0 0.1 x3 0 0 190 0.1 If a single bid is withdrawn there is probability of 0.18 of a revenue of 100, given the fact that we cannot withdraw an item from the other winning bidder. We can therefore surmise that the second solution is preferable to the first based on expected revenue. Determining the maximum expected revenue in the presence of such uncertainty becomes computationally infeasible however, as the number of brittle bids grows. Consider the previous example where the bid amount for x3 becomes 175. If we modify our repair search so that a solution of at least a given revenue is guaranteed, the search for a repair solution becomes a satisfiability test rather than an optimization problem. DEFINITION 1 (ROBUST SOLUTION FOR A CA). Constraints on acceptable revenue, e.g. In constraint programming (CP), a constraint satisfaction problem (CSP) is modeled as a set of n variables X = {x1, . Constraint optimization seeks to find a solution to a CSP that optimizes some objective function. An (a,b)-super solution is one in which if at most a variables lose their values, a repair solution can be found by changing at most b other variables . Super solutions for combinatorial auctions minimize the number of bids whose status needs to be changed when forming a repair solution . The Weighted Super Solution (WSS) framework considers the cost of repair required, rather than simply the number of assignments modified, to form an alternative solution. It may be desirable to reassign items to different bidders in order to find a repair solution of satisfactory revenue. The depth-first search for a WSS (see pseudo-code description in Algorithm 1) maintains arc-consistency at each node of the tree. Backtracking may then occur for one of two reasons: we cannot extend the assignment to satisfy the given constraints, or the current partial assignment cannot be associated with a repair solution whose cost of repair is less than \u03b2 should a break occur. EXAMPLE 1. Therefore, in this example the set of CSP variables is V = {x1, x2, x3}, whose domains are all {0, 1}. When all variables are set to 0 (see Figure 1(a) branch 3), this is not a solution because the minimum revenue of 190 has not been met, so we try assigning bid3 to 1 (branch 4). With bid3 now losing (branch 4.5), this gives a repair solution of 200. When bid1 is assigned to 1 (branch 6) we seek a partial repair for this variable breaking (branch 5 is not considered since it offers insufficient revenue). Some auction solutions are inherently brittle and it may be impossible to find a robust solution. 0 0 0 1 1 1 1 1 1 Insufficient revenue Insufficient revenue Bid 1 Bid 2 Bid 3 inertia=0 inertia=0 inertia=0 4.1 4.2 4.3 4.4 4.5 (b) Search for a repair for bid 3 breakage. Figure 1: Search Tree for a WSS without item withdrawal. the winner\"s curse for the bidder whilst also compensating bidders in the case of item withdrawal from winning bids. Combinatorial auctions impose a heavy computational burden on the bidder so it is important that the hedging of risk should be a simple and transparent operation for the bidder so as not to further increase this burden unnecessarily. The probability of such a bid being withdrawn may be less, so a repair solution may be deemed unnecessary for this bid. bid amount . EXAMPLE 2. Bid 1 Bid 2 Bid 3 Insufficient revenue inertia=10 =10 inertia=10 =10 inertia=10 =10 8.2 8.3 9.3 9.4 (b) Search for a repair for bid 2 breakage. Figure 2: Repair Search Tree for breaks 1 and 2, \u03ba = 0.05. and is just within the limit given by \u03b2. We have used the Combinatorial Auction Test Suite (CATS) to generate sample auction data. The method used is as follows. Figure 3 shows the average revenue of such repair solutions as a percentage of the optimum. optimal revenue on average for auctions with more than 250 bids. There are some cases however when such withdrawals result in solutions whose revenue is significantly lower than optimum. Figure 3(b) shows how the matching distribution is more brittle on average than paths and also has an inferior worst-case revenue on average. 5.2 Robust Solutions using WSS In this section we focus upon both the arbitrary-npv and regions-npv distributions because the sensitivity analysis indicated that these types of auctions produce optimal solutions that tend to be most brittle, and therefore stand to benefit most from solution robustness. Global constraints exploit special-purpose filtering algorithms to improve performance . Additional techniques, that are outlined in , can aid the scalability of a CP approach but our main aim in these experiments is to examine the robustness of various auction distributions and consider the tradeoff between robustness and revenue. Combinatorial auctions are easily modeled as a constraint optimization problems. 5.3 Results Our experiments simulate three different constraints on repair solutions. Prior to finding a robust solution we solved the WDP optimally using lp_solve . # arbitrary-npv repair \u2265 90% 21 5 3 37 93 repair \u2265 85% 26 15 40 87 100 MBB & repair \u2265 90% 41 35 60 94 \u2265 93 regions-npv repair \u2265 90% 30 33 61 91 98 repair \u2265 85% 50 71 95 100 100 MBB & repair \u2265 90% 60 78 96 99 \u2265 98 Table 3: Occurrence of Robust Solutions (%). # arbitrary-npv repair \u2265 90% 58 39 51 98 repair \u2265 85% 86 88 94 99 MBB & repair \u2265 90% 78 86 98 100 regions-npv repair \u2265 90% 61 70 97 100 repair \u2265 85% 89 99 99 100 MBB & repair \u2265 90% 83 96 100 100 a withdrawing bidder cannot participate in a repair solution. Figure 4 shows the average revenue of the optimal robust solution as a percentage of the overall optimum. When backtracking on winning bids by the bid-taker is disallowed, this can only happen when the repair solution includes two or more bids that were not in the original. determine an optimal repair solution following a break, whilst safe in the knowledge that in advance of any possible bid withdrawal we can establish a lower bound on the revenue of a repair. Mutual bid bonds facilitate backtracking by the bid-taker on already assigned items. Table 4 shows the number of winning bids participating in optimal and optimal robust solutions given the three different constraints on repairing solutions listed at the beginning of this section. A notable side-effect of robust solutions is that fewer bids participate in the solutions. # arbitrary-npv Optimal 3.31 5.60 7.17 9.31 10.63 Repair \u2265 90% 1.40 2.18 6.10 9.03 (\u2248 10.63) Repair \u2265 85% 1.65 3.81 6.78 9.31 (10.63) MBB (\u2265 90%) 2.33 5.49 7.33 9.34 (\u2248 10.63) regions-npv Optimal 4.34 7.05 9.10 10.67 12.76 Repair \u2265 90% 3.03 5.76 8.67 10.63 (\u2248 12.76) Repair \u2265 85% 3.45 6.75 9.07 (10.67) (12.76) MBB (\u2265 90%) 3.90 6.86 9.10 10.68 (\u2248 12.76) bility, the number of bids in the solutions increases on average. This is to be expected because a greater fraction of these solutions are in fact optimal, as we saw in Table 2. Bidding strategies can become complex in non-incentive-compatible mechanisms where winner determination is no longer necessarily optimal. Penalty payments for bid withdrawal also create an incentive for more aggressive bidding by providing a form of insurance against the winner\"s curse . It is important that mutual bid bonds do not result in depressed bidding in equilibrium. Combinatorial auctions with mutual bid bonds add an extra incentive to bid aggressively because of the possibility of being compensated for having a winning bid withdrawn by a bid-taker. Although solutions to larger auctions tend to be naturally more robust, some bid-takers in such auctions may require robustness. There is another interesting direction this work may take, namely robust mechanism design.", "body2": "Such auctions are gaining in popularity and there is a proliferation in their usage across various industries such as telecoms, B2B procurement and transportation . We have called this the Bid-taker\"s Exposure Problem that bears similarities to the Exposure Problem faced by bidders seeking multiple items in separate single-unit auctions but holding little or no value for a subset of those items. We assume a probabilistic approach whereby the bid-taker has knowledge of the reliability of bidders from which the likelihood of an incomplete transaction may be inferred. As we shall see, this framework can enforce constraints on solutions so that possible breakages are reparable. Section 5 presents an extensive empirical evaluation of the approach presented in this paper, in the context of a number of well-known combinatorial auction distributions, with very encouraging results. Finally, in Section 7 a number of concluding remarks are made. Before presenting the technical details of our solution to the Bid-taker\"s Exposure Problem, we shall present a brief survey of combinatorial auctions and existing techniques for handling bid withdrawal. The following is the integer programming formulation for the WDP: max j=1 pjxj s.t. m}, xj \u2208 {0, 1}. CABOB ) can, in practice, solve very large problems involving thousands of bids very quickly. examined how to handle perturbations given a solution whilst minimizing necessary changes to that solution. They formulated an incremental integer linear program (ILP) that sought to maximize the valuation of the repair solution whilst preserving the previous solution as much as possible. This approach does not make allowances for risk-averse bid-takers who may view a small possibility of very low revenue as unacceptable. Computing the expected revenue for the solution with the first and second bids winning the items, denoted 1, 1, 0 , gives: (200\u00d70.9\u00d70.9)+(2\u00d7100\u00d70.9\u00d70.1)+(190\u00d70.1\u00d70.1) = 181.90. bidders\" valuations for the item may have decreased in a non-linear fashion. 184 Table 1: Example Combinatorial Auction. The expected revenue for 0, 0, 1 is: (190 \u00d7 0.9) + (200 \u00d7 0.1) = 191.00. We can therefore surmise that the second solution is preferable to the first based on expected revenue. The possible loss in revenue for breaks is also not tightly bounded using this approach, therefore a large loss may be possible for a small number of breaks. A risk-averse bid-taker may not tolerate such a possibility, preferring to sacrifice revenue for reduced risk. We regard this as the ideal approach for real-world combinatorial auctions. A robust solution for a combinatorial auction is one where any subset of successful bids whose probability of withdrawal is greater than or equal to \u03b1 can be repaired by reassigning items at a cost of at most \u03b2 to other previously losing bids to form a repair solution. In the following section we describe an ideal constraint-based framework for the establishment of such robust solutions. Its main advantages are its declarative nature and flexibility in tackling problems with arbitrary side constraints. It is a generalization of both fault tolerance in CP and supermodels in propositional satisfiability (SAT) . An (a,b)-super solution is one in which if at most a variables lose their values, a repair solution can be found by changing at most b other variables . In reality, changing some variable assignments in a repair solution incurs a lower cost than others thereby motivating the use of a different metric for determining the legality of repair sets. Inertia is a measure of a variable\"s aversion to change and depends on its current assignment, future assignment and the breakage variable(s). In summary, an (\u03b1,\u03b2)-WSS allows any set of variables whose probability of breaking is greater than or equal to \u03b1 be repaired with changes to the original robust solution with a cost of at most \u03b2. The WSS algorithm attempts to extend the current partial assignment by choosing a variable and assigning it a value. For a more detailed description of the WSS search algorithm, the reader is referred to , since a complete description of the algorithm is beyond the scope of this paper. Similarly, bids 2 and 3 cannot both win so another constraint is added between these two variables. In order to find a robust solution of optimal revenue we seek to maximize the sum of these amounts, max xi\u2208V aixi. Bid1 and bid2 are both assigned to 1 (branches 4.2 and 4.4) and the total cost of both these changes is still 0 because no compensation needs to be paid for bids that change from losing to winning. We continue our search in Figure 1(a) however, because we are seeking a robust solution of optimal revenue. This will enable the reallocation of items and permit the establishment of 1, 1, 0 as a second WSS for this example. We propose a model that incorporates mutual bid bonds to enable solution reparability for the bid-taker, a form of insurance against 186 0 0 0 1 1 1 1 1 1 Insufficient revenue Find repair solution for bid 3 breakage Find partial repair for bid 1 breakage Insufficient revenue (a) Extend partial repair for bid 1 breakage (b) Find partial repair for bid 2 breakage Bid 1 Bid 2 Bid 3 Find repair solutions for bid 1 & 2 breakages 3 4 7 8 Insufficient revenue (a) Search for WSS. 0 0 0 1 1 1 1 1 1 Insufficient revenue Insufficient revenue Bid 1 Bid 2 Bid 3 inertia=0 inertia=0 inertia=0 4.1 4.2 4.3 4.4 4.5 (b) Search for a repair for bid 3 breakage. Figure 1: Search Tree for a WSS without item withdrawal. In the interests of fairness, compensation is paid to bidders from whom items are withdrawn and is equivalent to the penalty that would have been imposed on the bidder should he have withdrawn the bid. If a bidder does not agree to the insurance, it may be inferred that he may have accurately determined the valuation for the items and therefore less likely to fall victim to the winner\"s curse. On the other hand it decreases the reparability of solutions. The penalties paid are used to fund a reassignment of items to form a repair solution of sufficient revenue by compensating previously successful bidders for withdrawal of the items from them. 187 Bid 1 Bid 2 Bid 3 Insufficient revenue inertia=5 =5 inertia=0 =5 inertia=5 =5 6.1 8.1 9.1 9.2 (a) Search for a repair for bid 1 breakage. Bid 1 Bid 2 Bid 3 Insufficient revenue inertia=10 =10 inertia=10 =10 inertia=10 =10 8.2 8.3 9.3 9.4 (b) Search for a repair for bid 2 breakage. Figure 2: Repair Search Tree for breaks 1 and 2, \u03ba = 0.05. Using mutual bid bonds thus increases reparability and allows a robust solution of revenue 200 as opposed to 190, as was previously the case. These distributions were chosen because they describe a broad array of bidding patterns in different application domains. This process is repeated for all winning bids in the overall optimal solution, thus assuming that all bids are brittle. We assumed that all bids in a given XOR bid with the same dummy item were from the same bidder. optimal revenue on average for auctions with more than 250 bids. bids there are occasions when a single bid withdrawal can result in a drop in revenue of over 5%, although the average worst-case drop in revenue is only 1%. These distributions are clearly sensitive to bid withdrawal when no other winning bids in the solution may be involuntarily withdrawn by the bid-taker. A pure CP approach needs to be augmented with global constraints that incorporate operations research techniques to increase pruning sufficiently so that thousands of bids may be examined. s.t. m}, xj \u2265 0, xj \u2208 R. This solver is, in turn, based upon the EFC constraint solver . As the number of bids in the auction increases however, there is an increase in the inherent robustness of solutions so the degree of a variable loses significance as the auction size increases. Thirdly, we allowed backtracking by the bid-taker on winning bids using mutual bid bonds but maintaining the revenue constraint at 90% of optimum. We also implemented a rule stipulating that any losing bids from 189 Table 2: Optimal Solutions that are Inherently Robust (%). # arbitrary-npv repair \u2265 90% 21 5 3 37 93 repair \u2265 85% 26 15 40 87 100 MBB & repair \u2265 90% 41 35 60 94 \u2265 93 regions-npv repair \u2265 90% 30 33 61 91 98 repair \u2265 85% 50 71 95 100 100 MBB & repair \u2265 90% 60 78 96 99 \u2265 98 Table 3: Occurrence of Robust Solutions (%). The MBB auction model performs very well however, and ensures that robust solutions are achievable for such inherently brittle auctions without sacrificing over 10% of optimal revenue to achieve repair solutions. Note that in some instances it is possible for a repair solution to have higher revenue than the original solution. Revenue(%ofoptimum) Bids Repair Revenue: Min 90% Optimal Repair Revenue: Min 85% Optimal MBB: Repair Revenue: Min 90% Optimal (a) regions- Revenue(%ofoptimum) Bids Repair Revenue: Min 90% Optimal Repair Revenue: Min 85% Optimal MBB: Repair Revenue: Min 90% Optimal (b) arbitrary-npv Figure 4: Revenue of optimal robust solutions. have provided such an incremental ILP formulation . It is immediately obvious that such a mechanism can significantly increase revenue whilst still maintaining solution robustness. We can therefore infer that the average number of winning bids in revenuemaximizing robust solutions converges towards that of the optimal overall solutions. Although MBBs aid solution repara190 Table 4: Number of winning bids. # arbitrary-npv Optimal 3.31 5.60 7.17 9.31 10.63 Repair \u2265 90% 1.40 2.18 6.10 9.03 (\u2248 10.63) Repair \u2265 85% 1.65 3.81 6.78 9.31 (10.63) MBB (\u2265 90%) 2.33 5.49 7.33 9.34 (\u2248 10.63) regions-npv Optimal 4.34 7.05 9.10 10.67 12.76 Repair \u2265 90% 3.03 5.76 8.67 10.63 (\u2248 12.76) Repair \u2265 85% 3.45 6.75 9.07 (10.67) (12.76) MBB (\u2265 90%) 3.90 6.86 9.10 10.68 (\u2248 12.76) bility, the number of bids in the solutions increases on average. This is to be expected because a greater fraction of these solutions are in fact optimal, as we saw in Table 2. The perceived reparability of a bid may influence the bid amount, with reparable bids reaching a lower equilibrium point and perceived irreparable bids being more aggressive. For many distributions of costs and estimates, equilibrium bidding is more aggressive with withdrawal if the variability of the estimating distribution is sufficiently large. They assumed all bidders were risk-neutral, but surmised that it is entirely possible for the bid-taker to collect a risk premium from risk-averse bidders with the offer of such insurance. Whilst the WSS framework provides ample flexibility and expressiveness, scalability becomes a problem for larger auctions. The impact of such a constraint on revenue of robust solutions is also left for future work. In this manner the least brittle solution of adequate revenue may be chosen.", "introduction": "A combinatorial auction (CA) provides an efficient means of allocating multiple distinguishable items amongst bidders whose perceived valuations for combinations of items differ. Such auctions are gaining in popularity and there is a proliferation in their usage across various industries such as telecoms, B2B procurement and transportation . Revenue is the most obvious optimization criterion for such auctions, but another desirable attribute is solution robustness. In terms of combinatorial auctions, a robust solution is one that can withstand bid withdrawal (a break) by making changes easily to form a repair solution of adequate revenue. A brittle solution to a CA is one in which an unacceptable loss in revenue is unavoidable if a winning bid is withdrawn. In such situations the bid-taker may be left with a set of items deemed to be of low value by all other bidders. These bidders may associate a higher value for these items if they were combined with items already awarded to others, hence the bid-taker is left in an undesirable local optimum in which a form of backtracking is required to reallocate the items in a manner that results in sufficient revenue. We have called this the Bid-taker\"s Exposure Problem that bears similarities to the Exposure Problem faced by bidders seeking multiple items in separate single-unit auctions but holding little or no value for a subset of those items. However, reallocating items may be regarded as disruptive to a solution in many real-life scenarios. Consider a scenario where procurement for a business is conducted using a CA. It would be highly undesirable to retract contracts from a group of suppliers because of the failure of a third party. A robust solution that is tolerant of such breaks is preferable. Robustness may be regarded as a preventative measure protecting against future uncertainty by sacrificing revenue in place of solution stability and reparability. We assume a probabilistic approach whereby the bid-taker has knowledge of the reliability of bidders from which the likelihood of an incomplete transaction may be inferred. Repair solutions are required for bids that are seen as brittle (i.e. Repairs may also be required for sets of bids deemed brittle. We propose the use of the Weighted Super 183 Solutions (WSS) framework for constraint programming, that is ideal for establishing such robust solutions. As we shall see, this framework can enforce constraints on solutions so that possible breakages are reparable. This paper is organized as follows. Section 2 presents the Winner Determination Problem (WDP) for combinatorial auctions, outlines some possible reasons for bid withdrawal and shows how simply maximizing expected revenue can lead to intolerable revenue losses for risk-averse bid-takers. This motivates the use of robust solutions and Section 3 introduces a constraint programming (CP) framework, Weighted Super Solutions , that finds such solutions. We then propose an auction model in Section 4 that enhances reparability by introducing mandatory mutual bid bonds, that may be seen as a form of leveled commitment contract . Section 5 presents an extensive empirical evaluation of the approach presented in this paper, in the context of a number of well-known combinatorial auction distributions, with very encouraging results. Section 6 discusses possible extensions and questions raised by our research that deserve future work. Finally, in Section 7 a number of concluding remarks are made.", "conclusion": "Fairness is often cited as a reason for choosing the optimal solution in terms of revenue only .. Robust solutions militate against bids deemed brittle, therefore bidders must earn a reputation for being reliable to relax the reparability constraint attached to their bids.. This may be seen as being fair to long-standing business partners whose reliability is unquestioned.. Internet-based auctions are often seen as unwelcome price-gouging exercises by suppliers in many sectors .. Traditional business partnerships are being severed by increased competition amongst suppliers.. Quality of Service can suffer because of the increased focus on short-term profitability to the detriment of the bid-taker in the long-term.. Robust solutions can provide a means of selectively discriminating against distrusted bidders in a measured manner.. As combinatorial auction deployment moves from large value auctions with a small pool of trusted bidders (e.g.. spectrum-rights sales) towards lower value auctions with potentially unknown bidders (e.g.. Supply Chain Management ), solution robustness becomes more relevant.. As well as being used to ensure that the bid-taker is not left vulnerable to bid withdrawal, it may also be used to cement relationships with preferred, possibly incumbent, suppliers.. 191 We have shown that it is possible to attain robust solutions for CAs with only a small loss in revenue.. We have also illustrated how such solutions tend to have fewer winning bids than overall optimal solutions, thereby reducing any overheads associated with dealing with more bidders.. We have also demonstrated that introducing mutual bid bonds, a form of leveled commitment contract, can significantly increase the revenue of optimal robust solutions by improving reparability.. We contend that robust solutions using such a mechanism can allow a bid-taker to offer the possibility of bid withdrawal to bidders whilst remaining confident about postrepair revenue and also facilitating increased bidder aggressiveness."}
{"id": "C-4", "keywords": ["voic over ip", "loss sensit", "loss conceal", "loss metric", "object speech qualiti measur", "queue manag", "differenti servic"], "title": "Intra-flow Loss Recovery and Control for VolP", "abstract": "Best effort packet-switched networks, like the Internet, do not offer a reliable transmission of packets to applications with real-time constraints such as voice. Thus, the loss of packets impairs the application-level utility. For voice this utility impairment is twofold: on one hand, even short bursts of lost packets may decrease significantly the ability of the receiver to conceal the packet loss and the speech signal playout is interrupted. On the other hand, some packets may be particular sensitive to loss as they carry more important information in terms of user perception than other packets. We first develop an end-to-end model based on loss runlengths with which we can describe the loss distribution within a flow. These packet-level metrics are then linked to user-level objective speech quality metrics. Using this framework, we find that for low-compressing sample-based codecs (PCM) with loss concealment isolated packet losses can be concealed well, whereas burst losses have a higher perceptual impact. For high-compressing frame-based codecs (G.729) on one hand the impact of loss is amplified through error propagation caused by the decoder filter memories, though on the other hand such coding schemes help to perform loss concealment by extrapolation of decoder state. Contrary to sample-based codecs we show that the concealment performance may \"break\" at transitions within the speech signal however. We then propose mechanisms which differentiate between packets within a voice data ow to minimize the impact of packet loss. We designate these methods as \"intra-flow\" loss recovery and control. At the end-to-end level, identification of packets sensitive to loss (sender) as well as loss concealment (receiver) takes place. Hop-by-hop support schemes then allow to (statistically) trade the loss of one packet, which is considered more important, against another one of the same flow which is of lower importance. As both packets require the same cost in terms of network transmission, a gain in user perception is obtainable. We show that significant speech quality improvements can be achieved and additional data and delay overhead can be avoided while still maintaining a network service which is virtually identical to best effort in the long term.", "references": ["Understanding end-to-end internet traffic dynamics", "Adaptive FEC-based error control for interactive audio in the Internet", "Explicit allocation of best effort packet delivery service", "Low bit-rate speech coders for multimedia communication", "Low bit-rate speech coders for multimedia communication", "QoS measurement of Internet real-time multimedia services", "RTP payload for redundant audio data", "Simulation of FEC-based error control for packet audio on the nternet", "729 error recovery for Internet Telephony", "Integrating packet FEC into adaptive voice playout buffer algorithms on the Internet", "Integrating packet FEC into adaptive voice playout buffer algorithms on the Internet", "Packet Loss Recovery and Control for Voice tinsmission over the Internet", "Packet Loss Recovery and Control for Voice tinsmission over the Internet", "Efficient QoS support for Voice-over-IP applications using selective packet marking", "Loss correlation for queues with bursty input streams", "LDAf TCP-friendly adaptation: A measurement and comparison study", "Coding of speech at 8 kbit/s using conjugate-structure algebraic-code-excited linear-prediction (CS-ACELP)", "Objective oualitv measurement of telephone-band (300-3400 Hz) speech codecs", "Towsley. Packet loss correlation in the MBone multicast network: Experimental measurements and markov chain models", "Measurement and modelling of the temporal dependence in packet loss", "Improvement of MBSD scaling noise masking threshold and correlation analysis with MOS difference instead of MOS"], "full_text": "1. INTRODUCTION Considering that a real-time flow may experience some packet loss, the impact of loss may vary significantly dependent on which packets are lost within a flow. In the following we distinguish two reasons for such a variable loss sensitivity: Temporal sensitivity: Loss of ADUs\" which is correlated in time may lead to disruptions in the service. Note that this effect is further aggravated by some interdependence between ADUs (i.e., that one ADU can only be decoded when a previous ADU before has successfully been received and decoded). For voice, as a single packet contains typically several ADUs (voice frames) this effect is thus more significant than e.g. for video. It translates basically to isolated packet losses versus losses that occur in bursts. Sensitivity due to ADU heterogeneity: Certain ADUs might contain parts of the encoded signal which are \u2018Application Data Unit: the unit of data emitted by a source coder such as a video or voice frame. 441 Figure 1: Schematic utility functions dependent on the loss of more (+l) and less (-1) important packets more important with regard to user perception than others of the same flow. Let us consider a flow with two frame types of largely different perceptual importance (we assume same size, frequency and no interdependence between the frames). Under the loss of 50% of the packets, the perceptual quality varies hugely between the case where the 50% of the frames with high perceptual importance are received and the tax where the 50% less important frames are received. Network support for real-time multimedia flows can on one hand aim at offering a lossless service, which, however, to be implemented within a pa&et-switched network, will be costly for the network provider and thus for the user. On the other hand, within a lossy service, the above sensitivity constraints must be taken into account. It is our strong belief that this needs to be done in a generic way, i.e., no application-specific knowledge (about particular coding schemes e.g.) should be necessary within the network and, vice versa, no knowledge about network specifics should be necessary within an application. Let us now consider the case that 50% of packets of a flow are identified as more important (designated by +l) or less important (-1) due to any of the above sensitivity constraints. Figure 1 a) shows a generic utility function describing the applicationlevel Quality of Service (QoS) dependent on the percentage of packets lost. For real-time multimedia traffic, such utility should correspond to perceived video/voice quality. If the relative importance of the packets is not known by the transmission system, the loss rates for the +l and -1 packets are equal. Due to the over-proportional sensitivity of the +l packets to loss as well as the dependence of the end-t* end loss recovery performance on the fl packets, the utility function is decreasing significantly in a non-linear way (approximated in the figure by piece-wise linear functions) with an increasing loss rate. Figure 1 b) presents the CBS~ where all +l packets are protected at the expense of -1 pa&&. The decay of the utility function (for loss rates < 50%) is reduced, because the +l packets are protected and the endto-end loss recovery can thus operate properly over a wider range of loss rates indicated by the shaded area. This results in a graceful degradation of the application\"s utility. Note that the higher the non-linearity of the utility contribution of the fl packets is (deviation from the dotted curve in Fig. 1 a), the higher is the potential gain in utility when the protection for +l packets is enabled. Results for actual perceived quality as utility for multimedia applications exhibit such a non-linear behavior*. To describe this effect and provide a taxonomy for different QoS enhancement approaches, we introduce a novel terminology: we designate mechanisms which influence QoS parameters between Bows (thus decrease the loss rate of one flow at the expense of other flows) as inter-flow QoS. Schemes which, in the presence of loss, differentiate between pa&& within a flow as demonstrated in Figure 1 above, provide intra-flow QoS enhancement. As QoS mechanisms have to be implemented within the network (hopby-hop) and/or in the end systems (end-to-end), we have another axis of classification. The adaptation of the sender\"s bitrate to the current network congestion state as an intraflow QoS scheme (loss avoidance, [IS]) is difficult to apply to voice. Considering that voice flows have a very low bitrate, the relative cost of transmitting the feedback information is high (when compared e.g. to a video flow). To reduce this cost the feedback interval would need to be increased, then leading to a higher probability of wrong adaptation decisions. The major di6culty, however, is the lack of a codec which is truly scalable in terms of its output bitrate and corresponding perceptual quality. Currently standardized voice codecs usually only have a 6xed output bitrate. While it has been proposed to switch between voice codeca , the MOS (subjective quality) values for the codecs employed do not differ much: e.g., theITU codecs G.723.1, G.729, G.728, G.726and G.711 cover a bitrate range from 5.3 kbitjs to 64 kbitjs while the subjective quality differs by less than 0.25 on a l-to-5 MOS scale (, 1: bad, 5: excellent quality). So when the availability of sutticient computing power is assumed, the lowest bitrate codec can be chosen permanently without actually decreasing the perceptual quality. For loss recovery on an end-to-end basis, due to the realtime delay constraints, open-loop schemes like Forward Error Correction (FEC) have been proposed . While such schemes are attractive because they can be used on the Internet today, they also have several drawbacks. The amount of redundant information needs to be adaptive to avoid taking bandwidth away from other flows. This adaptation is crucial especially when the fraction of traflic using redundancy schemes is large (181). If the redundancy is a source coding itself, like it has often been proposed , the comments from above on adaptation also apply. Using redundancy has also implications to the playout delay adaptation ([lo]) employed to de-jitter the packets at the receiver. Note that the presented types of loss sensitivity also apply to ap\u2018While we have obtained results which confirm the shape of the overall utility curve shown in Fig. 1, clearly the utility functions of the +1/-l sub.flows and their relationship are more complex and only approximately additive. 442 Table 1: State and transition probabilities computed for an end-to-end Internet trace using a general Markov model (third order) by Yajnik et. al. state 000 001 010 011 100 101 110 111 Probability of Probability Probability being in the state of I(s)=0 of l(s)= 0 . 0 . 0 2 0 8 0 . 6 1 1 2 0 . 3 8 8 8 0 . 0 1 4 2 0 . 0 . 0 1 0 2 0 . 2 7 1 0 0 . 7 2 9 0 0 . 0 2 0 8 0 . 9 2 7 8 0 . 0 7 2 2 0 . 0 0 3 6 0 . 4 1 9 8 0 . 5 8 0 2 0 . 0 1 0 2 0 . 0 . plications which are enhanced by end-to-end loss recovery mechanisms. End-to-end mechanisms can reduce and shift such sensitivities but cannot come close to eliminate them. Therefore in this work we assume that the lowest possible bitrate which provides the desired quality is chosen. Neither feedback/adaptation nor redundancy is used, however, at the end-to-end level, identification/marking of packets sensitive to loss (sender) as well as loss concealment (receiver) takes place. Hop-by-hop support schemes then allow trading the loss of one packet, which is considered more important, against another one of the same flow which is of lower importance. We employ actual codecs and measure their utility in the presence of packet loss using objective speech quality measurement. The paper is structured as follows: Section 2 introduces packet- and user-level metrics. We employ these metrics to describe the sensitivity of VoIP traffic to packet loss in section 3. Section 4 briefly introduces a queue management algorithm which can be used for intra-flow loss control. In section 5, we present results documenting the performance of the proposed mechanisms at both the end-to-end and hopby-hop level. Section 6 concludes the paper. 2. METRICS 2.1 Packet-level metrics A general Markov model ([19, S]) which describes the loss process is defined as follows: Let P( Z(s) ] Z(s - m),... ,Z(s - 2),Z(s - 1) ) be the state transition probability of a general Markov model of order m, where Z(s) is the loss indicator function for the packet with the sequence number s. All combinations for the values (0 and 1) of the sequence Z(s-m), . . . , Z(s-2),Z(s-1) appear in the state space. As an example P( Z(s) = 1 ] Z(s - 2),Z(s1) = 01 ) gives the state transition probability when the current packet s is lost, the previous packet s - 1 has also been lost and packet s - 2 has not been lost. The number of states of the model is 2. Two state transitions can take place from any of the states. Thus, the number of parameters which have to be computed is 2m+1. Even for relatively small m this amount of parameters is difficult to be evaluated and compared. Table 1 shows some values for the state and transition probabilities for a general Markov model of third order measured end-to-end in the Internet by Yajnik et. al. . It is interesting to note that for all states with Z(s - 1) = 0 the probability for the next packet not to be lost (Z(s) = 0) is generally very high (> 0.8, in bold typeface) whereas when Z(s- 1) = 1 the state transition probabilities to that event cover the range of 0.15 to 0.61. That means that past no loss events do not affect the loss process as much as past loss events. Intuitively this seems to make sense, because a successfully arriving packet can be seen as a indicator for congestion relief. Andren et. al. ([l]) as well as Yajnik et. al. both confirmed this by measuring the cross correlation of the loss- and no-loss-runlengths. They came to the result that such correlation is very weak. This implies that patterns of short loss bursts interspersed with short periods of successful packet arrivals occur rarely (note in this context that in \u2018&able 1 the pattern 101 has by far the lowest state probability). Thus, in the following we employ a model which only considers the past loss events for the state transition probability. The number of states of the model can be reduced from 2m to m + 1. This means that we only consider the state transition probability P( Z(s) ] Z(s - k), . . . ,Z(s - 1) ) with Z(s - Ic + i) = 1 V i E [0, Ic - 11, where Ic (0 < k 5 m) is a variable parameter. We define a loss run length lc for a sequence of Ic consecutively lost packets detected at sj (sj > Ic > 0) with Z(sj - k - 1) = O,Z(sj) = 0 and Z(sj - k + i) = 1 V i E [0, Ic - 11, j being the j-th \u2018<burst loss event. Note that the parameters of the model become independent of the sequence number s and can now rather be described by the occurence ok of a loss run length k. We define the random variable X as follows: X = 0: no packet lost, X = k (0 < lc < m): ezactly lc consecutive packets lost, and X 2 k (0 < k < m): at least k consecutive packets lost. With this definition, we establish a loss run-length model with a finite number of states (m + 1) which gives loss probabilities dependent on the burst length. In the model, for every additional lost packet which adds to the length of a loss burst a state transition takes place. If a packet is successfully received, the state returns to X = 0. Thus, the state probability of the system for 0 < Ic < m is P(X 2 Zc). Due to the limited memory of the system, the last state X = m is just defined as crm consecutive packets lost, with P(X = m) being the state probability. Given the case of a finite number of packets a for a flow, which experiences d = cpC1 ]cok packet drops, we have the relative frequency pL,k = 2 (P(X = k) for a + 00) for the occurence of a loss burst of length lc. An approximation for the expectation of the random variable X can be computed as pi = cr==, #+L,k and identified with the mean loss rate. We can also approximate the state probabilities of the model by the cumulative loss rate p~,~,,~(k) = CrSkp~,,, (0 < k < m) and p~,~ = Cr.p=, (n--mzl)ot (k = m). The transition probabilities for 1 < lc < m can be computed easily as: p(k--l)(k) = P(X>klX>k-l)= P(X>knX>k-1) P(X>k-1) ZZ P ( X > k ) P(X>k-1) These conditional loss probabilities again can be approxPzl,cum(k) _ Cnm,k on i m a t e d by PL,cond@) = PL,cum(k-l) - C;Ek--l 0,. For 443 Figure 2: Loss run-length model with m + 1 states P(X = m]X = m) we have: PL,,,d(rn) = %Lf;:P Fig. 2 shows the Markov chain for the model. (1) Additionally, we also define a random variable Y which de scribes the distribution of burst loss lengths with respect to the bunt loss events j (and not to packet events like in the definition of X). We have the burst loss length rate ga = * as an estimate for P(Y = k). Thus, the mean burst loss length is g = h = e = CT==, kgb corresponding to E[Y] (average loss gap, ). The run-length model implies a geometric distribution for residing in state X = m. For the probability of a burst loss length of k packets we thus can compute estimates for performance parameters in a higher order model representation (note that here Y represent the random variable used in the higher-order models). For a three state model we have e.g. for P(Y = k): P(Y = k) = P(Y=k)=l-pm: k=l pn p;;\" (1 -pm): k 2 2 c-4 For the special case of a system with a memory of only the previous packet (m = l), we can use the runlength distribution for a simple computation of the parameters of the commonly-used Gilbert model (Fig. 3) to characterize the loss process (X being the associated random variable with X = 0: no packet lost, X = 1 a packet lost). Then the probability of being in state m can be seen BS the uncow ditional loss probability ulp and approximated by the mean loss (pr. = pi,,,,). Only one conditional loss probability clp for the transition 1 --t 1 exists: p~,E-d = CFL(~ - lbd (3) If losses of one flow are correlated (i.e., the loss probability of an arriving packet is itiuenced by the contribution to the state of the queue by a previous packet of the same flow and/or both the previous and the current packet see busty arrivals of other traffic, ) we have pal < clp and thus ulp 5 clp. For pal = elp the Gilbert model is equivalent to a l-state (Bernotilli) model with zllp = elp (no loss correlation). As in equation 2 we can compute an estimate for the probability of a burst loss length of k packets for a higher order model representation: B(Y = k) = clpk-\u2018(1 - clp) (4) Figure 3: Loss run-length (Gilbert model) model with two states Figure 4: Components of the end-to-end loss recovery/control measurement setup. 2.2 Objective Speech Quality Metrics Unlike the conventional methods like the Signal-t-Noise Ratio (SNR), novel objective quality measures attempt to estimate the subjective quality as closely as possible by modeling the human auditory system. In our evaluation we use two objective quality measures: the Enhanced Mod&d Bark Spectral Distortion (EMBSD, ) and the Measw ing Normalizing Blocks (MNB) described in the Appendix II of the ITU-T Recommendation P.861 . These two objective quality measure.~ are reported to have a very high correlation with subjective tests, their relation to the range of subjective test result values (MOS) is close to being linear and they are recommended as being suitable for the evaluation of speech degraded by transmission errors in real network environments such as bit errors and frame erasures ([18, 211). Both metrics are distance measures, i.e., a IB suit value of 0 implies perfect quality, the larger the value, the worse the speech quality is (in Fig. 5 we show an axis with approximate corresponding MOS values). For all simulations in this paper we employed both schemes. As they yielded very similar results (though MNB results generally exhibited less variability) we only present EMBSD results. 3. VOIP LOSS SENSITIVITY Figure 4 shows the components of the measurement setup which we will use to evaluate our approach to combined end-t-end and hopby-hop loss recovery and control. The 444 shaded boxes show the components in the data path where mechanisms of loss recovery are located. Together with the parameters of the network model (section 2.1) and the perceptual model we obtain a measurement setup which allows us to map a specific PCM signal input to a speech quality measure. While using a simple end-to-end loss characterization, we generate a large number of loss patterns by using different seeds for the pseudcerandom number generator (for the results presented here we used 300 patterns for each simulated condition for a single speech sample). This procedure takes thus into account that the impact of loss on an input signal may not be homogenous (i.e., a loss burst within one segment of that signal can have a different perceptual impact than a loss burst of the same size within another segment). By averaging the result of the objective quality measure for several loss patterns, we have a reliable indication for the performance of the codec operating under a certain network loss condition. We employed a Gilbert model (Fig. 3) as the network model for the simulations, as we have found that here higher order models do not provide much additional information. 3.1 Temporal sensitivity 3.1.1 Pa4 We first analyze the behaviour for ~-law PCM flows (64 kbit/s) with and without loss concealment, where the loss concealment repairs isolated losses only (speech stationarity can only be assumed for small time intervals). Results are shown for the AP/C concealment algorithm ([ll]). Similar results were obtained with other concealment algorithms. Figure 5 shows the case without loss concealment enabled where Gilbert model parameters are varied. The resulting speech quality is insensitive to the loss distribution parameter (elp). The results are even slightly decressing for an increasing clp, pointing to a significant variability of theresuits. In Figure 6 the results with loss concealment are depicted. When the loss correlation (dp) is low, loss concealment provides a significant performance improvement. The relative improvement increases with increasing loss (pulp). For higher clp values the cases with and without concealment become increasingly similar and show the same performance at clp x 0.3. Figures 5 and 6 respectively also contain a curve showing the performance under the assumption of random losses (Bernouilli model, ulp = elp). Thus, considering a given ulp, a worst case loss pattern of alternating losses (I(s mod 2) = l,l([s + l] mod 2) = 0) would enable a considerable performance improvement (with ok = OVk > 1: p~,cond = 0, Eq. 3). As we found by visual inspection that the distributions of the perceptual distortion values for one loss condition seem to approximately follow a normal distribution we employ mean and standard deviation to describe the statistical variability of the measured values. Figure 7 presents the perceptual distortion as in the previous figure but also give the standard deviation as error bars for the respective loss condition. It shows the increasing variability of the results with increasing loss correlation (clp), while the variability does not seem to change much with an increasing amount of loss (alp). On one hand this points to some, though weak, sensitivity with regard to heterogeneity (i.e., it matters which area of the speech signal (voiced/unvoiced) experiences a burst loss). On the other hand it shows, that a large number of different Figure 5: Utility curve for PCM without loss conc e a l m e n t (EMBSD) Figure 6: Utility curve for PCM with loss concealment (EMBSD) Figure 7: Variability of the perceptual distortion with 10s~ concealment (EMBSD) 445 Figure 8: Utility curve for the G.729 codec (EMBSD) loss patterns is necessary for a certain speech sample when using objective speech quality measurement to assess the impact of loss corwlation on user perception. 3.1.2 G.729 G.729 uses the Conjugate Structure Algebraic Code Excited Linear Prediction (CS-ACELP) coding scheme and ooerates at 8 kbit/s. In G.729. a speech fmme is 10 ms in d&ion. For each frame, the\"G.7i9 encoder analyzes the input data and extracts the parameters of the Code Excited Linear Prediction (CELP) model such as linear prediction filter coefficients and excitation vectors. When a frame is lost or corrupted, the G.729 decoder uses the parameters of the previous frame to interpolate those of the lost frame. The line spectral pair coefficients (LSP3) of the last good frame are repeated and the gain coefficients are taken from the previous frame but they are damped to gradually reduce their impact. When a frame loss occurs, the decoder cannot update its state, resulting in a divergence of encoder and decoder state. Thus, errors are not only introduced during the time period represented by the current frame but also in the following ones. In addition to the impact of the missing codewords, distortion is increased by the missing update of the predictor filter memories for the line speo tral pairs and the linear prediction synthesis filter memo ries. Figure 8 shows that for similar network conditions the output quality of the G.729* is worse than PCM with loss concealment, demonstrating the compression versus quality tradeoff under packet loss. Interestingly the loss correlation (dp parameter) has some impact on the speech quality, however, the effect is weak pointing to a certain robustness of the G.729 codec with regard to the resiliency to consecutive packet losses due to the internal loss concealment. Rosenberg has done a similar experiment , showing that the difference between the original and the concealed signal with increasing loss burstiness in terms of a mean-squared error is significant, however. This demonstrates the importance of perceptual metrics which are able to include concealment \u2018LSPs are another representation of the linear prediction coefficients. \u2018Two G.729 frames are contained in a packet. Figure 9: Resynchronization time (in frames) of the G.729 decoder after the loss of k consecutive frames (k E ) as a function of frame position. (and not only reconstruction) operations in the quality assessment. 3.2 Sensitivity due to ADU heterogeneity PCM is a memoryless encoding. Therefore the ADU content is only weakly heterogeneous (Figure 7). Thus, in this section we concentrate on the G.729 coda. The experiment we carry out is to meawre the resynchronization time of the d* coder after k consecutive frames are lost. The G.729 decoder is said to have resynchronized with the encoder when the energy of the error signal falls below one percent of the energy of the decoded signal without frame loss (this is equivalent to a signal-t-noise ratio (SNR) threshold of 2OdB). The error signal energy (and thus the SNR) is computed on a per-frame basis. Figure 9 shows the resynchronization time (expressed in the number of frames needed until the threshold is exceeded) plotted against the position of the loss (i.e., the index of the first lost frame) for d&rent values of k. The speech sample is produced by a male speaker where an unvoiced/voiced (au) transition occurs in the eighth frame. We can see from Figure 9 that the position of a frame loss has a signilicant inlluence cm the resulting signal degradation\", while the degradation is not that sensitive to the length of the frame loss burst k. The loss of unvoiced frames seems to have a rather small impact on the signal degradation and the decoder recovers the state information fast thereafter. The loss of voiced frames causes a larger degradation of the speech signal and the decoder needs more time to resyncbronize with the sender. However, the loss of voiced frames at an unvoiced/voiced transition leads to a significant degradation of the signal. We have repeated the experiment for different male and female speakers and obtained similar results. lsking into account the wed coding scheme, the above phenomenon could be explained as follows: Because voiced sounds have a higher energy than unvoiced sounds, the loss of voiced frames causes a larger signal degradation than the loss of unvoiced frames. However, due to the periodic property of voiced sounds, the decoder can conceal \u2018While on one hand we see that SNR n~easu~es often do not correlate well with subjective speech quality, on the other hand the large differences in the SNR-threshold-based resynchronization time clearly point to a significant impact on subjective speech quality. 446 Figure 10: Differential RED drop probabilities as a function of average queue sizes the loss of voiced frames well once it has obtained suflicient information on them. The decoder fails to conceal the loss of voiced frames at an unvoiced/voiced transition because it attempts to conceal the loss of voiced frames using the filter coefficients and the excitation for an unvoiced sound. Moreover, because the G.729 encoder uses a moving average filter to predict the values of the line spectral pairs and only transmits the difference between the real and predicted vaues, it takes a lot of time for the decoder to resynchronize with the encoder once it has failed to build the appropriate linear prediction filter. 4. QUEUE MANAGEMENT FOR INTRAFLOW LOSS CONTROL While we have highlighted the sensitivity of VoIP traffic to the distribution of loss in the last sections, we now want to briefly introduce a queue management mechanism which is able to enforce the relative preferences of the ap plication with regard to loss. We consider flows with packets marked with +l and \u2018I1 (as described in the introduction) BS foreground traffic (FT) and other (best effort) flows as backaound traffic (BT). Packet marking, in addition to keeping the desirable property of state aggregration within the network core as proposed by the IETF Differentiated Services architecture, is exploited here to convey the intrbflow requirements of a llow. As it should be avoided to introduce reordering of the packets of a flow in the network we consider mechanisms for the management of a single queue with different priority levels. One approach to realize inter-flow service differentiation using a single queue is RIO (\u2018RED with IN and OUT\", ). With RIO, two average queue sizes as congestion indicators are computed: one just for the IN (high priority) packets and another for both IN and OUT (low priority) packets. Packets marked as OUT are dropped earlier (in terms of the average queue size) than IN packets. RIO has been designed to decrease the clip seen by particular Bows at the expense of other flows. In this work, however, we want to keep the ulp as given by other parameters while modifying the loss distribution for the foreground traffic (FT). This amounts to trading the loss of a +l packet against a -1 packet of the same flow (in a statistical sense). Fig. 10 shows the conventional RED drop probability curve (po as a function of the average queue size for all arrivals avg), which is applied to all unmarked (0) traffic (background traffic: BT). The necessary relationship between the drop probabilities for packets marked as -1 and +l can be derived 85 follows (note that this relationship is valid both at the end-tend level and every individual hop): Let a = a0 + a+, +a-~ be the overall number of emitted packets by an FT flow and a,, + E [-l,O, +l] be the number of packets belonging to a certain class (where the 0 class corresponds to (unmarked) best effort traflic). Then, with a+, = a-1 = a,~, and considering that the resulting service has to be best effort in the long term, we have: aopo + a+lp+l + LIP-1 A ape Qlll@+l fP-1) = (a--oo)m P - l = zpo-p+1 (5) Due to this relationship between the drop probability for +l and -1 packets, we designate this queue management algorithm as Differential RED (DiERED). Figure 10 shows the corresponding drop probability curves. Due to the condition of a+, = (I-L = all, in addition to the conventional RED behaviour, the DiffRED implementation should also monitor the +l and -1 arrival processes. If the ratio of +l to -1 packets at a gateway is not 1 (either due to misbehaving flows or a significant number of flows which have already experienced loss at earlier hops) the -1 loss prob ability is decreased and the +l probability is increased at the same time thus degrading the service for all users. The shaded areas above and below the po(avg) curve (Fig. 10) show the operating area when this correction is added. In it has been shown that using only the conventional RED average queue size avg for DSRED operation is not sufficient. This is due to the potentially missing correlation of the computed aug value between consecutive +l and -1 arrivals, especially when the share of the FT traftic is low. As this might result in a unfair distribution of losses between the FT and BT fractions, a specific avgl value is computed by sampling the queue size only at FT arrival instants. Thus, a service differentiation for foreground traffic is possible which does not differ from conventional RED behaviour in the long term average (i.e., in the ulp). 5. INTRA-FLOW LOSS RECOVERY AND CONTROL 5.1 Temporal sensitivity Considering a flow with temporal loss sensitivity, paragraph 3.1.1 has shown that a simple, periodic loss pattern enhances the performance of the end-to-end loss recovery. The pattern is not tied to particular packets, therefore a per-flow characterization with the introduced metrics is applicable. In this paragraph we assume that a flow expressed its temporal se*sitivity by marking its flow with an alternating pattern of ,c+l,>,\u2018c~l,,, 447 Figure 11: Comparison of actual and estimated Figure 12: Comparison of actual and estimated burst loss length rates as a function of burst length burst, loss length rates as a function of burst length k: three state run-length-based model k: two state run-length-based model (Gilbert.) Figures 11 and 12 show the rates for the actual and the estimated burst loss lengths for a three-state (m = 2) and a two=state (m = 1, Gilbert) model respectively6. We can observe that DiffRED shapes the burst probability curve in the desired way. Most of the probability mass is concentrated at, isolated losses (the ideal behaviour would be the occurence of only isolated losses (Ic = 1) which can be expressed with clp = 0 in terms of Gilbert model parameters). With Drop Tail an approximately geometrically decreasing burst loss probability with increasing burst length (Eq. 4) is obtainable, where the clp parameter is relatively large though. Thus, considering voice with temporal loss sensitivity as the foreground traffic of interest, with DifFRED a large number of short annoying bursts can be traded against a larger number of isolated losses and few long loss bursts (which occur when the queue is under temporary overload, i.e., awg > maxth, Fig. 10). We can see that the three-state model estimation (Eq. 2) reflects the two areas of the DifFRED operation (the sharp drop of the burst loss length rate for k = 2 and the decrease along a geometrically decreasing asymptote for k > 2). This effect cannot be captured by the two state model (Eq. 4) which thus overestimates the burst loss length rate for Ic = 2 and then hugely underestimates it for k > 2. Interestingly, for Drop Tail, while both models capture the shape of the actual curve, the lower order model is more accurate in the estimation. This can be explained as follows: if the burst loss length probabilities are in fact close to a geometrical distribution, the estimate is more robust if all data is included (note that the run-length based approximation of the conditional loss probability P(X = mlX = m) only includes loss run-length occurences larger or equal to m: Eq. 1). sWe only discuss the results qualitatively here to give an example how an intra-flow loss control algorithm performs and to show how loss models can capture this performance. Details on the simulation scenario and parameters can be found in . 5.2 Sensitivity due to ADU heterogeneity In paragraph 3.1.2 we have seen that sensitivity to ADU heterogeneity results in a certain non-periodic loss pattern. Thus, a mechanism at (or near) the sender is necessary which derives that pattern from the voice data. Furthermore, an explicit cooperation between end-to-end and hop by-hop mechanisms is necessary (Fig. 4). We use the result of paragraph 3.2 to develop a new packet marking scheme called Speech Property-Based Selective Differential Packet Marking (SPB-DIFFMARK). The DIFFMARK scheme concentrates the higher priority packets on the frames essential to the speech signal and relies on the decoder\"s concealment for other frames. Figure 13 shows the simple algorithm written in a pseudocode that is used to detect an unvoiced/voiced (uw) transition and protect the voiced frames at, the beginning of a voiced signal. The procedure analysis0 is used to classify a block of Ic frames as voiced, unvoiced, or uv transition. send0 is used to send a block of Ic frames as a single packet with the appropriate priority (either fl, 0 or -1). As the core algorithm gives only a binary marking decision (protect the packet or not), we employ a simple algorithm to send the necessary -1 packets for compensation (Eq. 5): after a burst of +l packets has been sent, a corresponding number of -1 packets is sent immediately. State about the necessary number of to-be-sent -1 packets is kept in the event that the SPB algorithm triggers the next +l burst before all -1 packets necessary for compensation are sent. Thus, seen over time intervals which are long compared to the +1/-l burst times, the mean loss for the flow will be equal to the best effort case (Eq. 5). N is a pre-defined value and defines how many frames at the beginning of a voiced signal are to be protected. Our simulations (Fig. 9) have shown that the range from 10 to 20 are appropriate values for N (depending on the network loss condition). In the simulations presented below, we choose Ic = 2, a typical 448 protect = 0 fcm?ach (k frames) classify = analysis(k frames)] if (protect > 0) if (classify == unvoiced) protect = 0 if (compensation > 0) compensation = compensation-k send(k frames, -1) else send(k frames, 0) endif else send(k frames, +l) protect = protect-k compensation = compensation+k endif else if (classify == uvfransition) send(k frames, +l) protect = N - k compensation = compensation+k else if (compensation > 0) compensation = compensation-k send(k frames, -1) else send(k frames, 0) endif endif endif endfor Figure 13: SPB-DIFFMARK Pseudo Code value for interactive speech transmissions over the Internet (20ms of audio data per packet). A larger number of Ic would help to reduce the relative overhead of the protocol header but also increases the packetization delay and makes sender classification and receiver concealment in case of packet loss (due to a larger loss gap) more difficult. 5.2.1 En.d-to-end simulation description Due to the non-periodic loss pattern, we need to explicitly associate a drop probability with a single packet within an end-to-end model. Therefore we use a separate one-state Markov model (Bernouilli model) to describe the network behaviour as seen by each class of packets. Best effort packets (designated by 0 in Fig. 14) are dropped with the probability pc, whereas packets marked with +l and -1 are dropped with probabilities of p+i and p-1 respectively. This is a reasonable assumption\" with regard to the interdependence of the different classes in fact, as it has been shown that DiffRED (Figs. 11 and 12) achieves a fair amount of decorrelation of +l and -1 packet losses. Nevertheless to include some correlation between the classes we have set p+i = 10m3 pc for the subsequent simulations. This should \u2018The appropriateness of the simple end-to-end modeling used has been investigated in with discrete event simulation using a multi-hop topology and detailed modeling of foreground and background traffic sources. Marking Scheme Network Model N O M A R K lolo/ 0~0~0~ 01 F U L L M A R K +I +I +I +I +l +I SPB MARK 1 0 1+I j +I I 0 I 0 / 0 ALT MARK o/+11 Ol+ll o/+1 +I Pcl 330 po S P B D I F F M A R K 1 IJ / +l I +l I -1 j-1 / 0 I ALT DIFFMARK I-1 / +I 1-1 1+I /-I 1+I 1 Figure 14: Marking schemes and corresponding network models. also allow a reasonable evaluation of how losses in the fl class affect the performance of the SPB-algorithms. For a direct comparison with SPB-DIFFMARK, we evaluate a scheme where packets are alternatingly marked as being either -1 or +l (ALT-DIFFMARK, Figure 14). We furthermore include related inter-flow loss protection schemes. The first scheme uses full protection (FULL MARK, all packets are marked as +l). The SPB-MARK scheme operates similarly to SPB-DIFFMARK, but no -1 packets are sent for compensation (those packets are also marked as 0). For comparison we again use a scheme where packets are alternatingly marked as being either 0 or +l (ALT-MARK). Finally, packets of pure best effort flows are dropped with the probability po (NO MARK case in Fig. 14). For the SPB marking schemes the percentage of +l- and -l-marked packets respectively is 40.4% for the speech material used. We obtained similar marking percentages for other speech samples. The ALT marking schemes mark exactly 50% of their packets as being fl. 5.2.2 Results Figure 15 shows the perceptual distortion for the marking schemes dependent on the drop probability pc. The unprotected case (NO MARK) has the highest perceptual distortion and thus the worst speech quality*. The differential marking scheme (SPB-DIFFMARK) offers a significantly better speech quality even when only using a network service which amounts to best effort in the long term. Note that the ALT-DIFFMARK marking strategy does not differ from the best effort case (which confirms the result of paragraph 3.1.2). SPB-DIFFMARK is also even better than the inter-flow QoS ALT-MARK scheme, especially for higher values of pe. These results validate the strategy of our SPB marking schemes that do not equally mark all packets with a higher priority but rather protect a subset of frames that are essential to the speech quality. The SPB-FEC scheme , *We have also perfo rmed informal listening tests which confirmed the results using the objective metrics. 449 Figure 15: Perceptual Distortion (EMBSD) for the marking schemes and SPB-FEC which uses redundancy9 piggybacked on the main payload packets ( ) to protect a subset of the packets, enables a very good output speech quality for low loss rates. However, it should be noted that the amount of data sent over the network is increased by 40.4%. Note that the simulation presumes that this additionally consumed bandwidth itself does not contribute significantly to congestion. This assumption is only valid if a small fraction of trafhc is voice ([S]). The SPB-FEC curve is convex with increasing po, as due to the increasing loss correlation an increasing number of consecutive packets carrying redundancy are lost leading to unrecoverable losses. The curve for SPB-DIFFMARK is concave, however, yielding better performance for pe & 0.22. The inter-flow QoS ALT-MARK scheme (50% of the packets are marked) enhances the perceptual quality. However, the auditory distance and the perceptual distortion of the SPB-MARK scheme (with 40.4% of all packets marked) is significantly lower and very close to the quality of the decoded signal when all packets are marked (FULL MARK). This also shows that by protecting the entire flow only a minor improvement in the perceptual quality is obtained. The results for the FULL MARK scheme also show that, while the loss of some of the +l packets has some measurable impact, the impact on perceptual quality can still be considered to be very low. 6. CONCLUSIONS In this paper we have characterized the behaviour of a samplebased codec (PCM) and a frame-based codec (G.729) in the presence of packet loss. We have then developed intraflow loss recovery and control mechanisms to increase the perceptual quality. While we have tested other codecs only informally, we think that our results reflect the fundamental difference between codecs which either encode the speech wave\u2018We also used the G.729 encoder for the redundant source coding. form directly or which are based on linear prediction. For PCM without loss concealment we have found that it neither exhibits significant temporal sensitivity nor sensitivity to payload heterogeneity. With loss concealment, however, the speech quality is increased but the amount of increase exhibits strong temporal sensitivity. Frame-based codecs amplify on one hand the impact of loss by error propagation, though on the other hand such coding schemes help to perform loss concealment by extrapolation of decoder state. Contrary to sample-based codecs we have shown that the concealment performance of the G.729 decoder may break at transitions within the speech signal however, thus showing strong sensitivity to payload heterogeneity. We have briefly introduced a queue management algorithm which is able to control loss patterns without changing the amount of loss and characterized its performance for the loss control of a flow exhibiting temporal sensitivity. Then we developed a new packet marking scheme called Speech Property-Based Selective Differential Packet Marking for an efficient protection of frame-based codecs. The SPBDIFFMARK scheme concentrates the higher priority packets on the frames essential to the speech signal and relies on the decoder\"s concealment for other frames. We have also evaluated the mapping of an end-to-end algorithm to interflow protection. We have found that the selective marking scheme performs almost as good as the protection of the entire flow at a significantly lower number of necessary highpriority packets. Thus, combined intra-flow end-to-end / hopby-hop schemes seem to be well-suited for heavily-loaded networks with a relatively large fraction of voice traffic. This is the case because they neither need the addition of redundancy nor feedback (which would incur additional data and delay overhead) and thus yield stable voice quality also for higher loss rates due to absence of FEC and feedback loss. Such schemes can better accomodate codecs with fixed output bitrates, which are difficult to integrate into FEC schemes requiring adaptivity of both the codec and the redundancy generator. Also, it is useful for adaptive codecs running at the lowest possible bitrate. Avoiding redundancy and feedback is also interesting in multicast conferencing scenarios where the end-to-end loss characteristics of the different paths leading to members of the session are largely different. Our work has clearly focused on linking simple end-to-end models which can be easily parametrized with the known characteristic of hopby-hop loss control to user-level metrics. An analysis of a large scale deployment of non-adaptive or adaptive FEC as compared to a deployment of our combined scheme requires clearly further study. 7. ACKNOWLEDGMENTS We would like to thank Wonho Yang and Robert Yantorno, Temple University, for providing the EMBSD software for the objective speech quality measurements. Michael Zander, GMD Fokus, helped with the simulations of the queue management schemes. 8. ADDITIONAL AUTHORS Additional author: Georg Carle (GMD Fokus, email: carle@fokus.gmd.de). 450 9. PI PI [31 PI [51 PI [71 PI PI WI WI", "body1": "Considering that a real-time flow may experience some packet loss, the impact of loss may vary significantly dependent on which packets are lost within a flow. Note that this effect is further aggravated by some interdependence between ADUs (i.e., that one ADU can only be decoded when a previous ADU before has successfully been received and decoded). 441 Figure 1: Schematic utility functions dependent on the loss of more (+l) and less (-1) important packets more important with regard to user perception than others of the same flow. Network support for real-time multimedia flows can on one hand aim at offering a lossless service, which, however, to be implemented within a pa&et-switched network, will be costly for the network provider and thus for the user. The decay of the utility function (for loss rates < 50%) is reduced, because the +l packets are protected and the endto-end loss recovery can thus operate properly over a wider range of loss rates indicated by the shaded area. Schemes which, in the presence of loss, differentiate between pa&& within a flow as demonstrated in Figure 1 above, provide intra-flow QoS enhancement. The adaptation of the sender\"s bitrate to the current network congestion state as an intraflow QoS scheme (loss avoidance, [IS]) is difficult to apply to voice. For loss recovery on an end-to-end basis, due to the realtime delay constraints, open-loop schemes like Forward Error Correction (FEC) have been proposed . 442 Table 1: State and transition probabilities computed for an end-to-end Internet trace using a general Markov model (third order) by Yajnik et. Therefore in this work we assume that the lowest possible bitrate which provides the desired quality is chosen. The paper is structured as follows: Section 2 introduces packet- and user-level metrics. 2.1 Packet-level metrics A general Markov model ([19, S]) which describes the loss process is defined as follows: Let P( Z(s) ] Z(s - m),... ,Z(s - 2),Z(s - 1) ) be the state transition probability of a general Markov model of order m, where Z(s) is the loss indicator function for the packet with the sequence number s. All combinations for the values (0 and 1) of the sequence Z(s-m), . That means that past no loss events do not affect the loss process as much as past loss events. ([l]) as well as Yajnik et. Thus, in the following we employ a model which only considers the past loss events for the state transition probability. In the model, for every additional lost packet which adds to the length of a loss burst a state transition takes place. Thus, the state probability of the system for 0 < Ic < m is P(X 2 Zc). (1) Additionally, we also define a random variable Y which de scribes the distribution of burst loss lengths with respect to the bunt loss events j (and not to packet events like in the definition of X). for P(Y = k): P(Y = k) = P(Y=k)=l-pm: k=l pn p;;\" (1 -pm): k 2 2 c-4 For the special case of a system with a memory of only the previous packet (m = l), we can use the runlength distribution for a simple computation of the parameters of the commonly-used Gilbert model (Fig. 2.2 Objective Speech Quality Metrics Unlike the conventional methods like the Signal-t-Noise Ratio (SNR), novel objective quality measures attempt to estimate the subjective quality as closely as possible by modeling the human auditory system. Figure 4 shows the components of the measurement setup which we will use to evaluate our approach to combined end-t-end and hopby-hop loss recovery and control. By averaging the result of the objective quality measure for several loss patterns, we have a reliable indication for the performance of the codec operating under a certain network loss condition. 3.1 Temporal sensitivity 3.1.1 Pa4 We first analyze the behaviour for ~-law PCM flows (64 kbit/s) with and without loss concealment, where the loss concealment repairs isolated losses only (speech stationarity can only be assumed for small time intervals). Figure 5 shows the case without loss concealment enabled where Gilbert model parameters are varied. For higher clp values the cases with and without concealment become increasingly similar and show the same performance at clp x 0.3. As we found by visual inspection that the distributions of the perceptual distortion values for one loss condition seem to approximately follow a normal distribution we employ mean and standard deviation to describe the statistical variability of the measured values. 3.1.2 G.729 G.729 uses the Conjugate Structure Algebraic Code Excited Linear Prediction (CS-ACELP) coding scheme and ooerates at 8 kbit/s. The line spectral pair coefficients (LSP3) of the last good frame are repeated and the gain coefficients are taken from the previous frame but they are damped to gradually reduce their impact. \u2018Two G.729 frames are contained in a packet. Figure 9: Resynchronization time (in frames) of the G.729 decoder after the loss of k consecutive frames (k E ) as a function of frame position. (and not only reconstruction) operations in the quality assessment. 3.2 Sensitivity due to ADU heterogeneity PCM is a memoryless encoding. We can see from Figure 9 that the position of a frame loss has a signilicant inlluence cm the resulting signal degradation\", while the degradation is not that sensitive to the length of the frame loss burst k. The loss of unvoiced frames seems to have a rather small impact on the signal degradation and the decoder recovers the state information fast thereafter. Moreover, because the G.729 encoder uses a moving average filter to predict the values of the line spectral pairs and only transmits the difference between the real and predicted vaues, it takes a lot of time for the decoder to resynchronize with the encoder once it has failed to build the appropriate linear prediction filter. While we have highlighted the sensitivity of VoIP traffic to the distribution of loss in the last sections, we now want to briefly introduce a queue management mechanism which is able to enforce the relative preferences of the ap plication with regard to loss. We consider flows with packets marked with +l and \u2018I1 (as described in the introduction) BS foreground traffic (FT) and other (best effort) flows as backaound traffic (BT). This amounts to trading the loss of a +l packet against a -1 packet of the same flow (in a statistical sense). The necessary relationship between the drop probabilities for packets marked as -1 and +l can be derived 85 follows (note that this relationship is valid both at the end-tend level and every individual hop): Let a = a0 + a+, +a-~ be the overall number of emitted packets by an FT flow and a,, + E [-l,O, +l] be the number of packets belonging to a certain class (where the 0 class corresponds to (unmarked) best effort traflic). In it has been shown that using only the conventional RED average queue size avg for DSRED operation is not sufficient. CONTROL 5.1 Temporal sensitivity Considering a flow with temporal loss sensitivity, paragraph 3.1.1 has shown that a simple, periodic loss pattern enhances the performance of the end-to-end loss recovery. With Drop Tail an approximately geometrically decreasing burst loss probability with increasing burst length (Eq. We can see that the three-state model estimation (Eq. Details on the simulation scenario and parameters can be found in . 5.2 Sensitivity due to ADU heterogeneity In paragraph 3.1.2 we have seen that sensitivity to ADU heterogeneity results in a certain non-periodic loss pattern. Thus, a mechanism at (or near) the sender is necessary which derives that pattern from the voice data. Furthermore, an explicit cooperation between end-to-end and hop by-hop mechanisms is necessary (Fig. We use the result of paragraph 3.2 to develop a new packet marking scheme called Speech Property-Based Selective Differential Packet Marking (SPB-DIFFMARK). Figure 13 shows the simple algorithm written in a pseudocode that is used to detect an unvoiced/voiced (uw) transition and protect the voiced frames at, the beginning of a voiced signal. send0 is used to send a block of Ic frames as a single packet with the appropriate priority (either fl, 0 or -1). As the core algorithm gives only a binary marking decision (protect the packet or not), we employ a simple algorithm to send the necessary -1 packets for compensation (Eq. Thus, seen over time intervals which are long compared to the +1/-l burst times, the mean loss for the flow will be equal to the best effort case (Eq. 5.2.1 En.d-to-end simulation description Due to the non-periodic loss pattern, we need to explicitly associate a drop probability with a single packet within an end-to-end model. This is a reasonable assumption\" with regard to the interdependence of the different classes in fact, as it has been shown that DiffRED (Figs. also allow a reasonable evaluation of how losses in the fl class affect the performance of the SPB-algorithms. For a direct comparison with SPB-DIFFMARK, we evaluate a scheme where packets are alternatingly marked as being either -1 or +l (ALT-DIFFMARK, Figure 14). The first scheme uses full protection (FULL MARK, all packets are marked as +l). 5.2.2 Results Figure 15 shows the perceptual distortion for the marking schemes dependent on the drop probability pc. However, it should be noted that the amount of data sent over the network is increased by 40.4%. This also shows that by protecting the entire flow only a minor improvement in the perceptual quality is obtained. The results for the FULL MARK scheme also show that, while the loss of some of the +l packets has some measurable impact, the impact on perceptual quality can still be considered to be very low.", "body2": "In the following we distinguish two reasons for such a variable loss sensitivity: Temporal sensitivity: Loss of ADUs\" which is correlated in time may lead to disruptions in the service. Sensitivity due to ADU heterogeneity: Certain ADUs might contain parts of the encoded signal which are \u2018Application Data Unit: the unit of data emitted by a source coder such as a video or voice frame. Under the loss of 50% of the packets, the perceptual quality varies hugely between the case where the 50% of the frames with high perceptual importance are received and the tax where the 50% less important frames are received. Figure 1 b) presents the CBS~ where all +l packets are protected at the expense of -1 pa&&. To describe this effect and provide a taxonomy for different QoS enhancement approaches, we introduce a novel terminology: we designate mechanisms which influence QoS parameters between Bows (thus decrease the loss rate of one flow at the expense of other flows) as inter-flow QoS. As QoS mechanisms have to be implemented within the network (hopby-hop) and/or in the end systems (end-to-end), we have another axis of classification. So when the availability of sutticient computing power is assumed, the lowest bitrate codec can be chosen permanently without actually decreasing the perceptual quality. 1, clearly the utility functions of the +1/-l sub.flows and their relationship are more complex and only approximately additive. End-to-end mechanisms can reduce and shift such sensitivities but cannot come close to eliminate them. We employ actual codecs and measure their utility in the presence of packet loss using objective speech quality measurement. Section 6 concludes the paper. It is interesting to note that for all states with Z(s - 1) = 0 the probability for the next packet not to be lost (Z(s) = 0) is generally very high (> 0.8, in bold typeface) whereas when Z(s- 1) = 1 the state transition probabilities to that event cover the range of 0.15 to 0.61. al. This implies that patterns of short loss bursts interspersed with short periods of successful packet arrivals occur rarely (note in this context that in \u2018&able 1 the pattern 101 has by far the lowest state probability). With this definition, we establish a loss run-length model with a finite number of states (m + 1) which gives loss probabilities dependent on the burst length. If a packet is successfully received, the state returns to X = 0. 2 shows the Markov chain for the model. For a three state model we have e.g. As in equation 2 we can compute an estimate for the probability of a burst loss length of k packets for a higher order model representation: B(Y = k) = clpk-\u2018(1 - clp) (4) Figure 3: Loss run-length (Gilbert model) model with two states Figure 4: Components of the end-to-end loss recovery/control measurement setup. As they yielded very similar results (though MNB results generally exhibited less variability) we only present EMBSD results. This procedure takes thus into account that the impact of loss on an input signal may not be homogenous (i.e., a loss burst within one segment of that signal can have a different perceptual impact than a loss burst of the same size within another segment). 3) as the network model for the simulations, as we have found that here higher order models do not provide much additional information. Similar results were obtained with other concealment algorithms. The relative improvement increases with increasing loss (pulp). 3). On the other hand it shows, that a large number of different Figure 5: Utility curve for PCM without loss conc e a l m e n t (EMBSD) Figure 6: Utility curve for PCM with loss concealment (EMBSD) Figure 7: Variability of the perceptual distortion with 10s~ concealment (EMBSD) 445 Figure 8: Utility curve for the G.729 codec (EMBSD) loss patterns is necessary for a certain speech sample when using objective speech quality measurement to assess the impact of loss corwlation on user perception. When a frame is lost or corrupted, the G.729 decoder uses the parameters of the previous frame to interpolate those of the lost frame. This demonstrates the importance of perceptual metrics which are able to include concealment \u2018LSPs are another representation of the linear prediction coefficients. \u2018Two G.729 frames are contained in a packet. Figure 9: Resynchronization time (in frames) of the G.729 decoder after the loss of k consecutive frames (k E ) as a function of frame position. (and not only reconstruction) operations in the quality assessment. Figure 9 shows the resynchronization time (expressed in the number of frames needed until the threshold is exceeded) plotted against the position of the loss (i.e., the index of the first lost frame) for d&rent values of k. The speech sample is produced by a male speaker where an unvoiced/voiced (au) transition occurs in the eighth frame. The decoder fails to conceal the loss of voiced frames at an unvoiced/voiced transition because it attempts to conceal the loss of voiced frames using the filter coefficients and the excitation for an unvoiced sound. Moreover, because the G.729 encoder uses a moving average filter to predict the values of the line spectral pairs and only transmits the difference between the real and predicted vaues, it takes a lot of time for the decoder to resynchronize with the encoder once it has failed to build the appropriate linear prediction filter. While we have highlighted the sensitivity of VoIP traffic to the distribution of loss in the last sections, we now want to briefly introduce a queue management mechanism which is able to enforce the relative preferences of the ap plication with regard to loss. In this work, however, we want to keep the ulp as given by other parameters while modifying the loss distribution for the foreground traffic (FT). 10 shows the conventional RED drop probability curve (po as a function of the average queue size for all arrivals avg), which is applied to all unmarked (0) traffic (background traffic: BT). 10) show the operating area when this correction is added. Thus, a service differentiation for foreground traffic is possible which does not differ from conventional RED behaviour in the long term average (i.e., in the ulp). Most of the probability mass is concentrated at, isolated losses (the ideal behaviour would be the occurence of only isolated losses (Ic = 1) which can be expressed with clp = 0 in terms of Gilbert model parameters). 10). sWe only discuss the results qualitatively here to give an example how an intra-flow loss control algorithm performs and to show how loss models can capture this performance. Details on the simulation scenario and parameters can be found in . 5.2 Sensitivity due to ADU heterogeneity In paragraph 3.1.2 we have seen that sensitivity to ADU heterogeneity results in a certain non-periodic loss pattern. Thus, a mechanism at (or near) the sender is necessary which derives that pattern from the voice data. 4). The DIFFMARK scheme concentrates the higher priority packets on the frames essential to the speech signal and relies on the decoder\"s concealment for other frames. The procedure analysis0 is used to classify a block of Ic frames as voiced, unvoiced, or uv transition. send0 is used to send a block of Ic frames as a single packet with the appropriate priority (either fl, 0 or -1). State about the necessary number of to-be-sent -1 packets is kept in the event that the SPB algorithm triggers the next +l burst before all -1 packets necessary for compensation are sent. A larger number of Ic would help to reduce the relative overhead of the protocol header but also increases the packetization delay and makes sender classification and receiver concealment in case of packet loss (due to a larger loss gap) more difficult. 14) are dropped with the probability pc, whereas packets marked with +l and -1 are dropped with probabilities of p+i and p-1 respectively. Marking Scheme Network Model N O M A R K lolo/ 0~0~0~ 01 F U L L M A R K +I +I +I +I +l +I SPB MARK 1 0 1+I j +I I 0 I 0 / 0 ALT MARK o/+11 Ol+ll o/+1 +I Pcl 330 po S P B D I F F M A R K 1 IJ / +l I +l I -1 j-1 / 0 I ALT DIFFMARK I-1 / +I 1-1 1+I /-I 1+I 1 Figure 14: Marking schemes and corresponding network models. also allow a reasonable evaluation of how losses in the fl class affect the performance of the SPB-algorithms. We furthermore include related inter-flow loss protection schemes. The ALT marking schemes mark exactly 50% of their packets as being fl. 449 Figure 15: Perceptual Distortion (EMBSD) for the marking schemes and SPB-FEC which uses redundancy9 piggybacked on the main payload packets ( ) to protect a subset of the packets, enables a very good output speech quality for low loss rates. However, the auditory distance and the perceptual distortion of the SPB-MARK scheme (with 40.4% of all packets marked) is significantly lower and very close to the quality of the decoded signal when all packets are marked (FULL MARK). This also shows that by protecting the entire flow only a minor improvement in the perceptual quality is obtained. The results for the FULL MARK scheme also show that, while the loss of some of the +l packets has some measurable impact, the impact on perceptual quality can still be considered to be very low.", "introduction": "Considering that a real-time flow may experience some packet loss, the impact of loss may vary significantly dependent on which packets are lost within a flow. In the following we distinguish two reasons for such a variable loss sensitivity: Temporal sensitivity: Loss of ADUs\" which is correlated in time may lead to disruptions in the service. Note that this effect is further aggravated by some interdependence between ADUs (i.e., that one ADU can only be decoded when a previous ADU before has successfully been received and decoded). For voice, as a single packet contains typically several ADUs (voice frames) this effect is thus more significant than e.g. It translates basically to isolated packet losses versus losses that occur in bursts. Sensitivity due to ADU heterogeneity: Certain ADUs might contain parts of the encoded signal which are \u2018Application Data Unit: the unit of data emitted by a source coder such as a video or voice frame. 441 Figure 1: Schematic utility functions dependent on the loss of more (+l) and less (-1) important packets more important with regard to user perception than others of the same flow. Let us consider a flow with two frame types of largely different perceptual importance (we assume same size, frequency and no interdependence between the frames). Under the loss of 50% of the packets, the perceptual quality varies hugely between the case where the 50% of the frames with high perceptual importance are received and the tax where the 50% less important frames are received. Network support for real-time multimedia flows can on one hand aim at offering a lossless service, which, however, to be implemented within a pa&et-switched network, will be costly for the network provider and thus for the user. On the other hand, within a lossy service, the above sensitivity constraints must be taken into account. It is our strong belief that this needs to be done in a generic way, i.e., no application-specific knowledge (about particular coding schemes e.g.) should be necessary within the network and, vice versa, no knowledge about network specifics should be necessary within an application. Let us now consider the case that 50% of packets of a flow are identified as more important (designated by +l) or less important (-1) due to any of the above sensitivity constraints. Figure 1 a) shows a generic utility function describing the applicationlevel Quality of Service (QoS) dependent on the percentage of packets lost. For real-time multimedia traffic, such utility should correspond to perceived video/voice quality. If the relative importance of the packets is not known by the transmission system, the loss rates for the +l and -1 packets are equal. Due to the over-proportional sensitivity of the +l packets to loss as well as the dependence of the end-t* end loss recovery performance on the fl packets, the utility function is decreasing significantly in a non-linear way (approximated in the figure by piece-wise linear functions) with an increasing loss rate. Figure 1 b) presents the CBS~ where all +l packets are protected at the expense of -1 pa&&. The decay of the utility function (for loss rates < 50%) is reduced, because the +l packets are protected and the endto-end loss recovery can thus operate properly over a wider range of loss rates indicated by the shaded area. This results in a graceful degradation of the application\"s utility. Note that the higher the non-linearity of the utility contribution of the fl packets is (deviation from the dotted curve in Fig. 1 a), the higher is the potential gain in utility when the protection for +l packets is enabled. Results for actual perceived quality as utility for multimedia applications exhibit such a non-linear behavior*. To describe this effect and provide a taxonomy for different QoS enhancement approaches, we introduce a novel terminology: we designate mechanisms which influence QoS parameters between Bows (thus decrease the loss rate of one flow at the expense of other flows) as inter-flow QoS. Schemes which, in the presence of loss, differentiate between pa&& within a flow as demonstrated in Figure 1 above, provide intra-flow QoS enhancement. As QoS mechanisms have to be implemented within the network (hopby-hop) and/or in the end systems (end-to-end), we have another axis of classification. The adaptation of the sender\"s bitrate to the current network congestion state as an intraflow QoS scheme (loss avoidance, [IS]) is difficult to apply to voice. Considering that voice flows have a very low bitrate, the relative cost of transmitting the feedback information is high (when compared e.g. To reduce this cost the feedback interval would need to be increased, then leading to a higher probability of wrong adaptation decisions. The major di6culty, however, is the lack of a codec which is truly scalable in terms of its output bitrate and corresponding perceptual quality. Currently standardized voice codecs usually only have a 6xed output bitrate. While it has been proposed to switch between voice codeca , the MOS (subjective quality) values for the codecs employed do not differ much: e.g., theITU codecs G.723.1, G.729, G.728, G.726and G.711 cover a bitrate range from 5.3 kbitjs to 64 kbitjs while the subjective quality differs by less than 0.25 on a l-to-5 MOS scale (, 1: bad, 5: excellent quality). So when the availability of sutticient computing power is assumed, the lowest bitrate codec can be chosen permanently without actually decreasing the perceptual quality. For loss recovery on an end-to-end basis, due to the realtime delay constraints, open-loop schemes like Forward Error Correction (FEC) have been proposed . While such schemes are attractive because they can be used on the Internet today, they also have several drawbacks. The amount of redundant information needs to be adaptive to avoid taking bandwidth away from other flows. This adaptation is crucial especially when the fraction of traflic using redundancy schemes is large (181). If the redundancy is a source coding itself, like it has often been proposed , the comments from above on adaptation also apply. Using redundancy has also implications to the playout delay adaptation ([lo]) employed to de-jitter the packets at the receiver. Note that the presented types of loss sensitivity also apply to ap\u2018While we have obtained results which confirm the shape of the overall utility curve shown in Fig. 1, clearly the utility functions of the +1/-l sub.flows and their relationship are more complex and only approximately additive. 442 Table 1: State and transition probabilities computed for an end-to-end Internet trace using a general Markov model (third order) by Yajnik et. state 000 001 010 011 100 101 110 111 Probability of Probability Probability being in the state of I(s)=0 of l(s)= 0 . 0 . 0 2 0 8 0 . 6 1 1 2 0 . 3 8 8 8 0 . 0 1 4 2 0 . 0 . 0 1 0 2 0 . 2 7 1 0 0 . 7 2 9 0 0 . 0 2 0 8 0 . 9 2 7 8 0 . 0 7 2 2 0 . 0 0 3 6 0 . 4 1 9 8 0 . 5 8 0 2 0 . 0 1 0 2 0 . 0 . plications which are enhanced by end-to-end loss recovery mechanisms. End-to-end mechanisms can reduce and shift such sensitivities but cannot come close to eliminate them. Therefore in this work we assume that the lowest possible bitrate which provides the desired quality is chosen. Neither feedback/adaptation nor redundancy is used, however, at the end-to-end level, identification/marking of packets sensitive to loss (sender) as well as loss concealment (receiver) takes place. Hop-by-hop support schemes then allow trading the loss of one packet, which is considered more important, against another one of the same flow which is of lower importance. We employ actual codecs and measure their utility in the presence of packet loss using objective speech quality measurement. The paper is structured as follows: Section 2 introduces packet- and user-level metrics. We employ these metrics to describe the sensitivity of VoIP traffic to packet loss in section 3. Section 4 briefly introduces a queue management algorithm which can be used for intra-flow loss control. In section 5, we present results documenting the performance of the proposed mechanisms at both the end-to-end and hopby-hop level.", "conclusion": "In this paper we have characterized the behaviour of a samplebased codec (PCM) and a frame-based codec (G.729) in the presence of packet loss.. We have then developed intraflow loss recovery and control mechanisms to increase the perceptual quality.. While we have tested other codecs only informally, we think that our results reflect the fundamental difference between codecs which either encode the speech wave\u2018We also used the G.729 encoder for the redundant source coding.. form directly or which are based on linear prediction.. For PCM without loss concealment we have found that it neither exhibits significant temporal sensitivity nor sensitivity to payload heterogeneity.. With loss concealment, however, the speech quality is increased but the amount of increase exhibits strong temporal sensitivity.. Frame-based codecs amplify on one hand the impact of loss by error propagation, though on the other hand such coding schemes help to perform loss concealment by extrapolation of decoder state.. Contrary to sample-based codecs we have shown that the concealment performance of the G.729 decoder may break at transitions within the speech signal however, thus showing strong sensitivity to payload heterogeneity.. We have briefly introduced a queue management algorithm which is able to control loss patterns without changing the amount of loss and characterized its performance for the loss control of a flow exhibiting temporal sensitivity.. Then we developed a new packet marking scheme called Speech Property-Based Selective Differential Packet Marking for an efficient protection of frame-based codecs.. The SPBDIFFMARK scheme concentrates the higher priority packets on the frames essential to the speech signal and relies on the decoder\"s concealment for other frames.. We have also evaluated the mapping of an end-to-end algorithm to interflow protection.. We have found that the selective marking scheme performs almost as good as the protection of the entire flow at a significantly lower number of necessary highpriority packets.. Thus, combined intra-flow end-to-end / hopby-hop schemes seem to be well-suited for heavily-loaded networks with a relatively large fraction of voice traffic.. This is the case because they neither need the addition of redundancy nor feedback (which would incur additional data and delay overhead) and thus yield stable voice quality also for higher loss rates due to absence of FEC and feedback loss.. Such schemes can better accomodate codecs with fixed output bitrates, which are difficult to integrate into FEC schemes requiring adaptivity of both the codec and the redundancy generator.. Also, it is useful for adaptive codecs running at the lowest possible bitrate.. Avoiding redundancy and feedback is also interesting in multicast conferencing scenarios where the end-to-end loss characteristics of the different paths leading to members of the session are largely different.. Our work has clearly focused on linking simple end-to-end models which can be easily parametrized with the known characteristic of hopby-hop loss control to user-level metrics.. An analysis of a large scale deployment of non-adaptive or adaptive FEC as compared to a deployment of our combined scheme requires clearly further study.. ACKNOWLEDGMENTS We would like to thank Wonho Yang and Robert Yantorno, Temple University, for providing the EMBSD software for the objective speech quality measurements.. Michael Zander, GMD Fokus, helped with the simulations of the queue management schemes.. ADDITIONAL AUTHORS Additional author: Georg Carle (GMD Fokus, email: carle@fokus.gmd.de).. PI PI [31 PI [51 PI [71 PI PI WI WI"}
{"id": "C-66", "keywords": ["web servic", "qo", "work\ufb02ow", "schedul", "heurist"], "title": "Heuristics-Based Scheduling of Composite Web Service Workloads", "abstract": "Web services can be aggregated to create composite workflows that provide streamlined functionality for human users or other systems. Although industry standards and recent research have sought to define best practices and to improve end-to-end workflow composition, one area that has not fully been explored is the scheduling of a workflow's web service requests to actual service provisioning in a multi-tiered, multi-organisation environment. This issue is relevant to modern business scenarios where business processes within a workflow must complete within QoS-defined limits. Because these business processes are web service consumers, service requests must be mapped and scheduled across multiple web service providers, each with its own negotiated service level agreement. In this paper we provide heuristics for scheduling service requests from multiple business process workflows to web service providers such that a business value metric across all workflows is maximised. We show that a genetic search algorithm is appropriate to perform this scheduling, and through experimentation we show that our algorithm scales well up to a thousand workflows and produces better mappings than traditional approaches.", "references": ["DAML-S: Semantic Markup For Web Services,", "Job Shop Scheduling with Genetic Algorithms", "A Promising Genetic Algorithm Approach to Job-Shop Scheduling, Rescheduling, and Open-Shop Scheduling Problems", "Computers and Intractability: A Guide to the Theory of NP-Completeness", "Adaptation in Natural and Artificial Systems: An Introductory Analysis with Applications to Biology, Control, and Artificial Intelligence", "Genetic Algorithms in Search, Optimization and Machine Leaming", "Business Processes in a Web Services World", "Back-end Databases in Shared Dynamic Content Server Clusters", "Web Service Composition Current Solutions and Open Problems", "Dynamic Provisioning of Multi-Tier Internet Applications", "Quality Driven Web Services Composition"], "full_text": "1. INTRODUCTION Web services can be composed into workflows to provide streamlined end-to-end functionality for human users or other systems. Although previous research efforts have looked at ways to intelligently automate the composition of web services into workflows (e.g. ), an important remaining problem is the assignment of web service requests to the underlying web service providers in a multi-tiered runtime scenario within constraints. In this paper we address this scheduling problem and examine means to manage a large number of business process workflows in a scalable manner. The problem of scheduling web service requests to providers is relevant to modern business domains that depend on multi-tiered service provisioning. Consider the example shown in Figure 1 that illustrates our problem space. Workflows comprise multiple related business processes that are web service consumers; here we assume that the workflows represent requested service from customers or automated systems and that the workflow has already been composed with an existing choreography toolkit. These workflows are then submitted to a portal (not shown) that acts as a scheduling agent between the web service consumers and the web service providers. In this example, a workflow could represent the actions needed to instantiate a vacation itinerary, where one business process requests booking an airline ticket, another business process requests a hotel room, and so forth. Each of these requests target a particular service type (e.g. airline reservations, hotel reservations, car reservations, etc.), and for each service type, there are multiple instances of service providers that publish a web service interface. An important challenge is that the workflows must meet some quality-of-service (QoS) metric, such as end-to-end completion time of all its business processes, and that meeting or failing this goal results in the assignment of a quantitative business value metric for the workflow; intuitively, it is desired that all workflows meet their respective QoS goals. We further leverage the notion that QoS service agreements are generally agreed-upon between the web service providers and the scheduling agent such that the providers advertise some level of guaranteed QoS to the scheduler based upon runtime conditions such as turnaround time and maximum available concurrency. The resulting problem is then to schedule and assign the business processes\" requests for service types to one of the service providers for that type. The scheduling must be done such that the aggregate business value across all the workflows is maximised. In Section 3 we state the scenario as a combinatorial problem and utilise a genetic search algorithm to find the best assignment of web service requests to providers. This approach converges towards an assignment that maximises the overall business value for all the workflows. In Section 4 we show through experimentation that this search heuristic finds better assignments than other algorithms (greedy, round-robin, and proportional). Further, this approach allows us to scale the number of simultaneous workflows (up to one thousand workflows in our experiments) and yet still find effective schedules. 2. RELATED WORK In the context of service assignment and scheduling, maps web service calls to potential servers using linear programming, but their work is concerned with mapping only single workflows; our principal focus is on scalably scheduling multiple workflows (up 30 Service Type SuperHotels.com Business Process Business Process Workflow ... Business Process Business Process ... HostileHostels.com IncredibleInns.com Business Process Business Process Business Process ... Business Process Service Provider SkyHighAirlines.com SuperCrazyFlights.com Business Process Advertised QoS Service Agreement CarRentalService.com Figure 1: An example scenario demonstrating the interaction between business processes in workflows and web service providers. Each business process accesses a service type and is then mapped to a service provider for that type. to one thousand as we show later) using different business metrics and a search heuristic. presents a dynamic provisioning approach that uses both predictive and reactive techniques for multi-tiered Internet application delivery. However, the provisioning techniques do not consider the challenges faced when there are alternative query execution plans and replicated data sources. presents a feedback-based scheduling mechanism for multi-tiered systems with back-end databases, but unlike our work, it assumes a tighter coupling between the various components of the system. Our work also builds upon prior scheduling research. The classic job-shop scheduling problem, shown to be NP-complete , is similar to ours in that tasks within a job must be scheduled onto machinery (c.f. our scenario is that business processes within a workflow must be scheduled onto web service providers). The salient differences are that the machines can process only one job at a time (we assume servers can multi-task but with degraded performance and a maximum concurrency level), tasks within a job cannot simultaneously run on different machines (we assume business processes can be assigned to any available server), and the principal metric of performance is the makespan, which is the time for the last task among all the jobs to complete (and as we show later, optimising on the makespan is insufficient for scheduling the business processes, necessitating different metrics). 3. DESIGN In this section we describe our model and discuss how we can find scheduling assignments using a genetic search algorithm. 3.1 Model We base our model on the simplified scenario shown in Figure 1. Specifically, we assume that users or automated systems request the execution of a workflow. The workflows comprise business processes, each of which makes one web service invocation to a service type. Further, business processes have an ordering in the workflow. The arrangement and execution of the business processes and the data flow between them are all managed by a composition or choreography tool (e.g. ). Although composition languages can use sophisticated flow-control mechanisms such as conditional branches, for simplicity we assume the processes execute sequentially in a given order. This scenario can be naturally extended to more complex relationships that can be expressed in BPEL , which defines how business processes interact, messages are exchanged, activities are ordered, and exceptions are handled. Due to space constraints, we focus on the problem space presented here and will extend our model to more advanced deployment scenarios in the future. Each workflow has a QoS requirement to complete within a specified number of time units (e.g. on the order of seconds, as detailed in the Experiments section). Upon completion (or failure), the workflow is assigned a business value. We extended this approach further and considered different types of workflow completion in order to model differentiated QoS levels that can be applied by businesses (for example, to provide tiered customer service). We say that a workflow is successful if it completes within its QoS requirement, acceptable if it completes within a constant factor \u03ba 31 of its QoS bound (in our experiments we chose \u03ba=3), or failing if it finishes beyond \u03ba times its QoS bound. For each category, a business value score is assigned to the workflow, with the successful category assigned the highest positive score, followed by acceptable and then failing. The business value point distribution is non-uniform across workflows, further modelling cases where some workflows are of higher priority than others. Each service type is implemented by a number of different service providers. We assume that the providers make service level agreements (SLAs) to guarantee a level of performance defined by the completion time for completing a web service invocation. Although SLAs can be complex, in this paper we assume for simplicity that the guarantees can take the form of a linear performance degradation under load. This guarantee is defined by several parameters: \u03b1 is the expected completion time (for example, on the order of seconds) if the assigned workload of web service requests is less than or equal to \u03b2, the maximum concurrency, and if the workload is higher than \u03b2, the expected completion for a workload of size \u03c9 is \u03b1+ \u03b3(\u03c9 \u2212 \u03b2) where \u03b3 is a fractional coefficient. In our experiments we vary \u03b1, \u03b2, and \u03b3 with different distributions. Ideally, all workflows would be able to finish within their QoS limits and thus maximise the aggregate business value across all workflows. However, because we model service providers with degrading performance under load, not all workflows will achieve their QoS limit: it may easily be the case that business processes are assigned to providers who are overloaded and cannot complete within the respective workflow\"s QoS limit. The key research problem, then, is to assign the business processes to the web service providers with the goal of optimising on the aggregate business value of all workflows. Given that the scope of the optimisation is the entire set of workflows, it may be that the best scheduling assignments may result in some workflows having to fail in order for more workflows to succeed. This intuitive observation suggests that traditional scheduling approaches such as round-robin or proportional assignments will not fare well, which is what we observe and discuss in Section 4. On the other hand, an exhaustive search of all the possible assignments will find the best schedule, but the computational complexity is prohibitively high. Suppose there are W workflows with an average of B business processes per workflow. Further, in the worst case each business process requests one service type, for which there are P providers. There are thus W \u00b7 PB combinations to explore to find the optimal assignments of business processes to providers. Even for small configurations (e.g. W =10, B=5, P=10), the computational time for exhaustive search is significant, and in our work we look to scale these parameters. In the next subsection, discuss how a genetic search algorithm can be used to converge toward the optimum scheduling assignments. 3.2 Genetic algorithm Given an exponential search space of business process assignments to web service providers, the problem is to find the optimal assignment that produces the overall highest aggregate business value across all workflows. To explore the solution space, we use a genetic algorithm (GA) search heuristic that simulates Darwinian natural selection by having members of a population compete to survive in order to pass their genetic chromosomes onto the next generation; after successive generations, there is a tendency for the chromosomes to converge toward the best combination . Although other search heuristics exist that can solve optimization problems (e.g. simulated annealing or steepest-ascent hillclimbing), the business process scheduling problem fits well with a GA because potential solutions can be represented in a matrix form and allows us to use prior research in effective GA chromosome recombination to form new members of the population (e.g. ). 0 1 2 3 4 0 1 2 0 2 1 1 0 1 0 1 0 2 1 2 0 0 1 Figure 2: An example chromosome representing a scheduling assignment of (workflow,service type) \u2192 service provider. Each row represents a workflow, and each column represents a service type. For example, here there are 3 workflows (0 to 2) and 5 service types (0 to 4). In workflow 0, any request for service type 3 goes to provider 2. Note that the service provider identifier is within a range limited to its service type (i.e. its column), so the 2 listed for service type 3 is a different server from server 2 in other columns. Chromosome representation of a solution. In Figure 2 we show an example chromosome that encodes one scheduling assignment. The representation is a 2-dimensional matrix that maps {workflow, service type} to a service provider. For a business process in workflow i and utilising service type j, the (i, j)th entry in the table is the identifier for the service provider to which the business process is assigned. Note that the service provider identifier is within a range limited to its service type. GA execution. A GA proceeds as follows. Initially a random set of chromosomes is created for the population. The chromosomes are evaluated (hashed) to some metric, and the best ones are chosen to be parents. In our problem, the evaluation produces the net business value across all workflows after executing all business processes once they are assigned to their respective service providers according to the mapping in the chromosome. The parents recombine to produce children, simulating sexual crossover, and occasionally a mutation may arise which produces new characteristics that were not available in either parent. The principal idea is that we would like the children to be different from the parents (in order to explore more of the solution space) yet not too different (in order to contain the portions of the chromosome that result in good scheduling assignments). Note that finding the global optimum is not guaranteed because the recombination and mutation are stochastic. GA recombination and mutation. As mentioned, the chromosomes are 2-dimensional matrices that represent scheduling assignments. To simulate sexual recombination of two chromosomes to produce a new child chromosome, we applied a one-point crossover scheme twice (once along each dimension). The crossover is best explained by analogy to Cartesian space as follows. A random point is chosen in the matrix to be coordinate (0, 0). Matrix elements from quadrants II and IV from the first parent and elements from quadrants I and III from the second parent are used to create the new child. This approach follows GA best practices by keeping contiguous chromosome segments together as they are transmitted from parent to child. The uni-chromosome mutation scheme randomly changes one of the service provider assignments to another provider within the available range. Other recombination and mutation schemes are an area of research in the GA community, and we look to explore new operators in future work. GA evaluation function. An important GA component is the evaluation function. Given a particular chromosome representing one scheduling mapping, the function deterministically calculates the net business value across all workloads. The business processes in each workload are assigned to service providers, and each provider\"s completion time is calculated based on the service agreement guarantee using the parameters mentioned in Section 3.1, namely the unloaded completion time \u03b1, the maximum concur32 rency \u03b2, and a coefficient \u03b3 that controls the linear performance degradation under heavy load. Note that the evaluation function can be easily replaced if desired; for example, other evaluation functions can model different service provider guarantees or parallel workflows. 4. EXPERIMENTS AND RESULTS In this section we show the benefit of using our GA-based scheduler. Because we wanted to scale the scenarios up to a large number of workflows ( in our experiments), we implemented a simulation program that allowed us to vary parameters and to measure the results with different metrics. The simulator was written in standard C++ and was run on a Linux (Fedora Core) desktop computer running at 2.8 GHz with 1GB of RAM. We compared our algorithm against alternative candidates: \u2022 A well-known round-robin algorithm that assigns each business process in circular fashion to the service providers for a particular service type. This approach provides the simplest scheme for load-balancing. \u2022 A random-proportional algorithm that proportionally assigns business processes to the service providers; that is, for a given service type, the service providers are ranked by their guaranteed completion time, and business processes are assigned proportionally to the providers based on their completion time. (We also tried a proportionality scheme based on both the completion times and maximum concurrency but attained the same results, so only the former scheme\"s results are shown here.) \u2022 A strawman greedy algorithm that always assigns business processes to the service provider that has the fastest guaranteed completion time. This algorithm represents a naive approach based on greedy, local observations of each workflow without taking into consideration all workflows. In the experiments that follow, all results were averaged across 20 trials, and to help normalise the effects of randomisation used during the GA, each trial started by reading in pre-initialised data from disk. In Table 1 we list our experimental parameters. In Figure 3 we show the results of running our GA against the three candidate alternatives. The x- and the y-axis shows the aggregate business value for all workflows. As can be seen, the GA consistently produces the highest business value even as the number of workflows grows; workflows, the GA produces a 115% improvement over the next-best alternative. (Note that although we are optimising against the business value metric we defined earlier, genetic algorithms are able to converge towards the optimal value of any metric, as long as the evaluation function can consistently measure a chromosome\"s value with that metric.) As expected, the greedy algorithm performs very poorly because it does the worst job at balancing load: all business processes for a given service type are assigned to only one server (the one advertised to have the fastest completion time), and as more business processes arrive, the provider\"s performance degrades linearly. The round-robin scheme is initially outperformed by the randomproportional scheme up to around 120 workflows (as shown in the magnified graph of Figure 4), but as the number of workflows increases, the round-robin scheme consistently wins over randomproportional. The reason is that although the random-proportional scheme assigns business processes to providers proportionally according to the advertised completion times (which is a measure of the power of the service provider), even the best providers will eventually reach a real-world maximum concurrency for the large - - Aggregatebusinessvalueacrossallworkflows Total number of workflows Business value scores of scheduling algorithms Genetic algorithm Round robin Random proportional Greedy Figure 3: Net business value scores of different scheduling algorithms. - 0 50 100 150 200Aggregatebusinessvalueacrossallworkflows Total number of workflows Business value scores of scheduling algorithms Genetic algorithm Round robin Random proportional Greedy Figure 4: Magnification of the left-most region in Figure 3. number of workflows that we are considering. For a very large number of workflows, the round-robin scheme is able to better balance the load across all service providers. To better understand the behaviour resulting from the scheduling assignments, we show the workflow completion results in Figures 5, 6, and 7 for 100, 500, and 900 workflows, respectively. These figures show the percentage of workflows that are successful (can complete with their QoS limit), acceptable (can complete within \u03ba=3 times their QoS limit), and failed (cannot complete within \u03ba=3 times their QoS limit). The GA consistently produces the highest percentage of successful workflows (resulting in higher business values for the aggregate set of workflows). Further, the round-robin scheme produces better results than the random-proportional for a large number of workflows but does not perform as well as the GA. In Figure 8 we graph the makespan resulting from the same experiments above. Makespan is a traditional metric from the job scheduling community measuring elapsed time for the last job to complete. While useful, it does not capture the high-level business value metric that we are optimising against. Indeed, the makespan is oblivious to the fact that we provide multiple levels of completion (successful, acceptable, and failed) and assign business value scores accordingly. For completeness, we note that the GA provides the fastest makespan, but it is matched by the round robin algorithm. The GA produces better business values (as shown in Figure 3) because it is able to search the solution space to find better mappings that produce more successful workflows (as shown in Figures 5 to 7). We also looked at the effect of the scheduling algorithms on balancing the load. Figure 9 shows the percentage of services providers that were accessed while the workflows ran. As expected, the greedy algorithm always hits one service provider; on the other hand, the round- Business processes per workflow uniform random: 1 - 10 Service types 10 Service providers per service type uniform random: 1 - 10 Workflow QoS goal uniform random: 10-30 seconds Service provider completion time (\u03b1) uniform random: 1 - 12 seconds Service provider maximum concurrency (\u03b2) uniform random: 1 - 12 Service provider degradation coefficient (\u03b3) uniform random: 0.1 - 0.9 Business value for successful workflows uniform random: 10 - 50 points Business value for acceptable workflows uniform random: 0 - 10 points Business value for failed workflows uniform random: -10 - 0 points GA: number of parents 20 GA: number of children 80 GA: Table 1: Experimental parameters Failed Acceptable (completed but not within QoS) Successful (completed within QoS) 0% 20% 40% 60% 80% 100% RoundRobinRandProportionalGreedyGeneticAlg Percentageofallworkflows Workflow behaviour, 100 workflows Figure 5: Workflow behaviour for 100 workflows. Failed Acceptable (completed but not within QoS) Successful (completed within QoS) 0% 20% 40% 60% 80% 100% RoundRobinRandProportionalGreedyGeneticAlg Percentageofallworkflows Workflow behaviour, 500 workflows Figure 6: Workflow behaviour for 500 workflows. Failed Acceptable (completed but not within QoS) Successful (completed within QoS) 0% 20% 40% 60% 80% 100% RoundRobinRandProportionalGreedyGeneticAlg Percentageofallworkflows Workflow behaviour, 500 workflows Figure 7: Workflow behaviour for 900 workflows. Makespan[seconds] Number of workflows Maximum completion time for all workflows Genetic algorithm Round robin Random proportional Greedy Figure 8: Maximum completion time for all workflows. This value is the makespan metric used in traditional scheduling research. Although useful, the makespan does not take into consideration the business value scoring in our problem domain. processes. Figure 10 is the percentage of accessed service providers (that is, the percentage of service providers represented in Figure 9) that had more assigned business processes than their advertised maximum concurrency. For example, in the greedy algorithm only one service provider is utilised, and this one provider quickly becomes saturated. On the other hand, the random-proportional algorithm uses many service providers, but because business processes are proportionally assigned with more assignments going to the better providers, there is a tendency for a smaller percentage of providers to become saturated. For completeness, we show the performance of the genetic algorithm itself in Figure 11. The algorithm scales linearly with an increasing number of workflows. We note that the round-robin, random-proportional, and greedy algorithms all finished within 1 second even for the largest workflow configuration. However, we feel that the benefit of finding much higher business value scores justifies the running time of the GA; further we would expect that the running time will improve with both software tuning as well as with a computer faster than our off-the-shelf PC. 5. CONCLUSION Business processes within workflows can be orchestrated to access web services. In this paper we looked at multi-tiered service provisioning where web service requests to service types can be mapped to different service providers. The resulting problem is that in order to support a very large number of workflows, the assignment of business process to web service provider must be intelligent. We used a business value metric to measure the be34 0.2 0.4 0.6 0. Percentageofallserviceproviders Number of workflows Service providers utilised Genetic algorithm Round robin Random proportional Greedy Figure 9: The percentage of service providers utilized during workload executions. The Greedy algorithm always hits the one service provider, while the Round Robin algorithm spreads requests evenly across the providers. 0.2 0.4 0.6 0. Percentageofallserviceproviders Number of workflows Service providers saturated Genetic algorithm Round robin Random proportional Greedy Figure 10: The percentage of service providers that are saturated among those providers who were utilized (that is, percentage of the service providers represented in Figure 9). A saturated service provider is one whose workload is greater that its advertised maximum concurrency. Runningtimeinseconds Total number of workflows Running time of genetic algorithm GA running time Figure 11: Running time of the genetic algorithm. haviour of workflows meeting or failing QoS values, and we optimised our scheduling to maximise the aggregate business value across all workflows. Since the solution space of scheduler mappings is exponential, we used a genetic search algorithm to search the space and converge toward the best schedule. With a default configuration for all parameters and using our business value scoring, the GA produced up to 115% business value improvement over the next best algorithm. Finally, because a genetic algorithm will converge towards the optimal value using any metric (even other than the business value metric we used), we believe our approach has strong potential for continuing work. In future work, we look to acquire real-world traces of web service instances in order to get better estimates of service agreement guarantees, although we expect that such guarantees between the providers and their consumers are not generally available to the public. We will also look at other QoS metrics such as CPU and I/O usage. For example, we can analyse transfer costs with varying bandwidth, latency, data size, and data distribution. Further, we hope to improve our genetic algorithm and compare it to more scheduler alternatives. Finally, since our work is complementary to existing work in web services choreography (because we rely on pre-configured workflows), we look to integrate our approach with available web service workflow systems expressed in BPEL.", "body1": "Web services can be composed into workflows to provide streamlined end-to-end functionality for human users or other systems. Although previous research efforts have looked at ways to intelligently automate the composition of web services into workflows (e.g. The problem of scheduling web service requests to providers is relevant to modern business domains that depend on multi-tiered service provisioning. In this example, a workflow could represent the actions needed to instantiate a vacation itinerary, where one business process requests booking an airline ticket, another business process requests a hotel room, and so forth. In Section 4 we show through experimentation that this search heuristic finds better assignments than other algorithms (greedy, round-robin, and proportional). In the context of service assignment and scheduling, maps web service calls to potential servers using linear programming, but their work is concerned with mapping only single workflows; our principal focus is on scalably scheduling multiple workflows (up 30 Service Type SuperHotels.com Business Process Business Process Workflow ... Business Process Business Process ... HostileHostels.com IncredibleInns.com Business Process Business Process Business Process ... Business Process Service Provider SkyHighAirlines.com SuperCrazyFlights.com Business Process Advertised QoS Service Agreement CarRentalService.com Figure 1: An example scenario demonstrating the interaction between business processes in workflows and web service providers. Each business process accesses a service type and is then mapped to a service provider for that type. to one thousand as we show later) using different business metrics and a search heuristic. Our work also builds upon prior scheduling research. In this section we describe our model and discuss how we can find scheduling assignments using a genetic search algorithm. 3.1 Model We base our model on the simplified scenario shown in Figure 1. Each workflow has a QoS requirement to complete within a specified number of time units (e.g. We say that a workflow is successful if it completes within its QoS requirement, acceptable if it completes within a constant factor \u03ba 31 of its QoS bound (in our experiments we chose \u03ba=3), or failing if it finishes beyond \u03ba times its QoS bound. Although SLAs can be complex, in this paper we assume for simplicity that the guarantees can take the form of a linear performance degradation under load. Ideally, all workflows would be able to finish within their QoS limits and thus maximise the aggregate business value across all workflows. On the other hand, an exhaustive search of all the possible assignments will find the best schedule, but the computational complexity is prohibitively high. Although other search heuristics exist that can solve optimization problems (e.g. 0 1 2 3 4 0 1 2 0 2 1 1 0 1 0 1 0 2 1 2 0 0 1 Figure 2: An example chromosome representing a scheduling assignment of (workflow,service type) \u2192 service provider. Chromosome representation of a solution. GA execution. GA recombination and mutation. GA evaluation function. In this section we show the benefit of using our GA-based scheduler. \u2022 A random-proportional algorithm that proportionally assigns business processes to the service providers; that is, for a given service type, the service providers are ranked by their guaranteed completion time, and business processes are assigned proportionally to the providers based on their completion time. In Figure 3 we show the results of running our GA against the three candidate alternatives. - 0 50 100 150 200Aggregatebusinessvalueacrossallworkflows Total number of workflows Business value scores of scheduling algorithms Genetic algorithm Round robin Random proportional Greedy Figure 4: Magnification of the left-most region in Figure 3. number of workflows that we are considering. To better understand the behaviour resulting from the scheduling assignments, we show the workflow completion results in Figures 5, 6, and 7 for 100, 500, and 900 workflows, respectively. In Figure 8 we graph the makespan resulting from the same experiments above. We also looked at the effect of the scheduling algorithms on balancing the load. Failed Acceptable (completed but not within QoS) Successful (completed within QoS) 0% 20% 40% 60% 80% 100% RoundRobinRandProportionalGreedyGeneticAlg Percentageofallworkflows Workflow behaviour, 500 workflows Figure 7: Workflow behaviour for 900 workflows. Makespan[seconds] Number of workflows Maximum completion time for all workflows Genetic algorithm Round robin Random proportional Greedy Figure 8: Maximum completion time for all workflows. Although useful, the makespan does not take into consideration the business value scoring in our problem domain. processes. For completeness, we show the performance of the genetic algorithm itself in Figure 11.", "body2": "Web services can be composed into workflows to provide streamlined end-to-end functionality for human users or other systems. In this paper we address this scheduling problem and examine means to manage a large number of business process workflows in a scalable manner. These workflows are then submitted to a portal (not shown) that acts as a scheduling agent between the web service consumers and the web service providers. This approach converges towards an assignment that maximises the overall business value for all the workflows. Further, this approach allows us to scale the number of simultaneous workflows (up to one thousand workflows in our experiments) and yet still find effective schedules. In the context of service assignment and scheduling, maps web service calls to potential servers using linear programming, but their work is concerned with mapping only single workflows; our principal focus is on scalably scheduling multiple workflows (up 30 Service Type SuperHotels.com Business Process Business Process Workflow ... Business Process Business Process ... HostileHostels.com IncredibleInns.com Business Process Business Process Business Process ... Business Process Service Provider SkyHighAirlines.com SuperCrazyFlights.com Business Process Advertised QoS Service Agreement CarRentalService.com Figure 1: An example scenario demonstrating the interaction between business processes in workflows and web service providers. Each business process accesses a service type and is then mapped to a service provider for that type. presents a feedback-based scheduling mechanism for multi-tiered systems with back-end databases, but unlike our work, it assumes a tighter coupling between the various components of the system. The salient differences are that the machines can process only one job at a time (we assume servers can multi-task but with degraded performance and a maximum concurrency level), tasks within a job cannot simultaneously run on different machines (we assume business processes can be assigned to any available server), and the principal metric of performance is the makespan, which is the time for the last task among all the jobs to complete (and as we show later, optimising on the makespan is insufficient for scheduling the business processes, necessitating different metrics). In this section we describe our model and discuss how we can find scheduling assignments using a genetic search algorithm. Due to space constraints, we focus on the problem space presented here and will extend our model to more advanced deployment scenarios in the future. We extended this approach further and considered different types of workflow completion in order to model differentiated QoS levels that can be applied by businesses (for example, to provide tiered customer service). We assume that the providers make service level agreements (SLAs) to guarantee a level of performance defined by the completion time for completing a web service invocation. In our experiments we vary \u03b1, \u03b2, and \u03b3 with different distributions. This intuitive observation suggests that traditional scheduling approaches such as round-robin or proportional assignments will not fare well, which is what we observe and discuss in Section 4. To explore the solution space, we use a genetic algorithm (GA) search heuristic that simulates Darwinian natural selection by having members of a population compete to survive in order to pass their genetic chromosomes onto the next generation; after successive generations, there is a tendency for the chromosomes to converge toward the best combination . ). its column), so the 2 listed for service type 3 is a different server from server 2 in other columns. Note that the service provider identifier is within a range limited to its service type. Note that finding the global optimum is not guaranteed because the recombination and mutation are stochastic. Other recombination and mutation schemes are an area of research in the GA community, and we look to explore new operators in future work. Note that the evaluation function can be easily replaced if desired; for example, other evaluation functions can model different service provider guarantees or parallel workflows. This approach provides the simplest scheme for load-balancing. In Table 1 we list our experimental parameters. The reason is that although the random-proportional scheme assigns business processes to providers proportionally according to the advertised completion times (which is a measure of the power of the service provider), even the best providers will eventually reach a real-world maximum concurrency for the large - - Aggregatebusinessvalueacrossallworkflows Total number of workflows Business value scores of scheduling algorithms Genetic algorithm Round robin Random proportional Greedy Figure 3: Net business value scores of different scheduling algorithms. - 0 50 100 150 200Aggregatebusinessvalueacrossallworkflows Total number of workflows Business value scores of scheduling algorithms Genetic algorithm Round robin Random proportional Greedy Figure 4: Magnification of the left-most region in Figure 3. For a very large number of workflows, the round-robin scheme is able to better balance the load across all service providers. Further, the round-robin scheme produces better results than the random-proportional for a large number of workflows but does not perform as well as the GA. The GA produces better business values (as shown in Figure 3) because it is able to search the solution space to find better mappings that produce more successful workflows (as shown in Figures 5 to 7). Failed Acceptable (completed but not within QoS) Successful (completed within QoS) 0% 20% 40% 60% 80% 100% RoundRobinRandProportionalGreedyGeneticAlg Percentageofallworkflows Workflow behaviour, 500 workflows Figure 6: Workflow behaviour for 500 workflows. Failed Acceptable (completed but not within QoS) Successful (completed within QoS) 0% 20% 40% 60% 80% 100% RoundRobinRandProportionalGreedyGeneticAlg Percentageofallworkflows Workflow behaviour, 500 workflows Figure 7: Workflow behaviour for 900 workflows. This value is the makespan metric used in traditional scheduling research. Although useful, the makespan does not take into consideration the business value scoring in our problem domain. On the other hand, the random-proportional algorithm uses many service providers, but because business processes are proportionally assigned with more assignments going to the better providers, there is a tendency for a smaller percentage of providers to become saturated. However, we feel that the benefit of finding much higher business value scores justifies the running time of the GA; further we would expect that the running time will improve with both software tuning as well as with a computer faster than our off-the-shelf PC.", "introduction": "Web services can be composed into workflows to provide streamlined end-to-end functionality for human users or other systems. Although previous research efforts have looked at ways to intelligently automate the composition of web services into workflows (e.g. ), an important remaining problem is the assignment of web service requests to the underlying web service providers in a multi-tiered runtime scenario within constraints. In this paper we address this scheduling problem and examine means to manage a large number of business process workflows in a scalable manner. The problem of scheduling web service requests to providers is relevant to modern business domains that depend on multi-tiered service provisioning. Consider the example shown in Figure 1 that illustrates our problem space. Workflows comprise multiple related business processes that are web service consumers; here we assume that the workflows represent requested service from customers or automated systems and that the workflow has already been composed with an existing choreography toolkit. These workflows are then submitted to a portal (not shown) that acts as a scheduling agent between the web service consumers and the web service providers. In this example, a workflow could represent the actions needed to instantiate a vacation itinerary, where one business process requests booking an airline ticket, another business process requests a hotel room, and so forth. Each of these requests target a particular service type (e.g. airline reservations, hotel reservations, car reservations, etc. ), and for each service type, there are multiple instances of service providers that publish a web service interface. An important challenge is that the workflows must meet some quality-of-service (QoS) metric, such as end-to-end completion time of all its business processes, and that meeting or failing this goal results in the assignment of a quantitative business value metric for the workflow; intuitively, it is desired that all workflows meet their respective QoS goals. We further leverage the notion that QoS service agreements are generally agreed-upon between the web service providers and the scheduling agent such that the providers advertise some level of guaranteed QoS to the scheduler based upon runtime conditions such as turnaround time and maximum available concurrency. The resulting problem is then to schedule and assign the business processes\" requests for service types to one of the service providers for that type. The scheduling must be done such that the aggregate business value across all the workflows is maximised. In Section 3 we state the scenario as a combinatorial problem and utilise a genetic search algorithm to find the best assignment of web service requests to providers. This approach converges towards an assignment that maximises the overall business value for all the workflows. In Section 4 we show through experimentation that this search heuristic finds better assignments than other algorithms (greedy, round-robin, and proportional). Further, this approach allows us to scale the number of simultaneous workflows (up to one thousand workflows in our experiments) and yet still find effective schedules.", "conclusion": "Business processes within workflows can be orchestrated to access web services.. In this paper we looked at multi-tiered service provisioning where web service requests to service types can be mapped to different service providers.. The resulting problem is that in order to support a very large number of workflows, the assignment of business process to web service provider must be intelligent.. We used a business value metric to measure the be34 0.2 0.4 0.6 0. Percentageofallserviceproviders Number of workflows Service providers utilised Genetic algorithm Round robin Random proportional Greedy Figure 9: The percentage of service providers utilized during workload executions.. The Greedy algorithm always hits the one service provider, while the Round Robin algorithm spreads requests evenly across the providers.. 0.2 0.4 0.6 0. Percentageofallserviceproviders Number of workflows Service providers saturated Genetic algorithm Round robin Random proportional Greedy Figure 10: The percentage of service providers that are saturated among those providers who were utilized (that is, percentage of the service providers represented in Figure 9).. A saturated service provider is one whose workload is greater that its advertised maximum concurrency.. Runningtimeinseconds Total number of workflows Running time of genetic algorithm GA running time Figure 11: Running time of the genetic algorithm.. haviour of workflows meeting or failing QoS values, and we optimised our scheduling to maximise the aggregate business value across all workflows.. Since the solution space of scheduler mappings is exponential, we used a genetic search algorithm to search the space and converge toward the best schedule.. With a default configuration for all parameters and using our business value scoring, the GA produced up to 115% business value improvement over the next best algorithm.. Finally, because a genetic algorithm will converge towards the optimal value using any metric (even other than the business value metric we used), we believe our approach has strong potential for continuing work.. In future work, we look to acquire real-world traces of web service instances in order to get better estimates of service agreement guarantees, although we expect that such guarantees between the providers and their consumers are not generally available to the public.. We will also look at other QoS metrics such as CPU and I/O usage.. For example, we can analyse transfer costs with varying bandwidth, latency, data size, and data distribution.. Further, we hope to improve our genetic algorithm and compare it to more scheduler alternatives.. Finally, since our work is complementary to existing work in web services choreography (because we rely on pre-configured workflows), we look to integrate our approach with available web service workflow systems expressed in BPEL."}
{"id": "H-77", "keywords": ["inform extract", "metada extract", "machin learn", "search"], "title": "Automatic Extraction of Titles from General Documents using Machine Learning", "abstract": "We propose a machine learning approach to title extraction from general documents. By general documents, we mean documents that can belong to any one of a number of specific genres, including presentations, book chapters, technical papers, brochures, reports, and letters. Previously, methods have been proposed mainly for title extraction from research papers. It has not been clear whether it could be possible to conduct automatic title extraction from general documents. As a case study, we consider extraction from Office including Word and PowerPoint. In our approach, we annotate titles in sample documents (for Word and PowerPoint respectively) and take them as training data, train machine learning models, and perform title extraction using the trained models. Our method is unique in that we mainly utilize formatting information such as font size as features in the models. It turns out that the use of formatting information can lead to quite accurate extraction from general documents. Precision and recall for title extraction from Word is 0.810 and 0.837 respectively, and precision and recall for title extraction from PowerPoint is 0.875 and 0.895 respectively in an experiment on intranet data. Other important new findings in this work include that we can train models in one domain and apply them to another domain, and more surprisingly we can even train models in one language and apply them to another language. Moreover, we can significantly improve search ranking results in do document retrieval by using the extracted titles.", "references": ["A maximum entropy approach to natural language processing", "Discriminative training methods for hidden markov models: theory and experiments with perceptron algorithms", "Support-vector networks", "A maximum entropy approach to information extraction from semi-structured and free text", "Columbia newsblaster: multilingual news summarization on the Web", "Factorial hidden markov models", "Data and metadata for finding and reminding", "eBizSearch: a niche search engine for e-Business", "Knowledge-based metadata extraction from PostScript files", "Automatic document metadata extraction using support vector machines", "Information retrieval on the Web", "Conditional random fields: probabilistic models for segmenting and labeling sequence data", "The perceptron algorithm with uneven margins", "Automatic Metadata generation & evaluation", "Effective enterprise information retrieval across new content formats", "A dynamic feature generation system for automated metadata extraction in preservation of digital materials", "Maximum entropy markov models for information extraction and segmentation", "Digital document metadata in organizations: roles analytical approaches and future research directions", "Table extraction using conditional random fields", "Unsupervised statistical models for prepositional phrase attachment", "Simple BM25 extension to multiple weighted fields", "Metadata based Web mining for relevance", "MetaExtract: An NLP system to automatically assign metadata", "Internet search engines' response to metadata Dublin Core implementation", "Recognising and using named entities: focused named entity recognition using machine learning"], "full_text": "1. INTRODUCTION Metadata of documents is useful for many kinds of document processing such as search, browsing, and filtering. Ideally, metadata is defined by the authors of documents and is then used by various systems. However, people seldom define document metadata by themselves, even when they have convenient metadata definition tools . Thus, how to automatically extract metadata from the bodies of documents turns out to be an important research issue. Methods for performing the task have been proposed. However, the focus was mainly on extraction from research papers. For instance, Han et al. proposed a machine learning based method to conduct extraction from research papers. They formalized the problem as that of classification and employed Support Vector Machines as the classifier. They mainly used linguistic features in the model.1 In this paper, we consider metadata extraction from general documents. By general documents, we mean documents that may belong to any one of a number of specific genres. General documents are more widely available in digital libraries, intranets and the internet, and thus investigation on extraction from them is sorely needed. Research papers usually have well-formed styles and noticeable characteristics. In contrast, the styles of general documents can vary greatly. It has not been clarified whether a machine learning based approach can work well for this task. There are many types of metadata: title, author, date of creation, etc. As a case study, we consider title extraction in this paper. General documents can be in many different file formats: Microsoft Office, PDF (PS), etc. As a case study, we consider extraction from Office including Word and PowerPoint. We take a machine learning approach. We annotate titles in sample documents (for Word and PowerPoint respectively) and take them as training data to train several types of models, and perform title extraction using any one type of the trained models. In the models, we mainly utilize formatting information such as font size as features. We employ the following models: Maximum Entropy Model, Perceptron with Uneven Margins, Maximum Entropy Markov Model, and Voted Perceptron. In this paper, we also investigate the following three problems, which did not seem to have been examined previously. (1) Comparison between models: among the models above, which model performs best for title extraction; (2) Generality of model: whether it is possible to train a model on one domain and apply it to another domain, and whether it is possible to train a model in one language and apply it to another language; (3) Usefulness of extracted titles: whether extracted titles can improve document processing such as search. Experimental results indicate that our approach works well for title extraction from general documents. Our method can significantly outperform the baselines: one that always uses the first lines as titles and the other that always uses the lines in the largest font sizes as titles. Precision and recall for title extraction from Word are 0.810 and 0.837 respectively, and precision and recall for title extraction from PowerPoint are 0.875 and 0.895 respectively. It turns out that the use of format features is the key to successful title extraction. (1) We have observed that Perceptron based models perform better in terms of extraction accuracies. (2) We have empirically verified that the models trained with our approach are generic in the sense that they can be trained on one domain and applied to another, and they can be trained in one language and applied to another. (3) We have found that using the extracted titles we can significantly improve precision of document retrieval (by 10%). We conclude that we can indeed conduct reliable title extraction from general documents and use the extracted results to improve real applications. The rest of the paper is organized as follows. In section 2, we introduce related work, and in section 3, we explain the motivation and problem setting of our work. In section 4, we describe our method of title extraction, and in section 5, we describe our method of document retrieval using extracted titles. Section 6 gives our experimental results. We make concluding remarks in section 7. 2. RELATED WORK 2.1 Document Metadata Extraction Methods have been proposed for performing automatic metadata extraction from documents; however, the main focus was on extraction from research papers. The proposed methods fall into two categories: the rule based approach and the machine learning based approach. Giuffrida et al. , for instance, developed a rule-based system for automatically extracting metadata from research papers in Postscript. They used rules like titles are usually located on the upper portions of the first pages and they are usually in the largest font sizes. Liddy et al. and Yilmazel el al. performed metadata extraction from educational materials using rule-based natural language processing technologies. Mao et al. also conducted automatic metadata extraction from research papers using rules on formatting information. The rule-based approach can achieve high performance. However, it also has disadvantages. It is less adaptive and robust when compared with the machine learning approach. Han et al. , for instance, conducted metadata extraction with the machine learning approach. They viewed the problem as that of classifying the lines in a document into the categories of metadata and proposed using Support Vector Machines as the classifier. They mainly used linguistic information as features. They reported high extraction accuracy from research papers in terms of precision and recall. 2.2 Information Extraction Metadata extraction can be viewed as an application of information extraction, in which given a sequence of instances, we identify a subsequence that represents information in which we are interested. Hidden Markov Model , Maximum Entropy Model , Maximum Entropy Markov Model , Support Vector Machines , Conditional Random Field , and Voted Perceptron are widely used information extraction models. Information extraction has been applied, for instance, to part-ofspeech tagging , named entity recognition and table extraction . 2.3 Search Using Title Information Title information is useful for document retrieval. In the system Citeseer, for instance, Giles et al. managed to extract titles from research papers and make use of the extracted titles in metadata search of papers . In web search, the title fields (i.e., file properties) and anchor texts of web pages (HTML documents) can be viewed as \u2018titles\" of the pages . Many search engines seem to utilize them for web page retrieval . Zhang et al., found that web pages with well-defined metadata are more easily retrieved than those without well-defined metadata . To the best of our knowledge, no research has been conducted on using extracted titles from general documents (e.g., Office documents) for search of the documents. 146 3. MOTIVATION AND PROBLEM SETTING We consider the issue of automatically extracting titles from general documents. By general documents, we mean documents that belong to one of any number of specific genres. The documents can be presentations, books, book chapters, technical papers, brochures, reports, memos, specifications, letters, announcements, or resumes. General documents are more widely available in digital libraries, intranets, and internet, and thus investigation on title extraction from them is sorely needed. Figure 1 shows an estimate on distributions of file formats on intranet and internet . Office and PDF are the main file formats on the intranet. Even on the internet, the documents in the formats are still not negligible, given its extremely large size. In this paper, without loss of generality, we take Office documents as an example. Figure 1. Distributions of file formats in internet and intranet. For Office documents, users can define titles as file properties using a feature provided by Office. We found in an experiment, however, that users seldom use the feature and thus titles in file properties are usually very inaccurate. That is to say, titles in file properties are usually inconsistent with the \u2018true\" titles in the file bodies that are created by the authors and are visible to readers. We collected 6,000 Word and 6,000 PowerPoint documents from an intranet and the internet and examined how many titles in the file properties are correct. We found that surprisingly the accuracy was only 0.265 (cf., Section 6.3 for details). A number of reasons can be considered. For example, if one creates a new file by copying an old file, then the file property of the new file will also be copied from the old file. In another experiment, we found that Google uses the titles in file properties of Office documents in search and browsing, but the titles are not very accurate. We created 50 queries to search Word and PowerPoint documents and examined the top 15 results of each query returned by Google. We found that nearly all the titles presented in the search results were from the file properties of the documents. However, only 0.272 of them were correct. Actually, \u2018true\" titles usually exist at the beginnings of the bodies of documents. If we can accurately extract the titles from the bodies of documents, then we can exploit reliable title information in document processing. This is exactly the problem we address in this paper. More specifically, given a Word document, we are to extract the title from the top region of the first page. Given a PowerPoint document, we are to extract the title from the first slide. A title sometimes consists of a main title and one or two subtitles. We only consider extraction of the main title. As baselines for title extraction, we use that of always using the first lines as titles and that of always using the lines with largest font sizes as titles. Figure 2. Title extraction from Word document. Figure 3. Title extraction from PowerPoint document. Next, we define a \u2018specification\" for human judgments in title data annotation. The annotated data will be used in training and testing of the title extraction methods. Summary of the specification: The title of a document should be identified on the basis of common sense, if there is no difficulty in the identification. However, there are many cases in which the identification is not easy. There are some rules defined in the specification that guide identification for such cases. The rules include a title is usually in consecutive lines in the same format, a document can have no title, titles in images are not considered, a title should not contain words like \u2018draft\", 147 \u2018whitepaper\", etc, if it is difficult to determine which is the title, select the one in the largest font size, and if it is still difficult to determine which is the title, select the first candidate. (The specification covers all the cases we have encountered in data annotation.) Figures 2 and 3 show examples of Office documents from which we conduct title extraction. In Figure 2, \u2018Differences in Win32 API Implementations among Windows Operating Systems\" is the title of the Word document. \u2018Microsoft Windows\" on the top of this page is a picture and thus is ignored. In Figure 3, \u2018Building Competitive Advantages through an Agile Infrastructure\" is the title of the PowerPoint document. We have developed a tool for annotation of titles by human annotators. Figure 4 shows a snapshot of the tool. Figure 4. Title annotation tool. 4. TITLE EXTRACTION METHOD 4.1 Outline Title extraction based on machine learning consists of training and extraction. The same pre-processing step occurs before training and extraction. During pre-processing, from the top region of the first page of a Word document or the first slide of a PowerPoint document a number of units for processing are extracted. If a line (lines are separated by \u2018return\" symbols) only has a single format, then the line will become a unit. If a line has several parts and each of them has its own format, then each part will become a unit. Each unit will be treated as an instance in learning. A unit contains not only content information (linguistic information) but also formatting information. The input to pre-processing is a document and the output of pre-processing is a sequence of units (instances). Figure 5 shows the units obtained from the document in Figure 2. Figure 5. Example of units. In learning, the input is sequences of units where each sequence corresponds to a document. We take labeled units (labeled as title_begin, title_end, or other) in the sequences as training data and construct models for identifying whether a unit is title_begin title_end, or other. We employ four types of models: Perceptron, Maximum Entropy (ME), Perceptron Markov Model (PMM), and Maximum Entropy Markov Model (MEMM). In extraction, the input is a sequence of units from one document. We employ one type of model to identify whether a unit is title_begin, title_end, or other. We then extract units from the unit labeled with \u2018title_begin\" to the unit labeled with \u2018title_end\". The result is the extracted title of the document. The unique characteristic of our approach is that we mainly utilize formatting information for title extraction. Our assumption is that although general documents vary in styles, their formats have certain patterns and we can learn and utilize the patterns for title extraction. This is in contrast to the work by Han et al., in which only linguistic features are used for extraction from research papers. 4.2 Models The four models actually can be considered in the same metadata extraction framework. That is why we apply them together to our current problem. Each input is a sequence of instances kxxx L21 together with a sequence of labels kyyy L21 . ix and iy represents an instance and its label, respectively ( ki ,,2,1 L= ). Recall that an instance here represents a unit. A label represents title_begin, title_end, or other. Here, k is the number of units in a document. In learning, we train a model which can be generally denoted as a conditional probability distribution )|( 11 kk XXYYP LL where iX and iY denote random variables taking instance ix and label iy as values, respectively ( ki ,,2,1 L= ). nknnknn kk kk yyyxxx yyyxxx yyyxxx LL LL LL LL )|(maxarg 11 mkmmkm xxyyP LL )|( 11 kk XXYYP LL Conditional Distribution mkmm xxx L21 Figure 6. Metadata extraction model. We can make assumptions about the general model in order to make it simple enough for training. 148 For example, we can assume that kYY ,,1 L are independent of each other given kXX ,,1 L . Thus, we have )||( )|( 11 11 kk kk XYPXYP XXYYP LL In this way, we decompose the model into a number of classifiers. We train the classifiers locally using the labeled data. As the classifier, we employ the Perceptron or Maximum Entropy model. We can also assume that the first order Markov property holds for kYY ,,1 L given kXX ,,1 L . Thus, we have )||( )|( 111 11 kkk kk XYYPXYP XXYYP \u2212= L LL Again, we obtain a number of classifiers. However, the classifiers are conditioned on the previous label. When we employ the Percepton or Maximum Entropy model as a classifier, the models become a Percepton Markov Model or Maximum Entropy Markov Model, respectively. That is to say, the two models are more precise. In extraction, given a new sequence of instances, we resort to one of the constructed models to assign a sequence of labels to the sequence of instances, i.e., perform extraction. For Perceptron and ME, we assign labels locally and combine the results globally later using heuristics. Specifically, we first identify the most likely title_begin. Then we find the most likely title_end within three units after the title_begin. Finally, we extract as a title the units between the title_begin and the title_end. For PMM and MEMM, we employ the Viterbi algorithm to find the globally optimal label sequence. In this paper, for Perceptron, we actually employ an improved variant of it, called Perceptron with Uneven Margin . This version of Perceptron can work well especially when the number of positive instances and the number of negative instances differ greatly, which is exactly the case in our problem. We also employ an improved version of Perceptron Markov Model in which the Perceptron model is the so-called Voted Perceptron . In addition, in training, the parameters of the model are updated globally rather than locally. 4.3 Features There are two types of features: format features and linguistic features. We mainly use the former. The features are used for both the title-begin and the title-end classifiers. 4.3.1 Format Features Font Size: There are four binary features that represent the normalized font size of the unit (recall that a unit has only one type of font). If the font size of the unit is the largest in the document, then the first feature will be 1, otherwise 0. If the font size is the smallest in the document, then the fourth feature will be 1, otherwise 0. If the font size is above the average font size and not the largest in the document, then the second feature will be 1, otherwise 0. If the font size is below the average font size and not the smallest, the third feature will be 1, otherwise 0. It is necessary to conduct normalization on font sizes. For example, in one document the largest font size might be \u201812pt\", while in another the smallest one might be \u201818pt\". Boldface: This binary feature represents whether or not the current unit is in boldface. Alignment: There are four binary features that respectively represent the location of the current unit: \u2018left\", \u2018center\", \u2018right\", and \u2018unknown alignment\". The following format features with respect to \u2018context\" play an important role in title extraction. Empty Neighboring Unit: There are two binary features that represent, respectively, whether or not the previous unit and the current unit are blank lines. Font Size Change: There are two binary features that represent, respectively, whether or not the font size of the previous unit and the font size of the next unit differ from that of the current unit. Alignment Change: There are two binary features that represent, respectively, whether or not the alignment of the previous unit and the alignment of the next unit differ from that of the current one. Same Paragraph: There are two binary features that represent, respectively, whether or not the previous unit and the next unit are in the same paragraph as the current unit. 4.3.2 Linguistic Features The linguistic features are based on key words. Positive Word: This binary feature represents whether or not the current unit begins with one of the positive words. The positive words include \u2018title:\", \u2018subject:\", \u2018subject line:\" For example, in some documents the lines of titles and authors have the same formats. However, if lines begin with one of the positive words, then it is likely that they are title lines. Negative Word: This binary feature represents whether or not the current unit begins with one of the negative words. The negative words include \u2018To\", \u2018By\", \u2018created by\", \u2018updated by\", etc. There are more negative words than positive words. The above linguistic features are language dependent. Word Count: A title should not be too long. We heuristically create four intervals: , , and [9, \u221e) and define one feature for each interval. If the number of words in a title falls into an interval, then the corresponding feature will be 1; otherwise 0. Ending Character: This feature represents whether the unit ends with \u2018:\", \u2018-\", or other special characters. A title usually does not end with such a character. 5. DOCUMENT RETRIEVAL METHOD We describe our method of document retrieval using extracted titles. Typically, in information retrieval a document is split into a number of fields including body, title, and anchor text. A ranking function in search can use different weights for different fields of 149 the document. Also, titles are typically assigned high weights, indicating that they are important for document retrieval. As explained previously, our experiment has shown that a significant number of documents actually have incorrect titles in the file properties, and thus in addition of using them we use the extracted titles as one more field of the document. By doing this, we attempt to improve the overall precision. In this paper, we employ a modification of BM25 that allows field weighting . As fields, we make use of body, title, extracted title and anchor. First, for each term in the query we count the term frequency in each field of the document; each field frequency is then weighted according to the corresponding weight parameter: \u2211= tfft tfwwtf Similarly, we compute the document length as a weighted sum of lengths of each field. Average document length in the corpus becomes the average of all weighted document lengths. \u2211= ff dlwwdl In our experiments we used 75.0,8.11 == bk . Weight for content was 1.0, title was 10.0, anchor was 10.0, and extracted title was 5.0. 6. EXPERIMENTAL RESULTS 6.1 Data Sets and Evaluation Measures We used two data sets in our experiments. First, we downloaded and randomly selected 5,000 Word documents and 5,000 PowerPoint documents from an intranet of Microsoft. We call it MS hereafter. Second, we downloaded and randomly selected 500 Word and 500 PowerPoint documents from the DotGov and DotCom domains on the internet, respectively. Figure 7 shows the distributions of the genres of the documents. We see that the documents are indeed \u2018general documents\" as we define them. Figure 7. Distributions of document genres. Third, a data set in Chinese was also downloaded from the internet. It includes 500 Word documents and 500 PowerPoint documents in Chinese. We manually labeled the titles of all the documents, on the basis of our specification. Not all the documents in the two data sets have titles. Table 1 shows the percentages of the documents having titles. We see that DotCom and DotGov have more PowerPoint documents with titles than MS. This might be because PowerPoint documents published on the internet are more formal than those on the intranet. Table 1. The portion of documents with titles Domain Type MS DotCom DotGov Word 75.7% 77.8% 75.6% PowerPoint 82.1% 93.4% 96.4% In our experiments, we conducted evaluations on title extraction in terms of precision, recall, and F-measure. The evaluation measures are defined as follows: Precision: P = A / ( A + B ) Recall: R = A / ( A + C ) F-measure: F1 = 2PR / ( P + R ) Here, A, B, C, and D are numbers of documents as those defined in Table 2. Table 2. Contingence table with regard to title extraction Is title Is not title Extracted A B Not extracted C D 6.2 Baselines We test the accuracies of the two baselines described in section 4.2. They are denoted as \u2018largest font size\" and \u2018first line\" respectively. 6.3 Accuracy of Titles in File Properties We investigate how many titles in the file properties of the documents are reliable. We view the titles annotated by humans as true titles and test how many titles in the file properties can approximately match with the true titles. We use Edit Distance to conduct the approximate match. (Approximate match is only used in this evaluation). This is because sometimes human annotated titles can be slightly different from the titles in file properties on the surface, e.g., contain extra spaces). Given string A and string B: if ( (D == 0) or ( D / ( La + Lb ) < \u03b8 ) ) then string A = string B D: Edit Distance between string A and string B La: length of string A Lb: length of string B \u03b8: 0.1 \u2211 \u00d7 ++\u2212 wtf avwdl wdl bbk kwtf FBM )log( ))1(( )1( 25 150 Table 3. Accuracies of titles in file properties File Type Domain Precision Recall F1 MS 0.299 0.311 0.305 DotCom 0.210 0.214 0.212Word DotGov 0.182 0.177 0.180 MS 0.229 0.245 0.237 DotCom 0.185 0.186 0.186PowerPoint DotGov 0.180 0.182 0.181 6.4 Comparison with Baselines We conducted title extraction from the first data set (Word and PowerPoint in MS). As the model, we used Perceptron. We conduct 4-fold cross validation. Thus, all the results reported here are those averaged over 4 trials. Tables 4 and 5 show the results. We see that Perceptron significantly outperforms the baselines. In the evaluation, we use exact matching between the true titles annotated by humans and the extracted titles. Table 4. Accuracies of title extraction with Word Precision Recall F1 Model Perceptron 0.810 0.837 0.823 Largest font size 0.700 0.758 0.727 Baselines First line 0.707 0.767 0.736 Table 5. Accuracies of title extraction with PowerPoint Precision Recall F1 Model Perceptron 0.875 0. 895 0.885 Largest font size 0.844 0.887 0.865 Baselines First line 0.639 0.671 0.655 We see that the machine learning approach can achieve good performance in title extraction. For Word documents both precision and recall of the approach are 8 percent higher than those of the baselines. For PowerPoint both precision and recall of the approach are 2 percent higher than those of the baselines. We conduct significance tests. The results are shown in Table 6. Here, \u2018Largest\" denotes the baseline of using the largest font size, \u2018First\" denotes the baseline of using the first line. The results indicate that the improvements of machine learning over baselines are statistically significant (in the sense p-value < 0.05) Table 6. Sign test results Documents Type Sign test between p-value Perceptron vs. Largest 3.59e-26 Word Perceptron vs. First 7.12e-10 Perceptron vs. Largest 0.010 PowerPoint Perceptron vs. First 5.13e-40 We see, from the results, that the two baselines can work well for title extraction, suggesting that font size and position information are most useful features for title extraction. However, it is also obvious that using only these two features is not enough. There are cases in which all the lines have the same font size (i.e., the largest font size), or cases in which the lines with the largest font size only contain general descriptions like \u2018Confidential\", \u2018White paper\", etc. For those cases, the \u2018largest font size\" method cannot work well. For similar reasons, the \u2018first line\" method alone cannot work well, either. With the combination of different features (evidence in title judgment), Perceptron can outperform Largest and First. We investigate the performance of solely using linguistic features. We found that it does not work well. It seems that the format features play important roles and the linguistic features are supplements.. Figure 8. An example Word document. Figure 9. An example PowerPoint document. We conducted an error analysis on the results of Perceptron. We found that the errors fell into three categories. (1) About one third of the errors were related to \u2018hard cases\". In these documents, the layouts of the first pages were difficult to understand, even for humans. Figure 8 and 9 shows examples. (2) Nearly one fourth of the errors were from the documents which do not have true titles but only contain bullets. Since we conduct extraction from the top regions, it is difficult to get rid of these errors with the current approach. (3). Confusions between main titles and subtitles were another type of error. Since we only labeled the main titles as titles, the extractions of both titles were considered incorrect. This type of error does little harm to document processing like search, however. 6.5 Comparison between Models To compare the performance of different machine learning models, we conducted another experiment. Again, we perform 4-fold cross 151 validation on the first data set (MS). Table 7, 8 shows the results of all the four models. It turns out that Perceptron and PMM perform the best, followed by MEMM, and ME performs the worst. In general, the Markovian models perform better than or as well as their classifier counterparts. This seems to be because the Markovian models are trained globally, while the classifiers are trained locally. The Perceptron based models perform better than the ME based counterparts. This seems to be because the Perceptron based models are created to make better classifications, while ME models are constructed for better prediction. Table 7. Comparison between different learning models for title extraction with Word Model Precision Recall F1 Perceptron 0.810 0.837 0.823 MEMM 0.797 0.824 0.810 PMM 0.827 0.823 0.825 ME 0.801 0.621 0.699 Table 8. Comparison between different learning models for title extraction with PowerPoint Model Precision Recall F1 Perceptron 0.875 0. 895 0. 885 MEMM 0.841 0.861 0.851 PMM 0.873 0.896 0.885 ME 0.753 0.766 0.759 6.6 Domain Adaptation We apply the model trained with the first data set (MS) to the second data set (DotCom and DotGov). Tables 9-12 show the results. Table 9. Accuracies of title extraction with Word in DotGov Precision Recall F1 Model Perceptron 0.716 0.759 0.737 Largest font size 0.549 0.619 0.582Baselines First line 0.462 0.521 0.490 Table 10. Accuracies of title extraction with PowerPoint in DotGov Precision Recall F1 Model Perceptron 0.900 0.906 0.903 Largest font size 0.871 0.888 0.879Baselines First line 0.554 0.564 0.559 Table 11. Accuracies of title extraction with Word in DotCom Precisio Recall F1 Model Perceptron 0.832 0.880 0.855 Largest font size 0.676 0.753 0.712Baselines First line 0.577 0.643 0.608 Table 12. Performance of PowerPoint document title extraction in DotCom Precisio Recall F1 Model Perceptron 0.910 0.903 0.907 Largest font size 0.864 0.886 0.875Baselines First line 0.570 0.585 0.577 From the results, we see that the models can be adapted to different domains well. There is almost no drop in accuracy. The results indicate that the patterns of title formats exist across different domains, and it is possible to construct a domain independent model by mainly using formatting information. 6.7 Language Adaptation We apply the model trained with the data in English (MS) to the data set in Chinese. Tables 13-14 show the results. Table 13. Accuracies of title extraction with Word in Chinese Precision Recall F1 Model Perceptron 0.817 0.805 0.811 Largest font size 0.722 0.755 0.738Baselines First line 0.743 0.777 0.760 Table 14. Accuracies of title extraction with PowerPoint in Chinese Precision Recall F1 Model Perceptron 0.766 0.812 0.789 Largest font size 0.753 0.813 0.782Baselines First line 0.627 0.676 0.650 We see that the models can be adapted to a different language. There are only small drops in accuracy. Obviously, the linguistic features do not work for Chinese, but the effect of not using them is negligible. The results indicate that the patterns of title formats exist across different languages. From the domain adaptation and language adaptation results, we conclude that the use of formatting information is the key to a successful extraction from general documents. 6.8 Search with Extracted Titles We performed experiments on using title extraction for document retrieval. As a baseline, we employed BM25 without using extracted titles. The ranking mechanism was as described in Section 5. The weights were heuristically set. We did not conduct optimization on the weights. The evaluation was conducted on a corpus of 1.3 M documents crawled from the intranet of Microsoft using 100 evaluation queries obtained from this intranet\"s search engine query logs. 50 queries were from the most popular set, while 50 queries other were chosen randomly. Users were asked to provide judgments of the degree of document relevance from a scale of 1to 5 (1 meaning detrimental, 2 - bad, 3 - fair, 4 - good and 5 - excellent). 152 Figure 10 shows the results. In the chart two sets of precision results were obtained by either considering good or excellent documents as relevant (left 3 bars with relevance threshold 0.5), or by considering only excellent documents as relevant (right 3 bars with relevance threshold 1.0) 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 P@10 P@5 Reciprocal P@10 P@5 Reciprocal 0.5 1 BM25 Anchor, Title, Body BM25 Anchor, Title, Body, ExtractedTitle Name All RelevanceThreshold Data Description Figure 10. Search ranking results. Figure 10 shows different document retrieval results with different ranking functions in terms of precision @10, precision @5 and reciprocal rank: \u2022 Blue bar - BM25 including the fields body, title (file property), and anchor text. \u2022 Purple bar - BM25 including the fields body, title (file property), anchor text, and extracted title. With the additional field of extracted title included in BM25 the precision @10 increased from 0.132 to 0.145, or by ~10%. Thus, it is safe to say that the use of extracted title can indeed improve the precision of document retrieval. 7. CONCLUSION In this paper, we have investigated the problem of automatically extracting titles from general documents. We have tried using a machine learning approach to address the problem. Previous work showed that the machine learning approach can work well for metadata extraction from research papers. In this paper, we showed that the approach can work for extraction from general documents as well. Our experimental results indicated that the machine learning approach can work significantly better than the baselines in title extraction from Office documents. Previous work on metadata extraction mainly used linguistic features in documents, while we mainly used formatting information. It appeared that using formatting information is a key for successfully conducting title extraction from general documents. We tried different machine learning models including Perceptron, Maximum Entropy, Maximum Entropy Markov Model, and Voted Perceptron. We found that the performance of the Perceptorn models was the best. We applied models constructed in one domain to another domain and applied models trained in one language to another language. We found that the accuracies did not drop substantially across different domains and across different languages, indicating that the models were generic. We also attempted to use the extracted titles in document retrieval. We observed a significant improvement in document ranking performance for search when using extracted title information. All the above investigations were not conducted in previous work, and through our investigations we verified the generality and the significance of the title extraction approach.", "body1": "Metadata of documents is useful for many kinds of document processing such as search, browsing, and filtering. Methods for performing the task have been proposed. General documents can be in many different file formats: Microsoft Office, PDF (PS), etc. We take a machine learning approach. In the models, we mainly utilize formatting information such as font size as features. In this paper, we also investigate the following three problems, which did not seem to have been examined previously. (1) Comparison between models: among the models above, which model performs best for title extraction; (2) Generality of model: whether it is possible to train a model on one domain and apply it to another domain, and whether it is possible to train a model in one language and apply it to another language; (3) Usefulness of extracted titles: whether extracted titles can improve document processing such as search. Experimental results indicate that our approach works well for title extraction from general documents. (1) We have observed that Perceptron based models perform better in terms of extraction accuracies. The rest of the paper is organized as follows. 2.1 Document Metadata Extraction Methods have been proposed for performing automatic metadata extraction from documents; however, the main focus was on extraction from research papers. Giuffrida et al. The rule-based approach can achieve high performance. Han et al. 2.2 Information Extraction Metadata extraction can be viewed as an application of information extraction, in which given a sequence of instances, we identify a subsequence that represents information in which we are interested. Information extraction has been applied, for instance, to part-ofspeech tagging , named entity recognition and table extraction . 2.3 Search Using Title Information Title information is useful for document retrieval. In the system Citeseer, for instance, Giles et al. In web search, the title fields (i.e., file properties) and anchor texts of web pages (HTML documents) can be viewed as \u2018titles\" of the pages . SETTING We consider the issue of automatically extracting titles from general documents. General documents are more widely available in digital libraries, intranets, and internet, and thus investigation on title extraction from them is sorely needed. Figure 1 shows an estimate on distributions of file formats on intranet and internet . For Office documents, users can define titles as file properties using a feature provided by Office. We collected 6,000 Word and 6,000 PowerPoint documents from an intranet and the internet and examined how many titles in the file properties are correct. In another experiment, we found that Google uses the titles in file properties of Office documents in search and browsing, but the titles are not very accurate. Actually, \u2018true\" titles usually exist at the beginnings of the bodies of documents. More specifically, given a Word document, we are to extract the title from the top region of the first page. Figure 2. Figure 3. Next, we define a \u2018specification\" for human judgments in title data annotation. Summary of the specification: The title of a document should be identified on the basis of common sense, if there is no difficulty in the identification. Figure 4. 4.1 Outline Title extraction based on machine learning consists of training and extraction. During pre-processing, from the top region of the first page of a Word document or the first slide of a PowerPoint document a number of units for processing are extracted. Figure 5. In learning, the input is sequences of units where each sequence corresponds to a document. We employ one type of model to identify whether a unit is title_begin, title_end, or other. The unique characteristic of our approach is that we mainly utilize formatting information for title extraction. Each input is a sequence of instances kxxx L21 together with a sequence of labels kyyy L21 . nknnknn kk kk yyyxxx yyyxxx yyyxxx LL LL LL LL )|(maxarg 11 mkmmkm xxyyP LL )|( 11 kk XXYYP LL Conditional Distribution mkmm xxx L21 Figure 6. We can make assumptions about the general model in order to make it simple enough for training. 148 For example, we can assume that kYY ,,1 L are independent of each other given kXX ,,1 L . We train the classifiers locally using the labeled data. We can also assume that the first order Markov property holds for kYY ,,1 L given kXX ,,1 L . For Perceptron and ME, we assign labels locally and combine the results globally later using heuristics. In this paper, for Perceptron, we actually employ an improved variant of it, called Perceptron with Uneven Margin . We also employ an improved version of Perceptron Markov Model in which the Perceptron model is the so-called Voted Perceptron . 4.3 Features There are two types of features: format features and linguistic features. If the font size of the unit is the largest in the document, then the first feature will be 1, otherwise 0. Boldface: This binary feature represents whether or not the current unit is in boldface. Alignment: There are four binary features that respectively represent the location of the current unit: \u2018left\", \u2018center\", \u2018right\", and \u2018unknown alignment\". The following format features with respect to \u2018context\" play an important role in title extraction. Empty Neighboring Unit: There are two binary features that represent, respectively, whether or not the previous unit and the current unit are blank lines. Font Size Change: There are two binary features that represent, respectively, whether or not the font size of the previous unit and the font size of the next unit differ from that of the current unit. Alignment Change: There are two binary features that represent, respectively, whether or not the alignment of the previous unit and the alignment of the next unit differ from that of the current one. Same Paragraph: There are two binary features that represent, respectively, whether or not the previous unit and the next unit are in the same paragraph as the current unit. 4.3.2 Linguistic Features The linguistic features are based on key words. Positive Word: This binary feature represents whether or not the current unit begins with one of the positive words. There are more negative words than positive words. Word Count: A title should not be too long. We describe our method of document retrieval using extracted titles. Typically, in information retrieval a document is split into a number of fields including body, title, and anchor text. In this paper, we employ a modification of BM25 that allows field weighting . 6.1 Data Sets and Evaluation Measures We used two data sets in our experiments. Second, we downloaded and randomly selected 500 Word and 500 PowerPoint documents from the DotGov and DotCom domains on the internet, respectively. Figure 7 shows the distributions of the genres of the documents. We see that the documents are indeed \u2018general documents\" as we define them. Figure 7. Third, a data set in Chinese was also downloaded from the internet. It includes 500 Word documents and 500 PowerPoint documents in Chinese. We manually labeled the titles of all the documents, on the basis of our specification. Not all the documents in the two data sets have titles. Table 1. Table 2. 6.3 Accuracy of Titles in File Properties We investigate how many titles in the file properties of the documents are reliable. Given string A and string B: if ( (D == 0) or ( D / ( La + Lb ) < \u03b8 ) ) then string A = string B D: Edit Distance between string A and string B La: length of string A Lb: length of string B \u03b8: 0.1 \u2211 \u00d7 ++\u2212 wtf avwdl wdl bbk kwtf FBM )log( ))1(( )1( 25 150 Table 3. We conduct 4-fold cross validation. Table 4. Here, \u2018Largest\" denotes the baseline of using the largest font size, \u2018First\" denotes the baseline of using the first line. We found that it does not work well. Figure 8. Figure 9. We conducted an error analysis on the results of Perceptron. 6.5 Comparison between Models To compare the performance of different machine learning models, we conducted another experiment. It turns out that Perceptron and PMM perform the best, followed by MEMM, and ME performs the worst. Table 7. Table 9. Tables 13-14 show the results. Table 13. There are only small drops in accuracy. 6.8 Search with Extracted Titles We performed experiments on using title extraction for document retrieval. The evaluation was conducted on a corpus of 1.3 M documents crawled from the intranet of Microsoft using 100 evaluation queries obtained from this intranet\"s search engine query logs. 152 Figure 10 shows the results. \u2022 Purple bar - BM25 including the fields body, title (file property), anchor text, and extracted title. With the additional field of extracted title included in BM25 the precision @10 increased from 0.132 to 0.145, or by ~10%.", "body2": "Thus, how to automatically extract metadata from the bodies of documents turns out to be an important research issue. As a case study, we consider title extraction in this paper. As a case study, we consider extraction from Office including Word and PowerPoint. We annotate titles in sample documents (for Word and PowerPoint respectively) and take them as training data to train several types of models, and perform title extraction using any one type of the trained models. We employ the following models: Maximum Entropy Model, Perceptron with Uneven Margins, Maximum Entropy Markov Model, and Voted Perceptron. In this paper, we also investigate the following three problems, which did not seem to have been examined previously. (1) Comparison between models: among the models above, which model performs best for title extraction; (2) Generality of model: whether it is possible to train a model on one domain and apply it to another domain, and whether it is possible to train a model in one language and apply it to another language; (3) Usefulness of extracted titles: whether extracted titles can improve document processing such as search. It turns out that the use of format features is the key to successful title extraction. We conclude that we can indeed conduct reliable title extraction from general documents and use the extracted results to improve real applications. We make concluding remarks in section 7. The proposed methods fall into two categories: the rule based approach and the machine learning based approach. also conducted automatic metadata extraction from research papers using rules on formatting information. It is less adaptive and robust when compared with the machine learning approach. They reported high extraction accuracy from research papers in terms of precision and recall. Hidden Markov Model , Maximum Entropy Model , Maximum Entropy Markov Model , Support Vector Machines , Conditional Random Field , and Voted Perceptron are widely used information extraction models. Information extraction has been applied, for instance, to part-ofspeech tagging , named entity recognition and table extraction . 2.3 Search Using Title Information Title information is useful for document retrieval. managed to extract titles from research papers and make use of the extracted titles in metadata search of papers . To the best of our knowledge, no research has been conducted on using extracted titles from general documents (e.g., Office documents) for search of the documents. The documents can be presentations, books, book chapters, technical papers, brochures, reports, memos, specifications, letters, announcements, or resumes. General documents are more widely available in digital libraries, intranets, and internet, and thus investigation on title extraction from them is sorely needed. Distributions of file formats in internet and intranet. That is to say, titles in file properties are usually inconsistent with the \u2018true\" titles in the file bodies that are created by the authors and are visible to readers. For example, if one creates a new file by copying an old file, then the file property of the new file will also be copied from the old file. However, only 0.272 of them were correct. This is exactly the problem we address in this paper. As baselines for title extraction, we use that of always using the first lines as titles and that of always using the lines with largest font sizes as titles. Title extraction from Word document. Title extraction from PowerPoint document. The annotated data will be used in training and testing of the title extraction methods. Figure 4 shows a snapshot of the tool. Title annotation tool. The same pre-processing step occurs before training and extraction. Figure 5 shows the units obtained from the document in Figure 2. Example of units. In extraction, the input is a sequence of units from one document. The result is the extracted title of the document. That is why we apply them together to our current problem. In learning, we train a model which can be generally denoted as a conditional probability distribution )|( 11 kk XXYYP LL where iX and iY denote random variables taking instance ix and label iy as values, respectively ( ki ,,2,1 L= ). Metadata extraction model. We can make assumptions about the general model in order to make it simple enough for training. Thus, we have )||( )|( 11 11 kk kk XYPXYP XXYYP LL In this way, we decompose the model into a number of classifiers. As the classifier, we employ the Perceptron or Maximum Entropy model. In extraction, given a new sequence of instances, we resort to one of the constructed models to assign a sequence of labels to the sequence of instances, i.e., perform extraction. For PMM and MEMM, we employ the Viterbi algorithm to find the globally optimal label sequence. This version of Perceptron can work well especially when the number of positive instances and the number of negative instances differ greatly, which is exactly the case in our problem. In addition, in training, the parameters of the model are updated globally rather than locally. 4.3.1 Format Features Font Size: There are four binary features that represent the normalized font size of the unit (recall that a unit has only one type of font). For example, in one document the largest font size might be \u201812pt\", while in another the smallest one might be \u201818pt\". Boldface: This binary feature represents whether or not the current unit is in boldface. Alignment: There are four binary features that respectively represent the location of the current unit: \u2018left\", \u2018center\", \u2018right\", and \u2018unknown alignment\". The following format features with respect to \u2018context\" play an important role in title extraction. Empty Neighboring Unit: There are two binary features that represent, respectively, whether or not the previous unit and the current unit are blank lines. Font Size Change: There are two binary features that represent, respectively, whether or not the font size of the previous unit and the font size of the next unit differ from that of the current unit. Alignment Change: There are two binary features that represent, respectively, whether or not the alignment of the previous unit and the alignment of the next unit differ from that of the current one. Same Paragraph: There are two binary features that represent, respectively, whether or not the previous unit and the next unit are in the same paragraph as the current unit. 4.3.2 Linguistic Features The linguistic features are based on key words. The negative words include \u2018To\", \u2018By\", \u2018created by\", \u2018updated by\", etc. The above linguistic features are language dependent. A title usually does not end with such a character. We describe our method of document retrieval using extracted titles. By doing this, we attempt to improve the overall precision. Weight for content was 1.0, title was 10.0, anchor was 10.0, and extracted title was 5.0. We call it MS hereafter. Second, we downloaded and randomly selected 500 Word and 500 PowerPoint documents from the DotGov and DotCom domains on the internet, respectively. Figure 7 shows the distributions of the genres of the documents. We see that the documents are indeed \u2018general documents\" as we define them. Distributions of document genres. Third, a data set in Chinese was also downloaded from the internet. It includes 500 Word documents and 500 PowerPoint documents in Chinese. We manually labeled the titles of all the documents, on the basis of our specification. This might be because PowerPoint documents published on the internet are more formal than those on the intranet. The evaluation measures are defined as follows: Precision: P = A / ( A + B ) Recall: R = A / ( A + C ) F-measure: F1 = 2PR / ( P + R ) Here, A, B, C, and D are numbers of documents as those defined in Table 2. They are denoted as \u2018largest font size\" and \u2018first line\" respectively. This is because sometimes human annotated titles can be slightly different from the titles in file properties on the surface, e.g., contain extra spaces). As the model, we used Perceptron. In the evaluation, we use exact matching between the true titles annotated by humans and the extracted titles. The results are shown in Table 6. We investigate the performance of solely using linguistic features. It seems that the format features play important roles and the linguistic features are supplements.. An example Word document. An example PowerPoint document. This type of error does little harm to document processing like search, however. Table 7, 8 shows the results of all the four models. This seems to be because the Perceptron based models are created to make better classifications, while ME models are constructed for better prediction. Tables 9-12 show the results. 6.7 Language Adaptation We apply the model trained with the data in English (MS) to the data set in Chinese. Tables 13-14 show the results. Accuracies of title extraction with PowerPoint in Chinese Precision Recall F1 Model Perceptron 0.766 0.812 0.789 Largest font size 0.753 0.813 0.782Baselines First line 0.627 0.676 0.650 We see that the models can be adapted to a different language. From the domain adaptation and language adaptation results, we conclude that the use of formatting information is the key to a successful extraction from general documents. We did not conduct optimization on the weights. Users were asked to provide judgments of the degree of document relevance from a scale of 1to 5 (1 meaning detrimental, 2 - bad, 3 - fair, 4 - good and 5 - excellent). Figure 10 shows different document retrieval results with different ranking functions in terms of precision @10, precision @5 and reciprocal rank: \u2022 Blue bar - BM25 including the fields body, title (file property), and anchor text. \u2022 Purple bar - BM25 including the fields body, title (file property), anchor text, and extracted title. Thus, it is safe to say that the use of extracted title can indeed improve the precision of document retrieval.", "introduction": "Metadata of documents is useful for many kinds of document processing such as search, browsing, and filtering. Ideally, metadata is defined by the authors of documents and is then used by various systems. However, people seldom define document metadata by themselves, even when they have convenient metadata definition tools . Thus, how to automatically extract metadata from the bodies of documents turns out to be an important research issue. Methods for performing the task have been proposed. However, the focus was mainly on extraction from research papers. proposed a machine learning based method to conduct extraction from research papers. They formalized the problem as that of classification and employed Support Vector Machines as the classifier. They mainly used linguistic features in the model.1 In this paper, we consider metadata extraction from general documents. By general documents, we mean documents that may belong to any one of a number of specific genres. General documents are more widely available in digital libraries, intranets and the internet, and thus investigation on extraction from them is sorely needed. Research papers usually have well-formed styles and noticeable characteristics. In contrast, the styles of general documents can vary greatly. It has not been clarified whether a machine learning based approach can work well for this task. There are many types of metadata: title, author, date of creation, etc. As a case study, we consider title extraction in this paper. General documents can be in many different file formats: Microsoft Office, PDF (PS), etc. As a case study, we consider extraction from Office including Word and PowerPoint. We take a machine learning approach. We annotate titles in sample documents (for Word and PowerPoint respectively) and take them as training data to train several types of models, and perform title extraction using any one type of the trained models. In the models, we mainly utilize formatting information such as font size as features. We employ the following models: Maximum Entropy Model, Perceptron with Uneven Margins, Maximum Entropy Markov Model, and Voted Perceptron. In this paper, we also investigate the following three problems, which did not seem to have been examined previously. (1) Comparison between models: among the models above, which model performs best for title extraction; (2) Generality of model: whether it is possible to train a model on one domain and apply it to another domain, and whether it is possible to train a model in one language and apply it to another language; (3) Usefulness of extracted titles: whether extracted titles can improve document processing such as search. Experimental results indicate that our approach works well for title extraction from general documents. Our method can significantly outperform the baselines: one that always uses the first lines as titles and the other that always uses the lines in the largest font sizes as titles. Precision and recall for title extraction from Word are 0.810 and 0.837 respectively, and precision and recall for title extraction from PowerPoint are 0.875 and 0.895 respectively. It turns out that the use of format features is the key to successful title extraction. (1) We have observed that Perceptron based models perform better in terms of extraction accuracies. (2) We have empirically verified that the models trained with our approach are generic in the sense that they can be trained on one domain and applied to another, and they can be trained in one language and applied to another. (3) We have found that using the extracted titles we can significantly improve precision of document retrieval (by 10%). We conclude that we can indeed conduct reliable title extraction from general documents and use the extracted results to improve real applications. The rest of the paper is organized as follows. In section 2, we introduce related work, and in section 3, we explain the motivation and problem setting of our work. In section 4, we describe our method of title extraction, and in section 5, we describe our method of document retrieval using extracted titles. Section 6 gives our experimental results. We make concluding remarks in section 7.", "conclusion": "In this paper, we have investigated the problem of automatically extracting titles from general documents.. We have tried using a machine learning approach to address the problem.. Previous work showed that the machine learning approach can work well for metadata extraction from research papers.. In this paper, we showed that the approach can work for extraction from general documents as well.. Our experimental results indicated that the machine learning approach can work significantly better than the baselines in title extraction from Office documents.. Previous work on metadata extraction mainly used linguistic features in documents, while we mainly used formatting information.. It appeared that using formatting information is a key for successfully conducting title extraction from general documents.. We tried different machine learning models including Perceptron, Maximum Entropy, Maximum Entropy Markov Model, and Voted Perceptron.. We found that the performance of the Perceptorn models was the best.. We applied models constructed in one domain to another domain and applied models trained in one language to another language.. We found that the accuracies did not drop substantially across different domains and across different languages, indicating that the models were generic.. We also attempted to use the extracted titles in document retrieval.. We observed a significant improvement in document ranking performance for search when using extracted title information.. All the above investigations were not conducted in previous work, and through our investigations we verified the generality and the significance of the title extraction approach."}
